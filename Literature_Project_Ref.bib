
@article{fitschen_removal_2017,
	title = {Removal of curtaining effects by a variational model with directional forward differences},
	volume = {155},
	doi = {10.1016/j.cviu.2016.12.008},
	abstract = {Focused ion beam (FIB) tomography provides high resolution volumetric images on a micro scale. However, due to the physical acquisition process the resulting images are often corrupted by a so-called curtaining or waterfall effect. In this paper, a new convex variational model for removing such effects is proposed. More precisely, an infimal convolution model is applied to split the corrupted 3D image into the clean image and two types of corruptions, namely a striped part and a laminar one. In order to accomplish the decomposition we exploit the fact that the single parts have certain spatial structures, which are penalized by different first and second order differences. By doing so, our approach generalizes discrete unidirectional total variational (TV) approaches. A minimizer of the proposed model is computed by well-known primal dual techniques. Numerical examples show the very good performance of our new method for artificial as well as real-world data. Besides FIB tomography, we have also successfully applied our technique for the removal of pure stripes in Moderate Resolution Imaging Spectroradiometer (MODIS) data.},
	journal = {Computer Vision and Image Understanding},
	author = {Fitschen, Jan Henrik and Ma, Jianwei and Schuff, Sebastian},
	month = feb,
	year = {2017},
	note = {Publisher: Academic Press Inc.},
	keywords = {Convex analysis, Curtaining effect, Directional differences, FIB tomography, Image processing, Variational method},
	pages = {24--32},
}

@article{hollon_near_2020,
	title = {Near real-time intraoperative brain tumor diagnosis using stimulated {Raman} histology and deep neural networks},
	url = {http://www.nature.com/articles/s41591-019-0715-9},
	doi = {10.1038/s41591-019-0715-9},
	journal = {Nature Medicine},
	author = {Hollon, Todd C. and Pandian, Balaji and Adapa, Arjun R. and Urias, Esteban and Save, Akshay V. and Khalsa, Siri Sahib S. and Eichberg, Daniel G. and D’Amico, Randy S. and Farooq, Zia U. and Lewis, Spencer and Petridis, Petros D. and Marie, Tamara and Shah, Ashish H. and Garton, Hugh J. L. and Maher, Cormac O. and Heth, Jason A. and McKean, Erin L. and Sullivan, Stephen E. and Hervey-Jumper, Shawn L. and Patil, Parag G. and Thompson, B. Gregory and Sagher, Oren and McKhann, Guy M. and Komotar, Ricardo J. and Ivan, Michael E. and Snuderl, Matija and Otten, Marc L. and Johnson, Timothy D. and Sisti, Michael B. and Bruce, Jeffrey N. and Muraszko, Karin M. and Trautman, Jay and Freudiger, Christian W. and Canoll, Peter and Lee, Honglak and Camelo-Piragua, Sandra and Orringer, Daniel A.},
	month = jan,
	year = {2020},
}

@techreport{shen_dsod_nodate,
	title = {{DSOD}: {Learning} {Deeply} {Supervised} {Object} {Detectors} from {Scratch}},
	url = {https://github.com/szq0214/DSOD.},
	abstract = {We present Deeply Supervised Object Detector (DSOD), a framework that can learn object detectors from scratch. State-of-the-art object objectors rely heavily on the off-the-shelf networks pre-trained on large-scale classification datasets like ImageNet, which incurs learning bias due to the difference on both the loss functions and the category distributions between classification and detection tasks. Model fine-tuning for the detection task could alleviate this bias to some extent but not fundamentally. Besides , transferring pre-trained models from classification to detection between discrepant domains is even more difficult (e.g. RGB to depth images). A better solution to tackle these two critical problems is to train object detectors from scratch, which motivates our proposed DSOD. Previous efforts in this direction mostly failed due to much more complicated loss functions and limited training data in object detection. In DSOD, we contribute a set of design principles for training object detectors from scratch. One of the key findings is that deep supervision, enabled by dense layer-wise connections, plays a critical role in learning a good detector. Combining with several other principles, we develop DSOD following the single-shot detection (SSD) framework. Experiments on PASCAL VOC 2007, 2012 and MS COCO datasets demonstrate that DSOD can achieve better results than the state-of-the-art solutions with much more compact models. For instance, DSOD outperforms SSD on all three benchmarks with real-time detection speed, while requires only 1/2 parameters to SSD and 1/10 parameters to Faster RCNN. Our code and models are available at: https://github.com/szq0214/DSOD.},
	author = {Shen, Zhiqiang and Liu, Zhuang and Li, Jianguo and Jiang, Yu-Gang and Chen, Yurong and Xue, Xiangyang},
}

@techreport{maninis_deep_nodate,
	title = {Deep {Extreme} {Cut}: {From} {Extreme} {Points} to {Object} {Segmentation}},
	url = {http://www.vision.},
	abstract = {Figure 1. Example results of DEXTR: The user provides the extreme clicks for an object, and the CNN produces the segmented masks. Abstract This paper explores the use of extreme points in an object (left-most, right-most, top, bottom pixels) as input to obtain precise object segmentation for images and videos. We do so by adding an extra channel to the image in the input of a convolutional neural network (CNN), which contains a Gaussian centered in each of the extreme points. The CNN learns to transform this information into a segmentation of an object that matches those extreme points. We demonstrate the usefulness of this approach for guided segmentation (grabcut-style), interactive segmenta-tion, video object segmentation, and dense segmentation annotation. We show that we obtain the most precise results to date, also with less user input, in an extensive and varied selection of benchmarks and datasets. All our models and code are publicly available on http://www.vision. ee.ethz.ch/ ˜ cvlsegmentation/dextr/.},
	author = {Maninis, K.-K and Caelles, S and Pont-Tuset, J and Van Gool, L},
}

@article{nguyen_transformers_2019,
	title = {Transformers without {Tears}: {Improving} the {Normalization} of {Self}-{Attention}},
	url = {http://arxiv.org/abs/1910.05895},
	doi = {10.5281/zenodo.3525484},
	abstract = {We evaluate three simple, normalization-centric changes to improve Transformer training. First, we show that pre-norm residual connections (PreNorm) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose \${\textbackslash}ell\_2\$ normalization with a single scale parameter (ScaleNorm) for faster training and better performance. Finally, we reaffirm the effectiveness of normalizing word embeddings to a fixed length (FixNorm). On five low-resource translation pairs from TED Talks-based corpora, these changes always converge, giving an average +1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on IWSLT'15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the high-resource setting (WMT'14 English-German), ScaleNorm and FixNorm remain competitive but PreNorm degrades performance.},
	author = {Nguyen, Toan Q. and Salazar, Julian},
	month = oct,
	year = {2019},
}

@techreport{goyal_recurrent_nodate,
	title = {{RECURRENT} {INDEPENDENT} {MECHANISMS}},
	abstract = {Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant. We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.},
	author = {Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani, Shagun and Levine, Sergey and Bengio, Yoshua and Schölkopf, Bernhard},
}

@techreport{ke_learning_nodate,
	title = {{LEARNING} {NEURAL} {CAUSAL} {MODELS} {FROM} {UNKNOWN} {INTERVENTIONS}},
	abstract = {Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing parameters and slowly-changing meta-parameters. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions robust to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph. Finally, we find that when the learner is unaware of the intervention variable, it is able to infer that information, improving results further and focusing the parameter and meta-parameter updates where needed.},
	author = {Ke, Nan Rosemary and Bilaniuk, Olexa and Goyal, Anirudh and Bauer, Stefan and Larochelle, Hugo and Pal, Chris and Bengio, Yoshua},
}

@techreport{bengio_meta-transfer_nodate,
	title = {A {Meta}-{Transfer} {Objective} for {Learning} to {Disentangle} {Causal} {Mechanisms}},
	abstract = {We propose to meta-learn causal structures based on how fast a learner adapts to new distributions arising from sparse distributional changes, e.g. due to interventions, actions of agents and other sources of non-stationarities. We show that under this assumption, the correct causal structural choices lead to faster adaptation to modified distributions because the changes are concentrated in one or just a few mechanisms when the learned knowledge is modularized appropriately. This leads to sparse expected gradients and a lower effective number of degrees of freedom needing to be relearned while adapting to the change. It motivates using the speed of adaptation to a modified distribution as a meta-learning objective. We demonstrate how this can be used to determine the cause-effect relationship between two observed variables. The distributional changes do not need to correspond to standard interventions (clamping a variable), and the learner has no direct knowledge of these interventions. We show that causal structures can be parameterized via continuous variables and learned end-to-end. We then explore how these ideas could be used to also learn an encoder that would map low-level observed variables to unobserved causal variables leading to faster adaptation out-of-distribution, learning a representation space where one can satisfy the assumptions of independent mechanisms and of small and sparse changes in these mechanisms due to actions and non-stationarities.},
	author = {Bengio, Yoshua and Deleu, Tristan and Rahaman, Nasim and Rosemary Ke, Nan and Lachapelle, Sébastien and Bilaniuk, Olexa and Goyal, Anirudh and Pal, Christopher},
}

@techreport{chevalier-boisvert_babyai_nodate,
	title = {{BABYAI}: {A} {PLATFORM} {TO} {STUDY} {THE} {SAMPLE} {EFFI}-{CIENCY} {OF} {GROUNDED} {LANGUAGE} {LEARNING}},
	url = {https://github.com/mila-iqia/babyai/tree/iclr19.},
	abstract = {Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons. Though, given the lack of sample efficiency in current learning methods, reaching this goal may require substantial research efforts. We introduce the BabyAI research platform, with the goal of supporting investigations towards including humans in the loop for grounded language learning. The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty. Each level gradually leads the agent towards acquiring a combinatorially rich synthetic language, which is a proper subset of English. The platform also provides a hand-crafted bot agent, which simulates a human teacher. We report estimated amount of supervision required for training neural reinforcement and behavioral-cloning agents on some BabyAI levels. We put forward strong evidence that current deep learning methods are not yet sufficiently sample-efficient in the context of learning a language with compositional properties.},
	author = {Chevalier-Boisvert, Maxime and Bahdanau, Dzmitry and Lahlou, Salem and Willems, Lucas and Normale Supérieure, École and Chitwan Saharia, Paris and Huu Nguyen, Thien and Bengio, Yoshua},
}

@techreport{uppal_nonparametric_nodate,
	title = {Nonparametric density estimation \& convergence of {GANs} under {Besov} {IPM} losses},
	abstract = {We study the problem of estimating a nonparametric probability density under a large family of losses called Besov IPMs, which include, for example, L p distances, total variation distance, and generalizations of both Wasserstein and Kolmogorov-Smirnov distances. For a wide variety of settings, we provide both lower and upper bounds, identifying precisely how the choice of loss function and assumptions on the data interact to determine the minimax optimal convergence rate. We also show that linear distribution estimates, such as the empirical distribution or kernel density estimator, often fail to converge at the optimal rate. Our bounds generalize, unify, or improve several recent and classical results. Moreover, IPMs can be used to formalize a statistical model of generative adversarial networks (GANs). Thus, we show how our results imply bounds on the statistical error of a GAN, showing, for example, that GANs can strictly outperform the best linear estimator.},
	author = {Uppal, Ananya and Singh, Shashank and Póczos, Barnabás},
}

@techreport{diakonikolas_distribution-independent_nodate,
	title = {Distribution-{Independent} {PAC} {Learning} of {Halfspaces} with {Massart} {Noise}},
	abstract = {We study the problem of distribution-independent PAC learning of halfspaces in the presence of Massart noise. Specifically, we are given a set of labeled examples (x, y) drawn from a distribution D on R d+1 such that the marginal distribution on the unlabeled points x is arbitrary and the labels y are generated by an unknown halfspace corrupted with Massart noise at noise rate η {\textless} 1/2. The goal is to find a hypothesis h that minimizes the misclassification error Pr (x,y)∼D [h(x) = y]. We give a poly (d, 1//) time algorithm for this problem with misclassification error η +. We also provide evidence that improving on the error guarantee of our algorithm might be computationally hard. Prior to our work, no efficient weak (distribution-independent) learner was known in this model, even for the class of disjunctions. The existence of such an algorithm for halfspaces (or even disjunctions) has been posed as an open question in various works, starting with Sloan (1988), Cohen (1997), and was most recently highlighted in Avrim Blum's FOCS 2003 tutorial.},
	author = {Diakonikolas, Ilias and Gouleakis, Themis and Tzamos, Christos},
}

@techreport{thiagarajan_distill--label_nodate,
	title = {Distill-to-{Label}: {Weakly} {Supervised} {Instance} {Labeling} {Using} {Knowledge} {Distillation}},
	abstract = {Weakly supervised instance labeling using only image-level labels, in lieu of expensive fine-grained pixel annotations , is crucial in several applications including medical image analysis. In contrast to conventional instance segmentation scenarios in computer vision, the problems that we consider are characterized by a small number of training images and non-local patterns that lead to the diagnosis. In this paper, we explore the use of multiple instance learning (MIL) to design an instance label generator under this weakly supervised setting. Motivated by the observation that an MIL model can handle bags of varying sizes, we propose to repurpose an MIL model originally trained for bag-level classification to produce reliable predictions for single instances, i.e., bags of size 1. To this end, we introduce a novel regularization strategy based on virtual adversarial training for improving MIL training, and subsequently develop a knowledge distillation technique for repurposing the trained MIL model. Using empirical studies on colon cancer and breast cancer detection from histopathological images, we show that the proposed approach produces high-quality instance-level prediction and significantly outperforms state-of-the MIL methods.},
	author = {Thiagarajan, Jayaraman J and Kashyap, Satyananda and Karagyris, Alexandros},
	keywords = {attention mechanism, Index Terms-Multiple instance learning, knowledge distilla-tion, medical imaging, weak supervision},
}

@inproceedings{ratner_snorkel_2017,
	title = {Snorkel: {Rapid} training data creation with weak supervision},
	volume = {11},
	doi = {10.14778/3157794.3157797},
	abstract = {Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train stateof- the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8 × faster and increase predictive performance an average 45.5\% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8× speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132\% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60\% of the predictive performance of large hand-curated training sets.},
	publisher = {Association for Computing Machinery},
	author = {Ratner, Alexander and Bach, Stephen H. and Ehrenberg, Henry and Fries, Jason and Wu, Sen and Ré, Christopher},
	month = nov,
	year = {2017},
	note = {Issue: 3},
	pages = {269--282},
}

@article{greer_fast_2019,
	title = {Fast objective coupled planar illumination microscopy},
	volume = {10},
	doi = {10.1038/s41467-019-12340-0},
	abstract = {Among optical imaging techniques light sheet fluorescence microscopy is one of the most attractive for capturing high-speed biological dynamics unfolding in three dimensions. The technique is potentially millions of times faster than point-scanning techniques such as two-photon microscopy. However light sheet microscopes are limited by volume scanning rate and/or camera speed. We present speed-optimized Objective Coupled Planar Illumination (OCPI) microscopy, a fast light sheet technique that avoids compromising image quality or photon efficiency. Our fast scan system supports 40 Hz imaging of 700 μm-thick volumes if camera speed is sufficient. We also address the camera speed limitation by introducing Distributed Planar Imaging (DPI), a scaleable technique that parallelizes image acquisition across cameras. Finally, we demonstrate fast calcium imaging of the larval zebrafish brain and find a heartbeat-induced artifact, removable when the imaging rate exceeds 15 Hz. These advances extend the reach of fluorescence microscopy for monitoring fast processes in large volumes.},
	number = {1},
	journal = {Nature Communications},
	author = {Greer, Cody J. and Holy, Timothy E.},
	month = dec,
	year = {2019},
	note = {Publisher: Nature Publishing Group},
}

@article{peng_supplementary_2019,
	title = {Supplementary {Information} : {Learned} {Large} {Field}-of-{View} {Imaging} {With} {Thin}-{Plate} {Optics}},
	volume = {38},
	number = {6},
	author = {Peng, Yifan and Wetzstein, Gordon},
	year = {2019},
	pages = {1--15},
}

@techreport{lowe_putting_nodate,
	title = {Putting {An} {End} to {End}-to-{End}: {Gradient}-{Isolated} {Learning} of {Representations}},
	url = {https://github.com/loeweX/Greedy_InfoMax.},
	abstract = {We propose a novel deep learning method for local self-supervised representation learning that does not require labels nor end-to-end backpropagation but exploits the natural order in data instead. Inspired by the observation that biological neural networks appear to learn without backpropagating a global error signal, we split a deep neural network into a stack of gradient-isolated modules. Each module is trained to maximally preserve the information of its inputs using the InfoNCE bound from Oord et al. [2018]. Despite this greedy training, we demonstrate that each module improves upon the output of its predecessor, and that the representations created by the top module yield highly competitive results on downstream classification tasks in the audio and visual domain. The proposal enables optimizing modules asynchronously, allowing large-scale distributed training of very deep neural networks on unlabelled datasets.},
	author = {Löwe, Sindy and O'connor, Peter and Veeling, Bastiaan S},
}

@techreport{nagarajan_uniform_nodate,
	title = {Uniform convergence may be unable to explain generalization in deep learning},
	abstract = {Aimed at explaining the surprisingly good generalization behavior of overparam-eterized deep networks, recent works have developed a variety of generalization bounds for deep learning, all based on the fundamental learning-theoretic technique of uniform convergence. While it is well-known that many of these existing bounds are numerically large, through numerous experiments, we bring to light a more concerning aspect of these bounds: in practice, these bounds can increase with the training dataset size. Guided by our observations, we then present examples of overparameterized linear classifiers and neural networks trained by gradient descent (GD) where uniform convergence provably cannot "explain generalization"-even if we take into account the implicit bias of GD to the fullest extent possible. More precisely, even if we consider only the set of classifiers output by GD, which have test errors less than some small in our settings, we show that applying (two-sided) uniform convergence on this set of classifiers will yield only a vacuous generalization guarantee larger than 1 −. Through these findings, we cast doubt on the power of uniform convergence-based generalization bounds to provide a complete picture of why overparameterized deep networks generalize well.},
	author = {Nagarajan, Vaishnavh and Kolter, J Zico},
}

@article{redmon_yolov3_2018,
	title = {{YOLOv3}: {An} {Incremental} {Improvement}},
	url = {http://arxiv.org/abs/1804.02767},
	abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = apr,
	year = {2018},
}

@techreport{radlak_organization_nodate,
	title = {Organization of {ML}-based product development as per {ISO} 26262},
	abstract = {Machine learning (ML) applications generate a continuous stream of success stories from various domains. ML enables many novel applications, also in a safety-related context. With the advent of Autonomous Driving, ML gets used in automotive domain. In such a context, ML-based systems are safety-related. In the automotive industry, the applicable functional safety standard is ISO 26262, which does not cover specific aspects of ML. In a safety-related ML project, all ISO 26262 work products are typically necessary and have to be delivered. However, specific aspects of ML (like data set requirements, special analyses for ML) must be addressed within some work products. In this paper, we propose how the key technical aspects and supporting processes related to development of ML-based systems can be organized according to ISO 26262 phases, sub-phases and work-products.},
	author = {Radlak, Krystian and Serwa, Piotr and Jones, Tim},
	keywords = {automotive software, autonomous driving, de-pendability, functional safety, ISO 26262, machine learning, software engineering},
}

@article{fu_dssd_2017,
	title = {{DSSD} : {Deconvolutional} {Single} {Shot} {Detector}},
	url = {http://arxiv.org/abs/1701.06659},
	abstract = {The main contribution of this paper is an approach for introducing additional context into state-of-the-art general object detection. To achieve this we first combine a state-of-the-art classifier (Residual-101[14]) with a fast detection framework (SSD[18]). We then augment SSD+Residual-101 with deconvolution layers to introduce additional large-scale context in object detection and improve accuracy, especially for small objects, calling our resulting system DSSD for deconvolutional single shot detector. While these two contributions are easily described at a high-level, a naive implementation does not succeed. Instead we show that carefully adding additional stages of learned transformations, specifically a module for feed-forward connections in deconvolution and a new output module, enables this new approach and forms a potential way forward for further detection research. Results are shown on both PASCAL VOC and COCO detection. Our DSSD with \$513 {\textbackslash}times 513\$ input achieves 81.5\% mAP on VOC2007 test, 80.0\% mAP on VOC2012 test, and 33.2\% mAP on COCO, outperforming a state-of-the-art method R-FCN[3] on each dataset.},
	author = {Fu, Cheng-Yang and Liu, Wei and Ranga, Ananth and Tyagi, Ambrish and Berg, Alexander C.},
	month = jan,
	year = {2017},
}

@article{zhao_object_2019,
	title = {Object {Detection} {With} {Deep} {Learning}: {A} {Review}},
	doi = {10.1109/TNNLS.2018.2876865},
	abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhao, Zhong Qiu and Zheng, Peng and Xu, Shou Tao and Wu, Xindong},
	year = {2019},
	keywords = {Computer architecture, Deep learning, Feature extraction, neural network, Neural networks, Object detection, object detection., Task analysis, Training},
	pages = {1--21},
}

@techreport{zhao_object_nodate,
	title = {Object {Detection} with {Deep} {Learning}: {A} {Review}},
	url = {https://arxiv.org/pdf/1807.05511.pdf},
	abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles which combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy and optimization function, etc. In this paper, we provide a review on deep learning based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely Convolutional Neural Network (CNN). Then we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network based learning systems.},
	author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},
	keywords = {neural network, Index Terms-deep learning, object detection},
}

@article{zhao_object_2019-1,
	title = {Object {Detection} {With} {Deep} {Learning}: {A} {Review}},
	doi = {10.1109/TNNLS.2018.2876865},
	abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhao, Zhong Qiu and Zheng, Peng and Xu, Shou Tao and Wu, Xindong},
	month = nov,
	year = {2019},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Deep learning, neural network, object detection},
}

@article{mazet_background_2005,
	title = {Background removal from spectra by designing and minimising a non-quadratic cost function},
	volume = {76},
	doi = {10.1016/j.chemolab.2004.10.003},
	abstract = {In this paper, the problem of estimating the background of a spectrum is addressed. We propose to fit this background to a low-order polynomial, but rather than determining the polynomial parameters that minimise a least-squares criterion (i.e. a quadratic cost function), non-quadratic cost functions well adapted to the problem are proposed. To minimise these cost functions, we use the half-quadratic minimisation. It yields a fast and simple method, which can be applied to a wide range of spectroscopic signal. Guidelines for the choice of the design parameters are given and illustrated on simulated spectra. Finally, the effectiveness of the method is shown by processing experimental infrared and Raman spectra. © 2004 Elsevier B.V. All rights reserved.},
	number = {2},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	author = {Mazet, Vincent and Carteret, Cédric and Brie, David and Idier, Jérôme and Humbert, Bernard},
	year = {2005},
	keywords = {Background removal, Half-quadratic minimisation, Huber function, Infrared and Raman spectra, Polynomial fitting, Truncated quadratic},
	pages = {121--133},
}

@book{miura_bio-image_nodate,
	title = {Bio-image data analysis},
	isbn = {978-3-030-22385-4},
	author = {Miura, Kota},
}

@article{perslev_one_2019,
	title = {One {Network} to {Segment} {Them} {All}: {A} {General}, {Lightweight} {System} for {Accurate} {3D} {Medical} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1911.01764},
	doi = {10.1007/978-3-030-32245-8_4},
	abstract = {Many recent medical segmentation systems rely on powerful deep learning models to solve highly specific tasks. To maximize performance, it is standard practice to evaluate numerous pipelines with varying model topologies, optimization parameters, pre- \& postprocessing steps, and even model cascades. It is often not clear how the resulting pipeline transfers to different tasks. We propose a simple and thoroughly evaluated deep learning framework for segmentation of arbitrary medical image volumes. The system requires no task-specific information, no human interaction and is based on a fixed model topology and a fixed hyperparameter set, eliminating the process of model selection and its inherent tendency to cause method-level over-fitting. The system is available in open source and does not require deep learning expertise to use. Without task-specific modifications, the system performed better than or similar to highly specialized deep learning methods across 3 separate segmentation tasks. In addition, it ranked 5-th and 6-th in the first and second round of the 2018 Medical Segmentation Decathlon comprising another 10 tasks. The system relies on multi-planar data augmentation which facilitates the application of a single 2D architecture based on the familiar U-Net. Multi-planar training combines the parameter efficiency of a 2D fully convolutional neural network with a systematic train- and test-time augmentation scheme, which allows the 2D model to learn a representation of the 3D image volume that fosters generalization.},
	author = {Perslev, Mathias and Dam, Erik Bjørnager and Pai, Akshay and Igel, Christian},
	month = nov,
	year = {2019},
}

@article{learning__2016,
	title = {技術者が知っておきたい {Deep} {Learning} の基礎と 組込みでの利用 ～ 今さら聞いてください {Deep} {Learning} ～},
	volume = {26},
	issn = {9780262035613},
	url = {http://dx.doi.org/10.1038/s41592-019-0622-5},
	doi = {10.1038/s41592-019-0622-5},
	abstract = {"Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and video games. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors"--Page 4 of cover. Introduction -- Applied math and machine learning basics. Linear algebra -- Probability and information theory -- Numerical computation -- Machine learning basics -- Deep networks: modern practices. Deep feedforward networks -- Regularization for deep learning -- Optimization for training deep models -- Convolutional networks -- Sequence modeling: recurrent and recursive nets -- Practical methodology -- Applications -- Deep learning research. Linear factor models -- Autoencoders -- Representation learning -- Structured probabilistic models for deep learning -- Monte Carlo methods -- Confronting the partition function -- Approximate inference -- Deep generative models.},
	number = {7553},
	journal = {Nature},
	author = {Learning, Deep},
	year = {2016},
	note = {Publisher: Springer US},
	pages = {436--436},
}

@article{zhang_baseline_2010,
	title = {Baseline correction using adaptive iteratively reweighted penalized least squares},
	volume = {135},
	doi = {10.1039/b922045c},
	abstract = {Baseline drift always blurs or even swamps signals and deteriorates analytical results, particularly in multivariate analysis. It is necessary to correct baseline drift to perform further data analysis. Simple or modified polynomial fitting has been found to be effective to some extent. However, this method requires user intervention and is prone to variability especially in low signal-to-noise ratio environments. A novel algorithm named adaptive iteratively reweighted Penalized Least Squares (airPLS) that does not require any user intervention and prior information, such as peak detection etc., is proposed in this work. The method works by iteratively changing weights of sum squares errors (SSE) between the fitted baseline and original signals, and the weights of the SSE are obtained adaptively using the difference between the previously fitted baseline and the original signals. The baseline estimator is fast and flexible. Theory, implementation, and applications in simulated and real datasets are presented. The algorithm is implemented in R language and MATLAB™, which is available as open source software (http://code.google. com/p/airpls). © 2010 The Royal Society of Chemistry.},
	number = {5},
	journal = {Analyst},
	author = {Zhang, Zhi Min and Chen, Shan and Liang, Yi Zeng},
	year = {2010},
	pages = {1138--1146},
}

@article{royer_practical_2018,
	title = {A practical guide to adaptive light-sheet microscopy},
	volume = {13},
	doi = {10.1038/s41596-018-0043-4},
	abstract = {We describe the implementation and use of an adaptive imaging framework for optimizing spatial resolution and signal strength in a light-sheet microscope. The framework, termed AutoPilot, comprises hardware and software modules for automatically measuring and compensating for mismatches between light-sheet and detection focal planes in living specimens. Our protocol enables researchers to introduce adaptive imaging capabilities in an existing light-sheet microscope or use our SiMView microscope blueprint to set up a new adaptive multiview light-sheet microscope. The protocol describes (i) the mechano-optical implementation of the adaptive imaging hardware, including technical drawings for all custom microscope components; (ii) the algorithms and software library for automated adaptive imaging, including the pseudocode and annotated source code for all software modules; and (iii) the execution of adaptive imaging experiments, as well as the configuration and practical use of the AutoPilot framework. Setup of the adaptive imaging hardware and software takes 1–2 weeks each. Previous experience with light-sheet microscopy and some familiarity with software engineering and building of optical instruments are recommended. Successful implementation of the protocol recovers near diffraction-limited performance in many parts of typical multicellular organisms studied with light-sheet microscopy, such as fruit fly and zebrafish embryos, for which resolution and signal strength are improved two- to fivefold.},
	number = {11},
	journal = {Nature Protocols},
	author = {Royer, Loïc A. and Lemon, William C. and Chhetri, Raghav K. and Keller, Philipp J.},
	year = {2018},
	pages = {2462--2500},
}

@article{idier_convex_2001,
	title = {Convex half-quadratic criteria and interacting auxiliary variables for image restoration},
	volume = {10},
	doi = {10.1109/83.931094},
	abstract = {This paper deals with convex half-quadratic criteria and associated minimization algorithms for the purpose of image restoration. It brings a number of original elements within a unified mathematical presentation based on convex duality. Firstly, Geman and Yang's [1] and Geman and Reynold's [2] constructions are revisited, with a view to establish convexity properties of the resulting half-quadratic augmented criteria, when the original convex Gibbsian energies that incorporate interacting auxiliary variables is revealed as a potentially fruitful extension of Geman and Reynold's construction.},
	number = {7},
	journal = {IEEE Transactions on Image Processing},
	author = {Idier, J.},
	year = {2001},
	keywords = {Convex duality, Coordinate descent algorithms, Edge-preserving restoration, Gibbs-Markov models, Line processes},
	pages = {1001--1009},
}

@techreport{gao_learning_nodate,
	title = {Learning to {Separate} {Object} {Sounds} by {Watching} {Unlabeled} {Video}},
	url = {http://vision.cs.utexas.edu/projects/separating_object_sounds/},
	abstract = {Perceiving a scene most fully requires all the senses. Yet modeling how objects look and sound is challenging: most natural scenes and events contain multiple objects, and the audio track mixes all the sound sources together. We propose to learn audiovisual object models from unlabeled video, then exploit the visual context to perform audio source separation in novel videos. Our approach relies on a deep multi-instance multi-label learning framework to disentangle the audio frequency bases that map to individual visual objects, even without ob-serving/hearing those objects in isolation. We show how the recovered disentangled bases can be used to guide audio source separation to obtain better-separated, object-level sounds. Our work is the first to learn audio source separation from large-scale "in the wild" videos containing multiple audio sources per video. We obtain state-of-the-art results on visually-aided audio source separation and audio denoising. Our video results:},
	author = {Gao, Ruohan and Feris, Rogerio and Grauman, Kristen},
}

@article{zhang_baseline_2010-1,
	title = {Baseline correction using adaptive iteratively reweighted penalized least squares},
	volume = {135},
	doi = {10.1039/b922045c},
	abstract = {Baseline drift always blurs or even swamps signals and deteriorates analytical results, particularly in multivariate analysis. It is necessary to correct baseline drift to perform further data analysis. Simple or modified polynomial fitting has been found to be effective to some extent. However, this method requires user intervention and is prone to variability especially in low signal-to-noise ratio environments. A novel algorithm named adaptive iteratively reweighted Penalized Least Squares (airPLS) that does not require any user intervention and prior information, such as peak detection etc., is proposed in this work. The method works by iteratively changing weights of sum squares errors (SSE) between the fitted baseline and original signals, and the weights of the SSE are obtained adaptively using the difference between the previously fitted baseline and the original signals. The baseline estimator is fast and flexible. Theory, implementation, and applications in simulated and real datasets are presented. The algorithm is implemented in R language and MATLAB™, which is available as open source software (http://code.google. com/p/airpls). © 2010 The Royal Society of Chemistry.},
	number = {5},
	journal = {Analyst},
	author = {Zhang, Zhi Min and Chen, Shan and Liang, Yi Zeng},
	year = {2010},
	pages = {1138--1146},
}

@techreport{nielsen_neural_nodate,
	title = {Neural {Networks} and {Deep} {Learning}},
	url = {http://neuralnetworksanddeeplearning.com},
	author = {Nielsen, Michael},
}

@article{bucher_zero-shot_2019,
	title = {Zero-{Shot} {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1906.00817},
	abstract = {Semantic segmentation models are limited in their ability to scale to large numbers of object classes. In this paper, we introduce the new task of zero-shot semantic segmentation: learning pixel-wise classifiers for never-seen object categories with zero training examples. To this end, we present a novel architecture, ZS3Net, combining a deep visual segmentation model with an approach to generate visual representations from semantic word embeddings. By this way, ZS3Net addresses pixel classification tasks where both seen and unseen categories are faced at test time (so called "generalized" zero-shot classification). Performance is further improved by a self-training step that relies on automatic pseudo-labeling of pixels from unseen classes. On the two standard segmentation datasets, Pascal-VOC and Pascal-Context, we propose zero-shot benchmarks and set competitive baselines. For complex scenes as ones in the Pascal-Context dataset, we extend our approach by using a graph-context encoding to fully leverage spatial context priors coming from class-wise segmentation maps.},
	author = {Bucher, Maxime and Vu, Tuan-Hung and Cord, Matthieu and Pérez, Patrick},
	month = jun,
	year = {2019},
}

@article{ho_rapid_2019,
	title = {Rapid identification of pathogenic bacteria using {Raman} spectroscopy and deep learning},
	volume = {10},
	url = {http://www.nature.com/articles/s41467-019-12898-9},
	doi = {10.1038/s41467-019-12898-9},
	number = {1},
	journal = {Nature Communications},
	author = {Ho, Chi-Sing and Jean, Neal and Hogan, Catherine A. and Blackmon, Lena and Jeffrey, Stefanie S. and Holodniy, Mark and Banaei, Niaz and Saleh, Amr A. E. and Ermon, Stefano and Dionne, Jennifer},
	month = dec,
	year = {2019},
	pages = {4927--4927},
}

@article{tan_efficientnet_2019,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = may,
	year = {2019},
	file = {Full Text:/home/zwerg/Zotero/storage/2DQ77ITF/Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf},
}

@techreport{nogueira_dos_santos_learning_nodate,
	title = {Learning {Implicit} {Generative} {Models} by {Matching} {Perceptual} {Features}},
	abstract = {Perceptual features (PFs) have been used with great success in tasks such as transfer learning, style transfer, and super-resolution. However, the efficacy of PFs as key source of information for learning generative models is not well studied. We investigate here the use of PFs in the context of learning implicit generative models through moment matching (MM). More specifically, we propose a new effective MM approach that learns implicit generative models by performing mean and covariance matching of features extracted from pretrained ConvNets. Our proposed approach improves upon existing MM methods by: (1) breaking away from the problematic min/max game of adversarial learning ; (2) avoiding online learning of kernel functions; and (3) being efficient with respect to both number of used moments and required minibatch size. Our experimental results demonstrate that, due to the expressiveness of PFs from pretrained deep ConvNets, our method achieves state-of-the-art results for challenging benchmarks.},
	author = {Nogueira dos Santos, Cicero and Mroueh, Youssef and Padhi, Inkit and Dognin, Pierre},
}

@techreport{ignatov_dslr-quality_nodate,
	title = {{DSLR}-{Quality} {Photos} on {Mobile} {Devices} with {Deep} {Convolutional} {Networks}},
	url = {http://dped-photos.vision.ee.ethz.ch},
	abstract = {Figure 1: iPhone 3GS photo enhanced to DSLR-quality by our method. Best zoomed on screen. Abstract Despite a rapid rise in the quality of built-in smartphone cameras , their physical limitations-small sensor size, compact lenses and the lack of specific hardware,-impede them to achieve the quality results of DSLR cameras. In this work we present an end-to-end deep learning approach that bridges this gap by translating ordinary photos into DSLR-quality images. We propose learning the translation function using a residual convolutional neural network that improves both color rendition and image sharpness. Since the standard mean squared loss is not well suited for measuring perceptual image quality, we introduce a composite perceptual error function that combines content , color and texture losses. The first two losses are defined analytically, while the texture loss is learned in an adversarial fashion. We also present DPED, a large-scale dataset that consists of real photos captured from three different phones and one high-end reflex camera. Our quantitative and qualitative assessments reveal that the enhanced image quality is comparable to that of DSLR-taken photos, while the methodology is generalized to any type of digital camera.},
	author = {Ignatov, Andrey and Kobyshev, Nikolay and Vanhoey, Kenneth and Timofte, Radu and Gool, Van and Zurich, Eth},
}

@article{dautume_episodic_2019,
	title = {Episodic {Memory} in {Lifelong} {Language} {Learning}},
	url = {http://arxiv.org/abs/1906.01076},
	abstract = {We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly ({\textasciitilde}50-90\%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.},
	author = {d'Autume, Cyprien de Masson and Ruder, Sebastian and Kong, Lingpeng and Yogatama, Dani},
	month = jun,
	year = {2019},
}

@article{dautume_episodic_2019-1,
	title = {Episodic {Memory} in {Lifelong} {Language} {Learning}},
	url = {http://arxiv.org/abs/1906.01076},
	abstract = {We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly ({\textasciitilde}50-90\%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.},
	author = {d'Autume, Cyprien de Masson and Ruder, Sebastian and Kong, Lingpeng and Yogatama, Dani},
	year = {2019},
}

@article{guo_accelerating_2019,
	title = {Accelerating iterative deconvolution and multiview fusion by orders of magnitude},
	url = {http://biorxiv.org/content/early/2019/05/23/647370.abstract},
	doi = {10.1101/647370},
	abstract = {We describe theoretical and practical advances in algorithm and software design, resulting in ten to several thousand-fold faster deconvolution and multiview fusion than previous methods. First, we adapt methods from medical imaging, showing that an unmatched back projector accelerates Richardson-Lucy deconvolution by at least 10-fold, in most cases requiring only a single iteration. Second, we show that improvements in 3D image-based registration with GPU processing result in speedups of 10-100-fold over CPU processing. Third, we show that deep learning can provide further accelerations, particularly for deconvolution with a spatially varying point spread function. We illustrate the power of our methods from the subcellular to millimeter spatial scale, on diverse samples including single cells, nematode and zebrafish embryos, and cleared mouse tissue. Finally, we show that our methods facilitate the use of new microscopes that improve spatial resolution, including dual-view cleared tissue light-sheet microscopy and reflective lattice light-sheet microscopy.},
	journal = {bioRxiv},
	author = {Guo, Min and Li, Yue and Su, Yijun and Lambert, Talley and Nogare, Damian Dalle and Moyle, Mark W and Duncan, Leighton H and Ikegami, Richard and Santella, Anthony and Rey-Suarez, Ivan and Green, Daniel and Chen, Jiji and Vishwasrao, Harshad and Ganesan, Sundar and Waters, Jennifer C and Annunziata, Christina M and Hafner, Markus and Mohler, William A and Chitnis, Ajay B and Upadhyaya, Arpita and Usdin, Ted B and Bao, Zhirong and Colón-Ramos, Daniel and Riviere, Patrick La and Liu, Huafeng and Wu, Yicong and Shroff, Hari and Guo, Min and Li, Yue and Su, Yijun and Lambert, Talley and Nogare, Damian Dalle and Moyle, Mark W and Duncan, Leighton H and Ikegami, Richard and Santella, Anthony and Rey-Suarez, Ivan and Green, Daniel and Chen, Jiji and Vishwasrao, Harshad and Ganesan, Sundar and Waters, Jennifer C and Annunziata, Christina M and Hafner, Markus and Mohler, William A and Chitnis, Ajay B and Upadhyaya, Arpita and Usdin, Ted B and Bao, Zhirong and Colón-Ramos, Daniel and Riviere, Patrick La and Liu, Huafeng and Wu, Yicong and Shroff, Hari},
	year = {2019},
	pages = {647370--647370},
}

@article{antipa_diffusercam_2018,
	title = {{DiffuserCam}: lensless single-exposure {3D} imaging},
	volume = {5},
	doi = {10.1364/optica.5.000001},
	abstract = {We demonstrate a compact and easy-to-build computational camera for single-shot 3D imaging. Our lensless system consists solely of a diffuser placed in front of a standard image sensor. Every point within the volumetric field-of-view projects a unique pseudorandom pattern of caustics on the sensor. By using a physical approximation and simple calibration scheme, we solve the large-scale inverse problem in a computationally efficient way. The caustic patterns enable compressed sensing, which exploits sparsity in the sample to solve for more 3D voxels than pixels on the 2D sensor. Our 3D voxel grid is chosen to match the experimentally measured two-point optical resolution across the field-of-view, resulting in 100 million voxels being reconstructed from a single 1.3 megapixel image. However, the effective resolution varies significantly with scene content. Because this effect is common to a wide range of computational cameras, we provide new theory for analyzing resolution in such systems.},
	number = {1},
	journal = {Optica},
	author = {Antipa, Nick and Kuo, Grace and Heckel, Reinhard and Mildenhall, Ben and Bostan, Emrah and Ng, Ren and Waller, Laura},
	month = jan,
	year = {2018},
	note = {Publisher: The Optical Society},
	pages = {1--1},
}

@article{dautume_episodic_2019-2,
	title = {Episodic {Memory} in {Lifelong} {Language} {Learning}},
	url = {http://arxiv.org/abs/1906.01076},
	abstract = {We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly ({\textasciitilde}50-90\%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.},
	author = {d'Autume, Cyprien de Masson and Ruder, Sebastian and Kong, Lingpeng and Yogatama, Dani},
	year = {2019},
}

@article{chen_allen_2018,
	title = {The {Allen} {Cell} {Structure} {Segmenter}: a new open source toolkit for segmenting {3D} intracellular structures in fluorescence microscopy images},
	url = {https://www.biorxiv.org/content/10.1101/491035v1},
	doi = {10.1101/491035},
	abstract = {A continuing challenge in quantitative cell biology is the accurate and robust 3D segmentation of structures of interest from fluorescence microscopy images in an automated, reproducible, and widely accessible manner for subsequent interpretable data analysis. We describe the Allen Cell Structure Segmenter, a new Python-based open source toolkit developed for 3D segmentation of intracellular structures in fluorescence microscope images. This toolkit brings together classic image segmentation and iterative deep learning workflows first to generate initial high-quality 3D intracellular structure segmentations and then to easily curate these results to generate the ground truths for building robust and accurate deep learning models. The toolkit takes advantage of the high-replicate 3D live cell image data collected at the Allen Institute for Cell Science of over 30 endogenous fluorescently tagged human induced pluripotent stem cell (hiPSC) lines. Each cell line represents a different intracellular structure with one or more distinct localization patterns within undifferentiated hiPS cells and hiPSC-derived cardiomyocytes. The Allen Cell Structure Segmenter consists of two complementary elements, a classic image segmentation workflow with a restricted set of algorithms and parameters and an iterative deep learning segmentation workflow. We created a collection of 20 classic image segmentation workflows based on 20 distinct and representative intracellular structure localization patterns as a “lookup table” reference and starting point for users. The iterative deep learning workflow can take over when the classic segmentation workflow is insufficient. Two straightforward “human-in-the-loop” curation strategies convert a set of classic image segmentation workflow results into a set of 3D ground truth images for iterative model training without the need for manual painting in 3D. The deep learning model architectures used in this toolkit were designed and tested specifically for 3D fluorescence microscope images and implemented as readable scripts. This toolkit was applied to the robust segmentation of fluorescent lamin B1, which exhibits significant variability in its localization pattern during the cell cycle. The Allen Cell Structure Segmenter thus leverages state of the art computer vision algorithms in an accessible way to facilitate their application by the experimental biology researcher.},
	journal = {bioRxiv},
	author = {Chen, Jianxu and Ding, Liya and Viana, Matheus P. and Hendershott, Melissa C. and Yang, Ruian and Mueller, Irina A. and Rafelski, Susanne M.},
	year = {2018},
	pages = {491035--491035},
}

@article{zanella_towards_2013,
	title = {Towards real-time image deconvolution: {Application} to confocal and {STED} microscopy},
	volume = {3},
	doi = {10.1038/srep02523},
	abstract = {Although deconvolution can improve the quality of any type of microscope, the high computational time required has so far limited its massive spreading. Here we demonstrate the ability of the scaled-gradient-projection (SGP) method to provide accelerated versions of the most used algorithms in microscopy. To achieve further increases in efficiency, we also consider implementations on graphic processing units (GPUs). We test the proposed algorithms both on synthetic and real data of confocal and STED microscopy. Combining the SGP method with the GPU implementation we achieve a speed-up factor from about a factor 25 to 690 (with respect the conventional algorithm). The excellent results obtained on STED microscopy images demonstrate the synergy between super-resolution techniques and image-deconvolution. Further, the real-time processing allows conserving one of the most important property of STED microscopy, i.e the ability to provide fast sub-diffraction resolution recordings. © 2013 Macmillan Publishers Limited. All rights reserved.},
	journal = {Scientific Reports},
	author = {Zanella, R. and Zanghirati, G. and Cavicchioli, R. and Zanni, L. and Boccacci, P. and Bertero, M. and Vicidomini, G.},
	year = {2013},
}

@article{antipa_diffusercam_2018-1,
	title = {{DiffuserCam}: lensless single-exposure {3D} imaging},
	volume = {5},
	doi = {10.1364/optica.5.000001},
	abstract = {We demonstrate a compact and easy-to-build computational camera for single-shot 3D imaging. Our lensless system consists solely of a diffuser placed in front of a standard image sensor. Every point within the volumetric field-of-view projects a unique pseudorandom pattern of caustics on the sensor. By using a physical approximation and simple calibration scheme, we solve the large-scale inverse problem in a computationally efficient way. The caustic patterns enable compressed sensing, which exploits sparsity in the sample to solve for more 3D voxels than pixels on the 2D sensor. Our 3D voxel grid is chosen to match the experimentally measured two-point optical resolution across the field-of-view, resulting in 100 million voxels being reconstructed from a single 1.3 megapixel image. However, the effective resolution varies significantly with scene content. Because this effect is common to a wide range of computational cameras, we provide new theory for analyzing resolution in such systems.},
	number = {1},
	journal = {Optica},
	author = {Antipa, Nick and Kuo, Grace and Heckel, Reinhard and Mildenhall, Ben and Bostan, Emrah and Ng, Ren and Waller, Laura},
	month = jan,
	year = {2018},
	note = {Publisher: The Optical Society},
	pages = {1--1},
	file = {Submitted Version:/home/zwerg/Zotero/storage/TRIYIUK2/Antipa et al. - 2018 - DiffuserCam lensless single-exposure 3D imaging.pdf:application/pdf},
}

@article{bucher_zero-shot_2019-1,
	title = {Zero-{Shot} {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1906.00817},
	abstract = {Semantic segmentation models are limited in their ability to scale to large numbers of object classes. In this paper, we introduce the new task of zero-shot semantic segmentation: learning pixel-wise classifiers for never-seen object categories with zero training examples. To this end, we present a novel architecture, ZS3Net, combining a deep visual segmentation model with an approach to generate visual representations from semantic word embeddings. By this way, ZS3Net addresses pixel classification tasks where both seen and unseen categories are faced at test time (so called "generalized" zero-shot classification). Performance is further improved by a self-training step that relies on automatic pseudo-labeling of pixels from unseen classes. On the two standard segmentation datasets, Pascal-VOC and Pascal-Context, we propose zero-shot benchmarks and set competitive baselines. For complex scenes as ones in the Pascal-Context dataset, we extend our approach by using a graph-context encoding to fully leverage spatial context priors coming from class-wise segmentation maps.},
	author = {Bucher, Maxime and Vu, Tuan-Hung and Cord, Matthieu and Pérez, Patrick},
	month = jun,
	year = {2019},
}

@article{nguyen_deepusps_2019,
	title = {{DeepUSPS}: {Deep} {Robust} {Unsupervised} {Saliency} {Prediction} {With} {Self}-{Supervision}},
	url = {http://arxiv.org/abs/1909.13055},
	abstract = {Deep neural network (DNN) based salient object detection in images based on high-quality labels is expensive. Alternative unsupervised approaches rely on careful selection of multiple handcrafted saliency methods to generate noisy pseudo-ground-truth labels.In this work, we propose a two-stage mechanism for robust unsupervised object saliency prediction, where the first stage involves refinement of the noisy pseudo-labels generated from different handcrafted methods.Each handcrafted method is substituted by a deep network that learns to generate the pseudo-labels. These labels are refined incrementally in multiple iterations via our proposed self-supervision technique. In the second stage, the refined labels produced from multiple networks representing multiple saliency methods are used to train the actual saliency detection network. We show that this self-learning procedure outperforms all the existing unsupervised methods over different datasets. Results are even comparable to those of fully-supervised state-of-the-art approaches.},
	author = {Nguyen, Duc Tam and Dax, Maximilian and Mummad, Chaithanya Kumar and Ngo, Thi Phuong Nhung and Nguyen, Thi Hoai Phuong and Lou, Zhongyu and Brox, Thomas},
	month = sep,
	year = {2019},
}

@article{zhang_freeanchor_2019,
	title = {{FreeAnchor}: {Learning} to {Match} {Anchors} for {Visual} {Object} {Detection}},
	url = {http://arxiv.org/abs/1909.02466},
	abstract = {Modern CNN-based object detectors assign anchors for ground-truth objects under the restriction of object-anchor Intersection-over-Unit (IoU). In this study, we propose a learning-to-match approach to break IoU restriction, allowing objects to match anchors in a flexible manner. Our approach, referred to as FreeAnchor, updates hand-crafted anchor assignment to "free" anchor matching by formulating detector training as a maximum likelihood estimation (MLE) procedure. FreeAnchor targets at learning features which best explain a class of objects in terms of both classification and localization. FreeAnchor is implemented by optimizing detection customized likelihood and can be fused with CNN-based detectors in a plug-and-play manner. Experiments on MS-COCO demonstrate that FreeAnchor consistently outperforms their counterparts with significant margins.},
	author = {Zhang, Xiaosong and Wan, Fang and Liu, Chang and Ji, Rongrong and Ye, Qixiang},
	month = sep,
	year = {2019},
}

@article{wu_stochastic_2019,
	title = {Stochastic {Shared} {Embeddings}: {Data}-driven {Regularization} of {Embedding} {Layers}},
	url = {http://arxiv.org/abs/1905.10630},
	abstract = {In deep neural nets, lower level embedding layers account for a large portion of the total number of parameters. Tikhonov regularization, graph-based regularization, and hard parameter sharing are approaches that introduce explicit biases into training in a hope to reduce statistical complexity. Alternatively, we propose stochastically shared embeddings (SSE), a data-driven approach to regularizing embedding layers, which stochastically transitions between embeddings during stochastic gradient descent (SGD). Because SSE integrates seamlessly with existing SGD algorithms, it can be used with only minor modifications when training large scale neural networks. We develop two versions of SSE: SSE-Graph using knowledge graphs of embeddings; SSE-SE using no prior information. We provide theoretical guarantees for our method and show its empirical effectiveness on 6 distinct tasks, from simple neural networks with one hidden layer in recommender systems, to the transformer and BERT in natural languages. We find that when used along with widely-used regularization methods such as weight decay and dropout, our proposed SSE can further reduce overfitting, which often leads to more favorable generalization results.},
	author = {Wu, Liwei and Li, Shuqing and Hsieh, Cho-Jui and Sharpnack, James},
	month = may,
	year = {2019},
}

@techreport{ramachandran_stand-alone_nodate,
	title = {Stand-{Alone} {Self}-{Attention} in {Vision} {Models}},
	abstract = {Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12\% fewer FLOPS and 29\% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39\% fewer FLOPS and 34\% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.},
	author = {Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jonathon},
}

@article{gonzalez-acuna_general_2018,
	title = {General formula for bi-aspheric singlet lens design free of spherical aberration},
	volume = {57},
	doi = {10.1364/ao.57.009341},
	abstract = {In this paper, we present a rigorous analytical solution for the bi-aspheric singlet lens design problem. The input of the general formula presented here is the first surface of the singlet lens; this surface must be continuous and such that the rays inside the lens do not cross each other. The output is the correcting second surface of the singlet; the second surface is such that the singlet is free of spherical aberration.},
	number = {31},
	journal = {Applied Optics},
	author = {González-Acuña, Rafael G. and Chaparro-Romo, Héctor A.},
	year = {2018},
	pages = {9341--9341},
}

@article{bisrat_g_assefa_imaging-quality_2019,
	title = {Imaging-quality {3D}-printed centimeter-scale lens},
	volume = {27},
	url = {https://www.osapublishing.org/DirectPDFAccess/780DF538-DA4D-8B4E-5855B0A274F26E0F_409154/oe-27-9-12630.pdf?da=1&id=409154&seq=0&mobile=no},
	abstract = {Three-dimensional (3D) printing of imaging-quality optics has been challenging
due to the tight tolerances on surface shape and roughness. We report on manufacturing such
optics with Printoptical Technology, which is based on modified ink-jet printing. We demonstrate
for the first time a 3D-printed singlet lens with a surface profile deviation of ±500 nm within a
12-mm aperture diameter. Its RMS surface roughness is below 1 nm without surface polishing.
The printed lens exhibits an imaging resolution of some 140 lp mm−1
at 100-mm focal length in
the visible region.},
	number = {9},
	journal = {Optics Express},
	author = {{BISRAT G. ASSEFA} and 1, * MARKKU PEKKARINEN and {1 HENRI PARTANEN} and {1} and {JORIS BISKOP} and {2 JARI TURUNEN} and {1 AND JYRKI SAARINEN}},
	year = {2019},
	pages = {12630--12630},
}

@article{mockl_bgnet_2019,
	title = {{BGnet}: {Accurate} and rapid background estimation in single-molecule localization microscopy with deep neural nets},
	url = {http://arxiv.org/abs/1909.08151},
	abstract = {Background fluorescence, especially when it exhibits undesired spatial features, is a primary factor for reduced image quality in optical microscopy. Structured background is particularly detrimental when analyzing single-molecule images for 3D localization microscopy or single-molecule tracking. Here, we introduce BGnet, a deep neural network with a U-net-type architecture, as a general method to rapidly estimate the background underlying the image of a point source with excellent accuracy, even when point spread function (PSF) engineering is in use to create complex PSF shapes. We trained BGnet to extract the background from images of various PSFs and show that the identification is accurate for a wide range of different interfering background structures constructed from many spatial frequencies. Furthermore, we demonstrate that the obtained background-corrected PSF images, both for simulated and experimental data, lead to a substantial improvement in localization precision. Finally, we verify that structured background estimation with BGnet results in higher quality of super-resolution reconstructions of biological structures.},
	author = {Möckl, Leonhard and Roy, Anish R. and Petrov, Petar N. and Moerner, W. E.},
	month = sep,
	year = {2019},
}

@article{schmidt_cell_2018,
	title = {Cell {Detection} with {Star}-convex {Polygons}},
	url = {http://arxiv.org/abs/1806.03535},
	doi = {10.1007/978-3-030-00934-2_30},
	abstract = {Automatic detection and segmentation of cells and nuclei in microscopy images is important for many biological applications. Recent successful learning-based approaches include per-pixel cell segmentation with subsequent pixel grouping, or localization of bounding boxes with subsequent shape refinement. In situations of crowded cells, these can be prone to segmentation errors, such as falsely merging bordering cells or suppressing valid cell instances due to the poor approximation with bounding boxes. To overcome these issues, we propose to localize cell nuclei via star-convex polygons, which are a much better shape representation as compared to bounding boxes and thus do not need shape refinement. To that end, we train a convolutional neural network that predicts for every pixel a polygon for the cell instance at that position. We demonstrate the merits of our approach on two synthetic datasets and one challenging dataset of diverse fluorescence microscopy images.},
	author = {Schmidt, Uwe and Weigert, Martin and Broaddus, Coleman and Myers, Gene},
	month = jun,
	year = {2018},
}

@article{lu_vilbert_2019,
	title = {{ViLBERT}: {Pretraining} {Task}-{Agnostic} {Visiolinguistic} {Representations} for {Vision}-and-{Language} {Tasks}},
	url = {http://arxiv.org/abs/1908.02265},
	abstract = {We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.},
	author = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
	month = aug,
	year = {2019},
}

@article{weigert_star-convex_2019,
	title = {Star-convex {Polyhedra} for {3D} {Object} {Detection} and {Segmentation} in {Microscopy}},
	url = {http://arxiv.org/abs/1908.03636},
	abstract = {Accurate detection and segmentation of cell nuclei in volumetric (3D) fluorescence microscopy datasets is an important step in many biomedical research projects. Although many automated methods for these tasks exist, they often struggle for images with low signal-to-noise ratios and/or dense packing of nuclei. It was recently shown for 2D microscopy images that these issues can be alleviated by training a neural network to directly predict a suitable shape representation (star-convex polygon) for cell nuclei. In this paper, we adopt and extend this approach to 3D volumes by using star-convex polyhedra to represent cell nuclei and similar shapes. To that end, we overcome the challenges of 1) finding parameter-efficient star-convex polyhedra representations that can faithfully describe cell nuclei shapes, 2) adapting to anisotropic voxel sizes often found in fluorescence microscopy datasets, and 3) efficiently computing intersections between pairs of star-convex polyhedra (required for non-maximum suppression). Although our approach is quite general, since star-convex polyhedra subsume common shapes like bounding boxes and spheres as special cases, our focus is on accurate detection and segmentation of cell nuclei. That that end, we demonstrate on two challenging datasets that our approach (StarDist-3D) leads to superior results when compared to classical and deep-learning based methods.},
	author = {Weigert, Martin and Schmidt, Uwe and Haase, Robert and Sugawara, Ko and Myers, Gene},
	month = aug,
	year = {2019},
}

@article{grant-jacob_neural_2019,
	title = {A neural lens for super-resolution biological imaging},
	volume = {3},
	url = {https://iopscience.iop.org/article/10.1088/2399-6528/ab267d},
	doi = {10.1088/2399-6528/ab267d},
	number = {6},
	journal = {Journal of Physics Communications},
	author = {Grant-Jacob, James A and Mackay, Benita S and Baker, James A G and Xie, Yunhui and Heath, Daniel J and Loxham, Matthew and Eason, Robert W and Mills, Ben},
	month = jun,
	year = {2019},
	note = {Publisher: IOP Publishing},
	pages = {065004--065004},
}

@article{laine_high-quality_2019,
	title = {High-{Quality} {Self}-{Supervised} {Deep} {Image} {Denoising}.},
	url = {https://www.semanticscholar.org/paper/High-Quality-Self-Supervised-Deep-Image-Denoising.-Laine-Karras/eb07d87651f6622dc1d2d0fd64a38133ab40f283},
	journal = {NeurIPS 2019},
	author = {Laine, Samuli and Karras, Tero and Lehtinen, Jaakko and Aila, Timo},
	year = {2019},
}

@book{murray_spiegel_fourier_nodate,
	title = {Fourier {Analysis}},
	url = {http://plouffe.fr/IUT/GEII/MA3/Spiegel%20M.%20R.-Fourier%20Analysis%20Schaum%20-McGraw%20Hill.pdf},
	abstract = {with applications to boundary value problems},
	author = {{Murray Spiegel}},
}

@article{riba_kornia_2019,
	title = {Kornia: an {Open} {Source} {Differentiable} {Computer} {Vision} {Library} for {PyTorch}},
	url = {http://arxiv.org/abs/1910.02190},
	abstract = {This work presents Kornia -- an open source computer vision library which consists of a set of differentiable routines and modules to solve generic computer vision problems. At its core, the package uses PyTorch as its main backend both for efficiency and to take advantage of the reverse-mode auto-differentiation to define and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be inserted inside neural networks to train models to perform image transformations, camera calibration, epipolar geometry, and low level image processing techniques such as filtering and edge detection that operate directly on high dimensional tensor representations. Examples of classical vision problems implemented using our framework are also provided including a benchmark comparing to existing vision libraries.},
	author = {Riba, Edgar and Mishkin, Dmytro and Ponsa, Daniel and Rublee, Ethan and Bradski, Gary},
	month = oct,
	year = {2019},
}

@article{krull_noise2void_2018,
	title = {{Noise2Void} - {Learning} {Denoising} from {Single} {Noisy} {Images}},
	url = {http://arxiv.org/abs/1811.10980},
	abstract = {The field of image denoising is currently dominated by discriminative deep learning methods that are trained on pairs of noisy input and clean target images. Recently it has been shown that such methods can also be trained without clean targets. Instead, independent pairs of noisy images can be used, in an approach known as Noise2Noise (N2N). Here, we introduce Noise2Void (N2V), a training scheme that takes this idea one step further. It does not require noisy image pairs, nor clean target images. Consequently, N2V allows us to train directly on the body of data to be denoised and can therefore be applied when other methods cannot. Especially interesting is the application to biomedical image data, where the acquisition of training targets, clean or noisy, is frequently not possible. We compare the performance of N2V to approaches that have either clean target images and/or noisy image pairs available. Intuitively, N2V cannot be expected to outperform methods that have more information available during training. Still, we observe that the denoising performance of Noise2Void drops in moderation and compares favorably to training-free denoising methods.},
	author = {Krull, Alexander and Buchholz, Tim-Oliver and Jug, Florian},
	month = nov,
	year = {2018},
}

@inproceedings{dabov_image_2006,
	title = {Image denoising with block-matching and {3D} filtering},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.643267},
	doi = {10.1117/12.643267},
	author = {Dabov, Kostadin and Foi, Alessandro and Katkovnik, Vladimir and Egiazarian, Karen},
	editor = {Dougherty, Edward R. and Astola, Jaakko T. and Egiazarian, Karen O. and Nasrabadi, Nasser M. and Rizvi, Syed A.},
	month = feb,
	year = {2006},
	pages = {606414--606414},
}

@article{davoudi_sparse_2019,
	title = {sparse data},
	volume = {1},
	issn = {4225601900},
	url = {http://dx.doi.org/10.1038/s42256-019-0095-3},
	doi = {10.1038/s42256-019-0095-3},
	number = {October},
	journal = {Nature Machine Intelligence},
	author = {Davoudi, Neda and Deán-ben, Xosé Luís and Razansky, Daniel},
	year = {2019},
	note = {Publisher: Springer US},
	pages = {453--460},
}

@article{noauthor_alternatives_nodate,
	title = {Alternatives to the discrete fourier transform},
	url = {http://www.cs.cmu.edu/~dbalcan/papers/asympt.pdf},
	abstract = {It is well-known that the discrete Fourier transform (DFT) of a finite length discrete-time signal samples the discrete-time Fourier
transform of the same signal at equidistant points on the unit circle. Hence, as the signal length goes to infinity, the DFT approaches
the DTFT. Associated with the DFT are circular convolution and a
periodic signal extension. In this paper we identify a large class
of alternatives to the DFT using the theory of polynomial algebras.
Each of these Fourier transforms approaches the DTFT just as the
DFT does, but has its own signal extension and notion of convolution. Further, these Fourier transforms have Vandermonde structure,
which enables their fast computation. We provide a few experimental examples that confirm our theoretical results.},
}

@article{tang_alternative_1985,
	title = {An alternative to fourier transform spectral analysis with improved resolution},
	volume = {62},
	url = {https://www.sciencedirect.com/science/article/pii/0022236485903129},
	doi = {10.1016/0022-2364(85)90312-9},
	number = {1},
	journal = {Journal of Magnetic Resonance (1969)},
	author = {Tang, J and Lin, C.P and Bowman, M.K and Norris, J.R},
	month = mar,
	year = {1985},
	note = {Publisher: Academic Press},
	pages = {167--171},
}

@article{salili_eliminating_2017,
	title = {Eliminating {Stripe} {Artifacts} in {Light}-{Sheet} {Fluorescence} {Imaging}},
	url = {http://arxiv.org/abs/1711.07393},
	doi = {10.1063/1.5016546},
	abstract = {We report two techniques to mitigate stripe artifacts in light-sheet fluorescence imaging. The first uses an image processing algorithm called the multidirectional stripe remover (MDSR) method to filter stripes from an existing image. The second uses an elliptical holographic diffuser (EHD) with strong scattering anisotropy to prevent stripe formation during image acquisition. These techniques facilitate accurate interpretation of image data, especially in denser samples. They are also facile and cost-effective.},
	author = {Salili, Seyyed Muhammad and Harrington, Matt and Durian, Douglas J.},
	month = nov,
	year = {2017},
}

@article{radosavovic_network_2019,
	title = {On {Network} {Design} {Spaces} for {Visual} {Recognition}},
	url = {http://arxiv.org/abs/1905.13214},
	abstract = {Over the past several years progress in designing better neural network architectures for visual recognition has been substantial. To help sustain this rate of progress, in this work we propose to reexamine the methodology for comparing network architectures. In particular, we introduce a new comparison paradigm of distribution estimates, in which network design spaces are compared by applying statistical techniques to populations of sampled models, while controlling for confounding factors like network complexity. Compared to current methodologies of comparing point and curve estimates of model families, distribution estimates paint a more complete picture of the entire design landscape. As a case study, we examine design spaces used in neural architecture search (NAS). We find significant statistical differences between recent NAS design space variants that have been largely overlooked. Furthermore, our analysis reveals that the design spaces for standard model families like ResNeXt can be comparable to the more complex ones used in recent NAS work. We hope these insights into distribution analysis will enable more robust progress toward discovering better networks for visual recognition.},
	author = {Radosavovic, Ilija and Johnson, Justin and Xie, Saining and Lo, Wan-Yen and Dollár, Piotr},
	month = may,
	year = {2019},
}

@article{liu_liquid_2019,
	title = {Liquid {Warping} {GAN}: {A} {Unified} {Framework} for {Human} {Motion} {Imitation}, {Appearance} {Transfer} and {Novel} {View} {Synthesis}},
	url = {http://arxiv.org/abs/1909.12224},
	abstract = {We tackle the human motion imitation, appearance transfer, and novel view synthesis within a unified framework, which means that the model once being trained can be used to handle all these tasks. The existing task-specific methods mainly use 2D keypoints (pose) to estimate the human body structure. However, they only expresses the position information with no abilities to characterize the personalized shape of the individual person and model the limbs rotations. In this paper, we propose to use a 3D body mesh recovery module to disentangle the pose and shape, which can not only model the joint location and rotation but also characterize the personalized body shape. To preserve the source information, such as texture, style, color, and face identity, we propose a Liquid Warping GAN with Liquid Warping Block (LWB) that propagates the source information in both image and feature spaces, and synthesizes an image with respect to the reference. Specifically, the source features are extracted by a denoising convolutional auto-encoder for characterizing the source identity well. Furthermore, our proposed method is able to support a more flexible warping from multiple sources. In addition, we build a new dataset, namely Impersonator (iPER) dataset, for the evaluation of human motion imitation, appearance transfer, and novel view synthesis. Extensive experiments demonstrate the effectiveness of our method in several aspects, such as robustness in occlusion case and preserving face identity, shape consistency and clothes details. All codes and datasets are available on https://svip-lab.github.io/project/impersonator.html},
	author = {Liu, Wen and Piao, Zhixin and Min, Jie and Luo, Wenhan and Ma, Lin and Gao, Shenghua},
	month = sep,
	year = {2019},
}

@article{xue_reliable_2019,
	title = {Reliable deep-learning-based phase imaging with uncertainty quantification},
	url = {http://arxiv.org/abs/1901.02038},
	doi = {10.1364/OPTICA.6.000618},
	abstract = {Emerging deep-learning (DL)-based techniques have significant potential to revolutionize biomedical imaging. However, one outstanding challenge is the lack of reliability assessment in the DL predictions, whose errors are commonly revealed only in hindsight. Here, we propose a new Bayesian convolutional neural network (BNN)-based framework that overcomes this issue by quantifying the uncertainty of DL predictions. Foremost, we show that BNN-predicted uncertainty maps provide surrogate estimates of the true error from the network model and measurement itself. The uncertainty maps characterize imperfections often unknown in real-world applications, such as noise, model error, incomplete training data, and out-of-distribution testing data. Quantifying this uncertainty provides a per-pixel estimate of the confidence level of the DL prediction as well as the quality of the model and dataset. We demonstrate this framework in the application of large space-bandwidth product phase imaging using a physics-guided coded illumination scheme. From only five multiplexed illumination measurements, our BNN predicts gigapixel phase images in both static and dynamic biological samples with quantitative credibility assessment. Furthermore, we show that low-certainty regions can identify spatially and temporally rare biological phenomena. We believe our uncertainty learning framework is widely applicable to many DL-based biomedical imaging techniques for assessing the reliability of DL predictions.},
	author = {Xue, Yujia and Cheng, Shiyi and Li, Yunzhe and Tian, Lei},
	month = jan,
	year = {2019},
}

@article{rivenson_deep_2019,
	title = {Deep learning in holography and coherent imaging},
	volume = {8},
	url = {http://www.nature.com/articles/s41377-019-0196-0},
	doi = {10.1038/s41377-019-0196-0},
	abstract = {Recent advances in deep learning have given rise to a new paradigm of holographic image reconstruction and phase recovery techniques with real-time performance. Through data-driven approaches, these emerging techniques have overcome some of the challenges associated with existing holographic image reconstruction methods while also minimizing the hardware requirements of holography. These recent advances open up a myriad of new opportunities for the use of coherent imaging systems in biomedical and engineering research and related applications. Deep learning enabled by neural networks is bringing a host of new opportunities for improving the reconstruction of images obtained from digital holography and coherent imaging schemes. In a discussion of the topic, Yair Rivenson, Yichen Wu, and Aydogan Ozcan from the University of California at Los Angeles, USA explain how once “trained” with appropriate datasets, neural networks can learn to reconstruct images with added benefits such as improved phase recovery and extended depth of field as well as enhanced spatial resolution and superior signal-to-noise ratio. Advances in computing power, especially graphics processing units (GPUs), mean that such image reconstructions can now typically take place in a fraction of a second on a low-cost platform. The advances are expected to impact various applications in e.g., biomedical imaging, environmental sensing and materials science.},
	number = {1},
	journal = {Light: Science \& Applications},
	author = {Rivenson, Yair and Wu, Yichen and Ozcan, Aydogan},
	month = dec,
	year = {2019},
	note = {Publisher: Nature Publishing Group},
	keywords = {contrast microscopy, Imaging and sensing, Microscopy, Phase},
	pages = {85--85},
}

@article{gessert_automatic_2018,
	title = {Automatic {Plaque} {Detection} in {IVOCT} {Pullbacks} {Using} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1808.04187},
	doi = {10.1109/TMI.2018.2865659},
	abstract = {Coronary heart disease is a common cause of death despite being preventable. To treat the underlying plaque deposits in the arterial walls, intravascular optical coherence tomography can be used by experts to detect and characterize the lesions. In clinical routine, hundreds of images are acquired for each patient which requires automatic plaque detection for fast and accurate decision support. So far, automatic approaches rely on classic machine learning methods and deep learning solutions have rarely been studied. Given the success of deep learning methods with other imaging modalities, a thorough understanding of deep learning-based plaque detection for future clinical decision support systems is required. We address this issue with a new dataset consisting of in-vivo patient images labeled by three trained experts. Using this dataset, we employ state-of-the-art deep learning models that directly learn plaque classification from the images. For improved performance, we study different transfer learning approaches. Furthermore, we investigate the use of cartesian and polar image representations and employ data augmentation techniques tailored to each representation. We fuse both representations in a multi-path architecture for more effective feature exploitation. Last, we address the challenge of plaque differentiation in addition to detection. Overall, we find that our combined model performs best with an accuracy of 91.7\%, a sensitivity of 90.9\% and a specificity of 92.4\%. Our results indicate that building a deep learning-based clinical decision support system for plaque detection is feasible.},
	author = {Gessert, Nils and Lutz, Matthias and Heyder, Markus and Latus, Sarah and Leistner, David M. and Abdelwahed, Youssef S. and Schlaefer, Alexander},
	month = aug,
	year = {2018},
}

@article{cai_once_2019,
	title = {Once for {All}: {Train} {One} {Network} and {Specialize} it for {Efficient} {Deployment}},
	url = {http://arxiv.org/abs/1908.09791},
	abstract = {Efficient deployment of deep learning models requires specialized neural network architectures to best fit different hardware platforms and efficiency constraints (defined as deployment scenarios). Traditional approaches either manually design or use AutoML to search a specialized neural network and train it from scratch for each case. It is expensive and unscalable since their training cost is linear w.r.t. the number of deployment scenarios. In this work, we introduce Once for All (OFA) for efficient neural network design to handle many deployment scenarios, a new methodology that decouples model training from architecture search. Instead of training a specialized model for each case, we propose to train a once-for-all network that supports diverse architectural settings (depth, width, kernel size, and resolution). Given a deployment scenario, we can later search a specialized sub-network by selecting from the once-for-all network without training. As such, the training cost of specialized models is reduced from O(N) to O(1). However, it's challenging to prevent interference between many sub-networks. Therefore we propose the progressive shrinking algorithm, which is capable of training a once-for-all network to support more than \$10{\textasciicircum}\{19\}\$ sub-networks while maintaining the same accuracy as independently trained networks, saving the non-recurring engineering (NRE) cost. Extensive experiments on various hardware platforms (Mobile/CPU/GPU) and efficiency constraints show that OFA consistently achieves the same level (or better) ImageNet accuracy than SOTA neural architecture search (NAS) methods. Remarkably, OFA is orders of magnitude faster than NAS in handling multiple deployment scenarios (N). With N=40, OFA requires 14x fewer GPU hours than ProxylessNAS, 16x fewer GPU hours than FBNet and 1,142x fewer GPU hours than MnasNet. The more deployment scenarios, the more savings over NAS.},
	author = {Cai, Han and Gan, Chuang and Han, Song},
	month = aug,
	year = {2019},
}

@article{mayer_attenuation_2018,
	title = {Attenuation artifacts in light sheet fluorescence microscopy corrected by {OPTiSPIM}},
	volume = {7},
	url = {http://www.nature.com/articles/s41377-018-0068-z},
	doi = {10.1038/s41377-018-0068-z},
	abstract = {Light sheet fluorescence microscopy (LSFM) is rapidly becoming an essential technology for mesoscopic imaging of samples such as embryos and adult mouse organs. However, LSFM can suffer from optical artifacts for which there is no intrinsic solution. The attenuation of light due to absorbing material causes “shadow” artifacts along both the illumination and detection paths. Several approaches have been introduced to reduce this problem, including scanning illumination and multi-view imaging. However, neither of these approaches completely eliminates the problem. If the distribution of the absorbing material is complex, shadows cannot be avoided. We introduce a new approach that relies on multi-modal integration of two very different mesoscopic techniques. Unlike LSFM, optical projection tomography (OPT) can operate in transmission mode to create a voxel map of the 3D distribution of the sample’s optical attenuation. Here, we demonstrate a hybrid instrument (OPTiSPIM) that can quantify this attenuation and use the information to correct the shadow artifacts of LSFM. Scientists in Spain have developed a new method that combines two different techniques for improving the quality of images produced by light sheet fluorescence microscopy (LSFM), a tool that has revolutionized the imaging of biological samples. Although LSFM is a becoming an increasingly essential tool in biological imaging, it suffers from ‘shadows’ or ‘stripe artifacts’ when samples contain regions that significantly attenuate light, such as eye pigmentation. Now, Jim Swoger and colleagues from the Center for Genomic Regulation in Barcelona have developed a technique that measures the attenuation and uses three-dimensional mapping to correct for artifacts. By combining Optical Projection Tomography and multi-view LSFM imaging modalities into a single hybrid system, called OPTiSPIM, the researchers have developed a method that produces more accurate images of biological materials, such as mice and rodent embryos and organs.},
	number = {1},
	journal = {Light: Science \& Applications},
	author = {Mayer, Jürgen and Robert-Moreno, Alexandre and Sharpe, James and Swoger, Jim},
	month = dec,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Light, sheet microscopy, Transmission light microscopy},
	pages = {70--70},
}

@article{yuille_deep_2018,
	title = {Deep {Nets}: {What} have they ever done for {Vision}?},
	url = {http://arxiv.org/abs/1805.04025},
	abstract = {This is an opinion paper about the strengths and weaknesses of Deep Nets for vision. They are at the center of recent progress on artificial intelligence and are of growing importance in cognitive science and neuroscience. They have enormous successes but also clear limitations. There is also only partial understanding of their inner workings. It seems unlikely that Deep Nets in their current form will be the best long-term solution either for building general purpose intelligent machines or for understanding the mind/brain, but it is likely that many aspects of them will remain. At present Deep Nets do very well on specific types of visual tasks and on specific benchmarked datasets. But Deep Nets are much less general purpose, flexible, and adaptive than the human visual system. Moreover, methods like Deep Nets may run into fundamental difficulties when faced with the enormous complexity of natural images which can lead to a combinatorial explosion. To illustrate our main points, while keeping the references small, this paper is slightly biased towards work from our group.},
	author = {Yuille, Alan L. and Liu, Chenxi},
	month = may,
	year = {2018},
}

@article{batson_noise2self_2019,
	title = {{Noise2Self}: {Blind} {Denoising} by {Self}-{Supervision}},
	url = {http://arxiv.org/abs/1901.11365},
	abstract = {We propose a general framework for denoising high-dimensional measurements which requires no prior on the signal, no estimate of the noise, and no clean training data. The only assumption is that the noise exhibits statistical independence across different dimensions of the measurement, while the true signal exhibits some correlation. For a broad class of functions ("\${\textbackslash}mathcal\{J\}\$-invariant"), it is then possible to estimate the performance of a denoiser from noisy data alone. This allows us to calibrate \${\textbackslash}mathcal\{J\}\$-invariant versions of any parameterised denoising algorithm, from the single hyperparameter of a median filter to the millions of weights of a deep neural network. We demonstrate this on natural image and microscopy data, where we exploit noise independence between pixels, and on single-cell gene expression data, where we exploit independence between detections of individual molecules. This framework generalizes recent work on training neural nets from noisy images and on cross-validation for matrix factorization.},
	author = {Batson, Joshua and Royer, Loic},
	month = jan,
	year = {2019},
}

@article{araslanov_actor-critic_2019,
	title = {Actor-{Critic} {Instance} {Segmentation}},
	url = {http://arxiv.org/abs/1904.05126},
	abstract = {Most approaches to visual scene analysis have emphasised parallel processing of the image elements. However, one area in which the sequential nature of vision is apparent, is that of segmenting multiple, potentially similar and partially occluded objects in a scene. In this work, we revisit the recurrent formulation of this challenging problem in the context of reinforcement learning. Motivated by the limitations of the global max-matching assignment of the ground-truth segments to the recurrent states, we develop an actor-critic approach in which the actor recurrently predicts one instance mask at a time and utilises the gradient from a concurrently trained critic network. We formulate the state, action, and the reward such as to let the critic model long-term effects of the current prediction and incorporate this information into the gradient signal. Furthermore, to enable effective exploration in the inherently high-dimensional action space of instance masks, we learn a compact representation using a conditional variational auto-encoder. We show that our actor-critic model consistently provides accuracy benefits over the recurrent baseline on standard instance segmentation benchmarks.},
	author = {Araslanov, Nikita and Rothkopf, Constantin and Roth, Stefan},
	month = apr,
	year = {2019},
}

@article{bauer_automatic_2018,
	title = {Automatic {Estimation} of {Modulation} {Transfer} {Functions}},
	url = {http://arxiv.org/abs/1805.01872},
	abstract = {The modulation transfer function (MTF) is widely used to characterise the performance of optical systems. Measuring it is costly and it is thus rarely available for a given lens specimen. Instead, MTFs based on simulations or, at best, MTFs measured on other specimens of the same lens are used. Fortunately, images recorded through an optical system contain ample information about its MTF, only that it is confounded with the statistics of the images. This work presents a method to estimate the MTF of camera lens systems directly from photographs, without the need for expensive equipment. We use a custom grid display to accurately measure the point response of lenses to acquire ground truth training data. We then use the same lenses to record natural images and employ a data-driven supervised learning approach using a convolutional neural network to estimate the MTF on small image patches, aggregating the information into MTF charts over the entire field of view. It generalises to unseen lenses and can be applied for single photographs, with the performance improving if multiple photographs are available.},
	author = {Bauer, Matthias and Volchkov, Valentin and Hirsch, Michael and Schölkopf, Bernhard},
	month = may,
	year = {2018},
}

@article{liu_deep_2019,
	title = {Deep {Learning} {Theory} {Review}: {An} {Optimal} {Control} and {Dynamical} {Systems} {Perspective}},
	url = {http://arxiv.org/abs/1908.10920},
	abstract = {Attempts from different disciplines to provide a fundamental understanding of deep learning have advanced rapidly in recent years, yet a unified framework remains relatively limited. In this article, we provide one possible way to align existing branches of deep learning theory through the lens of dynamical system and optimal control. By viewing deep neural networks as discrete-time nonlinear dynamical systems, we can analyze how information propagates through layers using mean field theory. When optimization algorithms are further recast as controllers, the ultimate goal of training processes can be formulated as an optimal control problem. In addition, we can reveal convergence and generalization properties by studying the stochastic dynamics of optimization algorithms. This viewpoint features a wide range of theoretical study from information bottleneck to statistical physics. It also provides a principled way for hyper-parameter tuning when optimal control theory is introduced. Our framework fits nicely with supervised learning and can be extended to other learning problems, such as Bayesian learning, adversarial training, and specific forms of meta learning, without efforts. The review aims to shed lights on the importance of dynamics and optimal control when developing deep learning theory.},
	author = {Liu, Guan-Horng and Theodorou, Evangelos A.},
	month = aug,
	year = {2019},
}

@incollection{coolen_beginners_1998,
	title = {A {Beginner}’s {Guide} to the {Mathematics} of {Neural} {Networks}},
	abstract = {In this paper I try to describe both the role of mathematics in shaping our understanding of ho w neural net orks operate, and the curious w new mathematical concepts generated b y our attempts to capture neural net orks in equations. My target reader being the non-expert, I will present a biased selection of relativ ely simple examples of neural net ork w tasks, models and calculations, rather than try to giv e a full encyclopedic review-lik e accoun t of the man y mathematical dev elopmen ts in this field.},
	author = {Coolen, A. C. C.},
	year = {1998},
	doi = {10.1007/978-1-4471-3427-5_2},
	pages = {13--70},
}

@article{lee_coding_2019,
	title = {Coding {Scheme} {Optimization} for {Fast} {Fluorescence} {Lifetime} {Imaging}},
	volume = {38},
	url = {https://doi.org/10.1145/3325136},
	doi = {10.1145/3325136},
	abstract = {Fig. 1. Fast frequency domain fluorescence lifetime imaging (FLIM). (a)-(b) Fluorescence excitations by different modulation functions (orange) lead to different fluorescence emissions (green). The intensities obtained by correlating the emissions with demodulation functions (blue) determine the SNR. We propose a theoretical framework for analysis and design of FD-FLIM coding schemes (modulation and demodulation functions), and use that to design (b) novel coding schemes that achieve considerably higher SNR as compared to conventional methods. (c) We developed a prototype FD-FLIM system to implement various coding schemes. (d) A fluorescent sample with two different fluorescence lifetimes for the foreground and the background is excited by a low power light source. (e) With the conventional coding scheme and 0.8ms/pixel acquisition time, no clear boundary is observed between the foreground and background. (f) A considerably longer (10ms/pixel) acquisition time is required to obtain a clear boundary. (g) With the proposed coding schemes, 0.8ms/pixel acquisition time is sufficient to detect a clear boundary. Fluorescence lifetime imaging (FLIM) is used for measuring material properties in a wide range of applications, including biology, medical imaging, chemistry, and material science. In frequency-domain FLIM (FD-FLIM), the object of interest is illuminated with a temporally modulated light source. The fluorescence lifetime is measured by computing the correlations of the emitted light with a demodulation function at the sensor. The signal-to-noise ratio (SNR) and the acquisition time of a FD-FLIM system is determined by the coding scheme (modulation and demodulation functions). In this article, we develop theory and algorithms for designing high-performance FD-FLIM coding schemes that can achieve high SNR and short acquisition time, given a fixed source power budget. Based on a geometric analysis of the image formation and noise model, we propose a novel surrogate objective for the performance of a given coding scheme. The sur-rogate objective is extremely fast to compute, and can be used to efficiently explore the entire space of coding schemes. Based on this objective, we design novel, high-performance coding schemes that achieve up to an order of magnitude shorter acquisition time as compared to existing approaches. We demonstrate the performance advantage of the proposed schemes in a variety of imaging conditions, using a modular hardware prototype that can implement various coding schemes.},
	number = {3},
	journal = {ACM Transactions on Graphics},
	author = {Lee, Jongho and Varghese Chacko, Jenu and Dai, Bing and Azer Reza, Syed and Kader Sagar, Abdul and Eliceiri, Kevin W and Velten, Andreas and Gupta, Mohit and Chacko, J V and Dai, B and Sagar, A K and Eliceiri, K W and Reza, S A and Velten, A},
	year = {2019},
	keywords = {Additional Key Words and Phrases: Fluorescence lifetime, CCS Concepts: • Computing methodologies → Computational pho-tography, coding optimiza-tion, time-of-flight, waveform optimization},
}

@article{mancini_adagraph_2019,
	title = {{AdaGraph}: {Unifying} {Predictive} and {Continuous} {Domain} {Adaptation} through {Graphs}},
	url = {http://arxiv.org/abs/1903.07062},
	abstract = {The ability to categorize is a cornerstone of visual intelligence, and a key functionality for artificial, autonomous visual machines. This problem will never be solved without algorithms able to adapt and generalize across visual domains. Within the context of domain adaptation and generalization, this paper focuses on the predictive domain adaptation scenario, namely the case where no target data are available and the system has to learn to generalize from annotated source images plus unlabeled samples with associated metadata from auxiliary domains. Our contributionis the first deep architecture that tackles predictive domainadaptation, able to leverage over the information broughtby the auxiliary domains through a graph. Moreover, we present a simple yet effective strategy that allows us to take advantage of the incoming target data at test time, in a continuous domain adaptation scenario. Experiments on three benchmark databases support the value of our approach.},
	author = {Mancini, Massimiliano and Bulò, Samuel Rota and Caputo, Barbara and Ricci, Elisa},
	month = mar,
	year = {2019},
}

@article{wang_deep_2019,
	title = {Deep learning enables cross-modality super-resolution in fluorescence microscopy},
	volume = {16},
	url = {http://www.nature.com/articles/s41592-018-0239-0},
	doi = {10.1038/s41592-018-0239-0},
	number = {1},
	journal = {Nature Methods},
	author = {Wang, Hongda and Rivenson, Yair and Jin, Yiyin and Wei, Zhensong and Gao, Ronald and Günaydın, Harun and Bentolila, Laurent A. and Kural, Comert and Ozcan, Aydogan},
	month = jan,
	year = {2019},
	pages = {103--110},
}

@article{moen_deep_2019,
	title = {Deep learning for cellular image analysis},
	url = {http://www.nature.com/articles/s41592-019-0403-1},
	doi = {10.1038/s41592-019-0403-1},
	journal = {Nature Methods},
	author = {Moen, Erick and Bannon, Dylan and Kudo, Takamasa and Graf, William and Covert, Markus and Van Valen, David},
	month = may,
	year = {2019},
}

@article{fang_deep_2019,
	title = {Deep {Learning}-{Based} {Point}-{Scanning} {Super}-{Resolution} {Imaging}},
	url = {https://www.biorxiv.org/content/10.1101/740548v5},
	doi = {10.1101/740548},
	abstract = {Point scanning imaging systems (e.g. scanning electron or laser scanning confocal microscopes) are perhaps the most widely used tools for high resolution cellular and tissue imaging. Like all other imaging modalities, the resolution, speed, sample preservation, and signal-to-noise ratio (SNR) of point scanning systems are difficult to optimize simultaneously. In particular, point scanning systems are uniquely constrained by an inverse relationship between imaging speed and pixel resolution. Here we show these limitations can be mitigated via the use of deep learning-based super-sampling of undersampled images acquired on a point-scanning system, which we termed point-scanning super-resolution (PSSR) imaging. Oversampled, high SNR ground truth images acquired on scanning electron or Airyscan laser scanning confocal microscopes were ‘crappified’ to generate semi-synthetic training data for PSSR models that were then used to restore real-world undersampled images. Remarkably, our EM PSSR model could restore undersampled images acquired with different optics, detectors, samples, or sample preparation methods in other labs. PSSR enabled previously unattainable 2 nm resolution images with our serial block face scanning electron microscope system. For fluorescence, we show that undersampled confocal images combined with a multiframe PSSR model trained on Airyscan timelapses facilitates Airyscan-equivalent spatial resolution and SNR with ∼100x lower laser dose and 16x higher frame rates than corresponding high-resolution acquisitions. In conclusion, PSSR facilitates point-scanning image acquisition with otherwise unattainable resolution, speed, and sensitivity.
![Figure][1]{\textless}/img{\textgreater}
[1]: pending:yes},
	journal = {bioRxiv},
	author = {Fang, Linjing and Monroe, Fred and Novak, Sammy Weiser and Kirk, Lyndsey and Schiavon, Cara R. and Yu, Seungyoon B. and Zhang, Tong and Wu, Melissa and Kastner, Kyle and Kubota, Yoshiyuki and Zhang, Zhao and Pekkurnaz, Gulcin and Mendenhall, John and Harris, Kristen and Howard, Jeremy and Manor, Uri},
	month = sep,
	year = {2019},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {740548--740548},
}

@article{januszewski_high-precision_2017,
	title = {High-{Precision} {Automated} {Reconstruction} of {Neurons} with {Flood}-filling {Networks}},
	url = {https://www.biorxiv.org/content/10.1101/200675v1},
	doi = {10.1101/200675},
	abstract = {Reconstruction of neural circuits from volume electron microscopy data requires the tracing of complete cells including all their neurites. Automated approaches have been developed to perform the tracing, but without costly human proofreading their error rates are too high to obtain reliable circuit diagrams. We present a method for automated segmentation that, like the majority of previous efforts, employs convolutional neural networks, but contains in addition a recurrent pathway that allows the iterative optimization and extension of the reconstructed shape of individual neural processes. We used this technique, which we call flood-filling networks, to trace neurons in a data set obtained by serial block-face electron microscopy from a male zebra finch brain. Our method achieved a mean error-free neurite path length of 1.1 mm, an order of magnitude better than previously published approaches applied to the same dataset. Only 4 mergers were observed in a neurite test set of 97 mm path length.},
	journal = {bioRxiv},
	author = {Januszewski, Michał and Kornfeld, Jörgen and Li, Peter H. and Pope, Art and Blakely, Tim and Lindsey, Larry and Maitin-Shepard, Jeremy and Tyka, Mike and Denk, Winfried and Jain, Viren},
	month = oct,
	year = {2017},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {200675--200675},
}

@article{zhao_data_2019,
	title = {Data augmentation using learned transformations for one-shot medical image segmentation},
	url = {http://arxiv.org/abs/1902.09383},
	abstract = {Image segmentation is an important task in many medical applications. Methods based on convolutional neural networks attain state-of-the-art accuracy; however, they typically rely on supervised training with large labeled datasets. Labeling medical images requires significant expertise and time, and typical hand-tuned approaches for data augmentation fail to capture the complex variations in such images. We present an automated data augmentation method for synthesizing labeled medical images. We demonstrate our method on the task of segmenting magnetic resonance imaging (MRI) brain scans. Our method requires only a single segmented scan, and leverages other unlabeled scans in a semi-supervised approach. We learn a model of transformations from the images, and use the model along with the labeled example to synthesize additional labeled examples. Each transformation is comprised of a spatial deformation field and an intensity change, enabling the synthesis of complex effects such as variations in anatomy and image acquisition procedures. We show that training a supervised segmenter with these new examples provides significant improvements over state-of-the-art methods for one-shot biomedical image segmentation. Our code is available at https://github.com/xamyzhao/brainstorm.},
	author = {Zhao, Amy and Balakrishnan, Guha and Durand, Frédo and Guttag, John V. and Dalca, Adrian V.},
	month = feb,
	year = {2019},
}

@article{papernot_scalable_2018,
	title = {Scalable {Private} {Learning} with {PATE}},
	url = {http://arxiv.org/abs/1802.08908},
	abstract = {The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a "student" model the knowledge of an ensemble of "teacher" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers' answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets. In this work, we show how PATE can scale to learning tasks with large numbers of output classes and uncurated, imbalanced training data with errors. For this, we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise, and prove their tighter differential-privacy guarantees. Our new mechanisms build on two insights: the chance of teacher consensus is increased by using more concentrated noise and, lacking consensus, no answer need be given to a student. The consensus answers used are more likely to be correct, offer better intuitive privacy, and incur lower-differential privacy cost. Our evaluation shows our mechanisms improve on the original PATE on all measures, and scale to larger tasks with both high utility and very strong privacy (\${\textbackslash}varepsilon\$ {\textless} 1.0).},
	author = {Papernot, Nicolas and Song, Shuang and Mironov, Ilya and Raghunathan, Ananth and Talwar, Kunal and Erlingsson, Úlfar},
	month = feb,
	year = {2018},
}

@techreport{bonawitz_towards_2019,
	title = {{TOWARDS} {FEDERATED} {LEARNING} {AT} {SCALE}: {SYSTEM} {DESIGN}},
	url = {https://arxiv.org/pdf/1902.01046.pdf},
	abstract = {Federated Learning is a distributed machine learning approach which enables model training on a large corpus of decentralized data. We have built a scalable production system for Federated Learning in the domain of mobile devices, based on TensorFlow. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions.},
	author = {Bonawitz, Keith and Eichner, Hubert and Grieskamp, Wolfgang and Huba, Dzmitry and Ingerman, Alex and Ivanov, Vladimir and Kiddon, Chloé and Konečn´y, Jakub Konečn´ and Mazzocchi, Stefano and Mcmahan, H Brendan and Overveldt, Timon Van and Petrou, David and Ramage, Daniel and Roselander, Jason},
	year = {2019},
}

@article{konecny_federated_2016,
	title = {Federated {Learning}: {Strategies} for {Improving} {Communication} {Efficiency}},
	url = {https://ai.google/research/pubs/pub45648},
	author = {Konečný, Jakub and McMahan, H. Brendan and Yu, Felix X. and Richtarik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
	year = {2016},
}

@techreport{van_der_maaten_visualizing_2008,
	title = {Visualizing {Data} using t-{SNE}},
	url = {http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf},
	abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza-tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
	author = {Van Der Maaten, Laurens and Hinton, Geoffrey},
	year = {2008},
	note = {Volume: 9},
	keywords = {dimensionality reduction, embedding algorithms, manifold learning, multidimensional scaling, visualization},
	pages = {2579--2605},
}

@techreport{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44873.pdf},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	author = {Hinton, Geoffrey and Dean, Jeff},
	year = {2015},
	keywords = {()},
}

@article{papernot_semi-supervised_2016,
	title = {Semi-supervised {Knowledge} {Transfer} for {Deep} {Learning} from {Private} {Training} {Data}},
	url = {http://arxiv.org/abs/1610.05755},
	abstract = {Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as "teachers" for a "student" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.},
	author = {Papernot, Nicolas and Abadi, Martín and Erlingsson, Úlfar and Goodfellow, Ian and Talwar, Kunal},
	month = oct,
	year = {2016},
}

@techreport{konecn_federated_nodate,
	title = {{FEDERATED} {LEARNING}: {STRATEGIES} {FOR} {IMPROVING} {COMMUNICATION} {EFFICIENCY}},
	url = {https://arxiv.org/pdf/1610.05492.pdf},
	abstract = {Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.},
	author = {Konečn, Jakub and Brendan McMahan, H and Yu, Felix X and Theertha Suresh, Ananda and Bacon Google, Dave and Richtárik, Peter},
}

@article{xie_unsupervised_2019,
	title = {Unsupervised {Data} {Augmentation} for {Consistency} {Training}},
	url = {http://arxiv.org/abs/1904.12848},
	abstract = {Despite much success, deep learning generally does not perform well with small labeled training sets. In these scenarios, data augmentation has shown much promise in alleviating the need for more labeled data, but it so far has mostly been applied in supervised settings and achieved limited gains. In this work, we propose to apply data augmentation to unlabeled data in a semi-supervised learning setting. Our method, named Unsupervised Data Augmentation or UDA, encourages the model predictions to be consistent between an unlabeled example and an augmented unlabeled example. Unlike previous methods that use random noise such as Gaussian noise or dropout noise, UDA has a small twist in that it makes use of harder and more realistic noise generated by state-of-the-art data augmentation methods. This small twist leads to substantial improvements on six language tasks and three vision tasks even when the labeled set is extremely small. For example, on the IMDb text classification dataset, with only 20 labeled examples, UDA achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On standard semi-supervised learning benchmarks CIFAR-10 and SVHN, UDA outperforms all previous approaches and achieves an error rate of 2.7\% on CIFAR-10 with only 4,000 examples and an error rate of 2.85\% on SVHN with only 250 examples, nearly matching the performance of models trained on the full sets which are one or two orders of magnitude larger. UDA also works well on large-scale datasets such as ImageNet. When trained with 10\% of the labeled set, UDA improves the top-1/top-5 accuracy from 55.1/77.3\% to 68.7/88.5\%. For the full ImageNet with 1.3M extra unlabeled data, UDA further pushes the performance from 78.3/94.4\% to 79.0/94.5\%.},
	author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
	month = apr,
	year = {2019},
}

@article{berthelot_mixmatch_2019,
	title = {{MixMatch}: {A} {Holistic} {Approach} to {Semi}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/1905.02249},
	abstract = {Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that works by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp. We show that MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38\% to 11\%) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success.},
	author = {Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin},
	month = may,
	year = {2019},
}

@article{belthangady_applications_2019,
	title = {Applications, promises, and pitfalls of deep learning for fluorescence image reconstruction},
	url = {http://www.nature.com/articles/s41592-019-0458-z},
	doi = {10.1038/s41592-019-0458-z},
	journal = {Nature Methods},
	author = {Belthangady, Chinmay and Royer, Loic A.},
	year = {2019},
	note = {Publisher: Springer US},
}

@article{kulkarni_reconnet_2016,
	title = {{ReconNet}: {Non}-{Iterative} {Reconstruction} of {Images} from {Compressively} {Sensed} {Random} {Measurements}},
	url = {http://arxiv.org/abs/1601.06892},
	abstract = {The goal of this paper is to present a non-iterative and more importantly an extremely fast algorithm to reconstruct images from compressively sensed (CS) random measurements. To this end, we propose a novel convolutional neural network (CNN) architecture which takes in CS measurements of an image as input and outputs an intermediate reconstruction. We call this network, ReconNet. The intermediate reconstruction is fed into an off-the-shelf denoiser to obtain the final reconstructed image. On a standard dataset of images we show significant improvements in reconstruction results (both in terms of PSNR and time complexity) over state-of-the-art iterative CS reconstruction algorithms at various measurement rates. Further, through qualitative experiments on real data collected using our block single pixel camera (SPC), we show that our network is highly robust to sensor noise and can recover visually better quality images than competitive algorithms at extremely low sensing rates of 0.1 and 0.04. To demonstrate that our algorithm can recover semantically informative images even at a low measurement rate of 0.01, we present a very robust proof of concept real-time visual tracking application.},
	author = {Kulkarni, Kuldeep and Lohit, Suhas and Turaga, Pavan and Kerviche, Ronan and Ashok, Amit},
	month = jan,
	year = {2016},
}

@techreport{smith_super-convergence_nodate,
	title = {Super-{Convergence}: {Very} {Fast} {Training} of {Neural} {Networks} {Using} {Large} {Learning} {Rates}},
	url = {https://arxiv.org/pdf/1708.07120.pdf},
	abstract = {In this paper, we describe a phenomenon, which we named "super-convergence", where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures to replicate this work will be made available upon publication.},
	author = {Smith, Leslie N and Topin, Nicholay},
}

@inproceedings{lu_metric_2017,
	title = {Metric learning based data augmentation for environmental sound classification},
	isbn = {978-1-5386-1632-1},
	doi = {10.1109/WASPAA.2017.8169983},
	abstract = {Eighteen patients with stable intermittent claudication were randomized in a double blind cross-over study comparing the effects of the Ginkgo biloba extract GB-8 at a dose of 120 mg o.d. with placebo. All patients were treated for three months with the active extract and three months with placebo. The effects of treatment on arterial insufficiency were quantified by measurements of systemic and peripheral systolic blood pressures, and pain-free and maximal walking distances on a tread-mill. Questionnaires based on visual analogue scales were used to quantify the severity of leg pain, impairment of concentration, and inability to remember. Short-term memory was objectively assessed. We did not find any significant changes in either peripheral blood pressures, walking distances or the severity of leg pain. Systemic blood pressure was reduced both by placebo and GB-8. The impairment of concentration and the inability to remember were both reduced, when comparing results during active treatment to placebo. Short-term memory did not change significantly. In conclusion, our study has shown that treatment with the Ginkgo biloba extract GB-8 improves some cognitive functions in elderly patients with moderate arterial insufficiency, whereas the extract did not change signs and symptoms of vascular disease in the patients.},
	author = {Lu, Rui and Duan, Zhiyao and Zhang, Changshui},
	year = {2017},
	keywords = {Data augmentation, deep neural networks, environmental sound classification, metric learning},
}

@techreport{salamon_fusing_nodate,
	title = {{FUSING} {SHALLOW} {AND} {DEEP} {LEARNING} {FOR} {BIOACOUSTIC} {BIRD} {SPECIES} {CLASSIFICATION}},
	abstract = {Automated classification of organisms to species based on their vo-calizations would contribute tremendously to abilities to monitor biodiversity, with a wide range of applications in the field of ecology. In particular, automated classification of migrating birds' flight calls could yield new biological insights and conservation applications for birds that vocalize during migration. In this paper we explore state-of-the-art classification techniques for large-vocabulary bird species classification from flight calls. In particular, we contrast a "shallow learning" approach based on unsupervised dictionary learning with a deep convolutional neural network combined with data augmentation. We show that the two models perform comparably on a dataset of 5428 flight calls spanning 43 different species, with both significantly outperforming an MFCC baseline. Finally, we show that by combining the models using a simple late-fusion approach we can further improve the results, obtaining a state-of-the-art classification accuracy of 0.96.},
	author = {Salamon, Justin and Bello, Juan Pablo and Farnsworth, Andrew and Kelling, Steve},
	keywords = {bioacoustics, data augmentation, deep learning, flight calls, Index Terms-Convolutional neural networks},
}

@article{fonseca_simple_2018,
	title = {A {Simple} {Fusion} of {Deep} and {Shallow} {Learning} for {Acoustic} {Scene} {Classification}},
	abstract = {In the past, Acoustic Scene Classification systems have been based on hand crafting audio features that are input to a classifier. Nowadays, the common trend is to adopt data driven techniques, e.g., deep learning, where audio representations are learned from data. In this paper, we propose a system that consists of a simple fusion of two methods of the aforementioned types: a deep learning approach where log-scaled mel-spectrograms are input to a convolutional neural network, and a feature engineering approach, where a collection of hand-crafted features is input to a gradient boosting machine. We first show that both methods provide complementary information to some extent. Then, we use a simple late fusion strategy to combine both methods. We report classification accuracy of each method individually and the combined system on the TUT Acoustic Scenes 2017 dataset. The proposed fused system outperforms each of the individual methods and attains a classification accuracy of 72.8\% on the evaluation set, improving the baseline system by 11.8\%.},
	author = {Fonseca, Eduardo and Gong, Rong and Serra, Xavier},
	year = {2018},
}

@techreport{liu_sparsenet_2018,
	title = {{SparseNet}: {A} {Sparse} {DenseNet} for {Image} {Classification}},
	url = {https://arxiv.org/pdf/1804.05340.pdf},
	abstract = {Deep neural networks have made remarkable progresses on various computer vision tasks. Recent works have shown that depth, width and shortcut connections of networks are all vital to their performances. In this paper, we introduce a method to sparsify DenseNet which can reduce connections of a L-layer DenseNet from O(L 2) to O(L), and thus we can simultaneously increase depth, width and connections of neu-ral networks in a more parameter-efficient and computation-efficient way. Moreover, an attention module is introduced to further boost our net-work's performance. We denote our network as SparseN et. We evaluate SparseN et on datasets of CIFAR(including CIFAR10 and CIFAR100) and SVHN. Experiments show that SparseN et can obtain improvements over the state-of-the-art on CIFAR10 and SVHN. Furthermore, while achieving comparable performances as DenseNet on these datasets, SparseN et is ×2.6 smaller and ×3.7 faster than the original DenseNet.},
	author = {Liu, Wenqi and Zeng, Kun},
	year = {2018},
	keywords = {Dense, Net, neural networks, Sparse},
}

@techreport{xing_distance_nodate,
	title = {Distance {Metric} {Learning}, with {Application} to {Clustering} with {Side}-{Information}},
	url = {https://papers.nips.cc/paper/2164-distance-metric-learning-with-application-to-clustering-with-side-information.pdf},
	abstract = {Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many "plausible" ways, and if a clustering algorithm such as K-means initially fails to find one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufficiently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider "similar." For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in ¢ ¤ £ , learns a distance metric over ¢ ¥ £ that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efficient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to significantly improve clustering performance.},
	author = {Xing, Eric P and Ng, Andrew Y and Jordan, Michael I and Russell, Stuart},
}

@article{dravid_employing_nodate,
	title = {Employing {Deep} {Networks} for {Image} {Processing} on {Small} {Research} {Datasets}},
	url = {https://doi.org/10.1017/S1551929518001311},
	doi = {10.1017/S1551929518001311},
	abstract = {Deep neural networks have attracted considerable attention because of their state-of-the-art performance on a variety of image restoration tasks, including image completion, denoising, and segmen-tation. However, their record of performance is built upon extremely large datasets. In many cases (for example, electron microscopy), it is extremely labor intensive, if not impossible, to acquire tens of thousands of images for a single project. The present work shows the possibility of attaining high-accuracy image segmentation, isolating regions of interest, for small datasets of transmission electron micrographs by employing encoder-decoder neural networks and image augmentation.},
	author = {Dravid, Amil},
	keywords = {data augmentation, neural networks, accuracy, segmentation, Transmission electron microscopy (TEM)},
}

@article{falk_u-net_2018,
	title = {U-{Net}: deep learning for cell counting, detection, and morphometry},
	issn = {3328248515313},
	url = {http://www.nature.com/articles/s41592-018-0261-2},
	doi = {10.1038/s41592-018-0261-2},
	journal = {Nature Methods},
	author = {Falk, Thorsten and Mai, Dominic and Bensch, Robert and Çiçek, Özgün and Abdulkadir, Ahmed and Marrakchi, Yassine and Böhm, Anton and Deubner, Jan and Jäckel, Zoe and Seiwald, Katharina and Dovzhenko, Alexander and Tietz, Olaf and Dal Bosco, Cristina and Walsh, Sean and Saltukoglu, Deniz and Tay, Tuan Leng and Prinz, Marco and Palme, Klaus and Simons, Matias and Diester, Ilka and Brox, Thomas and Ronneberger, Olaf},
	year = {2018},
}

@techreport{liu_auto-deeplab_nodate,
	title = {Auto-{DeepLab}: {Hierarchical} {Neural} {Architecture} {Search} for {Semantic} {Image} {Segmentation}},
	url = {https://arxiv.org/pdf/1901.02985.pdf},
	abstract = {Recently, Neural Architecture Search (NAS) has successfully identified neural network architectures that exceed human designed ones on large-scale image classification problems. In this paper, we study NAS for semantic image segmentation, an important computer vision task that assigns a semantic label to every pixel in an image. Existing works often focus on searching the repeatable cell structure , while hand-designing the outer network structure that controls the spatial resolution changes. This choice simplifies the search space, but becomes increasingly problematic for dense image prediction which exhibits a lot more network level architectural variations. Therefore, we propose to search the network level structure in addition to the cell level structure, which forms a hierarchical architecture search space. We present a network level search space that includes many popular designs, and develop a formulation that allows efficient gradient-based architecture search (3 P100 GPU days on Cityscapes images). We demonstrate the effectiveness of the proposed method on the challenging Cityscapes, PASCAL VOC 2012, and ADE20K datasets. Without any ImageNet pretraining, our architecture searched specifically for semantic image segmentation attains state-of-the-art performance.},
	author = {Liu, Chenxi and Chen, Liang-Chieh and Schroff, Florian and Adam, Hartwig and Hua, Wei and Yuille, Alan and Fei-Fei, Li},
}

@article{chen_encoder-decoder_2018,
	title = {Encoder-{Decoder} with {Atrous} {Separable} {Convolution} for {Semantic} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1802.02611},
	abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0{\textbackslash}\% and 82.1{\textbackslash}\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at {\textbackslash}url\{https://github.com/tensorflow/models/tree/master/research/deeplab\}.},
	author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	month = feb,
	year = {2018},
}

@article{noauthor_full-text_nodate,
	title = {full-text},
}

@article{drawitsch_fluoem_2018,
	title = {{FluoEM}, virtual labeling of axons in three-dimensional electron microscopy data for long-range connectomics},
	volume = {7},
	url = {https://elifesciences.org/articles/38976},
	doi = {10.7554/eLife.38976},
	abstract = {{\textless}p{\textgreater}The labeling and identification of long-range axonal inputs from multiple sources within densely reconstructed electron microscopy (EM) datasets from mammalian brains has been notoriously difficult because of the limited color label space of EM. Here, we report FluoEM for the identification of multi-color fluorescently labeled axons in dense EM data without the need for artificial fiducial marks or chemical label conversion. The approach is based on correlated tissue imaging and computational matching of neurite reconstructions, amounting to a virtual color labeling of axons in dense EM circuit data. We show that the identification of fluorescent light- microscopically (LM) imaged axons in 3D EM data from mouse cortex is faithfully possible as soon as the EM dataset is about 40–50 µm in extent, relying on the unique trajectories of axons in dense mammalian neuropil. The method is exemplified for the identification of long-distance axonal input into layer 1 of the mouse cerebral cortex.{\textless}/p{\textgreater}},
	journal = {eLife},
	author = {Drawitsch, Florian and Karimi, Ali and Boergens, Kevin M and Helmstaedter, Moritz},
	month = aug,
	year = {2018},
}

@article{chan_everybody_2018,
	title = {Everybody {Dance} {Now}},
	url = {http://arxiv.org/abs/1808.07371},
	abstract = {This paper presents a simple method for "do as I do" motion transfer: given a source video of a person dancing we can transfer that performance to a novel (amateur) target after only a few minutes of the target subject performing standard moves. We pose this problem as a per-frame image-to-image translation with spatio-temporal smoothing. Using pose detections as an intermediate representation between source and target, we learn a mapping from pose images to a target subject's appearance. We adapt this setup for temporally coherent video generation including realistic face synthesis. Our video demo can be found at https://youtu.be/PCBTZh41Ris .},
	author = {Chan, Caroline and Ginosar, Shiry and Zhou, Tinghui and Efros, Alexei A.},
	month = aug,
	year = {2018},
}

@article{oktay_attention_2018,
	title = {Attention {U}-{Net}: {Learning} {Where} to {Look} for the {Pancreas}},
	issn = {1804.03999v3},
	url = {http://arxiv.org/abs/1804.03999},
	doi = {10.1016/S0957-4166(98)00134-7},
	abstract = {We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.},
	number = {Midl},
	author = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
	year = {2018},
}

@article{howard_universal_2018,
	title = {Universal {Language} {Model} {Fine}-tuning for {Text} {Classification}},
	url = {http://arxiv.org/abs/1801.06146},
	abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
	author = {Howard, Jeremy and Ruder, Sebastian},
	month = jan,
	year = {2018},
}

@article{knight_attention-gated_2018,
	title = {Attention-{Gated} {Networks} 2018},
	number = {Midl},
	journal = {Journal of the Medical Association of Georgia},
	author = {Knight, J. A.},
	year = {2018},
	pages = {1--12},
}

@article{wang_deep_2018,
	title = {Deep {Face} {Recognition}: {A} {Survey}},
	url = {http://arxiv.org/abs/1804.06655},
	abstract = {Deep learning applies multiple processing layers to learn representations of data with multiple levels of feature extraction. This emerging technique has reshaped the research landscape of face recognition since 2014, launched by the breakthroughs of Deepface and DeepID methods. Since then, deep face recognition (FR) technique, which leverages the hierarchical architecture to learn discriminative face representation, has dramatically improved the state-of-the-art performance and fostered numerous successful real-world applications. In this paper, we provide a comprehensive survey of the recent developments on deep FR, covering the broad topics on algorithms, data, and scenes. First, we summarize different network architectures and loss functions proposed in the rapid evolution of the deep FR methods. Second, the related face processing methods are categorized into two classes: `one-to-many augmentation' and `many-to-one normalization'. Then, we summarize and compare the commonly used databases for both model training and evaluation. Third, we review miscellaneous scenes in deep FR, such as cross-factor, heterogenous, multiple-media and industry scenes. Finally, potential deficiencies of the current methods and several future directions are highlighted.},
	author = {Wang, Mei and Deng, Weihong},
	month = apr,
	year = {2018},
}

@techreport{li_improving_nodate,
	title = {Improving {Pairwise} {Ranking} for {Multi}-label {Image} {Classification}},
	url = {https://arxiv.org/pdf/1704.03135.pdf},
	abstract = {Learning to rank has recently emerged as an attractive technique to train deep convolutional neural networks for various computer vision tasks. Pairwise ranking, in particular , has been successful in multi-label image classification, achieving state-of-the-art results on various benchmarks. However, most existing approaches use the hinge loss to train their models, which is non-smooth and thus is difficult to optimize especially with deep networks. Furthermore, they employ simple heuristics, such as top-k or thresholding, to determine which labels to include in the output from a ranked list of labels, which limits their use in the real-world setting. In this work, we propose two techniques to improve pairwise ranking based multi-label image classification: (1) we propose a novel loss function for pairwise ranking, which is smooth everywhere and thus is easier to optimize; and (2) we incorporate a label decision module into the model, estimating the optimal confidence thresholds for each visual concept. We provide theoretical analyses of our loss function in the Bayes consistency and risk minimization framework, and show its benefit over existing pairwise ranking formulations. We demonstrate the effectiveness of our approach on three large-scale datasets, VOC2007, NUS-WIDE and MS-COCO, achieving the best reported results in the literature.},
	author = {Li, Yuncheng and Song, Yale and Luo, Jiebo},
}

@article{zoph_learning_2017,
	title = {Learning {Transferable} {Architectures} for {Scalable} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1707.07012},
	abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4\% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on ImageNet. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0\% achieving 43.1\% mAP on the COCO dataset.},
	author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
	month = jul,
	year = {2017},
}

@techreport{hu_squeeze-and-excitation_2017,
	title = {Squeeze-and-{Excitation} {Networks}},
	url = {http://image-net.org/challenges/LSVRC/2017/results},
	abstract = {The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at minimal additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251\%, surpassing the winning entry of 2016 by a relative improvement of ∼25\%. Models and code are available at https://github.com/hujie-frank/SENet.},
	author = {Hu, Jie},
	year = {2017},
	keywords = {Convolutional Neural Network !, Image classification, Index Terms-Squeeze-and-Excitation},
}

@article{qiu_global_2018,
	title = {Global {Weighted} {Average} {Pooling} {Bridges} {Pixel}-level {Localization} and {Image}-level {Classification}},
	url = {http://arxiv.org/abs/1809.08264},
	abstract = {In this work, we first tackle the problem of simultaneous pixel-level localization and image-level classification with only image-level labels for fully convolutional network training. We investigate the global pooling method which plays a vital role in this task. Classical global max pooling and average pooling methods are hard to indicate the precise regions of objects. Therefore, we revisit the global weighted average pooling (GWAP) method for this task and propose the class-agnostic GWAP module and the class-specific GWAP module in this paper. We evaluate the classification and pixel-level localization ability on the ILSVRC benchmark dataset. Experimental results show that the proposed GWAP module can better capture the regions of the foreground objects. We further explore the knowledge transfer between the image classification task and the region-based object detection task. We propose a multi-task framework that combines our class-specific GWAP module with R-FCN. The framework is trained with few ground truth bounding boxes and large-scale image-level labels. We evaluate this framework on PASCAL VOC dataset. Experimental results show that this framework can use the data with only image-level labels to improve the generalization of the object detection model.},
	author = {Qiu, Suo},
	month = sep,
	year = {2018},
}

@article{woo_cbam_2018,
	title = {{CBAM}: {Convolutional} {Block} {Attention} {Module}},
	url = {http://arxiv.org/abs/1807.06521},
	abstract = {We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS{\textasciitilde}COCO detection, and VOC{\textasciitilde}2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.},
	author = {Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
	month = jul,
	year = {2018},
}

@article{wang_deep_2018-1,
	title = {Deep {Face} {Recognition}: {A} {Survey}},
	url = {http://arxiv.org/abs/1804.06655},
	abstract = {Deep learning applies multiple processing layers to learn representations of data with multiple levels of feature extraction. This emerging technique has reshaped the research landscape of face recognition since 2014, launched by the breakthroughs of Deepface and DeepID methods. Since then, deep face recognition (FR) technique, which leverages the hierarchical architecture to learn discriminative face representation, has dramatically improved the state-of-the-art performance and fostered numerous successful real-world applications. In this paper, we provide a comprehensive survey of the recent developments on deep FR, covering the broad topics on algorithms, data, and scenes. First, we summarize different network architectures and loss functions proposed in the rapid evolution of the deep FR methods. Second, the related face processing methods are categorized into two classes: `one-to-many augmentation' and `many-to-one normalization'. Then, we summarize and compare the commonly used databases for both model training and evaluation. Third, we review miscellaneous scenes in deep FR, such as cross-factor, heterogenous, multiple-media and industry scenes. Finally, potential deficiencies of the current methods and several future directions are highlighted.},
	author = {Wang, Mei and Deng, Weihong},
	month = apr,
	year = {2018},
}

@article{buslaev_albumentations_2018,
	title = {Albumentations: fast and flexible image augmentations},
	url = {http://arxiv.org/abs/1809.06839},
	abstract = {Data augmentation is a commonly used technique for increasing both the size and the diversity of labeled training sets by leveraging input transformations that preserve output labels. In computer vision domain, image augmentations have become a common implicit regularization technique to combat overfitting in deep convolutional neural networks and are ubiquitously used to improve performance. While most deep learning frameworks implement basic image transformations, the list is typically limited to some variations and combinations of flipping, rotating, scaling, and cropping. Moreover, the image processing speed varies in existing tools for image augmentation. We present Albumentations, a fast and flexible library for image augmentations with many various image transform operations available, that is also an easy-to-use wrapper around other augmentation libraries. We provide examples of image augmentations for different computer vision tasks and show that Albumentations is faster than other commonly used image augmentation tools on the most of commonly used image transformations. The source code for Albumentations is made publicly available online at https://github.com/albu/albumentations},
	author = {Buslaev, Alexander and Parinov, Alex and Khvedchenya, Eugene and Iglovikov, Vladimir I. and Kalinin, Alexandr A.},
	month = sep,
	year = {2018},
}

@article{yu_deep_2017,
	title = {Deep {Layer} {Aggregation}},
	url = {http://arxiv.org/abs/1707.06484},
	abstract = {Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been "shallow" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes. The code is at https://github.com/ucbdrive/dla.},
	author = {Yu, Fisher and Wang, Dequan and Shelhamer, Evan and Darrell, Trevor},
	month = jul,
	year = {2017},
}

@article{wu_group_2018,
	title = {Group {Normalization}},
	url = {http://arxiv.org/abs/1803.08494},
	abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
	author = {Wu, Yuxin and He, Kaiming},
	month = mar,
	year = {2018},
}

@article{szegedy_rethinking_2015,
	title = {Rethinking the {Inception} {Architecture} for {Computer} {Vision}},
	url = {http://arxiv.org/abs/1512.00567},
	abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
	month = dec,
	year = {2015},
}

@article{radosavovic_data_2017,
	title = {Data {Distillation}: {Towards} {Omni}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/1712.04440},
	abstract = {We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging real-world data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone.},
	author = {Radosavovic, Ilija and Dollár, Piotr and Girshick, Ross and Gkioxari, Georgia and He, Kaiming},
	month = dec,
	year = {2017},
}

@article{li_coco-cn_2018,
	title = {{COCO}-{CN} for {Cross}-{Lingual} {Image} {Tagging}, {Captioning} and {Retrieval}},
	url = {http://arxiv.org/abs/1805.08661},
	abstract = {This paper contributes to cross-lingual image annotation and retrieval in terms of data and baseline methods. We propose COCO-CN, a novel dataset enriching MS-COCO with manually written Chinese sentences and tags. For more effective annotation acquisition, we develop a recommendation-assisted collective annotation system, automatically providing an annotator with several tags and sentences deemed to be relevant with respect to the pictorial content. Having 20,342 images annotated with 27,218 Chinese sentences and 70,993 tags, COCO-CN is currently the largest Chinese-English dataset that provides a unified and challenging platform for cross-lingual image tagging, captioning and retrieval. We develop conceptually simple yet effective methods per task for learning from cross-lingual resources. Extensive experiments on the three tasks justify the viability of the proposed dataset and methods. Data and code are publicly available at https://github.com/li-xirong/coco-cn},
	author = {Li, Xirong and Xu, Chaoxi and Wang, Xiaoxu and Lan, Weiyu and Jia, Zhengxiong and Yang, Gang and Xu, Jieping},
	month = may,
	year = {2018},
}

@book{noauthor_pattern_nodate,
	title = {Pattern {Recognition}: 25th {DAGM} {Symposium}, {Magdeburg}, {Germany}, {September} 10 ... - {Google} {Books}},
	url = {https://books.google.de/books?id=gaaoCAAAQBAJ&pg=PA382&lpg=PA382&dq=from+a+set+of+images+create+one+average&source=bl&ots=zkhWYEwVYr&sig=ACfU3U2YIxdclCroMgja2v6ASu4JhJUgfg&hl=en&sa=X&ved=2ahUKEwj-5vqN_PbfAhVBz4UKHYwcAv4Q6AEwCXoECAgQAQ#v=onepage&q=from%20a},
}

@article{ghiasi_dropblock_2018,
	title = {{DropBlock}: {A} regularization method for convolutional networks},
	url = {http://arxiv.org/abs/1810.12890},
	abstract = {Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. Extensive experiments show that DropBlock works better than dropout in regularizing convolutional networks. On ImageNet classification, ResNet-50 architecture with DropBlock achieves \$78.13{\textbackslash}\%\$ accuracy, which is more than \$1.6{\textbackslash}\%\$ improvement on the baseline. On COCO detection, DropBlock improves Average Precision of RetinaNet from \$36.8{\textbackslash}\%\$ to \$38.4{\textbackslash}\%\$.},
	author = {Ghiasi, Golnaz and Lin, Tsung-Yi and Le, Quoc V.},
	month = oct,
	year = {2018},
}

@article{lin_focal_2017,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {https://arxiv.org/abs/1708.02002},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = aug,
	year = {2017},
}

@article{bortsova_deep_2018,
	title = {Deep {Learning} from {Label} {Proportions} for {Emphysema} {Quantification}},
	url = {http://arxiv.org/abs/1807.08601},
	abstract = {We propose an end-to-end deep learning method that learns to estimate emphysema extent from proportions of the diseased tissue. These proportions were visually estimated by experts using a standard grading system, in which grades correspond to intervals (label example: 1-5\% of diseased tissue). The proposed architecture encodes the knowledge that the labels represent a volumetric proportion. A custom loss is designed to learn with intervals. Thus, during training, our network learns to segment the diseased tissue such that its proportions fit the ground truth intervals. Our architecture and loss combined improve the performance substantially (8\% ICC) compared to a more conventional regression network. We outperform traditional lung densitometry and two recently published methods for emphysema quantification by a large margin (at least 7\% AUC and 15\% ICC), and achieve near-human-level performance. Moreover, our method generates emphysema segmentations that predict the spatial distribution of emphysema at human level.},
	author = {Bortsova, Gerda and Dubost, Florian and Ørting, Silas and Katramados, Ioannis and Hogeweg, Laurens and Thomsen, Laura and Wille, Mathilde and de Bruijne, Marleen},
	month = jul,
	year = {2018},
}

@article{pan_two_2018,
	title = {Two at {Once}: {Enhancing} {Learning} and {Generalization} {Capacities} via {IBN}-{Net}},
	url = {http://arxiv.org/abs/1807.09441},
	abstract = {Convolutional neural networks (CNNs) have achieved great successes in many computer vision problems. Unlike existing works that designed CNN architectures to improve performance on a single task of a single domain and not generalizable, we present IBN-Net, a novel convolutional architecture, which remarkably enhances a CNN's modeling ability on one domain (e.g. Cityscapes) as well as its generalization capacity on another domain (e.g. GTA5) without finetuning. IBN-Net carefully integrates Instance Normalization (IN) and Batch Normalization (BN) as building blocks, and can be wrapped into many advanced deep networks to improve their performances. This work has three key contributions. (1) By delving into IN and BN, we disclose that IN learns features that are invariant to appearance changes, such as colors, styles, and virtuality/reality, while BN is essential for preserving content related information. (2) IBN-Net can be applied to many advanced deep architectures, such as DenseNet, ResNet, ResNeXt, and SENet, and consistently improve their performance without increasing computational cost. (3) When applying the trained networks to new domains, e.g. from GTA5 to Cityscapes, IBN-Net achieves comparable improvements as domain adaptation methods, even without using data from the target domain. With IBN-Net, we won the 1st place on the WAD 2018 Challenge Drivable Area track, with an mIoU of 86.18\%.},
	author = {Pan, Xingang and Luo, Ping and Shi, Jianping and Tang, Xiaoou},
	month = jul,
	year = {2018},
}

@article{jalled_face_2017,
	title = {Face {Recognition} {Machine} {Vision} {System} {Using} {Eigenfaces}},
	url = {http://arxiv.org/abs/1705.02782},
	abstract = {Face Recognition is a common problem in Machine Learning. This technology has already been widely used in our lives. For example, Facebook can automatically tag people's faces in images, and also some mobile devices use face recognition to protect private security. Face images comes with different background, variant illumination, different facial expression and occlusion. There are a large number of approaches for the face recognition. Different approaches for face recognition have been experimented with specific databases which consist of single type, format and composition of image. Doing so, these approaches don't suit with different face databases. One of the basic face recognition techniques is eigenface which is quite simple, efficient, and yields generally good results in controlled circumstances. So, this paper presents an experimental performance comparison of face recognition using Principal Component Analysis (PCA) and Normalized Principal Component Analysis (NPCA). The experiments are carried out on the ORL (ATT) and Indian face database (IFD) which contain variability in expression, pose, and facial details. The results obtained for the two methods have been compared by varying the number of training images. MATLAB is used for implementing algorithms also.},
	author = {Jalled, Fares},
	month = may,
	year = {2017},
}

@article{wu_tagging_2018,
	title = {Tagging like {Humans}: {Diverse} and {Distinct} {Image} {Annotation}},
	url = {http://arxiv.org/abs/1804.00113},
	abstract = {In this work we propose a new automatic image annotation model, dubbed \{{\textbackslash}bf diverse and distinct image annotation\} (D2IA). The generative model D2IA is inspired by the ensemble of human annotations, which create semantically relevant, yet distinct and diverse tags. In D2IA, we generate a relevant and distinct tag subset, in which the tags are relevant to the image contents and semantically distinct to each other, using sequential sampling from a determinantal point process (DPP) model. Multiple such tag subsets that cover diverse semantic aspects or diverse semantic levels of the image contents are generated by randomly perturbing the DPP sampling process. We leverage a generative adversarial network (GAN) model to train D2IA. Extensive experiments including quantitative and qualitative comparisons, as well as human subject studies, on two benchmark datasets demonstrate that the proposed model can produce more diverse and distinct tags than the state-of-the-arts.},
	author = {Wu, Baoyuan and Chen, Weidong and Sun, Peng and Liu, Wei and Ghanem, Bernard and Lyu, Siwei},
	month = mar,
	year = {2018},
}

@article{noauthor_full-text_nodate-1,
	title = {full-text},
}

@article{xing_deep_2018,
	title = {Deep {Learning} in {Microscopy} {Image} {Analysis}: {A} {Survey}},
	volume = {29},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/29989994},
	doi = {10.1109/TNNLS.2017.2766168},
	abstract = {Computerized microscopy image analysis plays an important role in computer aided diagnosis and prognosis. Machine learning techniques have powered many aspects of medical investigation and clinical practice. Recently, deep learning is emerging as a leading machine learning tool in computer vision and has attracted considerable attention in biomedical image analysis. In this paper, we provide a snapshot of this fast-growing field, specifically for microscopy image analysis. We briefly introduce the popular deep neural networks and summarize current deep learning achievements in various tasks, such as detection, segmentation, and classification in microscopy image analysis. In particular, we explain the architectures and the principles of convolutional neural networks, fully convolutional networks, recurrent neural networks, stacked autoencoders, and deep belief networks, and interpret their formulations or modelings for specific tasks on various microscopy images. In addition, we discuss the open challenges and the potential trends of future research in microscopy image analysis using deep learning.},
	number = {10},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Xing, Fuyong and Xie, Yuanpu and Su, Hai and Liu, Fujun and Yang, Lin},
	month = oct,
	year = {2018},
	pages = {4550--4568},
}

@article{chen_neural_2018,
	title = {Neural {Ordinary} {Differential} {Equations}},
	url = {https://arxiv.org/abs/1806.07366},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	month = jun,
	year = {2018},
}

@techreport{geirhos_imagenet-trained_2018,
	title = {{IMAGENET}-{TRAINED} {CNNS} {ARE} {BIASED} {TOWARDS} {TEXTURE}; {INCREASING} {SHAPE} {BIAS} {IMPROVES} {ACCURACY} {AND} {ROBUSTNESS}},
	url = {https://github.com/rgeirhos/texture-vs-shape},
	abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies hint to a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation. (a) Texture image 81.4\% Indian elephant 10.3\% indri 8.2\% black swan (b) Content image 71.1\% tabby cat 17.3\% grey fox 3.3\% Siamese cat (c) Texture-shape cue conflict 63.9\% Indian elephant 26.4\% indri 9.6\% black swan Figure 1: Classification of a standard ResNet-50 of (a) a texture image (elephant skin: only texture cues); (b) a normal image of a cat (with both shape and texture cues), and (c) an image with a texture-shape cue conflict, generated by style transfer between the first two images.},
	author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A and Brendel, Wieland},
	year = {2018},
}

@techreport{dworak_middleware_nodate,
	title = {{MIDDLEWARE} {TRENDS} {AND} {MARKET} {LEADERS} 2011},
	url = {http://zeromq.wdfiles.com/local--files/intro%3Aread-the-manual/Middleware%20Trends%20and%20Market%20Leaders%202011.pdf},
	abstract = {The Controls Middleware (CMW) project was launched over ten years ago. Its main goal was to unify middleware solutions used to operate CERN accelerators. An important part of the project, the equipment access library RDA, was based on CORBA, an unquestionable standard at the time. RDA became an operational and critical part of the infrastructure, yet the demanding run-time environment revealed some shortcomings of the system. Accumulation of fixes and workarounds led to unnecessary complexity. RDA became difficult to maintain and to extend. CORBA proved to be rather a cumbersome product than a panacea. Fortunately, many new transport frameworks appeared since then. They boasted a better design and supported concepts that made them easy to use. Willing to profit from the new libraries, the CMW team updated user requirements and in their terms investigated eventual CORBA substitutes. The process consisted of several phases: a review of middleware solutions belonging to different categories (e.g. data-centric, object-, and message-oriented) and their applicability to a communication model in RDA; evaluation of several market recognized products and promising start-ups; prototyping of typical communication scenarios; testing the libraries against exceptional situations and errors; verifying that mandatory performance constraints were met. Thanks to the performed investigation the team have selected a few libraries that suit their needs better than CORBA. Further prototyping will select the best candidate. CERN MIDDLEWARE The Controls Middleware (CMW) project was launched at CERN over ten years ago. Its main goal was to unify middleware solutions used to operate CERN accelerators. Many software components were developed, among them the Remote Device Access (RDA) [1] library. The main responsibility of the library was to allow communication with servers that operate hardware sensors and actuators. The RDA design corresponds to the Accelerator Device Model [1] in which devices, named entities in the control system, can be controlled via properties. RDA implements this model in a distributed environment with devices residing in front-end servers that can run anywhere in the controls network. It provides a location-independent and reliable access to devices from control programs. By invoking the device access methods, clients can read, write, and subscribe to device property values. Currently over 4000 servers (processes) are deployed, which contain altogether almost 80,000 devices. In total the system gives access to more than 2,000,000 properties/IO points, on which clients may perform read/write operations or monitor their values. [2] Present Implementation From the beginning there were certain requirements [3] imposed on RDA that drove its implementation: relying only on standards; interoperability with the already existing communication infrastructure at CERN; portable on LynxOS, Linux, Windows, HP-UX and AIX (only the first three are still supported; LynxOS is being eradicated); C/C++ and Java bindings for client/server libraries; request-reply (read/write) and publish-subscribe operations on device data. Each call type should provide timeout settings and handling of communication errors. Moreover, complementary, centrally managed services like naming service, reservation service and access control should be supplied. There were no precisely defined constraints on communication latency or throughput. To facilitate development of the new library it was decided to base it on an already existing, mature product. CORBA [4] was a very popular middleware at that time and fulfilled all the requirements. Thus it was chosen as the communication layer. The C++ implementation was based on omniORB (currently 4.1.2,) and the Java implementation on JacORB (currently 2.2.4.) RDA library wrapped CORBA, hiding all its complexities and providing a simple to use API. The proposed solution was widely accepted and became an operational and critical part of the infrastructure.},
	author = {Dworak, A and Ehm, F and Sliwinski, W and Sobczak, M and Switzerland, Geneva},
}

@article{ha_world_2018,
	title = {World {Models}},
	issn = {978-92-64-23889-3},
	doi = {10.5281/zenodo.1207631},
	abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
	author = {Ha, David and Schmidhuber, Jürgen},
	year = {2018},
}

@article{durand_machine_2018,
	title = {A machine learning approach for online automated optimization of super-resolution optical microscopy},
	volume = {9},
	url = {http://www.nature.com/articles/s41467-018-07668-y},
	doi = {10.1038/s41467-018-07668-y},
	number = {1},
	journal = {Nature Communications},
	author = {Durand, Audrey and Wiesner, Theresa and Gardner, Marc-André and Robitaille, Louis-Émile and Bilodeau, Anthony and Gagné, Christian and De Koninck, Paul and Lavoie-Cardinal, Flavie},
	year = {2018},
	pages = {5247--5247},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	issn = {9781450358095},
	doi = {10.1038/nature14539},
	abstract = {Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.},
	journal = {Nature},
	author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	year = {2015},
}

@article{sadanandan_automated_2017,
	title = {Automated {Training} of {Deep} {Convolutional} {Neural} {Networks} for {Cell} {Segmentation}},
	volume = {7},
	url = {http://www.nature.com/articles/s41598-017-07599-6},
	doi = {10.1038/s41598-017-07599-6},
	abstract = {Deep Convolutional Neural Networks (DCNN) have recently emerged as superior for many image segmentation tasks. The DCNN performance is however heavily dependent on the availability of large amounts of problem-specific training samples. Here we show that DCNNs trained on ground truth created automatically using fluorescently labeled cells, perform similar to manual annotations.},
	number = {1},
	journal = {Scientific Reports},
	author = {Sadanandan, Sajith Kecheril and Ranefall, Petter and Le Guyader, Sylvie and Wählby, Carolina},
	month = dec,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {Image processing, Cellular imaging},
	pages = {7860--7860},
}

@article{ouyang_deep_2018,
	title = {Deep learning massively accelerates super-resolution localization microscopy},
	url = {http://www.microscopist.co.uk/wp-content/uploads/2018/04/nbt.4106.pdf},
	doi = {10.1038/nbt.4106},
	abstract = {A r t i c l e s Fluorescence microscopy methods that overcome the diffraction limit of resolution ({\textasciitilde}200-300 nm) allow imaging of biological structures with molecular specificity closer to the molecular scale. Among super-resolution microscopy approaches, those based on single-molecule localization, such as PALM 1 and STORM 2 (hereafter referred to collectively as PALM), are particularly attractive owing to their exquisite spatial resolution and ease of implementation. In these methods, random subsets of fluorophores are imaged in many consecutive diffraction-limited frames, computationally localized to high precision, and the combined localizations are used to generate a super-resolution view. In practice, typically 10 3-10 5 diffraction-limited frames are needed to assemble a single super-resolution image. This requirement follows from two conditions that must be simultaneously satisfied to ensure high spatial resolution: (i) a small number ({\textasciitilde}10-10 2) of active fluorophores per frame, to avoid overlaps between diffraction-limited spots and enable precise localization of individual molecules, and (ii) a large number of independent localizations to ensure a sufficiently dense sampling of the underlying biological structures 3,4. The large number of required frames makes localization microscopy inherently slow, thereby limiting its potential for high-throughput imaging, where many fields of view (FoVs) are to be imaged, and for imaging live-cell dynamics. As a result, most localization microscopy studies are restricted to analyzing a small number of cells (typically less than ten). Multiple approaches have been explored to accelerate localization microscopy. Using bright dyes with rapid switching kinetics, high-power lasers, and fast cameras allows minimization of exposure time without decreasing the signal-to-noise ratio 5,6 , but reaching submil-lisecond exposure remains challenging, and intense irradiation exacerbates phototoxicity in live-cell imaging 7,8. Increasing the number of active fluorophores per frame can reduce acquisition time, but despite algorithms designed to handle overlapping fluorescent spots 9-13 , this approach necessarily degrades spatial resolution 4,14. Here, we introduce a computational strategy that allows the total number of frames and independent localizations to be reduced without trading off spatial resolution. Unlike previous approaches, our method exploits the structural redundancy of most biological images to reconstruct high-quality images from vastly undersampled localization microscopy data. Our method leverages deep learning, which employs artificial neural networks (ANNs) to learn complex non-linear map-pings between numerical inputs and outputs 15. Accordingly, we call it 'artificial neural network accelerated PALM' , or ANNA-PALM. RESULTS A deep-learning approach to super-resolution image reconstruction We aim to reconstruct a super-resolution image of approximately similar information content as a standard PALM acquisition (with K frames and N localizations) from a much smaller number of raw frames (k {\textless}{\textless} K) without changing the average density of localizations, ρ, that is, from a much smaller number of total localizations (n = ρk {\textless}{\textless}N = ρK). If PALM images are defined as two-dimensional (2D) histograms of independent localizations, this task can be formulated as restoring an image corrupted by Poisson noise (and potentially additional forms of noise). Image restoration is an ill-posed problem that admits an infinity of solutions in the high-dimensional space of all possible images, unless additional constraints (priors) are imposed that restrict the solution to a lower dimensional manifold. Suitable manifolds exist because most natural images are highly redundant, and can be represented to very good approximation with a much smaller number of coefficients than pixels, via appropriate functions that map feature space to pixel The speed of super-resolution microscopy methods based on single-molecule localization, for example, PALM and STORM, is limited by the need to record many thousands of frames with a small number of observed molecules in each. Here, we present ANNA-PALM, a computational strategy that uses artificial neural networks to reconstruct super-resolution views from sparse, rapidly acquired localization images and/or widefield images. Simulations and experimental imaging of microtubules, nuclear pores, and mitochondria show that high-quality, super-resolution images can be reconstructed from up to two orders of magnitude fewer frames than usually needed, without compromising spatial resolution. Super-resolution reconstructions are even possible from widefield images alone, though adding localization data improves image quality. We demonstrate super-resolution imaging of {\textgreater}1,000 fields of view containing {\textgreater}1,000 cells in {\textasciitilde}3 h, yielding an image spanning spatial scales from {\textasciitilde}20 nm to {\textasciitilde}2 mm. The drastic reduction in acquisition time and sample irradiation afforded by ANNA-PALM enables faster and gentler high-throughput and live-cell super-resolution imaging.},
	author = {Ouyang, Wei and Aristov, Andrey and Lelek, Mickaël and Hao, Xian and Zimmer, Christophe},
	year = {2018},
}

@article{horl_bigstitcher_nodate,
	title = {{BigStitcher}: {Reconstructing} high-resolution image datasets of cleared and expanded samples},
	url = {http://dx.doi.org/10.1101/343954},
	doi = {10.1101/343954},
	author = {Hörl, David and Rojas Rusak, Fabio and Preusser, Friedrich and Tillberg, Paul and Randel, Nadine and Chhetri, Raghav K and Cardona, Albert and Keller, Philipp J and Harz, Hartmann and Leonhardt, Heinrich and Treier, Mathias and Preibisch, Stephan},
}

@article{zheng_complete_2018,
	title = {A {Complete} {Electron} {Microscopy} {Volume} of the {Brain} of {Adult} {Drosophila} melanogaster.},
	volume = {174},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/30033368},
	doi = {10.1016/j.cell.2018.06.019},
	abstract = {Drosophila melanogaster has a rich repertoire of innate and learned behaviors. Its 100,000-neuron brain is a large but tractable target for comprehensive neural circuit mapping. Only electron microscopy (EM) enables complete, unbiased mapping of synaptic connectivity; however, the fly brain is too large for conventional EM. We developed a custom high-throughput EM platform and imaged the entire brain of an adult female fly at synaptic resolution. To validate the dataset, we traced brain-spanning circuitry involving the mushroom body (MB), which has been extensively studied for its role in learning. All inputs to Kenyon cells (KCs), the intrinsic neurons of the MB, were mapped, revealing a previously unknown cell type, postsynaptic partners of KC dendrites, and unexpected clustering of olfactory projection neurons. These reconstructions show that this freely available EM volume supports mapping of brain-spanning circuits, which will significantly accelerate Drosophila neuroscience. VIDEO ABSTRACT.},
	number = {3},
	journal = {Cell},
	author = {Zheng, Zhihao and Lauritzen, J Scott and Perlman, Eric and Robinson, Camenzind G and Nichols, Matthew and Milkie, Daniel and Torrens, Omar and Price, John and Fisher, Corey B and Sharifi, Nadiya and Calle-Schuler, Steven A and Kmecova, Lucia and Ali, Iqbal J and Karsh, Bill and Trautman, Eric T and Bogovic, John A and Hanslovsky, Philipp and Jefferis, Gregory S X E and Kazhdan, Michael and Khairy, Khaled and Saalfeld, Stephan and Fetter, Richard D and Bock, Davi D},
	month = jul,
	year = {2018},
	note = {Publisher: Elsevier},
	keywords = {connectomics, Drosophila melanogaster, electron microscopy, image stitching, mushroom body, neural circuits, olfaction},
	pages = {730--743.e22},
}

@article{rotem_glow_2018,
	title = {Glow: {Graph} {Lowering} {Compiler} {Techniques} for {Neural} {Networks}},
	url = {http://arxiv.org/abs/1805.00907},
	abstract = {This paper presents the design of Glow, a machine learning compiler for heterogeneous hardware. It is a pragmatic approach to compilation that enables the generation of highly optimized code for multiple targets. Glow lowers the traditional neural network dataflow graph into a two-phase strongly-typed intermediate representation. The high-level intermediate representation allows the optimizer to perform domain-specific optimizations. The lower-level instruction-based address-only intermediate representation allows the compiler to perform memory-related optimizations, such as instruction scheduling, static memory allocation and copy elimination. At the lowest level, the optimizer performs machine-specific code generation to take advantage of specialized hardware features. Glow features a lowering phase which enables the compiler to support a high number of input operators as well as a large number of hardware targets by eliminating the need to implement all operators on all targets. The lowering phase is designed to reduce the input space and allow new hardware backends to focus on a small number of linear algebra primitives.},
	author = {Rotem, Nadav and Fix, Jordan and Abdulrasool, Saleem and Deng, Summer and Dzhabarov, Roman and Hegeman, James and Levenstein, Roman and Maher, Bert and Nadathur, Satish and Olesen, Jakob and Park, Jongsoo and Rakhov, Artem and Smelyanskiy, Misha},
	month = may,
	year = {2018},
}

@article{ellenberg_public_2018,
	title = {Public archives for biological image data},
	doi = {10.1038/s41592-018-0195-8},
	abstract = {Public data archives are the backbone of modern biological and biomedical research. While archives for biological molecules and structures are well-established, resources for imaging data do not yet cover the full range of spatial and temporal scales or application domains used by the scientific community. In the last few years, the technical barriers to building such resources have been solved and the first examples of scientific outputs from public image data resources, often through linkage to existing molecular resources, have been published. Using the successes of existing biomolecular resources as a guide, we present the rationale and principles for the construction of image data archives and databases that will be the foundation of the next revolution in biological and biomedical informatics and discovery.},
	author = {Ellenberg, Jan and Swedlow, Jason R and Barlow, Mary and Cook, Charles E and Patwardhan, Ardan and Brazma, Alvis and Birney, Ewan},
	year = {2018},
}

@article{costa_nblast_2016,
	title = {{NBLAST}: {Rapid}, {Sensitive} {Comparison} of {Neuronal} {Structure} and {Construction} of {Neuron} {Family} {Databases}},
	url = {http://dx.doi.org/10.1016/j.neuron.2016.06.012},
	doi = {10.1016/j.neuron.2016.06.012},
	author = {Costa, Marta and Manton, James D and Ostrovsky, Aaron D and Prohaska, Steffen and SXE Jefferis, Gregory},
	year = {2016},
	keywords = {cell type, clustering, NBLAST, neuroinformatics, neuron similarity, online resource},
}

@article{noauthor_medical_2018,
	title = {Medical {Image} {Synthesis} with {Deep} {Convolutional} {Adversarial} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8310638/},
	doi = {10.1109/TBME.2018.2814538},
	journal = {IEEE Transactions on Biomedical Engineering},
	year = {2018},
	pages = {1--1},
}

@article{hinton_dynamic_2017,
	title = {Dynamic {Routing} between capsules},
	issn = {0003-6935},
	doi = {10.1084/jem.53.1.27},
	abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
	author = {Hinton, Geoffrey},
	year = {2017},
}

@article{sabour_dynamic_2017,
	title = {Dynamic {Routing} {Between} {Capsules}},
	url = {http://arxiv.org/abs/1710.09829},
	abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
	year = {2017},
}

@article{lee_deep-learning_2017,
	title = {Deep-{Learning} {Based}, {Automated} {Segmentation} {Of} {Macular} {Edema} {In} {Optical} {Coherence} {Tomography}},
	url = {https://www.biorxiv.org/content/early/2017/05/09/135640},
	doi = {10.1101/135640},
	abstract = {Evaluation of clinical images is essential for diagnosis in many specialties and the development of computer vision algorithms to analyze biomedical images will be important. In ophthalmology, optical coherence tomography (OCT) is critical for managing retinal conditions. We developed a convolutional neural network (CNN) that detects intraretinal fluid (IRF) on OCT in a manner indistinguishable from clinicians. Using 1,289 OCT images, the CNN segmented images with a 0.911 cross-validated Dice coefficient, compared with segmentations by experts. Additionally, the agreement between experts and between experts and CNN were similar. Our results reveal that CNN can be trained to perform automated segmentations.},
	journal = {bioRxiv},
	author = {Lee, Cecilia S. and Tyring, Ariel J. and Deruyter, Nicolaas P. and Wu, Yue and Rokem, Ariel and Lee, Aaron Y.},
	month = may,
	year = {2017},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {135640--135640},
}

@techreport{kurtzer_singularity_nodate,
	title = {Singularity {Containers} for {Science}},
	url = {http://www.hpcadvisorycouncil.com/events/2017/stanford-workshop/pdf/GMKurtzer_Singularity_Keynote_Tuesday_02072017.pdf#43},
	author = {Kurtzer, Gregory M and {Gov{\textgreater}}},
}

@article{rivenson_deep_2017,
	title = {Deep learning microscopy},
	url = {https://doi.org/10.1364/OPTICA.4.001437},
	doi = {10.1364/OPTICA.4.001437},
	abstract = {We demonstrate that a deep neural network can significantly improve optical microscopy, enhancing its spatial resolution over a large field of view and depth of field. After its training, the only input to this network is an image acquired using a regular optical microscope, without any changes to its design. We blindly tested this deep learning approach using various tissue samples that are imaged with low-resolution and wide-field systems, where the network rapidly outputs an image with better resolution, matching the performance of higher numerical aperture lenses and also significantly surpassing their limited field of view and depth of field. These results are significant for various fields that use microscopy tools, including, e.g., life sciences, where optical microscopy is considered as one of the most widely used and deployed techniques. Beyond such applications, the presented approach might be applicable to other imaging modalities, also spanning different parts of the electromagnetic spectrum, and can be used to design computational imagers that get better as they continue to image specimens and establish new transformations among different modes of imaging.},
	author = {Rivenson, Yair and Göröcs, Zoltán and Günaydin, Harun and Zhang, Yibo and Wang, Hongda and Ozcan, Aydogan},
	year = {2017},
	keywords = {(1003010) Image reconstruction techniques, (1003190) Inverse problems, (1004996) Pattern recognition, neural networks, (1101758) Computational imaging, OCIS codes: (1800180) Microscopy},
}

@article{schorb_new_2017,
	title = {New hardware and workflows for semi-automated correlative cryo-fluorescence and cryo-electron microscopy/tomography},
	volume = {197},
	url = {https://www.sciencedirect.com/science/article/pii/S1047847716301356},
	doi = {10.1016/J.JSB.2016.06.020},
	abstract = {Correlative light and electron microscopy allows features of interest defined by fluorescence signals to be located in an electron micrograph of the same sample. Rare dynamic events or specific objects can be identified, targeted and imaged by electron microscopy or tomography. To combine it with structural studies using cryo-electron microscopy or tomography, fluorescence microscopy must be performed while maintaining the specimen vitrified at liquid-nitrogen temperatures and in a dry environment during imaging and transfer. Here we present instrumentation, software and an experimental workflow that improves the ease of use, throughput and performance of correlated cryo-fluorescence and cryo-electron microscopy. The new cryo-stage incorporates a specially modified high-numerical aperture objective lens and provides a stable and clean imaging environment. It is combined with a transfer shuttle for contamination-free loading of the specimen. Optimized microscope control software allows automated acquisition of the entire specimen area by cryo-fluorescence microscopy. The software also facilitates direct transfer of the fluorescence image and associated coordinates to the cryo-electron microscope for subsequent fluorescence-guided automated imaging. Here we describe these technological developments and present a detailed workflow, which we applied for automated cryo-electron microscopy and tomography of various specimens.},
	number = {2},
	journal = {Journal of Structural Biology},
	author = {Schorb, Martin and Gaechter, Leander and Avinoam, Ori and Sieckmann, Frank and Clarke, Mairi and Bebeacua, Cecilia and Bykov, Yury S. and Sonnen, Andreas F.-P. and Lihl, Reinhard and Briggs, John A.G.},
	month = feb,
	year = {2017},
	note = {Publisher: Academic Press},
	pages = {83--93},
}

@article{li_dlbi_2018,
	title = {{DLBI}: {Deep} learning guided {Bayesian} inference for structure reconstruction of super-resolution fluorescence microscopy},
	volume = {34},
	doi = {10.1093/bioinformatics/bty241},
	abstract = {Super-resolution fluorescence microscopy, with a resolution beyond the diffraction limit of light, has become an indispensable tool to directly visualize biological structures in living cells at a nanometer-scale resolution. Despite advances in high-density super-resolution fluorescent techniques, existing methods still have bottlenecks, including extremely long execution time, artificial thinning and thickening of structures, and lack of ability to capture latent structures. Here we propose a novel deep learning guided Bayesian inference approach, DLBI, for the time-series analysis of high-density fluorescent images. Our method combines the strength of deep learning and statistical inference, where deep learning captures the underlying distribution of the fluorophores that are consistent with the observed time-series fluorescent images by exploring local features and correlation along time-axis, and statistical inference further refines the ultrastructure extracted by deep learning and endues physical meaning to the final image. Comprehensive experimental results on both real and simulated datasets demonstrate that our method provides more accurate and realistic local patch and large-field reconstruction than the state-of-the-art method, the 3B analysis, while our method is more than two orders of magnitude faster. The main program is available at https://github.com/lykaust15/DLBI},
	number = {13},
	journal = {Bioinformatics},
	author = {Li, Yu and Xu, Fan and Zhang, Fa and Xu, Pingyong and Zhang, Mingshu and Fan, Ming and Li, Lihua and Gao, Xin and Han, Renmin},
	year = {2018},
	pages = {i284--i294},
}

@article{mukherjee_convolutional_2018,
	title = {Convolutional neural networks for whole slide image superresolution},
	url = {https://doi.org/10.1364/BOE.9.005368},
	doi = {10.1364/BOE.9.005368},
	abstract = {We present a computational approach for improving the quality of the resolution of images acquired from commonly available low magnification commercial slide scanners. Images from such scanners can be acquired cheaply and are efficient in terms of storage and data transfer. However, they are generally of poorer quality than images from high-resolution scanners and microscopes and do not have the necessary resolution needed in diagnostic or clinical environments, and hence are not used in such settings. The driving question of this presented research is whether the resolution of these images could be enhanced such that it would serve the same diagnostic purpose as high-resolution images from expensive scanners or microscopes. This need is generally known as the image super-resolution (SR) problem in image processing, and it has been studied extensively. Even so, none of the existing methods directly work for the slide scanner images, due to the unique challenges posed by this modality. Here, we propose a convolutional neural network (CNN) based approach, which is specifically trained to take low-resolution slide scanner images of cancer data and convert it into a high-resolution image. We validate these resolution improvements with computational analysis to show the enhanced images offer the same quantitative results. In summary, our extensive experiments demonstrate that this method indeed produces images that are similar to images from high-resolution scanners, both in quality and quantitative measures. This approach opens up new application possibilities for using low-resolution scanners, not only in terms of cost but also in access and speed of scanning for both research and possible clinical use.},
	author = {Mukherjee, Lopamudra and Keikhosravi, Adib and Bui, Dat and Eliceiri, Kevin W and Edu, Mukherjl@uww},
	year = {2018},
}

@article{jackson_style_2018,
	title = {Style {Augmentation}: {Data} {Augmentation} via {Style} {Randomization}},
	url = {http://arxiv.org/abs/1809.05375},
	abstract = {We introduce style augmentation, a new form of data augmentation based on random style transfer, for improving the robustness of convolutional neural networks (CNN) over both classification and regression based tasks. During training, our style augmentation randomizes texture, contrast and color, while preserving shape and semantic content. This is accomplished by adapting an arbitrary style transfer network to perform style randomization, by sampling input style embeddings from a multivariate normal distribution instead of inferring them from a style image. In addition to standard classification experiments, we investigate the effect of style augmentation (and data augmentation generally) on domain transfer tasks. We find that data augmentation significantly improves robustness to domain shift, and can be used as a simple, domain agnostic alternative to domain adaptation. Comparing style augmentation against a mix of seven traditional augmentation techniques, we find that it can be readily combined with them to improve network performance. We validate the efficacy of our technique with domain transfer experiments in classification and monocular depth estimation, illustrating consistent improvements in generalization.},
	author = {Jackson, Philip T. and Atapour-Abarghouei, Amir and Bonner, Stephen and Breckon, Toby and Obara, Boguslaw},
	month = sep,
	year = {2018},
}

@techreport{noauthor_by_2016,
	title = {By {Dr} {John} {Comley} {Latest} developments in {HIGH} {CONTENT} {SCREENING} systems},
	url = {https://pdfs.semanticscholar.org/9f51/5530616f06337a4d6d076cab214ca5d65065.pdf},
	year = {2016},
}

@article{rahni_week-long_2018,
	title = {Week-long imaging of cell divisions in the {Arabidopsis} root meristem},
	url = {https://www.biorxiv.org/content/early/2018/05/31/268102},
	doi = {10.1101/268102},
	abstract = {Characterizing the behaviors of dynamic systems requires capturing them with high temporal and spatial resolution. Owing to its transparency and genetic tractability, the Arabidopsis thaliana root lends itself well to live imaging when combined with cell and tissue-specific fluorescent reporters. We developed a novel 4D imaging method that utilizes simple confocal microscopy and readily available components to track cell divisions in the root stem cell niche and surrounding region for up to one week. This new setup allows us to finely analyze meristematic cell division rates that lead to patterning. Using this method, we performed a direct measurement of cell division intervals within and around the root stem cell niche. The results reveal a short, steep gradient of cell division in proximal stem cells, with progressively more rapid cell division rates from QC, to cells in direct contact with the QC (initials), to their immediate daughters, after which division rates appear to become more homogeneous. These results provide a baseline to study how perturbations in signaling could affect cell division patterns in the root meristem.},
	journal = {bioRxiv},
	author = {Rahni, Ramin and Birnbaum, Kenneth D},
	month = may,
	year = {2018},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {268102--268102},
}

@techreport{prenger_waveglow_nodate,
	title = {{WAVEGLOW}: {A} {FLOW}-{BASED} {GENERATIVE} {NETWORK} {FOR} {SPEECH} {SYNTHESIS}},
	abstract = {In this paper we propose WaveGlow: a flow-based network capable of generating high quality speech from mel-spectrograms. WaveGlow combines insights from Glow [1] and WaveNet [2] in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression. WaveGlow is implemented using only a single network, trained using only a single cost function: maximizing the likelihood of the training data, which makes the training procedure simple and stable. Our PyTorch implementation produces audio samples at a rate of more than 500 kHz on an NVIDIA V100 GPU. Mean Opinion Scores show that it delivers audio quality as good as the best publicly available WaveNet implementation. All code will be made publicly available online [3].},
	author = {Prenger, Ryan and Valle, Rafael and Catanzaro, Bryan},
	keywords = {Deep Learning, Gener-ative models, Index Terms-Audio Synthesis, Text-to-speech},
}

@techreport{brock_large_nodate,
	title = {{LARGE} {SCALE} {GAN} {TRAINING} {FOR} {HIGH} {FIDELITY} {NATURAL} {IMAGE} {SYNTHESIS}},
	url = {https://arxiv.org/pdf/1809.11096.pdf},
	abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick," allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Fréchet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.},
	author = {Brock, Andrew and Deepmind, Jeff Donahue and Deepmind, Karen Simonyan},
}

@article{mahajan_exploring_2018,
	title = {Exploring the {Limits} of {Weakly} {Supervised} {Pretraining} – {Facebook} {Research}},
	url = {https://research.fb.com/wp-content/uploads/2018/05/exploring_the_limits_of_weakly_supervised_pretraining.pdf?},
	abstract = {State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards "small". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4\% (97.6\% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.},
	journal = {Proceedings of the European Conference on Computer vision (ECCV)},
	author = {Mahajan, Dhruv and Girshick, Ross},
	year = {2018},
	pages = {181--196},
}

@article{zoph_learning_2017-1,
	title = {Learning {Transferable} {Architectures} for {Scalable} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1707.07012},
	abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4\% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on ImageNet. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0\% achieving 43.1\% mAP on the COCO dataset.},
	author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
	month = jul,
	year = {2017},
}

@article{pham_efficient_2018,
	title = {Efficient {Neural} {Architecture} {Search} via {Parameter} {Sharing}},
	url = {http://arxiv.org/abs/1802.03268},
	abstract = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89\%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65\%.},
	author = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
	month = feb,
	year = {2018},
}

@techreport{mullachery_image_2016,
	title = {Image {Captioning}},
	url = {https://arxiv.org/pdf/1805.09137.pdf},
	abstract = {This paper discusses and demonstrates the outcomes from our experimentation on Image Captioning. Image captioning is a much more involved task than image recognition or classification, because of the additional challenge of recognizing the interdependence between the objects/concepts in the image and the creation of a succinct sentential narration. Experiments on several labeled datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. As a toy application, we apply image captioning to create video captions, and we advance a few hypotheses on the challenges we encountered.},
	author = {Mullachery, Vikram and Motwani, Vishal},
	year = {2016},
}

@techreport{lee_pseudo-label_nodate,
	title = {Pseudo-{Label} : {The} {Simple} and {Efficient} {Semi}-{Supervised} {Learning} {Method} for {Deep} {Neural} {Networks}},
	url = {http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf},
	abstract = {We propose the simple and efficient method of semi-supervised learning for deep neural networks. Basically, the proposed network is trained in a supervised fashion with labeled and unlabeled data simultaneously. For un-labeled data, Pseudo-Label s, just picking up the class which has the maximum predicted probability, are used as if they were true labels. This is in effect equivalent to Entropy Regularization. It favors a low-density separation between classes, a commonly assumed prior for semi-supervised learning. With De-noising Auto-Encoder and Dropout, this simple method outperforms conventional methods for semi-supervised learning with very small labeled data on the MNIST handwritten digit dataset.},
	author = {Lee, Dong-Hyun},
}

@article{couture_image_2018,
	title = {Image analysis with deep learning to predict breast cancer grade, {ER} status, histologic subtype, and intrinsic subtype},
	volume = {4},
	url = {http://www.nature.com/articles/s41523-018-0079-1},
	doi = {10.1038/s41523-018-0079-1},
	abstract = {RNA-based, multi-gene molecular assays are available and widely used for patients with ER-positive/HER2-negative breast cancers. However, RNA-based genomic tests can be costly and are not available in many countries. Methods for inferring molecular subtype from histologic images may identify patients most likely to benefit from further genomic testing. To identify patients who could benefit from molecular testing based on H\&E stained histologic images, we developed an image analysis approach using deep learning. A training set of 571 breast tumors was used to create image-based classifiers for tumor grade, ER status, PAM50 intrinsic subtype, histologic subtype, and risk of recurrence score (ROR-PT). The resulting classifiers were applied to an independent test set (n = 288), and accuracy, sensitivity, and specificity of each was assessed on the test set. Histologic image analysis with deep learning distinguished low-intermediate vs. high tumor grade (82\% accuracy), ER status (84\% accuracy), Basal-like vs. non-Basal-like (77\% accuracy), Ductal vs. Lobular (94\% accuracy), and high vs. low-medium ROR-PT score (75\% accuracy). Sampling considerations in the training set minimized bias in the test set. Incorrect classification of ER status was significantly more common for Luminal B tumors. These data provide proof of principle that molecular marker status, including a critical clinical biomarker (i.e., ER status), can be predicted with accuracy {\textgreater}75\% based on H\&E features. Image-based methods could be promising for identifying patients with a greater need for further genomic testing, or in place of classically scored variables typically accomplished using human-based scoring.},
	number = {1},
	journal = {npj Breast Cancer},
	author = {Couture, Heather D. and Williams, Lindsay A. and Geradts, Joseph and Nyante, Sarah J. and Butler, Ebonee N. and Marron, J. S. and Perou, Charles M. and Troester, Melissa A. and Niethammer, Marc},
	month = dec,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Breast cancer, Cancer imaging, Tumour biomarkers},
	pages = {30--30},
}

@article{wang_tumor-infiltrating_2018,
	title = {Tumor-infiltrating {B} cells: their role and application in anti-tumor immunity in lung cancer},
	url = {http://www.nature.com/articles/s41423-018-0027-x},
	doi = {10.1038/s41423-018-0027-x},
	abstract = {Evidence indicates that lung cancer development is a complex process that involves interactions between tumor cells, stromal fibroblasts, and immune cells. Tumor-infiltrating immune cells play a significant role in the promotion or inhibition of tumor growth. As an integral component of the tumor microenvironment, tumor-infiltrating B lymphocytes (TIBs) exist in all stages of cancer and play important roles in shaping tumor development. Here, we review recent clinical and preclinical studies that outline the role of TIBs in lung cancer development, assess their prognostic significance, and explore the potential benefit of B cell-based immunotherapy for lung cancer treatment.},
	journal = {Cellular \& Molecular Immunology},
	author = {Wang, Si-si and Liu, Wei and Ly, Dalam and Xu, Hao and Qu, Limei and Zhang, Li},
	month = apr,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Antibodies, Biomedicine, general, Immunology, Medical Microbiology, Microbiology, Vaccine},
	pages = {1--1},
}

@article{feldman_state_2018,
	title = {The {State} of {Data} in {Healthcare}: {Path} {Towards} {Standardization}},
	volume = {2},
	url = {http://link.springer.com/10.1007/s41666-018-0019-8},
	doi = {10.1007/s41666-018-0019-8},
	number = {3},
	journal = {Journal of Healthcare Informatics Research},
	author = {Feldman, Keith and Johnson, Reid A. and Chawla, Nitesh V.},
	month = sep,
	year = {2018},
	note = {Publisher: Springer International Publishing},
	pages = {248--271},
}

@article{xu_show_2015,
	title = {Show, {Attend} and {Tell}: {Neural} {Image} {Caption} {Generation} with {Visual} {Attention}},
	url = {http://arxiv.org/abs/1502.03044},
	abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
	author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
	month = feb,
	year = {2015},
}

@article{vinyals_show_2014,
	title = {Show and {Tell}: {A} {Neural} {Image} {Caption} {Generator}},
	url = {http://arxiv.org/abs/1411.4555},
	abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
	author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
	month = nov,
	year = {2014},
}

@article{castillam_three-dimensional_2018,
	title = {Three-{Dimensional} {Quantification} of {Filopodia} in {Motile} {Cancer} {Cells}},
	url = {https://ieeexplore.ieee.org/document/8482350/},
	doi = {10.1109/TMI.2018.2873842},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Castillam, Carlos and Maskam, Martin and Sorokin, Dmitry V. and Meijering, Erik and Ortiz-de-Solorzano, Carlos},
	year = {2018},
	pages = {1--1},
}

@article{majchrzak_newest_2018,
	title = {The newest cathinone derivatives as designer drugs: an analytical and toxicological review},
	volume = {36},
	url = {http://link.springer.com/10.1007/s11419-017-0385-6},
	doi = {10.1007/s11419-017-0385-6},
	number = {1},
	journal = {Forensic Toxicology},
	author = {Majchrzak, Milena and Celiński, Rafał and Kuś, Piotr and Kowalska, Teresa and Sajewicz, Mieczysław},
	month = jan,
	year = {2018},
	note = {Publisher: Springer Japan},
	pages = {33--50},
	file = {Full Text:/home/zwerg/Zotero/storage/LWEJBKWG/Majchrzak et al. - 2018 - The newest cathinone derivatives as designer drugs.pdf:application/pdf},
}

@article{poplin_prediction_2018,
	title = {Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning},
	volume = {2},
	url = {http://www.nature.com/articles/s41551-018-0195-0},
	doi = {10.1038/s41551-018-0195-0},
	abstract = {Traditionally, medical discoveries are made by observing associations, making hypotheses from them and then designing and running experiments to test the hypotheses. However, with medical images, observing and quantifying associations can often be difficult because of the wide variety of features, patterns, colours, values and shapes that are present in real data. Here, we show that deep learning can extract new knowledge from retinal fundus images. Using deep-learning models trained on data from 284,335 patients and validated on two independent datasets of 12,026 and 999 patients, we predicted cardiovascular risk factors not previously thought to be present or quantifiable in retinal images, such as age (mean absolute error within 3.26 years), gender (area under the receiver operating characteristic curve (AUC) = 0.97), smoking status (AUC = 0.71), systolic blood pressure (mean absolute error within 11.23 mmHg) and major adverse cardiac events (AUC = 0.70). We also show that the trained deep-learning models used anatomical features, such as the optic disc or blood vessels, to generate each prediction.},
	number = {3},
	journal = {Nature Biomedical Engineering},
	author = {Poplin, Ryan and Varadarajan, Avinash V. and Blumer, Katy and Liu, Yun and McConnell, Michael V. and Corrado, Greg S. and Peng, Lily and Webster, Dale R.},
	month = mar,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cardiology, Computational science, Engineering, Risk factors},
	pages = {158--164},
}

@article{ucar_activity_2018,
	title = {The activity and discussion points of \#{Circumcision} through {Twitter}; a microblogging platform},
	volume = {30},
	url = {http://www.nature.com/articles/s41443-018-0058-y},
	doi = {10.1038/s41443-018-0058-y},
	abstract = {Our objective was to elucidate the discussion points of circumcision on social media (SoMe) by looking at the Twitter activity. Twitter searched for \#circumcision hashtag via 
www.tweetarchivist.com
,
www.twitonomy.com
,
www.symplur.com
. Total tweet numbers, most influencers, top users were documented. Tweets including female circumcision were excluded. The contents of the tweets were classified into four subgroups (medical, religious, social, and political) by two independent reviewers. All kinds of tweet activities were statistically analyzed. A total of 9795 users generated 15,989 tweets about circumcision in a 1 month period. Mean daily tweet activity was 532 for \#circumcision. The content analysis revealed that 2224 (15.8\%) medical, 1133 (8.0\%) religious, 323 (2.2\%) social and 10,470 (74.0\%) political tweets have been sent out by the users. Contributors originated from 174 countries from 6 continents. Media organizations were accounted for 52\% of the top 25 influencers in circumcision hashtag. The most common hashtags accompanying \#circumcision were \#HIV (4.9\%), \#babiesgotherpes (3.3\%), \#muslim (1.8\%), \#malegenitalmutilation (1.6\%) respectively. There is an increasing discussion about circumcision through SoMe . Our results provided that the discussion points are mostly driven by the media and the activists. The political tweets have been found to be the center of the discussion. SoMe usage should be increased by medical professionals for true information of the public.},
	number = {5},
	journal = {International Journal of Impotence Research},
	author = {Ucar, Taha and Culpan, Meftun and Caskurlu, Turhan and Karaman, M.İhsan and Silay, Mesrur Selcuk},
	month = oct,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Urethra, Urogenital diseases},
	pages = {249--252},
}

@article{li_single-cell_2018,
	title = {Single-cell multi-omics sequencing of human early embryos},
	volume = {20},
	url = {http://www.nature.com/articles/s41556-018-0123-2},
	doi = {10.1038/s41556-018-0123-2},
	abstract = {DNA methylation, chromatin states and their interrelationships represent critical epigenetic information, but these are largely unknown in human early embryos. Here, we apply single-cell chromatin overall omic-scale landscape sequencing (scCOOL-seq) to generate a genome-wide map of DNA methylation and chromatin accessibility at single-cell resolution during human preimplantation development. Unlike in mice, the chromatin of the paternal genome is already more open than that of the maternal genome at the mid-zygote stage in humans, and this state is maintained until the 4-cell stage. After fertilization, genes with high variations in DNA methylation, and those with high variations in chromatin accessibility, tend to be two different sets. Furthermore, 1,797 out of 5,155 (35\%) widely open chromatin regions in promoters closed when transcription activity was inhibited, indicating a feedback mechanism between transcription and open chromatin maintenance. Our work paves the way for dissecting the complex, yet highly coordinated, epigenetic reprogramming during human preimplantation development.},
	number = {7},
	journal = {Nature Cell Biology},
	author = {Li, Lin and Guo, Fan and Gao, Yun and Ren, Yixin and Yuan, Peng and Yan, Liying and Li, Rong and Lian, Ying and Li, Jingyun and Hu, Boqiang and Gao, Junpeng and Wen, Lu and Tang, Fuchou and Qiao, Jie},
	month = jul,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Chromatin structure, DNA methylation, Embryogenesis},
	pages = {847--858},
}

@article{abramoff_pivotal_2018,
	title = {Pivotal trial of an autonomous {AI}-based diagnostic system for detection of diabetic retinopathy in primary care offices},
	volume = {1},
	url = {http://www.nature.com/articles/s41746-018-0040-6},
	doi = {10.1038/s41746-018-0040-6},
	abstract = {Artificial Intelligence (AI) has long promised to increase healthcare affordability, quality and accessibility but FDA, until recently, had never authorized an autonomous AI diagnostic system. This pivotal trial of an AI system to detect diabetic retinopathy (DR) in people with diabetes enrolled 900 subjects, with no history of DR at primary care clinics, by comparing to Wisconsin Fundus Photograph Reading Center (FPRC) widefield stereoscopic photography and macular Optical Coherence Tomography (OCT), by FPRC certified photographers, and FPRC grading of Early Treatment Diabetic Retinopathy Study Severity Scale (ETDRS) and Diabetic Macular Edema (DME). More than mild DR (mtmDR) was defined as ETDRS level 35 or higher, and/or DME, in at least one eye. AI system operators underwent a standardized training protocol before study start. Median age was 59 years (range, 22–84 years); among participants, 47.5\% of participants were male; 16.1\% were Hispanic, 83.3\% not Hispanic; 28.6\% African American and 63.4\% were not; 198 (23.8\%) had mtmDR. The AI system exceeded all pre-specified superiority endpoints at sensitivity of 87.2\% (95\% CI, 81.8–91.2\%) ({\textgreater}85\%), specificity of 90.7\% (95\% CI, 88.3–92.7\%) ({\textgreater}82.5\%), and imageability rate of 96.1\% (95\% CI, 94.6–97.3\%), demonstrating AI’s ability to bring specialty-level diagnostics to primary care settings. Based on these results, FDA authorized the system for use by health care providers to detect more than mild DR and diabetic macular edema, making it, the first FDA authorized autonomous AI diagnostic system in any field of medicine, with the potential to help prevent vision loss in thousands of people with diabetes annually. ClinicalTrials.gov NCT02963441},
	number = {1},
	journal = {npj Digital Medicine},
	author = {Abràmoff, Michael D. and Lavin, Philip T. and Birch, Michele and Shah, Nilay and Folk, James C.},
	month = dec,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biomedical engineering, Eye manifestations},
	pages = {39--39},
}

@article{pinaire_patient_2017,
	title = {Patient healthcare trajectory. {An} essential monitoring tool: a systematic review},
	volume = {5},
	url = {http://link.springer.com/10.1007/s13755-017-0020-2},
	doi = {10.1007/s13755-017-0020-2},
	number = {1},
	journal = {Health Information Science and Systems},
	author = {Pinaire, Jessica and Azé, Jérôme and Bringay, Sandra and Landais, Paul},
	month = dec,
	year = {2017},
	note = {Publisher: Springer International Publishing},
	pages = {1--1},
}

@article{beger_metabolomics_2016,
	title = {Metabolomics enables precision medicine: “{A} {White} {Paper}, {Community} {Perspective}”},
	volume = {12},
	url = {http://link.springer.com/10.1007/s11306-016-1094-6},
	doi = {10.1007/s11306-016-1094-6},
	number = {9},
	journal = {Metabolomics},
	author = {Beger, Richard D. and Dunn, Warwick and Schmidt, Michael A. and Gross, Steven S. and Kirwan, Jennifer A. and Cascante, Marta and Brennan, Lorraine and Wishart, David S. and Oresic, Matej and Hankemeier, Thomas and Broadhurst, David I. and Lane, Andrew N. and Suhre, Karsten and Kastenmüller, Gabi and Sumner, Susan J. and Thiele, Ines and Fiehn, Oliver and Kaddurah-Daouk, Rima and Initiative, for “Precision Medicine {and} Pharmacometabolomics Task Group”-Metabolomics Society},
	month = sep,
	year = {2016},
	note = {Publisher: Springer US},
	pages = {149--149},
}

@article{oconnell_machine-learning_2016,
	title = {Machine-learning approach identifies a pattern of gene expression in peripheral blood that can accurately detect ischaemic stroke},
	volume = {1},
	url = {http://www.nature.com/articles/npjgenmed201638},
	doi = {10.1038/npjgenmed.2016.38},
	abstract = {A blood test that identifies gene expression patterns could help diagnose stroke early. Collaborators at West Virginia University and CereDx Inc in the US measured gene expression levels in the peripheral blood of 39 acute ischemic stroke (AIS) patients and 24 controls. The data were analyzed with a machine learning technique that looks for patterns in gene expression, and found ten co-expressed genes that could collectively distinguish AIS patients from the controls with 98\% accuracy. The co-expression of these ten genes was then successfully used as a biomarker signature to distinguish 39 AIS patients from 50 controls with 95\% accuracy in a second group of subjects. The blood test could help diagnose AIS early when imaging tools are not available.},
	number = {1},
	journal = {npj Genomic Medicine},
	author = {O’Connell, Grant C and Petrone, Ashley B and Treadway, Madison B and Tennant, Connie S and Lucke-Wold, Noelle and Chantler, Paul D and Barr, Taura L},
	month = nov,
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	keywords = {Diagnostic markers, Stroke},
	pages = {16038--16038},
}

@article{dunican_laboratory_2018,
	title = {Laboratory and home comparison of wrist-activity monitors and polysomnography in middle-aged adults},
	volume = {16},
	url = {http://link.springer.com/10.1007/s41105-017-0130-x},
	doi = {10.1007/s41105-017-0130-x},
	number = {1},
	journal = {Sleep and Biological Rhythms},
	author = {Dunican, Ian C. and Murray, Kevin and Slater, James A. and Maddison, Kathleen J. and Jones, Maddison J. and Dawson, Brian and Straker, Leon M. and Caldwell, John A. and Halson, Shona L. and Eastwood, Peter R.},
	month = jan,
	year = {2018},
	note = {Publisher: Springer Japan},
	pages = {85--97},
}

@article{zhang_network-based_2017,
	title = {Network-based machine learning and graph theory algorithms for precision oncology},
	volume = {1},
	url = {http://www.nature.com/articles/s41698-017-0029-7},
	doi = {10.1038/s41698-017-0029-7},
	abstract = {Network-based analytics plays an increasingly important role in precision oncology. Growing evidence in recent studies suggests that cancer can be better understood through mutated or dysregulated pathways or networks rather than individual mutations and that the efficacy of repositioned drugs can be inferred from disease modules in molecular networks. This article reviews network-based machine learning and graph theory algorithms for integrative analysis of personal genomic data and biomedical knowledge bases to identify tumor-specific molecular mechanisms, candidate targets and repositioned drugs for personalized treatment. The review focuses on the algorithmic design and mathematical formulation of these methods to facilitate applications and implementations of network-based analysis in the practice of precision oncology. We review the methods applied in three scenarios to integrate genomic data and network models in different analysis pipelines, and we examine three categories of network-based approaches for repositioning drugs in drug–disease–gene networks. In addition, we perform a comprehensive subnetwork/pathway analysis of mutations in 31 cancer genome projects in the Cancer Genome Atlas and present a detailed case study on ovarian cancer. Finally, we discuss interesting observations, potential pitfalls and future directions in network-based precision oncology.},
	number = {1},
	journal = {npj Precision Oncology},
	author = {Zhang, Wei and Chien, Jeremy and Yong, Jeongsik and Kuang, Rui},
	month = dec,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cancer genomics, Machine learning, Network topology},
	pages = {25--25},
}

@article{khalilimeybodi_investigating_2018,
	title = {Investigating β-adrenergic-induced cardiac hypertrophy through computational approach: classical and non-classical pathways},
	volume = {68},
	url = {http://link.springer.com/10.1007/s12576-017-0557-5},
	doi = {10.1007/s12576-017-0557-5},
	number = {4},
	journal = {The Journal of Physiological Sciences},
	author = {Khalilimeybodi, Ali and Daneshmehr, Alireza and Sharif-Kashani, Babak},
	month = jul,
	year = {2018},
	note = {Publisher: Springer Japan},
	pages = {503--520},
}

@incollection{chen_how_2017,
	title = {How to {Become} a {Smart} {Patient} in the {Era} of {Precision} {Medicine}?},
	url = {http://link.springer.com/10.1007/978-981-10-6041-0_1},
	publisher = {Springer, Singapore},
	author = {Chen, Yalan and Yang, Lan and Hu, Hai and Chen, Jiajia and Shen, Bairong},
	year = {2017},
	doi = {10.1007/978-981-10-6041-0_1},
	pages = {1--16},
}

@article{shaw_genetic_2018,
	title = {Genetic variants and pathways implicated in a pediatric inflammatory bowel disease cohort},
	url = {http://www.nature.com/articles/s41435-018-0015-2},
	doi = {10.1038/s41435-018-0015-2},
	abstract = {In the United States, approximately 5\% of individuals with inflammatory bowel disease (IBD) are younger than 20 years old. Studies of pediatric cohorts can provide unique insights into genetic architecture of IBD, which includes Crohn’s disease (CD) and ulcerative colitis (UC). Large genome-wide association studies have found more than 200 IBD-associated loci but explain a minority of disease variance for CD and UC. We sought to characterize the contribution of rare variants to disease development, comparing exome sequencing of 368 pediatric IBD patients to publicly available exome sequencing (dbGaP) and aggregate frequency data (ExAC). Using dbGaP data, we performed logistic regression for common variants and optimal unified association tests (SKAT-O) for rare, likely-deleterious variants. We further compared rare variants to ExAC counts with Fisher’s exact tests. We did pathway enrichment analysis on the most significant genes from each comparison. Many variants overlapped with known IBD-associated genes (e.g. NOD2). Rare variants were enriched in CD-associated loci (p = 0.009) and showed suggestive enrichment in neutrophil function genes (p = 0.05). Pathway enrichment implicated immune-related pathways, especially cell killing and apoptosis. Variants in extracellular matrix genes also emerged as an important theme in our analysis.},
	journal = {Genes \& Immunity},
	author = {Shaw, Kelly A. and Cutler, David J. and Okou, David and Dodd, Anne and Aronow, Bruce J. and Haberman, Yael and Stevens, Christine and Walters, Thomas D. and Griffiths, Anne and Baldassano, Robert N. and Noe, Joshua D. and Hyams, Jeffrey S. and Crandall, Wallace V. and Kirschner, Barbara S. and Heyman, Melvin B. and Snapper, Scott and Guthery, Stephen and Dubinsky, Marla C. and Shapiro, Jason M. and Otley, Anthony R. and Daly, Mark and Denson, Lee A. and Kugathasan, Subra and Zwick, Michael E.},
	month = mar,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Disease genetics, DNA sequencing, Genetic association study, Medical genomics},
	pages = {1--1},
}

@article{zhang_functional_2018,
	title = {Functional relevance for central cornea thickness-associated genetic variants by using integrative analyses},
	volume = {11},
	url = {https://biodatamining.biomedcentral.com/articles/10.1186/s13040-018-0179-3},
	doi = {10.1186/s13040-018-0179-3},
	abstract = {The genetic architecture underlying central cornea thickness (CCT) is far from understood. Most of the CCT-associated variants are located in the non-coding regions, raising the difficulty of following functional characterizations. Thus, integrative functional analyses on CCT-associated loci might benefit in overcoming these issues by prioritizing the hub genes that are located in the center of CCT genetic network. Integrative analyses including functional annotations, enrichment analysis, and protein-protein interaction analyses were performed on all reported CCT GWAS lead SNPs, together with their proxy variants. Functional annotations were conducted by CADD, GWAVA, and Eigen. Enrichment analyses for CCT-associated genes were performed using ToppGene suite. Protein-protein interaction network and gene co-expression analyses were performed by GeneMANIA. Functional annotations prioritized eight genes (ADAMSTS6, ARID5B, FOXO1, AKAP13, COL4A3, COL8A2, TBL1XR1, and KCMB2) harboring SNPs with strong evidence of regulatory potential. It was also shown that CCT-associated genes were significantly enriched in collagen-related pathways and the phenotype of keratoconus, and some of them were found to be involved in one interaction network. This study revealed the hub genes that were located in the center of CCT genetic network and provided a new insight into the genetic regulation underlying CCT GWAS findings.},
	number = {1},
	journal = {BioData Mining},
	author = {Zhang, Jing and Wu, Dan and Dai, Yiqin and Xu, Jianjiang},
	month = dec,
	year = {2018},
	note = {Publisher: BioMed Central},
	keywords = {Algorithms, Bioinformatics, Computational Biology/Bioinformatics, Computer Appl. in Life Sciences, Data Mining and Knowledge Discovery},
	pages = {19--19},
}

@article{karczewski_integrative_2018,
	title = {Integrative omics for health and disease},
	volume = {19},
	url = {http://www.nature.com/doifinder/10.1038/nrg.2018.4},
	doi = {10.1038/nrg.2018.4},
	abstract = {This article discusses how integrating different omics data types — such as DNA sequencing, transcriptomics and metabolomics — can provide a rich view of healthy and disease states, including novel clinical diagnoses. The authors discuss the value of the different data types, as well as strategies, considerations and challenges for multi-omic integration in various disease contexts.},
	number = {5},
	journal = {Nature Reviews Genetics},
	author = {Karczewski, Konrad J. and Snyder, Michael P.},
	month = feb,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Medical genomics, Clinical genetics, generation sequencing, Genetic databases, Medical genetics, Next, Personalized medicine, Transcriptomics},
	pages = {299--310},
}

@article{lotsch_identification_2018,
	title = {Identification of disease-distinct complex biomarker patterns by means of unsupervised machine-learning using an interactive {R} toolbox ({Umatrix})},
	volume = {3},
	url = {https://bdataanalytics.biomedcentral.com/articles/10.1186/s41044-018-0032-1},
	doi = {10.1186/s41044-018-0032-1},
	abstract = {Unsupervised machine-learned analysis of cluster structures, applied using the emergent self-organizing feature maps (ESOM) combined with the unified distance matrix (U-matrix) has been shown to provide an unbiased method to identify true clusters. It outperforms classical hierarchical clustering algorithms that carry a considerable tendency to produce erroneous results. To facilitate the application of the ESOM/U-matrix method in biomedical research, we introduce the interactive R-based bioinformatics tool “Umatrix”, which enables valid identification of a biologically meaningful cluster structure in the data by training a Kohonen-type self-organizing map followed by interface-guided interactive clustering on the emergent U-matrix map. The ability to detect clinical relevant subgroups was applied to a data set comprising plasma concentrations of d = 25 lipid markers including endocannabinoids, lysophosphatidic acids, ceramides and sphingolipids acquired from n = 100 patients with Parkinson's disease and n = 100 controls. Following ESOM training, clear data structures in the high-dimensional data space were observed on the U-matrix, allowing separation of patients from controls almost perfectly. When the data structure was destroyed by Monte-Carlo random resampling, the U-matrix became unstructured and patients and controls were mixed. Obtained results are biologically plausible and supported by empirical evidence of a regulation of several classes of lipids in Parkinson's disease. Sophisticated analysis of structures in biomedical data provides a basis for the mechanistic interpretation of the observations and facilitates subsequent analyses focusing on hypothesis testing. The freely available R library “Umatrix” provides an interactive tool for broader application of unsupervised machine learning on complex biomedical data.},
	number = {1},
	journal = {Big Data Analytics},
	author = {Lötsch, Jörn and Lerch, Florian and Djaldetti, Ruth and Tegder, Irmgard and Ultsch, Alfred},
	month = dec,
	year = {2018},
	note = {Publisher: BioMed Central},
	keywords = {Bioinformatics, Computational Biology/Bioinformatics, Computer Appl. in Life Sciences, Mathematical and Computational Biology},
	pages = {5--5},
}

@article{shi_h7n9_2017,
	title = {{H7N9} virulent mutants detected in chickens in {China} pose an increased threat to humans},
	volume = {27},
	url = {http://www.nature.com/doifinder/10.1038/cr.2017.129},
	doi = {10.1038/cr.2017.129},
	abstract = {H7N9 virulent mutants detected in chickens in China pose an increased threat to humans},
	number = {12},
	journal = {Cell Research},
	author = {Shi, Jianzhong and Deng, Guohua and Kong, Huihui and Gu, Chunyang and Ma, Shujie and Yin, Xin and Zeng, Xianying and Cui, Pengfei and Chen, Yan and Yang, Huanliang and Wan, Xiaopeng and Wang, Xiurong and Liu, Liling and Chen, Pucheng and Jiang, Yongping and Liu, Jinxiong and Guan, Yuntao and Suzuki, Yasuo and Li, Mei and Qu, Zhiyuan and Guan, Lizheng and Zang, Jinkai and Gu, Wenli and Han, Shuyu and Song, Yangming and Hu, Yuzhen and Wang, Zeng and Gu, Linlin and Yang, Wenyu and Liang, Libin and Bao, Hongmei and Tian, Guobin and Li, Yanbing and Qiao, Chuanling and Jiang, Li and Li, Chengjun and Bu, Zhigao and Chen, Hualan},
	month = dec,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {Risk factors, Influenza virus, Mutation},
	pages = {1409--1421},
}

@article{qin_computer-aided_2018,
	title = {Computer-aided detection in chest radiography based on artificial intelligence: a survey},
	volume = {17},
	url = {https://biomedical-engineering-online.biomedcentral.com/articles/10.1186/s12938-018-0544-y},
	doi = {10.1186/s12938-018-0544-y},
	abstract = {As the most common examination tool in medical practice, chest radiography has important clinical value in the diagnosis of disease. Thus, the automatic detection of chest disease based on chest radiography has become one of the hot topics in medical imaging research. Based on the clinical applications, the study conducts a comprehensive survey on computer-aided detection (CAD) systems, and especially focuses on the artificial intelligence technology applied in chest radiography. The paper presents several common chest X-ray datasets and briefly introduces general image preprocessing procedures, such as contrast enhancement and segmentation, and bone suppression techniques that are applied to chest radiography. Then, the CAD system in the detection of specific disease (pulmonary nodules, tuberculosis, and interstitial lung diseases) and multiple diseases is described, focusing on the basic principles of the algorithm, the data used in the study, the evaluation measures, and the results. Finally, the paper summarizes the CAD system in chest radiography based on artificial intelligence and discusses the existing problems and trends.},
	number = {1},
	journal = {BioMedical Engineering OnLine},
	author = {Qin, Chunli and Yao, Demin and Shi, Yonghong and Song, Zhijian},
	month = dec,
	year = {2018},
	note = {Publisher: BioMed Central},
	keywords = {Biomaterials, Biomedical Engineering, Biomedical Engineering/Biotechnology, Biotechnology},
	pages = {113--113},
}

@article{de_fauw_clinically_2018,
	title = {Clinically applicable deep learning for diagnosis and referral in retinal disease},
	volume = {24},
	url = {http://www.nature.com/articles/s41591-018-0107-6},
	doi = {10.1038/s41591-018-0107-6},
	abstract = {The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue segmentations produced by our architecture act as a device-independent representation; referral accuracy is maintained when using tissue segmentations from a different type of device. Our work removes previous barriers to wider clinical use without prohibitive training data requirements across multiple pathologies in a real-world setting.},
	number = {9},
	journal = {Nature Medicine},
	author = {De Fauw, Jeffrey and Ledsam, Joseph R. and Romera-Paredes, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O’Donoghue, Brendan and Visentin, Daniel and van den Driessche, George and Lakshminarayanan, Balaji and Meyer, Clemens and Mackinder, Faith and Bouton, Simon and Ayoub, Kareem and Chopra, Reena and King, Dominic and Karthikesalingam, Alan and Hughes, Cían O. and Raine, Rosalind and Hughes, Julian and Sim, Dawn A. and Egan, Catherine and Tufail, Adnan and Montgomery, Hugh and Hassabis, Demis and Rees, Geraint and Back, Trevor and Khaw, Peng T. and Suleyman, Mustafa and Cornebise, Julien and Keane, Pearse A. and Ronneberger, Olaf},
	month = sep,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Eye manifestations, Machine learning, Diagnosis, dimensional imaging, Three},
	pages = {1342--1350},
}

@article{rajalakshmi_automated_2018,
	title = {Automated diabetic retinopathy detection in smartphone-based fundus photography using artificial intelligence},
	volume = {32},
	url = {http://www.nature.com/articles/s41433-018-0064-9},
	doi = {10.1038/s41433-018-0064-9},
	abstract = {To assess the role of artificial intelligence (AI)-based automated software for detection of diabetic retinopathy (DR) and sight-threatening DR (STDR) by fundus photography taken using a smartphone-based device and validate it against ophthalmologist’s grading. Three hundred and one patients with type 2 diabetes underwent retinal photography with Remidio ‘Fundus on phone’ (FOP), a smartphone-based device, at a tertiary care diabetes centre in India. Grading of DR was performed by the ophthalmologists using International Clinical DR (ICDR) classification scale. STDR was defined by the presence of severe non-proliferative DR, proliferative DR or diabetic macular oedema (DME). The retinal photographs were graded using a validated AI DR screening software (EyeArtTM) designed to identify DR, referable DR (moderate non-proliferative DR or worse and/or DME) or STDR. The sensitivity and specificity of automated grading were assessed and validated against the ophthalmologists’ grading. Retinal images of 296 patients were graded. DR was detected by the ophthalmologists in 191 (64.5\%) and by the AI software in 203 (68.6\%) patients while STDR was detected in 112 (37.8\%) and 146 (49.3\%) patients, respectively. The AI software showed 95.8\% (95\% CI 92.9–98.7) sensitivity and 80.2\% (95\% CI 72.6–87.8) specificity for detecting any DR and 99.1\% (95\% CI 95.1–99.9) sensitivity and 80.4\% (95\% CI 73.9–85.9) specificity in detecting STDR with a kappa agreement of k = 0.78 (p {\textless} 0.001) and k = 0.75 (p {\textless} 0.001), respectively. Automated AI analysis of FOP smartphone retinal imaging has very high sensitivity for detecting DR and STDR and thus can be an initial tool for mass retinal screening in people with diabetes.},
	number = {6},
	journal = {Eye},
	author = {Rajalakshmi, Ramachandran and Subashini, Radhakrishnan and Anjana, Ranjit Mohan and Mohan, Viswanathan},
	month = jun,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Outcomes research, Retinal diseases},
	pages = {1138--1144},
}

@article{hosny_artificial_2018,
	title = {Artificial intelligence in radiology},
	volume = {18},
	url = {http://www.nature.com/articles/s41568-018-0016-5},
	doi = {10.1038/s41568-018-0016-5},
	abstract = {Artificial intelligence (AI) algorithms, particularly deep learning, have demonstrated remarkable progress in image-recognition tasks. Methods ranging from convolutional neural networks to variational autoencoders have found myriad applications in the medical image analysis field, propelling it forward at a rapid pace. Historically, in radiology practice, trained physicians visually assessed medical images for the detection, characterization and monitoring of diseases. AI methods excel at automatically recognizing complex patterns in imaging data and providing quantitative, rather than qualitative, assessments of radiographic characteristics. In this Opinion article, we establish a general understanding of AI methods, particularly those pertaining to image-based tasks. We explore how these methods could impact multiple facets of radiology, with a general focus on applications in oncology, and demonstrate ways in which these methods are advancing the field. Finally, we discuss the challenges facing clinical implementation and provide our perspective on how the domain could be advanced.},
	number = {8},
	journal = {Nature Reviews Cancer},
	author = {Hosny, Ahmed and Parmar, Chintan and Quackenbush, John and Schwartz, Lawrence H. and Aerts, Hugo J. W. L.},
	month = aug,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cancer imaging, Machine learning, Cancer screening, Medical imaging},
	pages = {500--510},
}

@article{eguchi_integrative_2018,
	title = {An integrative network-based approach to identify novel disease genes and pathways: a case study in the context of inflammatory bowel disease},
	volume = {19},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2251-x},
	doi = {10.1186/s12859-018-2251-x},
	abstract = {There are different and complicated associations between genes and diseases. Finding the causal associations between genes and specific diseases is still challenging. In this work we present a method to predict novel associations of genes and pathways with inflammatory bowel disease (IBD) by integrating information of differential gene expression, protein-protein interaction and known disease genes related to IBD. We downloaded IBD gene expression data from NCBI’s Gene Expression Omnibus, performed statistical analysis to determine differentially expressed genes, collected known IBD genes from DisGeNet database, which were used to construct a IBD related PPI network with HIPPIE database. We adapted our graph-based clustering algorithm DPClusO to cluster the disease PPI network. We evaluated the statistical significance of the identified clusters in the context of determining the richness of IBD genes using Fisher’s exact test and predicted novel genes related to IBD. We showed 93.8\% of our predictions are correct in the context of other databases and published literatures related to IBD. Finding disease-causing genes is necessary for developing drugs with synergistic effect targeting many genes simultaneously. Here we present an approach to identify novel disease genes and pathways and discuss our approach in the context of IBD. The approach can be generalized to find disease-associated genes for other diseases.},
	number = {1},
	journal = {BMC Bioinformatics},
	author = {Eguchi, Ryohei and Karim, Mohammand Bozlul and Hu, Pingzhao and Sato, Tetsuo and Ono, Naoaki and Kanaya, Shigehiko and Altaf-Ul-Amin, Md.},
	month = dec,
	year = {2018},
	note = {Publisher: BioMed Central},
	keywords = {Algorithms, Bioinformatics, Computational Biology/Bioinformatics, Computer Appl. in Life Sciences, Microarrays},
	pages = {264--264},
}

@article{choi_end--end_2017,
	title = {End-to-{End} {Prediction} of {Buffer} {Overruns} from {Raw} {Source} {Code} via {Neural} {Memory} {Networks}},
	url = {http://arxiv.org/abs/1703.02458},
	abstract = {Detecting buffer overruns from a source code is one of the most common and yet challenging tasks in program analysis. Current approaches have mainly relied on rigid rules and handcrafted features devised by a few experts, limiting themselves in terms of flexible applicability and robustness due to diverse bug patterns and characteristics existing in sophisticated real-world software programs. In this paper, we propose a novel, data-driven approach that is completely end-to-end without requiring any hand-crafted features, thus free from any program language-specific structural limitations. In particular, our approach leverages a recently proposed neural network model called memory networks that have shown the state-of-the-art performances mainly in question-answering tasks. Our experimental results using source codes demonstrate that our proposed model is capable of accurately detecting simple buffer overruns. We also present in-depth analyses on how a memory network can learn to understand the semantics in programming languages solely from raw source codes, such as tracing variables of interest, identifying numerical values, and performing their quantitative comparisons.},
	author = {Choi, Min-je and Jeong, Sehun and Oh, Hakjoo and Choo, Jaegul},
	month = mar,
	year = {2017},
}

@article{weinstein_analyzing_2013,
	title = {Analyzing {Big} {Data} with {Dynamic} {Quantum} {Clustering}},
	url = {http://arxiv.org/abs/1310.2700},
	abstract = {How does one search for a needle in a multi-dimensional haystack without knowing what a needle is and without knowing if there is one in the haystack? This kind of problem requires a paradigm shift - away from hypothesis driven searches of the data - towards a methodology that lets the data speak for itself. Dynamic Quantum Clustering (DQC) is such a methodology. DQC is a powerful visual method that works with big, high-dimensional data. It exploits variations of the density of the data (in feature space) and unearths subsets of the data that exhibit correlations among all the measured variables. The outcome of a DQC analysis is a movie that shows how and why sets of data-points are eventually classified as members of simple clusters or as members of - what we call - extended structures. This allows DQC to be successfully used in a non-conventional exploratory mode where one searches data for unexpected information without the need to model the data. We show how this works for big, complex, real-world datasets that come from five distinct fields: i.e., x-ray nano-chemistry, condensed matter, biology, seismology and finance. These studies show how DQC excels at uncovering unexpected, small - but meaningful - subsets of the data that contain important information. We also establish an important new result: namely, that big, complex datasets often contain interesting structures that will be missed by many conventional clustering techniques. Experience shows that these structures appear frequently enough that it is crucial to know they can exist, and that when they do, they encode important hidden information. In short, we not only demonstrate that DQC can be flexibly applied to datasets that present significantly different challenges, we also show how a simple analysis can be used to look for the needle in the haystack, determine what it is, and find what this means.},
	author = {Weinstein, M. and Meirer, F. and Hume, A. and Sciau, Ph. and Shaked, G. and Hofstetter, R. and Persi, E. and Mehta, A. and Horn, D.},
	month = oct,
	year = {2013},
}

@article{shi_real-time_2016,
	title = {Real-{Time} {Single} {Image} and {Video} {Super}-{Resolution} {Using} an {Efficient} {Sub}-{Pixel} {Convolutional} {Neural} {Network}},
	url = {http://arxiv.org/abs/1609.05158},
	abstract = {Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.},
	author = {Shi, Wenzhe and Caballero, Jose and Huszár, Ferenc and Totz, Johannes and Aitken, Andrew P. and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
	month = sep,
	year = {2016},
}

@article{smith_cyclical_2015,
	title = {Cyclical {Learning} {Rates} for {Training} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1506.01186},
	abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
	author = {Smith, Leslie N.},
	month = jun,
	year = {2015},
	file = {Full Text:/home/zwerg/Zotero/storage/7P68PTBB/Smith - 2015 - Cyclical Learning Rates for Training Neural Networ.pdf:application/pdf},
}

@article{huang_densely_2016,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1608.06993},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and models are available at https://github.com/liuzhuang13/DenseNet .},
	author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q. and van der Maaten, Laurens},
	month = aug,
	year = {2016},
}

@article{ye_shape-based_nodate,
	title = {{SHAPE}-{BASED} {CT} {LUNG} {NODULE} {SEGMENTATION} {USING} {FIVE}-{DIMENSIONAL} {MEAN} {SHIFT} {CLUSTERING} {AND} {MEM} {WITH} {SHAPE} {INFORMATION}},
	url = {http://openaccess.city.ac.uk/pubs/file/29003/NoduleSegmentationISBI2009.pdf},
	abstract = {This paper presents a joint spatial-intensity-shape (JSIS) feature-based method for the segmentation of CT lung nodules. First, a volumetric shape index (SI) feature based on the second-order partial derivatives of the CT image is calculated. Next, the SI feature is combined with spatial and intensity features to form a five-dimensional feature vectors, which are then clustered using mean shift to produce intensity and shape mode maps. Finally, a modified expectation-maximization (MEM) algorithm is applied on the mean shift intensity mode map to merge the neighboring modes with spatial and shape mode maps as priors. The proposed method has been evaluated on a clinical dataset of thoracic CT scans that contains 80 nodules. A volume overlap ratio between each segmented nodule and the ground truth annotation is calculated. Using the proposed method, the mean overlap ratio over all the nodules is 0.81 with standard deviation of 0.05. Most of the nodules, including challenging juxta-vascular and juxta-pleural nodules, can be properly separated from adjoining tissues. Index Terms— Mean shift, mode map, expectation-maximization (EM), lung nodule, shape index, shape prior.},
	author = {Ye, Xujiong and Siddique, Musib and Douiri, Abdel and Beddoe, Gareth and Slabaugh, Greg},
}

@article{henaff_tracking_2016,
	title = {Tracking the {World} {State} with {Recurrent} {Entity} {Networks}},
	url = {http://arxiv.org/abs/1612.03969},
	abstract = {We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass.},
	author = {Henaff, Mikael and Weston, Jason and Szlam, Arthur and Bordes, Antoine and LeCun, Yann},
	month = dec,
	year = {2016},
}

@article{milletari_v-net_2016,
	title = {V-{Net}: {Fully} {Convolutional} {Neural} {Networks} for {Volumetric} {Medical} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1606.04797},
	abstract = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
	author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	month = jun,
	year = {2016},
}

@article{reed_generative_nodate,
	title = {Generative {Adversarial} {Text} to {Image} {Synthesis}},
	url = {https://arxiv.org/pdf/1605.05396.pdf},
	abstract = {Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neu-ral network architectures have been developed to learn discriminative text feature representa-tions. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to gen-erate highly compelling images of specific cat-egories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image model-ing, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.},
	author = {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran REEDSCOT, Lajanugen and Schiele, Bernt and Lee SCHIELE, Honglak},
}

@article{albanie_stopping_2017,
	title = {Stopping {GAN} {Violence}: {Generative} {Unadversarial} {Networks}},
	url = {http://arxiv.org/abs/1703.02528},
	abstract = {While the costs of human violence have attracted a great deal of attention from the research community, the effects of the network-on-network (NoN) violence popularised by Generative Adversarial Networks have yet to be addressed. In this work, we quantify the financial, social, spiritual, cultural, grammatical and dermatological impact of this aggression and address the issue by proposing a more peaceful approach which we term Generative Unadversarial Networks (GUNs). Under this framework, we simultaneously train two models: a generator G that does its best to capture whichever data distribution it feels it can manage, and a motivator M that helps G to achieve its dream. Fighting is strictly verboten and both models evolve by learning to respect their differences. The framework is both theoretically and electrically grounded in game theory, and can be viewed as a winner-shares-all two-player game in which both players work as a team to achieve the best score. Experiments show that by working in harmony, the proposed model is able to claim both the moral and log-likelihood high ground. Our work builds on a rich history of carefully argued position-papers, published as anonymous YouTube comments, which prove that the optimal solution to NoN violence is more GUNs.},
	author = {Albanie, Samuel and Ehrhardt, Sébastien and Henriques, João F.},
	month = mar,
	year = {2017},
}

@article{park_transformation-grounded_2017,
	title = {Transformation-{Grounded} {Image} {Generation} {Network} for {Novel} {3D} {View} {Synthesis}},
	url = {http://arxiv.org/abs/1703.02921},
	abstract = {We present a transformation-grounded image generation network for novel 3D view synthesis from a single image. Instead of taking a 'blank slate' approach, we first explicitly infer the parts of the geometry visible both in the input and novel views and then re-cast the remaining synthesis problem as image completion. Specifically, we both predict a flow to move the pixels from the input to the novel view along with a novel visibility map that helps deal with occulsion/disocculsion. Next, conditioned on those intermediate results, we hallucinate (infer) parts of the object invisible in the input image. In addition to the new network structure, training with a combination of adversarial and perceptual loss results in a reduction in common artifacts of novel view synthesis such as distortions and holes, while successfully generating high frequency details and preserving visual aspects of the input image. We evaluate our approach on a wide range of synthetic and real examples. Both qualitative and quantitative results show our method achieves significantly better results compared to existing methods.},
	author = {Park, Eunbyung and Yang, Jimei and Yumer, Ersin and Ceylan, Duygu and Berg, Alexander C.},
	month = mar,
	year = {2017},
}

@article{shi_real-time_nodate,
	title = {Real-{Time} {Single} {Image} and {Video} {Super}-{Resolution} {Using} an {Efficient} {Sub}-{Pixel} {Convolutional} {Neural} {Network}},
	url = {https://arxiv.org/pdf/1609.05158.pdf},
	abstract = {Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architec-ture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.},
	author = {Shi, Wenzhe and Caballero, Jose and Huszár, Ferenc and Totz, Johannes and Aitken, Andrew P and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
}

@article{li_triple_2017,
	title = {Triple {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1703.02291},
	abstract = {Generative adversarial nets (GANs) are good at generating realistic images and have been extended for semi-supervised classification. However, under a two-player formulation, existing work shares competing roles of identifying fake samples and predicting labels via a single discriminator network, which can lead to undesirable incompatibility. We present triple generative adversarial net (Triple-GAN), a flexible game-theoretical framework for classification and class-conditional generation in semi-supervised learning. Triple-GAN consists of three players - a generator, a discriminator and a classifier, where the generator and classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. With designed utilities, the distributions characterized by the classifier and generator both concentrate to the data distribution under nonparametric assumptions. We further propose unbiased regularization terms to make the classifier and generator strongly coupled and some biased techniques to boost the performance of Triple-GAN in practice. Our results on several datasets demonstrate the promise in semi-supervised learning, where Triple-GAN achieves comparable or superior performance than state-of-the-art classification results among DGMs; it is also able to disentangle the classes and styles and transfer smoothly on the data level via interpolation on the latent space class-conditionally.},
	author = {Li, Chongxuan and Xu, Kun and Zhu, Jun and Zhang, Bo},
	month = mar,
	year = {2017},
}

@article{pinto_robust_2017,
	title = {Robust {Adversarial} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1703.02702},
	abstract = {Deep neural networks coupled with fast simulation and improved computation have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H-infinity control methods, we note that both modeling errors and differences in training and test scenarios can be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced -- that is, it learns an optimal destabilization policy. We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper and Walker2d) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary.},
	author = {Pinto, Lerrel and Davidson, James and Sukthankar, Rahul and Gupta, Abhinav},
	month = mar,
	year = {2017},
}

@article{santurkar_generative_2017,
	title = {Generative {Compression}},
	url = {http://arxiv.org/abs/1703.01467},
	abstract = {Traditional image and video compression algorithms rely on hand-crafted encoder/decoder pairs (codecs) that lack adaptability and are agnostic to the data being compressed. Here we describe the concept of generative compression, the compression of data using generative models, and show its potential to produce more accurate and visually pleasing reconstructions at much deeper compression levels for both image and video data. We also demonstrate that generative compression is orders-of-magnitude more resilient to bit error rates (e.g. from noisy wireless channels) than traditional variable-length entropy coding schemes.},
	author = {Santurkar, Shibani and Budden, David and Shavit, Nir},
	month = mar,
	year = {2017},
}

@article{larsson_fractalnet_2016,
	title = {{FractalNet}: {Ultra}-{Deep} {Neural} {Networks} without {Residuals}},
	url = {http://arxiv.org/abs/1605.07648},
	abstract = {We introduce a design strategy for neural network macro-architecture based on self-similarity. Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals. These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks. Rather, the key may be the ability to transition, during training, from effectively shallow to deep. We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures. Such regularization allows extraction of high-performance fixed-depth subnetworks. Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.},
	author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
	month = may,
	year = {2016},
}

@article{thies_face2face_nodate,
	title = {{Face2Face}: {Real}-time {Face} {Capture} and {Reenactment} of {RGB} {Videos}},
	url = {http://www.graphics.stanford.edu/~niessner/papers/2016/1facetoface/thies2016face.pdf},
	abstract = {Proposed online reenactment setup: a monocular target video sequence (e.g., from Youtube) is reenacted based on the ex-pressions of a source actor who is recorded live with a commodity webcam. Abstract We present a novel approach for real-time facial reenact-ment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular video stream, captured live with a commodity webcam. Our goal is to animate the facial expressions of the target video by a source actor and re-render the manipulated output video in a photo-realistic fashion. To this end, we first address the under-constrained problem of facial identity recovery from monocular video by non-rigid model-based bundling. At run time, we track facial expressions of both source and tar-get video using a dense photometric consistency measure. Reenactment is then achieved by fast and efficient defor-mation transfer between source and target. The mouth inte-rior that best matches the re-targeted expression is retrieved from the target sequence and warped to produce an accu-rate fit. Finally, we convincingly re-render the synthesized target face on top of the corresponding video stream such that it seamlessly blends with the real-world illumination. We demonstrate our method in a live setup, where Youtube videos are reenacted in real time.},
	author = {Thies, Justus and Zollhöfer, Michael and Stamminger, Marc and Theobalt, Christian and Nießner, Matthias},
}

@article{thies_supplemental_nodate,
	title = {Supplemental {Material} for " {Face2Face}: {Real}-time {Face} {Capture} and {Reenactment} of {RGB} {Videos} "},
	url = {http://www.graphics.stanford.edu/~niessner/papers/2016/1facetoface/thies2016face_supplemental.pdf},
	abstract = {In this document, we provide supplementary information to the method by Thies et al. [4]. More specifically, we include additional detail about our optimization framework (see Section 1 and 2), and we show further comparisons against other methods (see Section 3). We also evaluate the reconstruction error in a self-reenactment scenario. In Section 4, a list of used mathematical symbols is given. The used video sources are listed in Table 1. 1. Optimization Framework Our Gauss-Newton optimization framework is based on the work of Thies et al. [3]. Our aim is to include every visible pixel p ∈ V in C S in the optimization process. To this end, we gather all visible pixels in the synthesized im-age using a parallel prefix scan. The computation of the Jacobian J of the residual vector F and the gradient J T F of the energy function are then parallelized across all GPU processors. This parallelization is feasible since all partial derivatives and gradient entries with respect to a variable can be computed independently. During evaluation of the gradient, all components of the Jacobian are computed and stored in global memory. In order to evaluate the gradient, we use a two-stage reduction to sum-up all local per pixel gradients. Finally, we add the regularizer and the sparse feature term to the Jacobian and the gradient. Using the computed Jacobian J and the gradient J T F , we solve the corresponding normal equation J T J∆x = −J T F for the parameter update ∆x using a preconditioned conjugate gradient (PCG) method. We apply a Jacobi pre-conditioner that is precomputed during the evaluation of the gradient. To avoid the high computational cost of J T J, our GPU-based PCG method splits up the computation of J T Jp into two successive matrix-vector products. In order to increase convergence speed and to avoid lo-cal minima, we use a coarse-to-fine hierarchical optimiza-tion strategy. During online tracking, we only consider the second and third level, where we run one and seven Gauss-Newton steps on the respective level. Within a Gauss-Newton step, we always run four PCG iterations. Our complete framework is implemented using DirectX for rendering and DirectCompute for optimization. The joint graphics and compute capability of DirectX11 enables the processing of rendered images by the graphics pipeline without resource mapping overhead. In the case of an analysis-by-synthesis approach like ours, this is essential to runtime performance, since many rendering-to-compute switches are required.},
	author = {Thies, Justus and Zollhöfer, Michael and Stamminger, Marc and Theobalt, Christian and Nießner, Matthias},
}

@article{li_combining_2016,
	title = {Combining {Markov} {Random} {Fields} and {Convolutional} {Neural} {Networks} for {Image} {Synthesis}},
	url = {http://arxiv.org/abs/1601.04589},
	abstract = {This paper studies a combination of generative Markov random field (MRF) models and discriminatively trained deep convolutional neural networks (dCNNs) for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNN feature pyramid, controling the image layout at an abstract level. We apply the method to both photographic and non-photo-realistic (artwork) synthesis tasks. The MRF regularizer prevents over-excitation artifacts and reduces implausible feature mixtures common to previous dCNN inversion approaches, permitting synthezing photographic content with increased visual plausibility. Unlike standard MRF-based texture synthesis, the combined system can both match and adapt local features with considerable variability, yielding results far out of reach of classic generative MRF methods.},
	author = {Li, Chuan and Wand, Michael},
	month = jan,
	year = {2016},
}

@article{zhang_stackgan_2016,
	title = {{StackGAN}: {Text} to {Photo}-realistic {Image} {Synthesis} with {Stacked} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1612.03242},
	abstract = {Synthesizing photo-realistic images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose stacked Generative Adversarial Networks (StackGAN) to generate photo-realistic images conditioned on text descriptions. The Stage-I GAN sketches the primitive shape and basic colors of the object based on the given text description, yielding Stage-I low resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high resolution images with photo-realistic details. The Stage-II GAN is able to rectify defects and add compelling details with the refinement process. Samples generated by StackGAN are more plausible than those generated by existing approaches. Importantly, our StackGAN for the first time generates realistic 256 x 256 images conditioned on only text descriptions, while state-of-the-art methods can generate at most 128 x 128 images. To demonstrate the effectiveness of the proposed StackGAN, extensive experiments are conducted on CUB and Oxford-102 datasets, which contain enough object appearance variations and are widely-used for text-to-image generation analysis.},
	author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Huang, Xiaolei and Wang, Xiaogang and Metaxas, Dimitris},
	month = dec,
	year = {2016},
}

@article{ledig_photo-realistic_2016,
	title = {Photo-{Realistic} {Single} {Image} {Super}-{Resolution} {Using} a {Generative} {Adversarial} {Network}},
	url = {https://arxiv.org/pdf/1609.04802v2.pdf},
	abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper con-volutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? During image downsampling information is lost, making super-resolution a highly ill-posed inverse problem with a large set of possible solutions. The behavior of optimization-based super-resolution methods is therefore principally driven by the choice of objective function. Recent work has largely focussed on minimizing the mean squared re-construction error (MSE). The resulting estimates have high peak signal-to-noise-ratio (PSNR), but they are often overly smoothed, lack high-frequency detail, making them perceptually unsatisfying. In this paper, we present super-resolution generative adversarial network (SRGAN). To our knowledge, it is the first framework capable of recovering photo-realistic natural images from 4× downsampling. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and origi-nal photo-realistic images. In addition, we use a content loss function motivated by perceptual similarity instead of similarity in pixel space. Trained on 350K images using the perceptual loss function, our deep residual network was able to recover photo-realistic textures from heavily downsampled images on public benchmarks.},
	author = {Ledig, Christian and Theis, Lucas and Huszár, Ferenc and Caballero, Jose and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	year = {2016},
}

@article{balog_deepcoder_nodate,
	title = {{DEEPCODER}: {LEARNING} {TO} {WRITE} {PROGRAMS}},
	url = {https://openreview.net/pdf?id=ByldLrqlx},
	abstract = {We develop a first line of attack for solving programming competition-style prob-lems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network's predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites.},
	author = {Balog, Matej and Gaunt, Alexander L and Brockschmidt, Marc and Nowozin, Sebastian and Tarlow, Daniel},
}

@article{johnson_perceptual_nodate,
	title = {Perceptual {Losses} for {Real}-{Time} {Style} {Transfer} and {Super}-{Resolution}: {Supplementary} {Material}},
	url = {https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16Supplementary.pdf},
	abstract = {Our style transfer networks use the architecture shown in Table 1 and our super-resolution networks use the architecture shown in Table 2. In these tables " C × H × W conv " denotes a convolutional layer with C filters size H × W which is immediately followed by spatial batch normalization [1] and a ReLU nonlinearity. Our residual blocks each contain two 3×3 convolutional layers with the same number of filters on both layer. We use the residual block design of Gross and Wilber [2] (shown in Figure 1), which differs from that of He et al [3] in that the ReLU nonlinearity following the addition is removed; this modified design was found in [2] to perform slightly better for image classification. For style transfer, we found that standard zero-padded convolutions resulted in severe artifacts around the borders of the generated image. We therefore remove padding from the convolutions in residual blocks. A 3 × 3 convolution with no padding reduces the size of a feature map by 1 pixel on each side, so in this case the identity connection of the residual block performs a center crop on the input feature map. We also add spatial reflection padding to the beginning of the network so that the input and output of the network have the same size. Layer Activation size Input 3 × 256 × 256 Reflection Padding (40 × 40) 3 × 336 × 336 32 × 9 × 9 conv, stride 1 32 × 336 × 336 64 × 3 × 3 conv, stride 2 64 × 168 × 168 128 × 3 × 3 conv, stride 2 128 × 84 × 84 Residual block, 128 filters 128 × 80 × 80 Residual block, 128 filters 128 × 76 × 76 Residual block, 128 filters 128 × 72 × 72 Residual block, 128 filters 128 × 68 × 68 Residual block, 128 filters 128 × 64 × 64 64 × 3 × 3 conv, stride 1/2 64 × 128 × 128 32 × 3 × 3 conv, stride 1/2 32 × 256 × 256 3 × 9 × 9 conv, stride 1 3 × 256 × 256 Table 1. Network architecture used for style transfer networks.},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
}

@article{novak_improving_2016,
	title = {Improving the {Neural} {Algorithm} of {Artistic} {Style}},
	url = {http://arxiv.org/abs/1605.04603},
	abstract = {In this work we investigate different avenues of improving the Neural Algorithm of Artistic Style (by Leon A. Gatys, Alexander S. Ecker and Matthias Bethge, arXiv:1508.06576). While showing great results when transferring homogeneous and repetitive patterns, the original style representation often fails to capture more complex properties, like having separate styles of foreground and background. This leads to visual artifacts and undesirable textures appearing in unexpected regions when performing style transfer. We tackle this issue with a variety of approaches, mostly by modifying the style representation in order for it to capture more information and impose a tighter constraint on the style transfer result. In our experiments, we subjectively evaluate our best method as producing from barely noticeable to significant improvements in the quality of style transfer.},
	author = {Novak, Roman and Nikulin, Yaroslav},
	month = may,
	year = {2016},
}

@article{shi_is_nodate,
	title = {Is the deconvolution layer the same as a convolutional layer?},
	url = {https://arxiv.org/pdf/1609.07009.pdf},
	abstract = {1 In our CVPR 2016 paper [1], we proposed a novel network architecture to perform single image super­resolution (SR). Most existing convolutional neural network (CNN) based super­resolution methods [10,11] first upsample the image using a bicubic interpolation, then apply a convolutional network. We will refer to these types of networks as high­resolution (HR) networks because the images are upsampled first. Instead, we feed the low­resolution (LR) input directly to a sub­pixel CNN as shown in Fig.1 : Figure 1: An illustration of the ESCPN framework where r denotes the upscaling ratio. Let denote the upscaling ratio ­ e.g if the input LR image is then the output HR image will be r 1 × 1 . We then output number of channels instead of one high­resolution (HR) image and use periodic r × r r 2 shuffling to recreate the HR image. The exact details about how our efficient sub­pixel convolutional layer works can be found in the paper. We will refer to our network as a LR network. In this note, we want to focus on two aspects related to two questions most people asked us at CVPR when they saw this network. Firstly, how can channels magically become a HR image? And secondly, r 2 why are convolution in LR space a better choice? These are actually the key questions we tried to answer in the paper, but we were not able to go into as much depth and clarity as we would've liked given the page limit. To better answer these questions, we first discuss the relationships between the deconvolution layer in the form of the transposed convolution layer, the sub­pixel convolutional layer and our efficient sub­pixel convolutional layer, which we'll go through in Sec. 1 and Sec. 2. We will refer to our efficient sub­pixel convolutional layer as a convolutional layer in LR space to distinguish it from the common sub­pixel convolutional layer [5]. We will then show that for a fixed computational budget and complexity, a network with convolutions exclusively in LR space has more representation power at the same speed than a network that first upsamples the input in HR space.},
	author = {Shi, Wenzhe and Caballero, Jose and Theis, Lucas and Huszar, Ferenc and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Ledig, Christian and Wang, Zehan},
}

@article{dumoulin_guide_2016,
	title = {A guide to convolution arithmetic for deep learning},
	url = {https://arxiv.org/pdf/1603.07285.pdf},
	author = {Dumoulin, Vincent and Visin, Francesco and Box, George E P},
	year = {2016},
}

@article{mao_image_2016,
	title = {Image {Restoration} {Using} {Convolutional} {Auto}-encoders with {Symmetric} {Skip} {Connections}},
	url = {http://arxiv.org/abs/1606.08921},
	abstract = {Image restoration, including image denoising, super resolution, inpainting, and so on, is a well-studied problem in computer vision and image processing, as well as a test bed for low-level image modeling algorithms. In this work, we propose a very deep fully convolutional auto-encoder network for image restoration, which is a encoding-decoding framework with symmetric convolutional-deconvolutional layers. In other words, the network is composed of multiple layers of convolution and de-convolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers capture the abstraction of image contents while eliminating corruptions. Deconvolutional layers have the capability to upsample the feature maps and recover the image details. To deal with the problem that deeper networks tend to be more difficult to train, we propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains better results.},
	author = {Mao, Xiao-Jiao and Shen, Chunhua and Yang, Yu-Bin},
	month = jun,
	year = {2016},
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
}

@article{openai_nips_nodate,
	title = {{NIPS} 2016 {Tutorial}: {Generative} {Adversarial} {Networks}},
	url = {https://arxiv.org/pdf/1701.00160.pdf},
	abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
	author = {Openai, Ian Goodfellow},
}

@article{antipov_face_nodate,
	title = {{FACE} {AGING} {WITH} {CONDITIONAL} {GENERATIVE} {ADVERSARIAL} {NETWORKS}},
	url = {https://arxiv.org/pdf/1702.01983.pdf},
	abstract = {It has been recently shown that Generative Adversarial Networks (GANs) can produce synthetic images of excep-tional visual fidelity. In this work, we propose the GAN-based method for automatic face aging. Contrary to previous works employing GANs for altering of facial attributes, we make a particular emphasize on preserving the original person's iden-tity in the aged version of his/her face. To this end, we intro-duce a novel approach for " Identity-Preserving " optimization of GAN's latent vectors. The objective evaluation of the re-sulting aged and rejuvenated face images by the state-of-the-art face recognition and age estimation solutions demonstrate the high potential of the proposed method.},
	author = {Antipov, Grigory and Baccouche, Moez and Dugelay, Jean-Luc},
	keywords = {Deep Learning, Face Synthesis, GAN, Index Terms— Face Aging},
}

@article{kingma_auto-encoding_2013,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	author = {Kingma, Diederik P and Welling, Max},
	month = dec,
	year = {2013},
}

@article{im_generating_2016,
	title = {Generating images with recurrent adversarial networks},
	url = {http://arxiv.org/abs/1602.05110},
	abstract = {Gatys et al. (2015) showed that optimizing pixels to match features in a convolutional network with respect reference image features is a way to render images of high visual quality. We show that unrolling this gradient-based optimization yields a recurrent computation that creates images by incrementally adding onto a visual "canvas". We propose a recurrent generative model inspired by this view, and show that it can be trained using adversarial training to generate very good image samples. We also propose a way to quantitatively compare adversarial networks by having the generators and discriminators of these networks compete against each other.},
	author = {Im, Daniel Jiwoong and Kim, Chris Dongjoo and Jiang, Hui and Memisevic, Roland},
	month = feb,
	year = {2016},
}

@article{de_brebisson_artificial_2015,
	title = {Artificial {Neural} {Networks} {Applied} to {Taxi} {Destination} {Prediction}},
	url = {http://arxiv.org/abs/1508.00021},
	abstract = {We describe our first-place solution to the ECML/PKDD discovery challenge on taxi destination prediction. The task consisted in predicting the destination of a taxi based on the beginning of its trajectory, represented as a variable-length sequence of GPS points, and diverse associated meta-information, such as the departure time, the driver id and client information. Contrary to most published competitor approaches, we used an almost fully automated approach based on neural networks and we ranked first out of 381 teams. The architectures we tried use multi-layer perceptrons, bidirectional recurrent neural networks and models inspired from recently introduced memory networks. Our approach could easily be adapted to other applications in which the goal is to predict a fixed-length output from a variable-length sequence.},
	author = {de Brébisson, Alexandre and Simon, Étienne and Auvolat, Alex and Vincent, Pascal and Bengio, Yoshua},
	month = jul,
	year = {2015},
}

@article{gatys_texture_2015,
	title = {Texture {Synthesis} {Using} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.07376},
	abstract = {Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	month = may,
	year = {2015},
}

@article{oord_wavenet_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
}

@article{kingma_adam_2015,
	title = {Adam: a {Method} for {Stochastic} {Optimization}},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
	journal = {International Conference on Learning Representations 2015},
	author = {Kingma, Diederik P. and Ba, Jimmy Lei},
	year = {2015},
	pages = {1--15},
}


@article{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	issn = {9781627480031},
	url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	doi = {10.1109/5.726791},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0 \% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2 \% achieved by the second-best entry. 1},
	journal = {Advances in Neural Information Processing Systems 25 (NIPS2012)},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Geoffrey E., Hinton},
	year = {2012},
	pages = {1--9--1--9},
}

@article{noauthor_ronneberger_u-net_segmentation_1505_nodate,
	title = {Ronneberger\_U-{Net}\_Segmentation\_1505},
}

@article{wang_recurrent_2013,
	title = {Recurrent {Neural} {Network}},
	url = {http://www.scholarpedia.org/article/Recurrent_neural_networks},
	author = {Wang, Xiaogang},
	year = {2013},
	pages = {1--9},
}

@article{ren_faster_2015,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	issn = {0162-8828 VO - PP},
	doi = {10.1016/j.nima.2015.05.028},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region pro-posal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolu-tional features. For the very deep VGG-16 model [18], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. The code will be released.},
	journal = {Nips},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = {2015},
	pages = {1--10},
}

@article{ghosh_quicknet_2017,
	title = {{QuickNet}: {Maximizing} {Efficiency} and {Efficacy} in {Deep} {Architectures}},
	url = {http://arxiv.org/abs/1701.02291},
	abstract = {We present QuickNet, a fast and accurate network architecture that is both faster and significantly more accurate than other fast deep architectures like SqueezeNet. Furthermore, it uses less parameters than previous networks, making it more memory efficient. We do this by making two major modifications to the reference Darknet model (Redmon et al, 2015): 1) The use of depthwise separable convolutions and 2) The use of parametric rectified linear units. We make the observation that parametric rectified linear units are computationally equivalent to leaky rectified linear units at test time and the observation that separable convolutions can be interpreted as a compressed Inception network (Chollet, 2016). Using these observations, we derive a network architecture, which we call QuickNet, that is both faster and more accurate than previous models. Our architecture provides at least four major advantages: (1) A smaller model size, which is more tenable on memory constrained systems; (2) A significantly faster network which is more tenable on computationally constrained systems; (3) A high accuracy of 95.7 percent on the CIFAR-10 Dataset which outperforms all but one result published so far, although we note that our works are orthogonal approaches and can be combined (4) Orthogonality to previous model compression approaches allowing for further speed gains to be realized.},
	author = {Ghosh, Tapabrata},
	year = {2017},
}

@article{fu_deep_2016,
	title = {Deep {Learning} with {INT8} {Optimization} on {Xilinx} {Devices} {White} {Paper} ({WP485})},
	volume = {486},
	url = {www.xilinx.com},
	abstract = {Xilinx INT8 optimization provide the best performance and most power efficient computational techniques for deep learning inference. Xilinx's integrated DSP architecture can achieve 1.75X solution-level performance at INT8 deep learning operations than other FPGA DSP architectures. ABSTRACT The intent of this white paper is to explore INT8 deep learning operations implemented on the Xilinx DSP48E2 slice, and how this contrasts with other FPGAs. With INT8, Xilinx's DSP architecture can achieve 1.75X peak solution-level performance at INT8 deep learning operation per second (OPS) compared to other FPGAs with the same resource count. As deep learning inference exploits lower bit precision without sacrificing accuracy, efficient INT8 implementations are needed. Xilinx's DSP architecture and libraries are optimized for INT8 deep learning inference. This white paper describes how the DSP48E2 slice in Xilinx's UltraScale and UltraScale+ FPGAs can be used to process two concurrent INT8 multiply and accumulate (MACC) operations while sharing the same kernel weights. It also explains why 24-bit is the minimal size for an input to utilize this technique, which is unique to Xilinx. The white paper also includes an example of this INT8 optimization technique to show its relevance by revisiting the fundamental operations of neural networks WP486 (v1.0) November 11, 2016 www.xilinx.com 2 Deep Learning with INT8 Optimization on Xilinx Devices INT8 for Deep Learning Deep neural networks have propelled an evolution in machine learning fields and redefined many existing applications with new human-level AI capabilities. While more accurate deep learning models have been developed, their complexity is accompanied by high compute and memory bandwidth challenges. Power efficiency is driving innovation in developing new deep learning inference models that require lower compute intensity and memory bandwidth but must not be at the cost of accuracy and throughput. Reducing this overheard will ultimately increase power efficiency and lower the total power required. In addition to saving power during computation, lower bit-width compute also lowers the power needed for memory bandwidth, because fewer bits are transferred with the same amount of memory transactions.},
	author = {Fu, Yao and Wu, Ephrem and Sirasao, Ashish and Attia, Sedny and Khan, Kamran and Wittig, Ralph},
	year = {2016},
	keywords = {Deep Learning, DSP48, INT8, UltraScale, UltraScale+, wp486},
	pages = {1--11},
}

@article{wang_learning_2014,
	title = {Learning fine-grained image similarity with deep ranking},
	issn = {9781479951178},
	doi = {10.1109/CVPR.2014.180},
	abstract = {Learning fine-grained image similarity is a challenging task. It needs to capture between-class and within-class image differences. This paper proposes a deep ranking model that employs deep learning techniques to learn similarity metric directly from images.It has higher learning capability than models based on hand-crafted features. A novel multiscale network structure has been developed to describe the images effectively. An efficient triplet sampling algorithm is proposed to learn the model with distributed asynchronized stochastic gradient. Extensive experiments show that the proposed algorithm outperforms models based on hand-crafted visual features and deep classification models.},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Wang, Jiang and Song, Yang and Leung, Thomas and Rosenberg, Chuck and Wang, Jingbin and Philbin, James and Chen, Bo and Wu, Ying},
	year = {2014},
	pages = {1386--1393},
}

@article{eck_first_2002,
	title = {A {First} {Look} at {Music} {Composition} using {LSTM} {Recurrent} {Neural} {Networks}},
	url = {http://www.idsia.ch/~juergen/blues/IDSIA-07-02.pdf},
	abstract = {In general music composed by recurrent neural networks (RNNs) suffers from a lack of global structure. Though networks can learn note-by-note transition probabilities and even reproduce phrases, attempts at learning an entire musical form and using that knowledge to guide composition have been unsuccessful. The reason for this failure seems to be that RNNs cannot keep track of temporally distant events that indicate global music structure. Long Short-Term Memory (LSTM) has succeeded in similar domains where other RNNs have failed, such as timing \& counting and CSL learning. In the current study we show that LSTM is also a good mechanism for learning to compose music. We compare this approach to previous attempts, with particular focus on issues of data representation. We present experimental results showing that LSTM successfully learns a form of blues music and is able to compose novel (and we believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure it does not drift from it: LSTM is able to play the blues with good timing and proper structure as long as one is willing to listen.},
	journal = {Idsia},
	author = {Eck, Douglas and Schmidhuber, Jürgen},
	year = {2002},
}

@article{ducoffe_qbdc_2015,
	title = {{QBDC}: {Query} by dropout committee for training deep supervised architecture},
	url = {http://arxiv.org/abs/1511.06412},
	abstract = {While the current trend is to increase the depth of neural networks to increase their performance, the size of their training database has to grow accordingly. We notice an emergence of tremendous databases, although providing labels to build a training set still remains a very expensive task. We tackle the problem of selecting the samples to be labelled in an online fashion. In this paper, we present an active learning strategy based on query by committee and dropout technique to train a Convolutional Neural Network (CNN). We derive a commmittee of partial CNNs resulting from batchwise dropout runs on the initial CNN. We evaluate our active learning strategy for CNN on MNIST benchmark, showing in particular that selecting less than 30 \% from the annotated database is enough to get similar error rate as using the full training set on MNIST. We also studied the robustness of our method against adversarial examples.},
	number = {1998},
	author = {Ducoffe, Melanie and Precioso, Frederic},
	year = {2015},
	pages = {1--10},
}

@article{gatys_neural_2015,
	title = {A {Neural} {Algorithm} of {Artistic} {Style}},
	issn = {2200000006},
	doi = {10.1561/2200000006},
	abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the con- tent and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. How- ever, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks.1,2 Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to sepa- rate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the strik- ing similarities between performance-optimised artificial neural networks and biological vision,3–7 our work offers a path forward to an algorithmic under- standing of how humans create and perceive artistic imagery.},
	journal = {arXiv preprint},
	author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
	year = {2015},
	keywords = {eural algorithm of artistic, style},
	pages = {3--7},
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	issn = {9781450341448},
	url = {http://arxiv.org/abs/1409.1556},
	doi = {10.1016/j.infsof.2008.09.005},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	journal = {International Conference on Learning Representations (ICRL)},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2015},
	pages = {1--14},
}

@article{numbers_n_2004,
	title = {N , {M} )},
	volume = {0},
	author = {Numbers, Quantum},
	year = {2004},
	pages = {53--66},
}

@article{liu_ssd_nodate,
	title = {{SSD} : {Single} {Shot} {MultiBox} {Detector}},
	issn = {9781479951178},
	doi = {10.1016/j.nima.2015.05.028},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-yang and Berg, Alexander C},
	keywords = {convolutional neural network, real-time object detection},
	pages = {1--15},
}

@article{iandola_squeezenet_2016,
	title = {{SqueezeNet}: {AlexNet}-{Level} {Accuracy} with 50x {Fewer} {Parameters} and {\textless}{1MB} {Model} {Size}},
	issn = {978-3-319-24552-2},
	url = {http://arxiv.org/abs/1602.07360},
	doi = {10.1007/978-3-319-24553-9},
	abstract = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 1MB (461x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet},
	journal = {arXiv},
	author = {Iandola, Forrest N. and Moskewicz, Matthew W. and Ashraf, Khalid and Han, Song and Dally, William J. and Keutzer, Kurt},
	year = {2016},
	pages = {1--5},
}

@article{jaderberg_spatial_2015,
	title = {Spatial {Transformer} {Networks}},
	issn = {9781627480031},
	doi = {10.1038/nbt.3343},
	abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
	journal = {Nips},
	author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
	year = {2015},
	pages = {1--14},
}

@article{wang_backward_2016,
	title = {A backward pass through a {CNN} using a generative model of its activations},
	url = {http://arxiv.org/abs/1611.02767},
	abstract = {Neural networks have shown to be a practical way of building a very complex mapping between a pre-specified input space and output space. For example, a convolutional neural network (CNN) mapping an image into one of a thousand object labels is approaching human performance in this particular task. However the mapping (neural network) does not automatically lend itself to other forms of queries, for example, to detect/reconstruct object instances, to enforce top-down signal on ambiguous inputs, or to recover object instances from occlusion. One way to address these queries is a backward pass through the network that fuses top-down and bottom-up information. In this paper, we show a way of building such a backward pass by defining a generative model of the neural network's activations. Approximate inference of the model would naturally take the form of a backward pass through the CNN layers, and it addresses the aforementioned queries in a unified framework.},
	number = {2014},
	author = {Wang, Huayan and Chen, Anna and Liu, Yi and George, Dileep and Phoenix, D. Scott},
	year = {2016},
	pages = {1--11},
}

@article{attenberg_inactive_2011,
	title = {Inactive learning?: difficulties employing active learning in practice},
	volume = {12},
	url = {http://portal.acm.org/citation.cfm?doid=1964897.1964906%5Cnpapers3://publication/doi/10.1145/1964897.1964906},
	doi = {10.1145/1964897.1964906},
	abstract = {Page 1. Inactive Learning? Difficulties Employing Active Learning in Practice Josh Attenberg NYU Polytechnic Institute Brooklyn, NY, 11201 josh@cis.poly.edu Foster Provost NYU Stern School of Business New York, NY, 10012 fprovost@stern.nyu.edu ABSTRACT ... {\textbackslash}n},
	number = {2},
	journal = {ACM SIGKDD Explorations Newsletter},
	author = {Attenberg, Josh and Provost, Foster},
	year = {2011},
	keywords = {active learning},
	pages = {36--41},
}

@article{cao_practical_2015,
	title = {A practical theory for designing very deep convolutional neural networks},
	abstract = {Going deep is essential for deep learning. However it is not easy, there are many ways of going deep but most of them are ineffective. In this work, we propose two novel constrains in the design of deep structure to guarantee the performance gain when going deep. Firstly, for each con-volutional layer, its capacity of learning more complex pat-terns should be guaranteed; Secondly, the receptive field of the topmost layer should be no larger than the image region. Given these two constrains, we cast the task of designing deep convolutional neural network into a constrained opti-mization problem. We present an analytic optimal solution under certain conditions.},
	author = {Cao, Xudong},
	year = {2015},
	pages = {1--6},
}

@article{simpson_deep_2015,
	title = {Deep karaoke: {Extracting} vocals from musical mixtures using a convolutional deep neural network},
	volume = {9237},
	issn = {9783319224817},
	doi = {10.1007/978-3-319-22482-4_50},
	abstract = {Identification and extraction of singing voice from within musical mixtures is a key challenge in source separation and machine audition. Recently, deep neural networks (DNN) have been used to estimate 'ideal' binary masks for carefully controlled cocktail party speech separation problems. However, it is not yet known whether these methods are capable of generalizing to the discrimination of voice and non-voice in the context of musical mixtures. Here, we trained a convolutional DNN (of around a billion parameters) to provide probabilistic estimates of the ideal binary mask for separation of vocal sounds from real-world musical mixtures. We contrast our DNN results with more traditional linear methods. Our approach may be useful for automatic removal of vocal sounds from musical mixtures for 'karaoke' type applications. Index},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Simpson, Andrew J R and Roma, Gerard and Plumbley, Mark D.},
	year = {2015},
	keywords = {Deep learning, Convolution, Source separation, Supervised learning},
	pages = {429--436},
}

@article{zeng_convolutional_2016,
	title = {Convolutional neural network architectures for predicting {DNA}-protein binding},
	volume = {32},
	doi = {10.1093/bioinformatics/btw255},
	abstract = {Motivation: Convolutional neural networks (CNN) have outperformed conventional methods in modeling the sequence specificity of DNA–protein binding. Yet inappropriate CNN architectures can yield poorer performance than simpler models. Thus an in-depth understanding of how to match CNN architecture to a given task is needed to fully harness the power of CNNs for computational biology applications.Results: We present a systematic exploration of CNN architectures for predicting DNA sequence binding using a large compendium of transcription factor datasets. We identify the best-performing architectures by varying CNN width, depth and pooling designs. We find that adding convolutional kernels to a network is important for motif-based tasks. We show the benefits of CNNs in learning rich higher-order sequence features, such as secondary motifs and local sequence context, by comparing network performance on multiple modeling tasks ranging in difficulty. We also demonstrate how careful construction of sequence benchmark datasets, using approaches that control potentially confounding effects like positional or motif strength bias, is critical in making fair comparisons between competing methods. We explore how to establish the sufficiency of training data for these learning tasks, and we have created a flexible cloud-based framework that permits the rapid exploration of alternative neural network architectures for problems in computational biology.Availability and Implementation: All the models analyzed are available at http://cnn.csail.mit.edu.Contact: gifford@mit.eduSupplementary information: Supplementary data are available at Bioinformatics online.},
	number = {12},
	journal = {Bioinformatics},
	author = {Zeng, Haoyang and Edwards, Matthew D. and Liu, Ge and Gifford, David K.},
	year = {2016},
	pages = {i121--i127},
}

@article{he_spatial_2015,
	title = {Spatial {Pyramid} {Pooling} in {Deep} {Convolutional} {Networks} for {Visual} {Recognition}},
	volume = {37},
	issn = {9783319105772},
	doi = {10.1109/TPAMI.2015.2389824},
	abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224x224) input image. This requirement is "artificial" and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101. The power of SPP-net is more significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method computes convolutional features 30-170x faster than the recent leading method R-CNN (and 24-64x faster overall), while achieving better or comparable accuracy on Pascal VOC 2007.},
	number = {9},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
	keywords = {Convolutional Neural Networks, Image Classification, Object Detection, Spatial Pyramid Pooling},
	pages = {1904--1916},
}

@article{dong_image_2016,
	title = {Image {Super}-{Resolution} {Using} {Deep} {Convolutional} {Networks}},
	volume = {38},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2015.2439281},
	abstract = {We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparsecoding- based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.},
	number = {2},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
	year = {2016},
	keywords = {deep convolutional neural networks, sparse coding, Super-resolution},
	pages = {295--307},
}

@article{tveit_deeplearningkit-open_2014,
	title = {{DeepLearningKit}-an {Open} {Source} {Deep} {Learning} {Framework} for {Apple}'s {iOS}, {OS} {X} and {tvOS} developed in {Metal} and {Swift}},
	url = {http://deeplearningkit.org/wp-content/uploads/2016/01/DeepLearningKitPaper.pdf%5Cnpapers3://publication/uuid/48CB437D-3178-4723-BBAA-88D550FE7110},
	abstract = {... per night), some Deep  Learning Models can take weeks of training on GPUs like the ... Have done a presentation of DeepLearningKit  GPU accelerated Deep  Learning for Metal / Swift . ... at http:// deeplearningkit .org/tutorials-for- ios - os - x -and- tvos /tutorial-using- deeplearningkit -with- ios  ... {\textbackslash}n},
	journal = {Deeplearningkit.Org},
	author = {Tveit, A and Morland, T and Røst, T B},
	year = {2014},
	keywords = {and swift, learning framework for apple, optimized deep, os x and, plearningkit - an gpu, s ios, tvos developed in metal},
	pages = {1--9},
}

@article{tutorial_convolutional_2016,
	title = {Convolutional {Neural} {Network}},
	url = {http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/},
	author = {Tutorial, UFLDL},
	year = {2016},
	pages = {1--21},
}

@article{szegedy_going_2015,
	title = {Going deeper with convolutions},
	volume = {07-12-June},
	issn = {9781467369640},
	doi = {10.1109/CVPR.2015.7298594},
	abstract = {Abstract We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	year = {2015},
	pages = {1--9},
}

@article{zamoshchin_alex_2012,
	title = {Alex {Zamoshchin} (alexzam), {Jonathan} {Gold} (johngold)},
	author = {Zamoshchin, Alex and Gold, Jonathan},
	year = {2012},
	pages = {1--8},
}

@article{lou_generative_2016,
	title = {Generative {Shape} {Models}: {Joint} {Text} {Recognition} and {Segmentation} with {Very} {Little} {Training} {Data}},
	abstract = {We demonstrate that a generative model for object shapes can achieve state of the art results on challenging scene text recognition tasks, and with orders of magnitude fewer training images than required for competing discriminative methods. In addition to transcribing text from challenging images, our method performs fine-grained instance segmentation of characters. We show that our model is more robust to both affine transformations and non-affine deformations compared to previous approaches.},
	number = {Nips 2016},
	author = {Lou, Xinghua and Kansky, Ken and Lehrach, Wolfgang and Laan, CC and Marthi, Bhaskara and Phoenix, D. Scott and George, Dileep},
	year = {2016},
}

@article{li_deep_2014,
	title = {Deep {Learning} for {Image} {Denoising}},
	volume = {7},
	issn = {9781467384933},
	url = {http://www.sersc.org/journals/IJSIP/vol7_no3/14.pdf},
	doi = {10.1109/ICDMW.2015.121},
	number = {3},
	author = {Li, HM},
	year = {2014},
	keywords = {deep learning, denoising auto-encoder, image denoising},
	pages = {171--180},
}

@article{dumoulin_guide_2016-1,
	title = {A guide to convolution arithmetic for deep learning},
	issn = {9783319105895},
	url = {http://arxiv.org/abs/1603.07285},
	doi = {10.1051/0004-6361/201527329},
	abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
	journal = {Arxiv},
	author = {Dumoulin, Vincent and Visin, Francesco},
	year = {2016},
	pages = {1--28},
}

@article{chollet_deep_2016,
	title = {Deep {Learning} with {Separable} {Convolutions}},
	url = {https://arxiv.org/abs/1610.02357},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the {\textbackslash}textit\{depthwise separable convolution\} operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameter as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
	journal = {arXiv preprint arXiv:1610.02357},
	author = {Chollet, François},
	year = {2016},
	pages = {1--14},
}

@article{analysis_gpu-based_2015,
	title = {{GPU}-{Based} {Deep} {Learning} {Inference} : {A} {Performance} and {Power} {Analysis}},
	url = {https://www.nvidia.com/content/tegra/embedded-systems/pdf/jetson_tx1_whitepaper.pdf},
	abstract = {Deep learning methods are revolutionizing various areas of machine perception. On a high level, working with deep neural networks is a two-stage process: First, a neural network is trained, i.e. its parameters are determined using labeled examples of inputs and desired output. Then, the network is deployed to run inference, using its previously trained parameters to classify, recognize, and generally process unknown inputs. It is widely recognized within academia and industry that GPUs are the state of the art in training deep neural networks, due to both speed and energy efficiency advantages compared to more traditional CPU-based platforms. In this whitepaper, we take the next step and investigate GPU performance and energy efficiency for deep learning inference. We compare two standard deep learning frameworks, Caffe and Intel’s Deep Learning Framework (IDLF), running on four publicly available hardware platforms, an NVIDIA Jetson™ TX1 developer kit, an NVIDIA GeForce GTX™ Titan X, an Intel Core™ i7 6700K, and an Intel Xeon™ E5- 2698 v3. Our results show that GPUs provide state-of-the-art inference performance and energy efficiency, making them the platform of choice for anyone wanting to deploy a trained neural network in the field. In particular, the Titan X delivers between 5.3 and 6.7 times higher performance than the 16-core Xeon E5 CPU while achieving 3.6 to 4.4 times higher energy efficiency. The Tegra X1 also achieves impressive results, achieving similar performance (258 vs. 242 images/second) with much higher energy efficiency (45.0 vs. 3.9 images/second/Watt) than the state-of-the-art Intel Core i7 6700K.},
	number = {November},
	author = {Analysis, Power},
	year = {2015},
	pages = {1--12},
}

@article{deng_reduced-precision_2015,
	title = {Reduced-{Precision} {Memory} {Value} {Approximation} for {Deep} {Learning}},
	url = {http://www.labs.hpe.com/techreports/2015/HPL-2015-100.html},
	abstract = {Neural networks (NNs) and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. As the complexity of the NNs increases over the years, the performance of them are highly limited by the hardware resources. We identified that memory capacity and memory bandwidth are bottlenecks for highly parallel and high-performance deep NN implementations. In this work, we propose reduced-precision memory value approximation to reduce the required memory bandwidth. Our design highlights an approximator in the memory controller for reduced-precision access to the weight parameters. Our results show that reduced precision to 8-bit representation for a random 20\% of the whole set of weights causes only 1\% of accuracy loss in the object classification tasks on ImageNet. Another important observation is that the error-resilience of weight parameters varies from layer to layer, with fully connected layers being more prone to the reduced precision approximation. Our proposal is expected to achieve significant memory bandwidth savings with less than 1\% loss of accuracy.},
	author = {Deng, Zhaoxia and Xu, Cong and Cai, Qiong and Faraboschi, Paolo},
	year = {2015},
	keywords = {deep learning, convolutional neural networks, memory, reduced precision floating point, s},
}

@article{lazaro-gredilla_hierarchical_2016,
	title = {Hierarchical compositional feature learning},
	url = {http://arxiv.org/abs/1611.02252},
	abstract = {We introduce the hierarchical compositional network (HCN), a directed generative model able to discover and disentangle, without supervision, the building blocks of a set of binary images. The building blocks are binary features defined hierarchically as a composition of some of the features in the layer immediately below, arranged in a particular manner. At a high level, HCN is similar to a sigmoid belief network with pooling. Inference and learning in HCN are very challenging and existing variational approximations do not work satisfactorily. A main contribution of this work is to show that both can be addressed using max-product message passing (MPMP) with a particular schedule (no EM required). Also, using MPMP as an inference engine for HCN makes new tasks simple: adding supervision information, classifying images, or performing inpainting all correspond to clamping some variables of the model to their known values and running MPMP on the rest. When used for classification, fast inference with HCN has exactly the same functional form as a convolutional neural network (CNN) with linear activations and binary weights. However, HCN's features are qualitatively very different.},
	author = {Lázaro-Gredilla, Miguel and Liu, Yi and Phoenix, D. Scott and George, Dileep},
	year = {2016},
	pages = {1--18},
}

@article{gupta_deep_2015,
	title = {Deep {Learning} with {Limited} {Numerical} {Precision}},
	volume = {37},
	issn = {9781510810587},
	url = {http://jmlr.org/proceedings/papers/v37/gupta15.pdf},
	doi = {10.1109/72.80206},
	abstract = {Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited preci- sion data representation and computation on neu- ral network training. Within the context of low- precision fixed-point computations, we observe the rounding scheme to play a crucial role in de- termining the network’s behavior during train- ing. Our results show that deep networks can be trained using only 16-bit wide fixed-point num- ber representation when using stochastic round- ing, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that imple- ments low-precision fixed-point arithmetic with stochastic rounding.},
	journal = {Proceedings of the 32nd International Conference on Machine Learning (ICML-15)},
	author = {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
	year = {2015},
	pages = {1737--1746},
}

@article{humphrey_feature_2013,
	title = {Feature learning and deep architectures: {New} directions for music informatics},
	volume = {41},
	issn = {0925-9902},
	doi = {10.1007/s10844-013-0248-5},
	abstract = {As we look to advance the state of the art in content-based music informatics, there is a general sense that progress is decelerating throughout the field. On closer inspection, performance trajectories across several applications reveal that this is indeed the case, raising some difficult questions for the discipline: why are we slowing down, and what can we do about it? Here, we strive to address both of these concerns. First, we critically review the standard approach to music signal analysis and identify three specific deficiencies to current methods: hand-crafted feature design is sub-optimal and unsustainable, the power of shallow architectures is fundamentally limited, and short-time analysis cannot encode musically meaningful structure. Acknowledging breakthroughs in other perceptual AI domains, we offer that deep learning holds the potential to overcome each of these obstacles. Through conceptual arguments for feature learning and deeper processing architectures, we demonstrate how deep processing models are more powerful extensions of current methods, and why now is the time for this paradigm shift. Finally, we conclude with a discussion of current challenges and the potential impact to further motivate an exploration of this promising research area.},
	number = {3},
	journal = {Journal of Intelligent Information Systems},
	author = {Humphrey, Eric J. and Bello, Juan P. and Lecun, Yann},
	year = {2013},
	keywords = {Deep learning, Music informatics, Signal processing},
	pages = {461--481},
}

@article{meng_two-bit_2017,
	title = {Two-{Bit} {Networks} for {Deep} {Learning} on {Resource}-{Constrained} {Embedded} {Devices}},
	url = {http://arxiv.org/abs/1701.00485},
	abstract = {With the rapid proliferation of Internet of Things and intelligent edge devices, there is an increasing need for implementing machine learning algorithms, including deep learning, on resource-constrained mobile embedded devices with limited memory and computation power. Typical large Convolutional Neural Networks (CNNs) need large amounts of memory and computational power, and cannot be deployed on embedded devices efficiently. We present Two-Bit Networks (TBNs) for model compression of CNNs with edge weights constrained to (-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the memory usage and improve computational efficiency significantly while achieving good performance in terms of classification accuracy, thus representing a reasonable tradeoff between model size and performance.},
	number = {Line 3},
	author = {Meng, Wenjia and Gu, Zonghua and Zhang, Ming and Wu, Zhaohui},
	year = {2017},
	pages = {8--9},
}

@article{szegedy_inception-v4_2016,
	title = {Inception-v4, {Inception}-{ResNet} and the {Impact} of {Residual} {Connections} on {Learning}},
	abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at rel-atively low computational cost. Recently, the introduction of residual connections in conjunction with a more tradi-tional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Incep-tion networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These varia-tions improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We fur-ther demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08\% top-5 error on the test set of the ImageNet classification (CLS) challenge.},
	journal = {Arxiv},
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent},
	year = {2016},
	pages = {12--12},
}

@article{szegedy_rethinking_2016,
	title = {Rethinking the {Inception} {Architecture} for {Computer} {Vision}},
	issn = {9781617796029},
	url = {http://arxiv.org/abs/1512.00567%5Cnhttp://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html},
	doi = {10.1002/2014GB005021},
	abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible. We benchmark our methods on the ILSVRC 2012 classification challenge validation set and demonstrate substantial gains over the state of the art via to carefully factorized convolutions and aggressive regularization: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters.},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
	year = {2016},
	pages = {2818--2826},
}

@article{huang_speedaccuracy_2016,
	title = {Speed/accuracy trade-offs for modern convolutional object detectors},
	abstract = {In this paper, we study the trade-off between accuracy and speed when building an object detection system based on convolutional neural networks. We consider three main families of detectors — Faster R-CNN, R-FCN and SSD — which we view as " meta-architectures " . Each of these can be combined with different kinds of feature extractors, such as VGG, Inception or ResNet. In addition, we can vary other parameters, such as the image resolution, and the number of box proposals. We develop a unified framework (in Tensorflow) that enables us to perform a fair comparison between all of these variants. We analyze the performance of many different previously published model combinations, as well as some novel ones, and thus identify a set of models which achieve different points on the speed-accuracy trade-off curve, ranging from fast models, suitable for use on a mobile phone, to a much slower model that achieves a new state of the art on the COCO detection challenge.},
	journal = {arXiv},
	author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin and Research, Google},
	year = {2016},
}

@article{li_fully_2016,
	title = {Fully {Convolutional} {Instance}-aware {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1611.07709},
	abstract = {We present the first fully convolutional end-to-end solution for instance-aware semantic segmentation task. It inherits all the merits of FCNs for semantic segmentation and instance mask proposal. It performs instance mask prediction and classification jointly. The underlying convolutional representation is fully shared between the two sub-tasks, as well as between all regions of interest. The proposed network is highly integrated and achieves state-of-the-art performance in both accuracy and efficiency. It wins the COCO 2016 segmentation competition by a large margin. The code would be released at {\textbackslash}url\{https://github.com/daijifeng001/TA-FCN\}.},
	journal = {arXiv preprint},
	author = {Li, Yi and Qi, Haozhi and Dai, Jifeng and Ji, Xiangyang and Wei, Yichen},
	year = {2016},
}

@article{zeiler_visualizing_2014,
	title = {Visualizing and {Understanding} {Convolutional} {Networks} {arXiv}:1311.2901v3 [cs.{CV}] 28 {Nov} 2013},
	volume = {8689},
	issn = {978-3-319-10589-5},
	url = {http://link.springer.com/10.1007/978-3-319-10590-1_53%5Cnhttp://arxiv.org/abs/1311.2901%5Cnpapers3://publication/uuid/44feb4b1-873a-4443-8baa-1730ecd16291},
	doi = {10.1007/978-3-319-10590-1_53},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	journal = {Computer Vision–ECCV 2014},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	year = {2014},
	pages = {818--833},
}

@article{long_fully_2015,
	title = {Fully convolutional networks for semantic segmentation},
	volume = {07-12-June},
	issn = {9781467369640},
	doi = {10.1109/CVPR.2015.7298965},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	year = {2015},
	pages = {3431--3440},
}

@article{johnson_perceptual_2016,
	title = {Perceptual {Losses} for {Real}-{Time} {Style} {Transfer} and {Super}-{Resolution}},
	url = {http://arxiv.org/abs/1603.08155},
	abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a {\textbackslash}emph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing {\textbackslash}emph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	month = mar,
	year = {2016},
}

@article{gatys_neural_2015-1,
	title = {A {Neural} {Algorithm} of {Artistic} {Style}},
	url = {http://arxiv.org/abs/1508.06576},
	abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	month = aug,
	year = {2015},
}

@article{arjovsky_wasserstein_2017,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = jan,
	year = {2017},
}

@article{loannidis_why_2005,
	title = {Why most published research findings are false.},
	volume = {2},
	issn = {3540239081},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1182327&tool=pmcentrez&rendertype=abstract\nhttp://medicine.plosjournals.org/perlserv/?request=get-document&doi=10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	number = {8},
	journal = {PLoS Medicine},
	author = {Loannidis, John P A and Ioannidis, John P a},
	year = {2005},
	note = {Publisher: Public Library of Science},
	keywords = {bias (epidemiology), data interpretation, likelihood functions, meta analysis as topic, Meta-Analysis as Topic, odds ratio, publishing, reproducibility of results, research design, sample size, statistical},
	pages = {e124--e124},
}

@article{ioannidis_why_2007,
	title = {Why most published research findings are false: {Author}'s reply to {Good} man and {Greenland} [7]},
	volume = {4},
	abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	journal = {PLoS Medicine},
	author = {Ioannidis, John P A},
	year = {2007},
	pages = {1132--1133},
}

@article{lenc_r-cnn_2015,
	title = {R-{CNN} minus {R}},
	issn = {1-901725-53-7},
	url = {http://arxiv.org/abs/1506.06981%5Cnhttp://www.bmva.org/bmvc/2015/papers/paper005/index.html},
	doi = {10.5244/C.29.5},
	abstract = {Deep convolutional neural networks (CNNs) have had a major impact in most areas of image understanding, including object category detection. In object detection, methods such as R-CNN have obtained excellent results by integrating CNNs with region proposal generation algorithms such as selective search. In this paper, we investigate the role of proposal generation in CNN-based detectors in order to determine whether it is a necessary modelling component, carrying essential geometric information not contained in the CNN, or whether it is merely a way of accelerating detection. We do so by designing and evaluating a detector that uses a trivial region generation scheme, constant for each image. Combined with SPP, this results in an excellent and fast detector that does not require to process an image with algorithms other than the CNN itself. We also streamline and simplify the training of CNN-based detectors by integrating several learning steps in a single algorithm, as well as by proposing a number of improvements that accelerate detection.},
	journal = {Procedings of the British Machine Vision Conference 2015},
	author = {Lenc, Karel and Vedaldi, Andrea},
	year = {2015},
	pages = {5.1--5.12},
}

@article{radford_unsupervised_2015,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = nov,
	year = {2015},
}

@article{goodfellow_generative_2014-1,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	file = {Full Text:/home/zwerg/Zotero/storage/XUBD88WE/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf},
}

@article{lamb_sensitivity_2000,
	title = {Sensitivity enhancement in fluorescence correlation spectroscopy of multiple species using time-gated detection.},
	volume = {79},
	issn = {0006-3495 (Print) 0006-3495 (Linking)},
	doi = {10.1016/S0006-3495(00)76366-1},
	abstract = {Fluorescence correlation spectroscopy (FCS) is a powerful technique to measure chemical reaction rates and diffusion coefficients of molecules in thermal equilibrium. The capabilities of FCS can be enhanced by measuring the energy, polarization, or delay time between absorption and emission of the collected fluorescence photons in addition to their arrival times. This information can be used to change the relative intensities of multiple fluorescent species in FCS measurements and, thus, the amplitude of the intensity autocorrelation function. Here we demonstrate this strategy using lifetime gating in FCS experiments. Using pulsed laser excitation and laser-synchronized gating in the detection channel, we suppress photons emitted within a certain time interval after excitation. Three applications of the gating technique are presented: suppression of background fluorescence, simplification of FCS reaction studies, and investigation of lifetime heterogeneity of fluorescently labeled biomolecules. The usefulness of this technique for measuring forward and backward rates of protein fluctuations in equilibrium and for distinguishing between static and dynamic heterogeneity makes it a promising tool in the investigation of chemical reactions and conformational fluctuations in biomolecules.},
	number = {August},
	journal = {Biophysical journal},
	author = {Lamb, D C and Schenk, a and Röcker, C and Scalfi-Happ, C and Nienhaus, G U},
	year = {2000},
	pages = {1129--1138},
}

@article{xu_imaging_2007,
	title = {Imaging protein interactions with bioluminescence resonance energy transfer ({BRET}) in plant and mammalian cells and tissues.},
	volume = {104},
	issn = {0027-8424},
	doi = {10.1073/pnas.0701987104},
	abstract = {FRET is a well established method for cellular and subcellular imaging of protein interactions. However, FRET obligatorily necessitates fluorescence excitation with its concomitant problems of photobleaching, autofluorescence, phototoxicity, and undesirable stimulation of photobiological processes. A sister technique, bioluminescence resonance energy transfer (BRET), avoids these problems because it uses enzyme-catalyzed luminescence; however, BRET signals usually have been too dim to image effectively in the past. Using a new generation electron bombardment-charge-coupled device camera coupled to an image splitter, we demonstrate that BRET can be used to image protein interactions in plant and animal cells and in tissues; even subcellular imaging is possible. We have applied this technology to image two different protein interactions: (i) dimerization of the developmental regulator, COP1, in plant seedlings; and (ii) CCAAT/enhancer binding protein alpha (C/EBPalpha) in the mammalian nucleus. This advance heralds a host of applications for imaging without fluorescent excitation and its consequent limitations.},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Xu, Xiaodong and Soutto, Mohammed and Xie, Qiguang and Servick, Stein and Subramanian, Chitra and von Arnim, Albrecht G and Johnson, Carl Hirschie},
	year = {2007},
	pages = {10264--10269},
}

@article{murata_texture_2001,
	title = {Texture analysis of fluorescence lifetime images of nuclear {DNA} with effect of fluorescence resonance energy transfer.},
	volume = {43},
	doi = {10.1002/1097-0320(20010201)43:2<94::AID-CYTO1023>3.0.CO;2-4},
	abstract = {BACKGROUND: Fluorescence lifetime imaging microscopy (FLIM) is becoming an important tool in cellular imaging. In FLIM, the image contrast is concentration insensitive, whereas it is sensitive to the local environment and interactions of fluorophores such as fluorescence resonance energy transfer (RET). METHODS: Fluorescence microscopy, lifetime imaging, and texture analysis were used to study the spatial distribution of fluorophores bound to nuclear DNA. 3T3-Swiss albino mice fibroblast nuclei were labeled with Hoechst 33258 (Ho), an AT-specific dye, and 7-aminoactinomycin D (7-AAD), a GC-specific dye. Ho is a RET donor to the 7-AAD acceptor. RESULTS: Texture analysis of 50 alcohol-fixed nuclei quantitatively showed changes of spatial distribution of apparent donor lifetimes. RET increased the spatial heterogeneity in the phase and modulation lifetime images. In most of the doubly stained cells (about 80\%), the phase and modulation lifetime distributions were spatially homogeneous. In about 20\% of the cells, we noticed that lower phase and modulation lifetimes caused by RET were correlated with regions of high Ho intensity in the nuclei. CONCLUSIONS: The spatial lifetime heterogeneity of Ho in presence of 7-AAD seems to be caused by RET between closely spaced strands in the three dimensionally condensed regions of DNA.},
	journal = {Cytometry},
	author = {Murata, S and Herman, P and Lakowicz, J R},
	year = {2001},
	pages = {94--100},
}

@article{parthasarathy_rapid_2012,
	title = {Rapid, accurate particle tracking by calculation of radial symmetry centers},
	volume = {9},
	issn = {1548-7091},
	doi = {10.1038/nmeth.2071},
	abstract = {I introduce an algorithm for subpixel localization of imaged objects based on an analytic, non-iterative calculation of the best-fit radial symmetry center. This approach yields tracking accuracies that are near theoretical limits, similarly to Gaussian fitting, but with orders-of-magnitude faster execution time, lower sensitivity to nearby particles and applicability to any radially symmetric intensity distribution. I demonstrate the method with several types of data, including super-resolution microscopy images.},
	number = {7},
	journal = {Nature Methods},
	author = {Parthasarathy, Raghuveer},
	year = {2012},
	pages = {724--726},
}

@article{becker_high_2003,
	title = {High resolution {TCSPC} lifetime imaging},
	volume = {4963},
	url = {http://spiedigitallibrary.org/data/Conferences/SPIEP/32137/175_1.pdf},
	doi = {10.1117/12.472866},
	journal = {Proc. …},
	author = {Becker, Wolfgang and Bergmann, Axel and Biskup, Christoph},
	year = {2003},
	pages = {1--10},
}

@article{rusk_experimental_2009,
	title = {Experimental micro-matchmaking},
	volume = {6},
	url = {http://www.nature.com/doifinder/10.1038/nmeth.f.242},
	doi = {10.1038/nmeth.f.242},
	number = {1},
	journal = {Nature Methods},
	author = {Rusk, Nicole},
	year = {2009},
	pages = {36--36},
}

@article{siegel_wide-field_2003,
	title = {Wide-field time-resolved fluorescence anisotropy imaging ({TR}-{FAIM}): {Imaging} the rotational mobility of a fluorophore},
	volume = {74},
	issn = {0034-6748},
	doi = {10.1063/1.1519934},
	abstract = {We report a picosecond time-gated fluorescence lifetime imaging (FLIM){\textbackslash}nsystem extended to perform time-resolved fluorescence anisotropy{\textbackslash}nimaging (TR-FAIM). Upon excitation with linearly polarized laser{\textbackslash}npulses, the parallel and perpendicular components of the fluorescence{\textbackslash}nemission from a sample are imaged simultaneously using a polarization-resolved{\textbackslash}nimager. The imaging technique presented here quantitatively reports{\textbackslash}nthe rotational mobility of a fluorophore as it varies according to{\textbackslash}nthe local environment. In a single acquisition run it yields maps{\textbackslash}nof both rotational correlation time and fluorescence lifetime as{\textbackslash}nthey vary across a sample. TR-FAIM has been applied to imaging standard{\textbackslash}nmultiwell plate samples of rhodamine 6G dissolved in methanol, ethylene{\textbackslash}nglycol, trimethylene glycol, and glycerol. The observed rotational{\textbackslash}ncorrelation times and fluorescence lifetimes, which report the local{\textbackslash}nviscosity and refractive index of the local rhodamine 6G environment,{\textbackslash}nrespectively, are in good agreement with previously published single{\textbackslash}npoint measurements. By considering the linear dependence of the rotational{\textbackslash}ncorrelation time on viscosity up to 20 cP, we are able to obtain{\textbackslash}na two-dimensional viscosity map. Wide-field maps of rotational correlation{\textbackslash}ntime, and therefore viscosity, have been obtained. This illustrates{\textbackslash}nthe potential to image the local viscosity and fluorescence lifetime{\textbackslash}ndistributions of fluorophore tagged proteins in cells. (C) 2003 American{\textbackslash}nInstitute of Physics.},
	number = {1},
	journal = {Review of Scientific Instruments},
	author = {Siegel, J. and Suhling, K. and Lévêque-Fort, S. and Webb, S. E D and Davis, D. M. and Phillips, D. and Sabharwal, Y. and French, P. M W},
	year = {2003},
	pages = {182--192},
}

@article{keller_magnifying_2009,
	title = {Magnifying power},
	journal = {Nature},
	author = {Keller, P and Schmidt, a and Marshall, G and Yoshida, K},
	year = {2009},
	pages = {629--629},
}

@article{becker_time-_nodate,
	title = {Time- and {Wavelength}-{Resolved} {Autofluorescence} {Detection} by {Multi}- {Dimensional} {TCSPC}},
	journal = {Measurement},
	author = {Becker, Wolfgang and Bergmann, Axel and Biskup, Christoph and Schweitzer, Dietrich and Hammer, Martin and Gmbh, Hickl and Ophthalmologie, Experimentelle},
	keywords = {autofluorescence, flim, ophthalmic imaging, tcspc},
	pages = {1--8},
}

@article{kim_fluorescence_2007,
	title = {Fluorescence correlation spectroscopy in living cells.},
	volume = {4},
	issn = {1548-7091 (Print){\textbackslash}n1548-7091 (Linking)},
	doi = {10.1038/nmeth1104},
	abstract = {Fluorescence correlation spectroscopy (FCS) is an ideal analytical tool for studying concentrations, propagation, interactions and internal dynamics of molecules at nanomolar concentrations in living cells. FCS analyzes minute fluorescence-intensity fluctuations about the equilibrium of a small ensemble ({\textless}10(3)) of molecules. These fluctuations act like a 'fingerprint' of a molecular species detected when entering and leaving a femtoliter-sized optically defined observation volume created by a focused laser beam. In FCS the fluorescence fluctuations are recorded as a function of time and then statistically analyzed by autocorrelation analysis. The resulting autocorrelation curve yields a measure of self-similarity of the system after a certain time delay, and its amplitude describes the normalized variance of the fluorescence fluctuations. By fitting the curves to an appropriate physical model, this method provides precise information about a multitude of measurement parameters, including diffusion coefficients, local concentration, states of aggregation and molecular interactions. FCS operates in real time with diffraction-limited spatial and sub-microsecond temporal resolution. Assessing diverse molecular dynamics within the living cell is a challenge well met by FCS because of its single-molecule sensitivity and high dynamic resolution. For these same reasons, however, intracellular FCS measurements also harbor the large risk of collecting artifacts and thus producing erroneous data. Here we provide a step-by-step guide to the application of FCS to cellular systems, including methods for minimizing artifacts, optimizing measurement conditions and obtaining parameter values in the face of diverse and complex conditions of the living cell. A discussion of advantages and disadvantages of one-photon versus two-photon excitation for FCS is available in Supplementary Methods online.},
	number = {11},
	journal = {Nature methods},
	author = {Kim, Sally a and Heinze, Katrin G and Schwille, Petra},
	year = {2007},
	pages = {963--973},
}

@article{lippincott-schwartz_photoactivatable_2009,
	title = {Photoactivatable fluorescent proteins for diffraction-limited and super-resolution imaging},
	volume = {19},
	issn = {1879-3088 (Electronic)},
	doi = {10.1016/j.tcb.2009.09.003},
	abstract = {Photoactivatable fluorescent proteins (PA-FPs) are molecules that switch to a new fluorescent state in response to activation to generate a high level of contrast. Over the past eight years, several types of PA-FPs have been developed. The PA-FPs fluoresce green or red, or convert from green to red in response to activating light. Others reversibly switch between 'off' and 'on' in response to light. The optical "highlighting" capability of PA-FPs has led to the rise of novel imaging techniques providing important new biological insights. These range from in cellulo pulse-chase labeling for tracking subpopulations of cells, organelles or proteins under physiological settings, to super-resolution imaging of single molecules for determining intracellular protein distributions at nanometer precision. This review surveys the expanding array of PA-FPs, including their advantages and disadvantages, and highlights their use in novel imaging methodologies. © 2009.},
	journal = {Trends in Cell Biology},
	author = {Lippincott-Schwartz, Jennifer and Patterson, George H.},
	year = {2009},
	pages = {555--565},
}

@article{noauthor_mcnamara2007fluorophorestable_nodate,
	title = {{McNamara2007FluorophoresTable}},
}

@article{chudakov_photoswitchable_2004,
	title = {Photoswitchable cyan fluorescent protein for protein tracking.},
	volume = {22},
	issn = {1087-0156},
	doi = {10.1038/nbt1025},
	abstract = {In recent years diverse photolabeling techniques using green fluorescent protein (GFP)-like proteins have been reported, including photoactivatable PA-GFP, photoactivatable protein Kaede, the DsRed 'greening' technique and kindling fluorescent proteins. So far, only PA-GFP, which is monomeric and gives 100-fold fluorescence contrast, could be applied for protein tracking. Here we describe a dual-color monomeric protein, photoswitchable cyan fluorescent protein (PS-CFP). PS-CFP is capable of efficient photoconversion from cyan to green, changing both its excitation and emission spectra in response to 405-nm light irradiation. Complete photoactivation of PS-CFP results in a 1,500-fold increase in the green-to-cyan fluorescence ratio, making it the highest-contrast monomeric photoactivatable fluorescent protein described to date. We used PS-CFP as a photoswitchable tag to study trafficking of human dopamine transporter in living cells. At moderate excitation intensities, PS-CFP can be used as a pH-stable cyan label for protein tagging and fluorescence resonance energy transfer applications.},
	number = {11},
	journal = {Nature biotechnology},
	author = {Chudakov, Dmitriy M and Verkhusha, Vladislav V and Staroverov, Dmitry B and Souslova, Ekaterina a and Lukyanov, Sergey and Lukyanov, Konstantin a},
	year = {2004},
	pages = {1435--1439},
}

@article{galletly_fluorescence_2008,
	title = {Fluorescence lifetime imaging distinguishes basal cell carcinoma from surrounding uninvolved skin},
	volume = {159},
	issn = {1365-2133 (Electronic){\textbackslash}r0007-0963 (Linking)},
	doi = {10.1111/j.1365-2133.2008.08577.x},
	abstract = {BACKGROUND: Fluorescence lifetime imaging (FLIM) is a novel imaging technique that generates image contrast between different states of tissue due to differences in fluorescence decay rates. OBJECTIVES: To establish whether FLIM of skin autofluorescence can provide useful contrast between basal cell carcinomas (BCCs) and surrounding uninvolved skin. METHODS: Unstained excision biopsies of 25 BCCs were imaged en face with FLIM following excitation of autofluorescence with a 355 nm pulsed ultraviolet laser. RESULTS: Using FLIM we were able to distinguish areas of BCC from surrounding skin in an ex vivo study. Significant reductions in mean fluorescence lifetimes between areas of BCC and areas of surrounding uninvolved skin were demonstrated (P {\textless} 0.0001). These differences were apparent irrespective of the decay model used to calculate the fluorescence lifetimes (single vs. stretched exponential) or the long-pass filter through which the emitted autofluorescence was collected (375 vs. 455 nm). Conversely, there was no significant difference between the BCC and uninvolved areas of each sample when mean autofluorescence intensities were examined. Moreover, wide-field false-colour images of fluorescence lifetimes clearly discriminated areas of BCC from the surrounding uninvolved skin. CONCLUSIONS: We therefore believe that FLIM has a potential future clinical role in imaging BCCs for rapid and noninvasive tumour delineation and as an aid to determine adequate excision margins with best preservation of normal tissue.},
	journal = {British Journal of Dermatology},
	author = {Galletly, N. P. and McGinty, J. and Dunsby, C. and Teixeira, F. and Requejo-Isidro, J. and Munro, I. and Elson, D. S. and Neil, M. a a and Chu, a. C. and French, P. M W and Stamp, G. W.},
	year = {2008},
	keywords = {Autofluorescence, Basal cell carcinoma, Fluorescence lifetime imaging},
	pages = {152--161},
}

@article{goedhart_sensitive_2007,
	title = {Sensitive detection of p65 homodimers using red-shifted and fluorescent protein-based {FRET} couples},
	volume = {2},
	issn = {1932-6203 (Electronic){\textbackslash}n1932-6203 (Linking)},
	doi = {10.1371/journal.pone.0001011},
	abstract = {BACKGROUND: Fluorescence Resonance Energy Transfer (FRET) between the green fluorescent protein (GFP) variants CFP and YFP is widely used for the detection of protein-protein interactions. Nowadays, several monomeric red-shifted fluorescent proteins are available that potentially improve the efficiency of FRET. METHODOLOGY/PRINCIPAL FINDINGS: To allow side-by-side comparison of several fluorescent protein combinations for detection of FRET, yellow or orange fluorescent proteins were directly fused to red fluorescent proteins. FRET from yellow fluorescent proteins to red fluorescent proteins was detected by both FLIM and donor dequenching upon acceptor photobleaching, showing that mCherry and mStrawberry were more efficient acceptors than mRFP1. Circular permutated yellow fluorescent protein variants revealed that in the tandem constructs the orientation of the transition dipole moment influences the FRET efficiency. In addition, it was demonstrated that the orange fluorescent proteins mKO and mOrange are both suitable as donor for FRET studies. The most favorable orange-red FRET pair was mKO-mCherry, which was used to detect homodimerization of the NF-kappaB subunit p65 in single living cells, with a threefold higher lifetime contrast and a twofold higher FRET efficiency than for CFP-YFP. CONCLUSIONS/SIGNIFICANCE: The observed high FRET efficiency of red-shifted couples is in accordance with increased Förster radii of up to 64 A, being significantly higher than the Förster radius of the commonly used CFP-YFP pair. Thus, red-shifted FRET pairs are preferable for detecting protein-protein interactions by donor-based FRET methods in single living cells.},
	number = {10},
	journal = {PLoS ONE},
	author = {Goedhart, Joachim and Vermeer, J. E M and Adjobo-Hermans, M. J W and van Weeren, Laura and Gadella, T. W J},
	year = {2007},
	pages = {2--10},
}

@article{kim_fully_2007,
	title = {Fully automated segmentation and morphometrical analysis of muscle fiber images.},
	volume = {71},
	doi = {10.1002/cyto.a},
	abstract = {BACKGROUND: Measurement of muscle fiber size and determination of size distribution is important in the assessment of neuromuscular disease. Fiber size estimation by simple inspection is inaccurate and subjective. Manual segmentation and measurement are time-consuming and tedious. We therefore propose an automated image analysis method for objective, reproducible, and time-saving measurement of muscle fibers in routinely hematoxylin-eosin stained cryostat sections. METHODS: The proposed segmentation technique makes use of recent advances in level set based segmentation, where classical edge based active contours are extended by region based cues, such as color and texture. Segmentation and measurement are performed fully automatically. Multiple morphometric parameters, i.e., cross sectional area, lesser diameter, and perimeter are assessed in a single pass. The performance of the computed method was compared to results obtained by manual measurement by experts. RESULTS: The correct classification rate of the computed method was high (98\%). Segmentation and measurement results obtained manually or automatically did not reveal any significant differences. CONCLUSIONS: The presented region based active contour approach has been proven to accurately segment and measure muscle fibers. Complete automation minimizes user interaction, thus, batch processing, as well as objective and reproducible muscle fiber morphometry are provided.},
	number = {February},
	journal = {Cytometry. Part A : the journal of the International Society for Analytical Cytology},
	author = {Kim, Yoo-Jin and Brox, Thomas and Feiden, Wolfgang and Weickert, Joachim},
	year = {2007},
	pages = {8--15},
}

@article{landecker_seeing_2009,
	title = {Seeing things: from microcinematography to live cell  imaging.},
	volume = {6},
	url = {http://dx.doi.org/10.1038/nmeth1009-707},
	doi = {10.1038/nmeth1009-707},
	abstract = {Nature Methods 6, 707 (2009). doi:10.1038/nmeth1009-707},
	number = {10},
	journal = {Nature methods},
	author = {Landecker, Hannah},
	year = {2009},
	note = {Publisher: Nature Publishing Group},
	pages = {707--709},
}

@article{thompson_recent_2002,
	title = {Recent advances in fluorescence correlation spectroscopy},
	volume = {12},
	issn = {0959-440X},
	doi = {10.1016/S0959-440X(02)00368-8},
	abstract = {Fluorescence correlation spectroscopy is a method in which fluctuations in the fluorescence arising from a very small sample volume are correlated to obtain information about the processes giving rise to the fluctuations. Recent progress has been made in methodologies such as two-photon excitation, photon counting histogram analysis, cross-correlation, image correlation and evanescent excitation. Fluorescence correlation spectroscopy techniques have been applied to several biological processes, including fluorescent protein photodynamics, binding equilibria and kinetics, protein oligomerization, nucleic acid interactions, and membrane and intracellular dynamics.},
	journal = {Current Opinion in Structural Biology},
	author = {Thompson, Nancy L. and Lieto, Alena M. and Allen, Noah W.},
	year = {2002},
	pages = {634--641},
}

@article{cole_time-domain_2001,
	title = {Time-domain whole-field fluorescence lifetime imaging with optical sectioning.},
	volume = {203},
	issn = {0022-2720 (Print){\textbackslash}n0022-2720 (Linking)},
	doi = {10.1046/j.1365-2818.2001.00894.x},
	abstract = {A whole-field time-domain fluorescence lifetime imaging (FLIM) microscope with the capability to perform optical sectioning is described. The excitation source is a mode-locked Ti:Sapphire laser that is regeneratively amplified and frequency doubled to 415 nm. Time-gated fluorescence intensity images at increasing delays after excitation are acquired using a gated microchannel plate image intensifier combined with an intensified CCD camera. By fitting a single or multiple exponential decay to each pixel in the field of view of the time-gated images, 2-D FLIM maps are obtained for each component of the fluorescence lifetime. This FLIM instrument was demonstrated to exhibit a temporal discrimination of better than 10 ps. It has been applied to chemically specific imaging, quantitative imaging of concentration ratios of mixed fluorophores and quantitative imaging of perturbations to fluorophore environment. Initially, standard fluorescent dyes were studied and then this FLIM microscope was applied to the imaging of biological tissue, successfully contrasting different tissues and different states of tissue using autofluorescence. To demonstrate the potential for real-world applications, the FLIM microscope has been configured using potentially compact, portable and low cost all-solid-state diode-pumped laser technology. Whole-field FLIM with optical sectioning (3D FLIM) has been realized using a structured illumination technique.},
	number = {September 2000},
	journal = {Journal of microscopy},
	author = {Cole, M J and Siegel, J and Webb, S E and Jones, R and Dowling, K and Dayel, M J and Parsons-Karavassilis, D and French, P M and Lever, M J and Sucharov, L O and Neil, M a and Juskaitis, R and Wilson, T},
	year = {2001},
	keywords = {3d microscopy, diode-pumped laser, fluorescence lifetime imaging, optical sectioning, whole-field imaging},
	pages = {246--257},
}

@article{henriksson_specific_2001,
	title = {Specific binding of proinsulin {C}-peptide to intact and to detergent-solubilized human skin fibroblasts.},
	volume = {280},
	doi = {10.1006/bbrc.2000.4135},
	abstract = {Proinsulin C-peptide exerts physiological effects on kidney and nerve function, but the mechanisms involved remain incompletely understood. Using fluorescence correlation spectroscopy, we have studied binding of rhodamine-labelled human C-peptide to intact human skin fibroblasts and to detergent-solubilised extracts of fibroblasts, K-562, and IEC-6 cells. Specificity was shown by displacement of rhodamine-labelled human C-peptide with unlabelled human C-peptide. C-peptide was found to bind to the cell membranes of intact fibroblasts with an association constant of 3 x 10(9) M(-1), giving full saturation at about 0.9 nM, close to the physiological C-peptide plasma concentration. Treatment of all investigated cells with the zwitter-ionic detergent Chaps was found to release macromolecules that bind specifically to C-peptide. The binding in Chaps extracts of fibroblasts was sensitive to time but remained reproducible for up to 2 h at room temperature. Lysophosphatidylcholine, Triton X-100, beta-octylglucopyranoside, SDS, or cholate gave extracts with only low or nonspecific binding. It is concluded that C-peptide binding components can be solubilised from cells, and that Chaps appears to be a suitable detergent.},
	journal = {Biochemical and biophysical research communications},
	author = {Henriksson, M and Pramanik, a and Shafqat, J and Zhong, Z and Tally, M and Ekberg, K and Wahren, J and Rigler, R and Johansson, J and Jörnvall, H},
	year = {2001},
	keywords = {c-peptide, diabetes mellitus, fcs, peptide-lipid interactions, proinsulin c-peptide, regarded as a, that proinsulin, there is increasing evidence, which generally has been},
	pages = {423--427},
}

@article{heinze_simultaneous_2000,
	title = {Simultaneous two-photon excitation of distinct labels for dual-color fluorescence crosscorrelation analysis.},
	volume = {97},
	issn = {0027-8424 (Print){\textbackslash}r0027-8424 (Linking)},
	doi = {10.1073/pnas.180317197},
	abstract = {Confocal fluorescence correlation spectroscopy as a time-averaging fluctuation analysis combining maximum sensitivity with high statistical confidence has proved to be a very versatile and powerful tool for detection and temporal investigation of biomolecules at ultralow concentrations on surfaces, in solutions, and in living cells. To probe the interaction of different molecular species for a detailed understanding of biologically relevant mechanisms, crosscorrelation studies on dual or multiple fluorophore assays with spectrally distinct excitation and emission are particularly promising. Despite the considerable improvement of detection specificity provided by fluorescence crosscorrelation analysis, few applications have so far been reported, presumably because of the practical challenges of properly aligning and controlling the stability of the experimental setup. In this work, we demonstrate that two-photon excitation combined with dual-color fluorescence correlation spectroscopy can be the key to simplifying simultaneous investigations of multiple fluorescent species significantly on a single-molecule scale. Two-photon excitation allows accession of common fluorophores of largely distinct emission by the same excitation wavelength, because differences in selection rules and vibronic coupling can induce considerable shifts between the one-photon and two-photon excitation spectra. The concept of dual-color two-photon fluorescence crosscorrelation analysis is introduced and experimentally demonstrated with an established assay probing the selective cleavage of dual-labeled DNA substrates by restriction endonuclease EcoRI.},
	number = {19},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Heinze, K G and Koltermann, a and Schwille, P},
	year = {2000},
	pages = {10377--10382},
}

@article{noauthor_title_nodate,
	title = {title},
}

@article{noauthor_applied_fluorescence_mafs5-seidel-ac5pdf_nodate,
	title = {Applied\_Fluorescence\_MAFS5-{Seidel}-{AC5}.pdf},
}

@article{spectroscopy_accelerated_2000,
	title = {Accelerated {Publications} {Simultaneous} {Binding} of {Two} {DNA} {Duplexes} to the {NtrC} - {Enhancer} {Complex}},
	volume = {39},
	issn = {4962214233},
	number = {9},
	author = {Spectroscopy, Two-color Fluorescence Cross-correlation and Rippe, Karsten},
	year = {2000},
	pages = {11--15},
}

@article{asai_fluorescence_1985,
	title = {Fluorescence correlation spectroscopy},
	volume = {83},
	abstract = {Fluorescence correlation spectroscopy (FCS) is a technique that allows for an extremely sensitive determination of molecular diffusion properties, down to the level of single molecules. It thus provides an attractive alternative to FRAP, requiring much less laser power and lower concentrations of fluorophores. FCS has recently been applied on live cells, and in comparison on domain-forming model membrane systems, to systematically study the influence of cholesterol on local membrane structure by investigating the mobility of selected lipid probes. The findings demonstrate the ability of FCS to sensitively distinguish between different local lipid structures, and emphasize the value of model systems for understanding membrane dynamics in general.},
	number = {October},
	journal = {Tanpakushitsu kakusan koso. Protein, nucleic acid, enzyme},
	author = {Asai, H},
	year = {1985},
	pages = {246--248},
}

@article{maus_new_2001,
	title = {New picosecond laser system for easy tunability over the whole ultraviolet/visible/near infrared wavelength range based on flexible harmonic generation and optical parametric oscillation},
	volume = {72},
	issn = {0034-6748},
	doi = {10.1063/1.1326930},
	abstract = {A new laser-based and time-correlated single photon counting (TCSPC) detection system which allows easy and fast tuning of excitation wavelengths over a broad range from 240 to 1300 nm, with small gaps from 335 to 360 nm and 660 to 720 nm, has been built. The unique combination of a mode-locked Ti:sapphire laser, an optical parametric oscillator, pulse selectors, and harmonic generators delivers ultrafast laser pulses (1-2 ps) with variable repetition rates and excitation wavelengths. Performance characteristics of the laser system at different excitation wavelengths are reported and the TCSPC setup, which is characterized by a total instrument response function of 25 ps full width at half maximum, is described. Typical TCSPC measurements demonstrate the capability of the system of deriving decay or species associated excitation spectra. (C) 2001 American Institute of Physics.},
	number = {1},
	journal = {Review of Scientific Instruments},
	author = {Maus, Michael and Rousseau, Els and Cotlet, Mircea and Schweitzer, Gerd and Hofkens, Johan and Van Der Auweraer, Mark and De Schryver, Frans C. and Krueger, Arnd},
	year = {2001},
	pages = {36--40},
}

@article{scorilas_streptavidin-polyvinylamine_2000,
	title = {Streptavidin-polyvinylamine conjugates labeled with a europium chelate: {Applications} in immunoassay, immunohistochemistry, and microarrays},
	volume = {46},
	issn = {0009-9147},
	abstract = {BACKGROUND: The favorable properties of lanthanide chelates compared with conventional fluorescent probes have attracted considerable interest. A Eu(3+) chelator, 4,7-bis(chlorosulfophenyl)-1, 10-phenanthroline-2,9-dicarboxylic acid (BCPDA), has been synthesized previously. METHODS: We here describe immunoassay, immunohistochemistry, and microarray applications of a new streptavidin-based universal polyvinylamine (PVA) detection reagent that is multiply labeled with the europium chelate of BCPDA. Solid-phase time-resolved immunofluorometric assays for biotinylated mouse IgG and prostate-specific antigen (PSA) were developed using the new conjugate as a detection reagent. The new conjugate was also used for the immunohistochemical localization of PSA expression in paraffin-embedded prostatic tissues. A model microarray with spotted biotinylated antibody as target was also performed. RESULTS: Approximately 50-100 BCPDA moieties were covalently bound to PVA, which was then linked to streptavidin via biotin interaction. The macromolecular complex successfully recognized and bound biotinylated detection reagents, e.g., antibodies. The new reagent enabled measurement of solid phase-immobilized biotinylated mouse IgG with a detection limit of approximately 1 pg/assay and demonstrated excellent linearity. In an ELISA-type sandwich PSA assay that included two PSA monoclonal antibodies using the new conjugate as detection reagent, we detected 0.001 microg/L PSA ( approximately 100 fg or approximately 3 amol/assay). Serum samples analyzed for PSA by this method and a commercial assay gave highly correlated results. The new reagent enabled excellent immunohistochemical localization of PSA expression in prostate tissues. Using the new reagent in a model microarray experiment with biotinylated mouse IgG as target, we demonstrated excellent spatial resolution of 5- to 10-nL microspots. CONCLUSIONS: The new detection reagent may find important applications in biotechnology.},
	journal = {Clinical Chemistry},
	author = {Scorilas, a. and Bjartell, a. and Lilja, H. and Moller, C. and Diamandis, E. P.},
	year = {2000},
	pages = {1450--1455},
}

@article{ai_exploration_2007,
	title = {Exploration of new chromophore structures leads to the identification of improved blue fluorescent proteins},
	volume = {46},
	issn = {0006-2960},
	doi = {10.1021/bi700199g},
	abstract = {The variant of Aequorea green fluorescent protein (GFP) known as blue fluorescent protein (BFP) was originally engineered by substituting histidine for tyrosine in the chromophore precursor sequence. Herein we report improved versions of BFP along with a variety of engineered fluorescent protein variants with novel and distinct chromophore structures that all share the property of a blue fluorescent hue. The two most intriguing of the new variants are a version of GFP in which the chromophore does not undergo excited-state proton transfer and a version of mCherry with a phenylalanine-derived chromophore. All of the new blue fluorescing proteins have been critically assessed for their utility in live cell fluorescent imaging. These new variants should greatly facilitate multicolor fluorescent imaging by legitimizing blue fluorescing proteins as practical and robust members of the fluorescent protein "toolkit".},
	journal = {Biochemistry},
	author = {Ai, Hui Wang and Shaner, Nathan C. and Cheng, Zihao and Tsien, Roger Y. and Campbell, Robert E.},
	year = {2007},
	pages = {5904--5910},
}

@article{noauthor_quest_2012,
	title = {The quest for quantitative microscopy},
	volume = {9},
	issn = {1548-7105 (Electronic) 1548-7091 (Linking)},
	doi = {10.1038/nmeth.2102},
	abstract = {With the aid of informatics, microscopy is in the midst of a crucial evolution into a more quantitative and powerful technique.},
	number = {7},
	journal = {Nature Methods},
	year = {2012},
	pages = {627--627},
}

@article{treanor_microclusters_2006,
	title = {Microclusters of inhibitory killer immunoglobulin-like receptor signaling at natural killer cell immunological synapses},
	volume = {174},
	issn = {0021-9525 (Print){\textbackslash}r0021-9525 (Linking)},
	doi = {10.1083/jcb.200601108},
	abstract = {We report the supramolecular organization of killer Ig-like receptor (KIR) phosphorylation using a technique applicable to imaging phosphorylation of any green fluorescent protein-tagged receptor at an intercellular contact or immune synapse. Specifically, we use fluorescence lifetime imaging (FLIM) to report Förster resonance energy transfer (FRET) between GFP-tagged KIR2DL1 and a Cy3-tagged generic anti-phosphotyrosine monoclonal antibody. Visualization of KIR phosphorylation in natural killer (NK) cells contacting target cells expressing cognate major histocompatibility complex class I proteins revealed that inhibitory signaling is spatially restricted to the immune synapse. This explains how NK cells respond appropriately when simultaneously surveying susceptible and resistant target cells. More surprising, phosphorylated KIR was confined to microclusters within the aggregate of KIR, contrary to an expected homogeneous distribution of KIR signaling across the immune synapse. Also, yellow fluorescent protein-tagged Lck, a kinase important for KIR phosphorylation, accumulated in a multifocal distribution at inhibitory synapses. Spatial confinement of receptor phosphorylation within the immune synapse may be critical to how activating and inhibitory signals are integrated in NK cells.},
	number = {1},
	journal = {Journal of Cell Biology},
	author = {Treanor, Bebhinn and Lanigan, Peter M P and Kumar, Sunil and Dunsby, Chris and Munro, Ian and Auksorius, Egidijus and Culley, Fiona J. and Purbhoo, Marco a. and Phillips, David and Neil, Mark a a and Burshtyn, Deborah N. and French, Paul M W and Davis, Daniel M.},
	year = {2006},
	pages = {153--161},
}

@article{campbell_monomeric_2002,
	title = {A monomeric red fluorescent protein.},
	volume = {99},
	issn = {0027-8424 (Print){\textbackslash}r0027-8424 (Linking)},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/12060735},
	doi = {10.1073/pnas.082243699},
	abstract = {All coelenterate fluorescent proteins cloned to date display some form of quaternary structure, including the weak tendency of Aequorea green fluorescent protein (GFP) to dimerize, the obligate dimerization of Renilla GFP, and the obligate tetramerization of the red fluorescent protein from Discosoma (DsRed). Although the weak dimerization of Aequorea GFP has not impeded its acceptance as an indispensable tool of cell biology, the obligate tetramerization of DsRed has greatly hindered its use as a genetically encoded fusion tag. We present here the stepwise evolution of DsRed to a dimer and then either to a genetic fusion of two copies of the protein, i.e., a tandem dimer, or to a true monomer designated mRFP1 (monomeric red fluorescent protein). Each subunit interface was disrupted by insertion of arginines, which initially crippled the resulting protein, but red fluorescence could be rescued by random and directed mutagenesis totaling 17 substitutions in the dimer and 33 in mRFP1. Fusions of the gap junction protein connexin43 to mRFP1 formed fully functional junctions, whereas analogous fusions to the tetramer and dimer failed. Although mRFP1 has somewhat lower extinction coefficient, quantum yield, and photostability than DsRed, mRFP1 matures {\textgreater}10 times faster, so that it shows similar brightness in living cells. In addition, the excitation and emission peaks of mRFP1, 584 and 607 nm, are approximately 25 nm red-shifted from DsRed, which should confer greater tissue penetration and spectral separation from autofluorescence and other fluorescent proteins.},
	number = {12},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Campbell, Robert E and Campbell, Robert E and Tour, Oded and Tour, Oded and Palmer, Amy E and Palmer, Amy E and Steinbach, Paul a and Steinbach, Paul a and Baird, Geoffrey S and Baird, Geoffrey S and Zacharias, David a and Zacharias, David a and Tsien, Roger Y and Tsien, Roger Y},
	year = {2002},
	keywords = {Amino Acid Sequence, Animals, Cell Line, Cnidaria, Cnidaria: metabolism, Dimerization, Kinetics, Luminescent Proteins, Luminescent Proteins: chemistry, Luminescent Proteins: genetics, Luminescent Proteins: metabolism, Mammals, Models, Molecular, Molecular Sequence Data, Mutagenesis, Site-Directed, Protein Structure, Quaternary, Recombinant Proteins, Recombinant Proteins: chemistry, Recombinant Proteins: metabolism, Transfection},
	pages = {7877--82},
}

@article{brock_rapid_1999,
	title = {Rapid characterization of green fluorescent protein fusion proteins on the molecular and cellular level by fluorescence correlation microscopy.},
	volume = {96},
	doi = {10.1073/pnas.96.18.10123},
	abstract = {Fluorescence correlation microscopy (FCM) was applied to characterize fusion proteins of the green fluorescent protein (GFP) on the cellular as well as molecular level within seconds in an integrated instrument. FCM combines the inherent sensitivity and high spatial resolution of fluorescence correlation spectroscopy with fluorescence imaging and micropositioning, thereby providing a spectrum of molecular information in the cellular context. Signatures of characteristic parameters derived from the autocorrelation functions served to distinguish a GFP fusion protein of the epidermal growth factor receptor from GFP fluorescence in the endoplasmic reticulum and cytoplasm. Diffusion constants measured for free transiently expressed GFP reproduced values reported previously with other techniques. The accessible concentration range extends from millions to only a few thousand molecules per cell, with single molecule detectability in the femtoliter detection volume. The detailed molecular characterization offered by FCM is fully compatible with automation in sample identification and detection, offering new possibilities for highly integrated high-throughput screening.},
	number = {August},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Brock, R and Vàmosi, G and Vereb, G and Jovin, T M},
	year = {1999},
	pages = {10123--10128},
}

@article{korn_gene_2003,
	title = {Gene expression analysis using single molecule detection.},
	volume = {31},
	doi = {10.1093/nar/gng089},
	abstract = {Recent developments of single molecule detection techniques and in particular the introduction of fluorescence correlation spectroscopy (FCS) led to a number of important applications in biological research. We present a unique approach for the gene expression analysis using dual-color cross-correlation. The expression assay is based on gene-specific hybridization of two dye-labeled DNA probes to a selected target gene. The counting of the dual-labeled molecules within the solution allows the quantification of the expressed gene copies in absolute numbers. As detection and analysis by FCS can be performed at the level of single molecules, there is no need for any type of amplification. We describe the gene expression assay and present data demonstrating the capacity of this novel technology. In order to prove the gene specificity, we performed experiments with gene-depleted total cDNA. The biological application was demonstrated by quantifying selected high, medium and low abundant genes in cDNA prepared from HL-60 cells.},
	number = {16},
	journal = {Nucleic acids research},
	author = {Korn, Kerstin and Gardellin, Paola and Liao, Bohao and Amacker, Mario and Bergström, Asa and Björkman, Henrik and Camacho, Agnès and Dörhöfer, Sabine and Dörre, Klaus and Enström, Johanna and Ericson, Thomas and Favez, Tatiana and Gösch, Michael and Honegger, Adrian and Jaccoud, Sandra and Lapczyna, Markus and Litborn, Erik and Thyberg, Per and Winter, Holger and Rigler, Rudolf},
	year = {2003},
	pages = {e89--e89},
}

@article{dertinger_two-focus_2007,
	title = {Two-focus fluorescence correlation spectroscopy: {A} new tool for accurate and absolute diffusion measurements},
	volume = {8},
	doi = {10.1002/cphc.200600638},
	abstract = {We present a new method to measure absolute diffusion coefficients at nanomolar concentrations with high precision. Based on a modified fluorescence correlation spectroscopy (FCS)-setup, this method is improved by introducing an external ruler for measuring the diffusion time by generating two laterally shifted and overlapping laser foci at a fixed and known distance. Data fitting is facilitated by a new two-parameter model to describe the molecule detection function (MDF). We present a recorded MDF and show the excellent agreement with the fitting model. We measure the diffusion coefficient of the red fluorescent dye Atto655 under various conditions and compare these values with a value achieved by gradient pulsed field NMR (GPF NMR). From these measurements we conclude, that the new measurement scheme is robust against optical and photophysical artefacts which are inherent to standard FCS. With two-focus-FCS, the diffusion coefficient of 4.26 x 10(-6) cm2s(-1) for Atto655 in water at 25 degrees C compares well with the GPF NMR value of 4.28 x 10(-6) cm2s(-1).},
	journal = {ChemPhysChem},
	author = {Dertinger, Thomas and Pacheco, Victor and Von Der Hocht, Iris and Hartmann, Rudolf and Gregor, Ingo and Enderlein, Jörg},
	year = {2007},
	keywords = {Diffusion coefficients, Fluorescence spectroscopy, Fluorescent dyes, Time-resolved spectroscopy},
	pages = {433--443},
}

@article{noauthor_l_zhao_nodate,
	title = {L\_Zhao et al 2003 - {New} {Caged} {Coumarin} 2003.pdf},
}

@article{correlation_confocal_nodate,
	title = {Confocal {Fluorescence} {Microscopy} and {Fluorescence} {Correlation} {Spectroscopy}},
	author = {Correlation, Fluorescence},
}

@article{straub_fluorescence_1998,
	title = {Fluorescence lifetime three-dimensional microscopy with picosecond precision using a multifocal multiphoton microscope},
	volume = {73},
	issn = {0003-6951},
	doi = {10.1063/1.122276},
	abstract = {The combination of pulsed-mode excitation multifocal multiphoton microscopy with a high-repetition, time-gated intensified CCD camera enables efficient three-dimensional (3D) fluorescence lifetime imaging. With a 200-ps gate opening at 76 MHz repetition rate, fluorescence decay can be traced in a sequence of images with varying delays between pulse and gate. Fluorophore lifetimes are measured with a precision of a few picoseconds. As an application we show that, upon two-photon excitation at 800 nm, certain pollen samples feature a multiexponential fluorescence relaxation. Our results indicate that efficient four-dimensional microscopy with hundreds of nanometer spatial and tens of picoseconds temporal resolution is within reach. (C) 1998 American Institute of Physics. [S0003-6951(98)01139-5].},
	number = {13},
	journal = {Applied Physics Letters},
	author = {Straub, M. and Hell, S. W.},
	year = {1998},
	pages = {1769--1771},
}

@article{noauthor_email_nodate,
	title = {email},
}

@article{shaner_improved_2004,
	title = {Improved monomeric red, orange and yellow fluorescent proteins derived from {Discosoma} sp. red fluorescent protein.},
	volume = {22},
	issn = {1087-0156 (Print){\textbackslash}r1087-0156 (Linking)},
	doi = {10.1038/nbt1037},
	abstract = {Fluorescent proteins are genetically encoded, easily imaged reporters crucial in biology and biotechnology. When a protein is tagged by fusion to a fluorescent protein, interactions between fluorescent proteins can undesirably disturb targeting or function. Unfortunately, all wild-type yellow-to-red fluorescent proteins reported so far are obligately tetrameric and often toxic or disruptive. The first true monomer was mRFP1, derived from the Discosoma sp. fluorescent protein "DsRed" by directed evolution first to increase the speed of maturation, then to break each subunit interface while restoring fluorescence, which cumulatively required 33 substitutions. Although mRFP1 has already proven widely useful, several properties could bear improvement and more colors would be welcome. We report the next generation of monomers. The latest red version matures more completely, is more tolerant of N-terminal fusions and is over tenfold more photostable than mRFP1. Three monomers with distinguishable hues from yellow-orange to red-orange have higher quantum efficiencies.},
	number = {12},
	journal = {Nature biotechnology},
	author = {Shaner, Nathan C and Campbell, Robert E and Steinbach, Paul a and Giepmans, Ben N G and Palmer, Amy E and Tsien, Roger Y},
	year = {2004},
	pages = {1567--1572},
}

@article{kim_fully_2007-1,
	title = {Fully automated segmentation and morphometrical analysis of muscle fiber images.},
	volume = {71},
	doi = {10.1002/cyto.a},
	abstract = {BACKGROUND: Measurement of muscle fiber size and determination of size distribution is important in the assessment of neuromuscular disease. Fiber size estimation by simple inspection is inaccurate and subjective. Manual segmentation and measurement are time-consuming and tedious. We therefore propose an automated image analysis method for objective, reproducible, and time-saving measurement of muscle fibers in routinely hematoxylin-eosin stained cryostat sections. METHODS: The proposed segmentation technique makes use of recent advances in level set based segmentation, where classical edge based active contours are extended by region based cues, such as color and texture. Segmentation and measurement are performed fully automatically. Multiple morphometric parameters, i.e., cross sectional area, lesser diameter, and perimeter are assessed in a single pass. The performance of the computed method was compared to results obtained by manual measurement by experts. RESULTS: The correct classification rate of the computed method was high (98\%). Segmentation and measurement results obtained manually or automatically did not reveal any significant differences. CONCLUSIONS: The presented region based active contour approach has been proven to accurately segment and measure muscle fibers. Complete automation minimizes user interaction, thus, batch processing, as well as objective and reproducible muscle fiber morphometry are provided.},
	journal = {Cytometry. Part A : the journal of the International Society for Analytical Cytology},
	author = {Kim, Yoo-Jin and Brox, Thomas and Feiden, Wolfgang and Weickert, Joachim},
	year = {2007},
	pages = {8--15},
}

@article{noauthor_type_nodate,
	title = {type},
}

@article{lee_application_2001,
	title = {Application of the stretched exponential function to fluorescence lifetime imaging.},
	volume = {81},
	doi = {10.1016/S0006-3495(01)75784-0},
	abstract = {Conventional analyses of fluorescence lifetime measurements resolve the fluorescence decay profile in terms of discrete exponential components with distinct lifetimes. In complex, heterogeneous biological samples such as tissue, multi-exponential decay functions can appear to provide a better fit to fluorescence decay data than the assumption of a mono-exponential decay, but the assumption of multiple discrete components is essentially arbitrary and is often erroneous. Moreover, interactions, both between fluorophores and with their environment, can result in complex fluorescence decay profiles that represent a continuous distribution of lifetimes. Such continuous distributions have been reported for tryptophan, which is one of the main fluorophores in tissue. This situation is better represented by the stretched-exponential function (StrEF). In this work, we have applied, for the first time to our knowledge, the StrEF to time-domain whole-field fluorescence lifetime imaging (FLIM), yielding both excellent tissue contrast and goodness of fit using data from rat tissue. We note that for many biological samples for which there is no a priori knowledge of multiple discrete exponential fluorescence decay profiles, the StrEF is likely to provide a truer representation of the underlying fluorescence dynamics. Furthermore, fitting to a StrEF significantly decreases the required processing time, compared with a multi-exponential component fit and typically provides improved contrast and signal/noise in the resulting FLIM images. In addition, the stretched-exponential decay model can provide a direct measure of the heterogeneity of the sample, and the resulting heterogeneity map can reveal subtle tissue differences that other models fail to show.},
	number = {September},
	journal = {Biophysical journal},
	author = {Lee, K C and Siegel, J and Webb, S E and Lévêque-Fort, S and Cole, M J and Jones, R and Dowling, K and Lever, M J and French, P M},
	year = {2001},
	pages = {1265--1274},
}

@article{de_souza_induced_2008,
	title = {Induced pluripotency in human cells},
	volume = {5},
	issn = {1548-7091},
	doi = {10.1038/nmeth1163},
	number = {1},
	journal = {Nature Methods},
	author = {de Souza, Natalie},
	year = {2008},
	pages = {24--24},
}

@article{de_souza_flashing_2007,
	title = {{FlAsHing} the neighbors},
	volume = {4},
	doi = {10.1038/nmeth1207-990},
	number = {12},
	journal = {Nature Methods},
	author = {de Souza, Natalie},
	year = {2007},
	pages = {990--990},
}

@article{noauthor_ncomms3207-s16_nodate,
	title = {ncomms3207-s16},
}

@article{schwille_dual-color_1997,
	title = {Dual-{Color} {Fluorescence} {Cross}-{Correlation} {Spectroscopy} for},
	volume = {72},
	number = {April},
	author = {Schwille, Petra and Rigler, Rudolf},
	year = {1997},
}

@article{noauthor_table_2011,
	title = {Table 1 - {Nature} {Methods} {C} \$ {B} {Table} 1 - {Nature} {Methods}},
	year = {2011},
	pages = {1--2},
}

@article{eggeling_multi-parameter_2001,
	title = {Multi-parameter fluorescence detection at the single-molecule level: techniques and applications},
	url = {http://tobias-lib.uni-tuebingen.de/volltexte/2001/353/},
	journal = {Biosensor Symposium},
	author = {Eggeling, C and Schaffer, J and Volkmer, a and Seidel, C},
	year = {2001},
}

@article{murata_fluorescence_2000,
	title = {Fluorescence lifetime imaging of nuclear {DNA}: effect of fluorescence resonance energy transfer.},
	volume = {41},
	doi = {10.1002/1097-0320(20001101)41:3<178::AID-CYTO4>3.0.CO;2-N},
	abstract = {BACKGROUND: DNA fluorescence dyes have been used to study DNA dynamics, chromatin structure, and cell cycle analysis. However, most microscopic fluorescence studies of DNA use only steady-state measurements and do not take advantage of the additional information content of the time-resolved fluorescence. In this paper, we combine fluorescence imaging of DNA with time-resolved measurements to examine the proximity of donors and acceptors bound to chromatin. METHODS: We used frequency-domain fluorescence lifetime imaging microscopy to study the spatial distribution of DNA-bound donors and acceptors in fixed 3T3 nuclei. Over 50 cell nuclei were imaged in the presence of an AT-specific donor, Hoechst 33258 (Ho), and a GC-specific acceptor, 7-aminoactinomycin D (7-AAD). RESULTS: The intensity images of Ho alone showed a spatially irregular distribution due to the various concentrations of DNA or AT-rich DNA throughout the nuclei. The lifetime imaging of the Ho-stained nuclei was typically flat. Addition of 7-AAD decreased the fluorescence intensity and lifetime of the Ho-stained DNA. The spatially dependent phase and modulation values of Ho in the presence of 7-AAD showed that the Ho decay becomes nonexponential, as is expected for a resonance energy transfer (RET) with multiple acceptors located over a range of distances. In approximately 40 nuclei, the intensity and lifetime decrease was spatially homogeneous. In approximately 10 nuclei, addition of 7-AAD resulted in a spatially nonhomogeneous decrease in intensity and lifetime. The RET efficiency was higher in G(2)/M than in G(0/1) phase cells. CONCLUSIONS: Because RET efficiency depends on the average distance between Ho and 7-AAD, data suggest that the heterogeneity of lifetimes and spatial variation of the RET efficiency are caused by the presence of highly condensed regions of DNA in nuclei.},
	journal = {Cytometry},
	author = {Murata, S and Herman, P and Lin, H J and Lakowicz, J R},
	year = {2000},
	pages = {178--185},
}

@article{asai_fluorescence_1985-1,
	title = {Fluorescence correlation spectroscopy},
	abstract = {Fluorescence correlation spectroscopy (FCS) is a technique that allows for an extremely sensitive determination of molecular diffusion properties, down to the level of single molecules. It thus provides an attractive alternative to FRAP, requiring much less laser power and lower concentrations of fluorophores. FCS has recently been applied on live cells, and in comparison on domain-forming model membrane systems, to systematically study the influence of cholesterol on local membrane structure by investigating the mobility of selected lipid probes. The findings demonstrate the ability of FCS to sensitively distinguish between different local lipid structures, and emphasize the value of model systems for understanding membrane dynamics in general.},
	journal = {Tanpakushitsu kakusan koso. Protein, nucleic acid, enzyme},
	author = {Asai, H},
	year = {1985},
	pages = {246--248},
}

@article{noauthor_optogenetic_2010,
	title = {Optogenetic investigation of nervous system functions using walking behavior and genome wide transcript analysis of {Synapsin} and {Sap47} mutants of {Drosophila}},
	year = {2010},
}

@article{eggeling_monitoring_1998,
	title = {Monitoring conformational dynamics of a single molecule by selective fluorescence spectroscopy.},
	volume = {95},
	doi = {10.1073/pnas.95.4.1556},
	abstract = {A recently developed, real-time spectroscopic technique, burst-integrated fluorescence lifetime (BIFL), is shown to be well suited for monitoring the individual molecular conformational dynamics of a single molecule diffusing through the microscopic, open measurement volume (approximately 10 fl) of a confocal epi-illuminated set-up. In a highly diluted aqueous solution of 20-mer oligonucleotide strand of DNA duplex labeled with the environment-sensitive fluorescent dye tetramethylrhodamine (TMR), fluorescence bursts indicating traces of individual molecules are registered and further subjected to selective burst analysis. The two-dimensional BIFL data allow the identification and detection of different temporally resolved conformational states. A complementary autocorrelation analysis was performed on the time-dependent fluctuations in fluorescence lifetime and intensity. The consistent results strongly support the hypothesized three-state model of the conformational dynamics of the TMR-DNA duplex with a polar, a nonpolar, and a quenching environment of TMR.},
	number = {February},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Eggeling, C and Fries, J R and Brand, L and Günther, R and Seidel, C a},
	year = {1998},
	pages = {1556--1561},
}

@article{noauthor_532_nodate,
	title = {532 vs 561 nm},
}

@article{cai_improved_2013,
	title = {Improved tools for the {Brainbow} toolbox.},
	volume = {10},
	issn = {1548-7105 (Electronic){\textbackslash}r1548-7091 (Linking)},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3713494&tool=pmcentrez&rendertype=abstract},
	doi = {10.1038/nmeth.2450},
	abstract = {In the transgenic multicolor labeling strategy called 'Brainbow', Cre-loxP recombination is used to create a stochastic choice of expression among fluorescent proteins, resulting in the indelible marking of mouse neurons with multiple distinct colors. This method has been adapted to non-neuronal cells in mice and to neurons in fish and flies, but its full potential has yet to be realized in the mouse brain. Here we present several lines of mice that overcome limitations of the initial lines, and we report an adaptation of the method for use in adeno-associated viral vectors. We also provide technical advice about how best to image Brainbow-expressing tissue.},
	journal = {Nature methods},
	author = {Cai, Dawen and Cohen, Kimberly B and Luo, Tuanlian and Lichtman, Jeff W and Sanes, Joshua R},
	year = {2013},
	pages = {540--7},
}

@article{piston_fluorescent_2007,
	title = {Fluorescent protein {FRET}: the good, the bad and the ugly.},
	volume = {32},
	issn = {0968-0004 (Print) 0968-0004 (Linking)},
	doi = {10.1016/j.tibs.2007.08.003},
	abstract = {Dynamic protein interactions play a significant part in many cellular processes. A technique that shows considerable promise in elucidating such interactions is Förster resonance energy transfer (FRET). When combined with multiple, colored fluorescent proteins, FRET permits high spatial resolution assays of protein-protein interactions in living cells. Because FRET signals are usually small, however, their measurement requires careful interpretation and several control experiments. Nevertheless, the use of FRET in cell biological experiments has exploded over the past few years. Here we describe the physical basis of FRET and the fluorescent proteins appropriate for these experiments. We also review the approaches that can be used to measure FRET, with particular emphasis on the potential artifacts associated with each approach.},
	number = {9},
	journal = {Trends in biochemical sciences},
	author = {Piston, David W and Kremers, Gert-Jan},
	year = {2007},
	pages = {407--414},
}

@article{noauthor_pie_0410122852_001pdf_nodate,
	title = {{PIE}\_0410122852\_001.pdf},
}

@article{szmacinski_sodium_1997,
	title = {Sodium {Green} as a potential probe for intracellular sodium imaging based on fluorescence lifetime.},
	volume = {250},
	doi = {10.1006/abio.1997.2203},
	abstract = {We characterized the use of the fluorescent probe Sodium Green for measurements of intracellular free sodium using frequency-domain, phase-modulation fluorometry. The intensity decays were found to be strongly Na+ dependent, with mean lifetime increasing from 1.13 ns in the absence of Na+ to 2.39 ns in the presence of 140 mM Na+. Detailed analysis of the intensity decays in the presence of Na+ and K+ in the concentration range from 0 to 500 mM is provided. Sodium sensing using data measured at a single modulation frequency is described. Phase and modulation data showed high sensitivity to Na+ and substantially lower sensitivity to K+. Additionally, exposure of Sodium Green to intense illumination indicated that Sodium Green is much more photostable than its precursor, fluorescein. These results indicate that lifetime-based measurements with Sodium Green can be used for imaging of intracellular free [Na+] in the range from about 0.5 to 50 mM with high accuracy.},
	number = {250},
	journal = {Analytical biochemistry},
	author = {Szmacinski, H and Lakowicz, J R},
	year = {1997},
	pages = {131--138},
}

@article{maruvada_cell_2004,
	title = {Cell cycle-dependent expression of thyroid hormone receptor-beta is a mechanism for variable hormone sensitivity.},
	volume = {15},
	doi = {10.1091/mbc.E03},
	abstract = {Thyroid hormone receptors (TRs) are ligand-regulatable transcription factors. Currently, little is known about the expression of TRs or other nuclear hormone receptors during the cell cycle. We thus developed a stable expression system to express green fluorescent protein-TRbeta in HeLa cells under tetracycline regulation, and studied TR expression during the cell cycle by laser scanning cytometry. Only approximately 9-15\% of the nonsynchronized cell population expressed TR because the majority of cells were in G(1) phase and did not express detectable amounts of TR. However, when cells were synchronized in early S phase with hydroxyurea and then released, TR expression levels increased in a cell cycle-dependent manner and peaked to 30-40\% cells expressing TR at late G(2)/M phase before declining to nonsynchronized levels. Moreover, we observed a direct correlation between transcriptional activity and TR expression during the cell cycle. Similar cell cycle-dependent findings also were observed for endogenous TR in rat pituitary GH(3) cells. Last, cycloheximide studies demonstrated that the increase in TR expression was primarily due to increased translation. These novel observations of cell cycle-dependent expression of TR suggest that differential hormone sensitivity can occur during the cell cycle and may contribute to cell cycle progression during normal development and oncogenesis.},
	number = {June},
	journal = {Molecular biology of the cell},
	author = {Maruvada, Padma and Dmitrieva, Natalia I and East-Palmer, Joyce and Yen, Paul M},
	year = {2004},
	pages = {1895--1903},
}

@article{lumma_flow_2003,
	title = {Flow profile near a wall measured by double-focus fluorescence cross-correlation.},
	volume = {67},
	doi = {10.1103/PhysRevE.67.056313},
	abstract = {We present an experimental approach to flow profiling within femtoliter sample volumes, which allows the high-precision measurements at the solid interface. The method is based on the spatial cross-correlation of the fluorescence response from labeled tracer particles (latex nanospheres or single dye molecules). Two excitation volumes, separated by a few micrometers, are created by two laser foci under a confocal microscope. The velocity of tracer particles is measured in a channel about 100 microm wide within a typical accuracy of 0.1\%, and the positions of the walls are estimated independently of any hydrodynamic data. The underlying theory for the optical method is given for an arbitrary velocity profile, explicitly presenting the numerical convolutions necessary for a quantitative analysis. It is illustrated by using the Poiseuille flow of a Newtonian liquid with slip as an example. Our analysis yields a large apparent fluid velocity at the wall, which is mostly due to the impact of the colloidal (electrostatic) forces. This colloidal lift is crucially important in accelerating the transport processes of molecules and nanoparticles in microfluidic devices.},
	journal = {Physical review. E, Statistical, nonlinear, and soft matter physics},
	author = {Lumma, D and Best, a and Gansen, a and Feuillebois, F and Rädler, J O and Vinogradova, O I},
	year = {2003},
	pages = {056313--056313},
}

@article{palo_fluorescence_2000,
	title = {Fluorescence intensity multiple distributions analysis: concurrent determination of diffusion times and molecular brightness.},
	volume = {79},
	issn = {0006-3495},
	doi = {10.1016/S0006-3495(00)76523-4},
	abstract = {Fluorescence correlation spectroscopy (FCS) has proven to be a powerful technique with single-molecule sensitivity. Recently, it has found a complement in the form of fluorescence intensity distribution analysis (FIDA). Here we introduce a fluorescence fluctuation method that combines the features of both techniques. It is based on the global analysis of a set of photon count number histograms, recorded with multiple widths of counting time intervals simultaneously. This fluorescence intensity multiple distributions analysis (FIMDA) distinguishes fluorescent species on the basis of both the specific molecular brightness and the translational diffusion time. The combined information, extracted from a single measurement, increases the readout effectively by one dimension and thus breaks the individual limits of FCS and FIDA. In this paper a theory is introduced that describes the dependence of photon count number distributions on diffusion coefficients. The theory is applied to a series of photon count number histograms corresponding to different widths of counting time intervals. Although the ability of the method to determine specific brightness values, diffusion times, and concentrations from mixtures is demonstrated on simulated data, its experimental utilization is shown by the determination of the binding constant of a protein-ligand interaction exemplifying its broad applicability in the life sciences.},
	number = {December},
	journal = {Biophysical journal},
	author = {Palo, K and Mets, U and Jäger, S and Kask, P and Gall, K},
	year = {2000},
	pages = {2858--2866},
}

@article{hutterer_dynamics_2001,
	title = {Dynamics in diether lipid bilayers and interdigitated bilayer structures studied by time-resolved emission spectra, decay time and anisotropy profiles},
	volume = {11},
	issn = {1053-0509},
	url = {://WOS:000172600900009},
	abstract = {We present a comparative fluorescence spectroscopic investigation of diacyl and diether phosphatidylcholine vesicles using different probes with well-defined localization within either the hydrophilic headgroup region or the hydrophobic part of the bilayer. Time-resolved emission spectra have been used to characterize the solvent relaxation behavior in both symmetric and asymmetric diether and diacyl phosphatidylcholines. It is shown that time-resolved emission spectra of Prodan (6-propionyl-2-(dimethylamino)-naphthalene) and its long-alkyl chain derivative Patman (6-palmitoyl-2-[[trimethylammoniumethyl]methylamino]-naphthalene chloride) are a sensitive tool for the detection of differences in the micropolarities and viscosities at the hydrophobic/hydrophilic membrane interface of diether and diacyl lipids, respectively. Moreover, a new approach for the detection of interdigitated bilayers is discussed. It relies on the construction of anisotropy and decay time profiles for the set of n-anthroyloxy fatty acids and is compared with an older fluorescence assay based on intensity measurements only. The shape of plots of the fluorescence steady-state anisotropy versus the position of the chromophore (anthracene-9-carboxylic acid) combined with fluorescence lifetime measurements can be used to differentiate among non-fully, and mixed interdigitated gel phase structures and to predict structures for new lipid species.},
	number = {3},
	journal = {Journal of Fluorescence},
	author = {Hutterer, R and Hof, M},
	year = {2001},
	keywords = {acyl chain interdigitation, anthroyloxy fatty-acids, ether lipids, femtosecond solvation dynamics, gel phase, hydrostatic-pressure, lipid interdigitation, model membranes, n-anthroyloxy fatty acids, patman, phosphatidylcholine bilayers, phospholipid-membranes, prodan, solvent relaxation, surrogate hamiltonian description, time-resolved emission spectra},
	pages = {227--236},
}

@article{fluor_table_2011,
	title = {Table of {Fluorochromes}},
	abstract = {This is a table of some characteristics of fluorochromes useful for flow cytometry or fluorescence microscopy. Within groups, roughly in order of excitation wavelength (families excepted). Peak excitation and emission wavelengths often vary depending on the environment in which the probe finds itself. Be sure to also look up the excitation and emission spectra for your dye of choice. Note that colors you might see with a capable browser are only a very rough approximation!},
	journal = {Source},
	author = {Fluor, Alexa and Dyes, Cy},
	year = {2011},
	pages = {6--9},
}

@article{noauthor_biomed_nodate,
	title = {{BioMed} {Central}  {Full} text  {Dimerization} of {Receptor} {Protein}-},
}

@article{noauthor_rainbowl_nodate,
	title = {rainbowl},
}

@article{siegel_whole-field_2001,
	title = {Whole-field five-dimensional fluorescence microscopy combining lifetime and spectral resolution with optical sectioning.},
	volume = {26},
	doi = {10.1364/OL.26.001338},
	abstract = {We report a novel whole-field three-dimensional fluorescence lifetime imaging microscope that incoporates multispectral imaging to provide five-dimensional (5-D) fluorescence microscopy. This instrument, which can acquire a 5-D data set in less than a minute, is based on potentially compact and inexpensive diode-pumped solid-state laser technology. We demonstrate that spectral discrimination as well as optical sectioning minimize artifacts in lifetime determination and illustrate how spectral discrimination improves the lifetime contrast of biological tissue.},
	number = {17},
	journal = {Optics letters},
	author = {Siegel, J and Elson, D S and Webb, S E and Parsons-Karavassilis, D and Lévêque-Fort, S and Cole, M J and Lever, M J and French, P M and Neil, M a and Juskaitis, R and Sucharov, L O and Wilson, T},
	year = {2001},
	pages = {1338--1340},
}

@article{eggeling_comparison_2005,
	title = {Comparison of different fluorescence fluctuation methods for their use in {FRET} assays: monitoring a protease reaction.},
	volume = {6},
	issn = {1389-2010},
	doi = {10.2174/138920105774370571},
	abstract = {We compare the accuracy of a variety of Fluorescence Fluctuation Spectroscopy (FFS) methods for the study of Förster Resonance Energy Transfer (FRET) assays. As an example, the cleavage of a doubly labeled, FRET-active peptide substrate by the protease Trypsin is monitored and analyzed using methods based on fluorescence intensity, Fluorescence Correlation Spectroscopy (FCS) and Fluorescence Intensity Distribution Analysis (FIDA). The presented fluorescence data are compared to High-Pressure Liquid Chromatography (HPLC) data obtained from the same assay. The HPLC analysis discloses general disadvantages of the FRET approach, such as incomplete labeling and the need for aliquots. However, the simultaneous use of two photon detectors monitoring the fluorescence signal of both labels significantly improves the analysis. In particular, the two global analysis tools Two-Dimensional Fluorescence Intensity Distribution Analysis (2D-FIDA) and Two-Color Global Fluorescence Correlation Spectroscopy (2CG-FCS) highlight the potential of a combination of FFS and FRET. While conventional FIDA and FCS auto- or cross-correlation analysis leaves the user with drawbacks inherent in two-color and FRET applications, these effects are overcome by the global analysis on the molecular level. Furthermore, it is advantageous to analyze the unnormalized as opposed to the normalized correlation data when combining any fluorescence correlation method with FRET, since the analysis of the unnormalized data introduces more accuracy and is less sensitive to the experimental drawbacks.},
	journal = {Current pharmaceutical biotechnology},
	author = {Eggeling, C and Jäger, S and Winkler, D and Kask, Peet},
	year = {2005},
	keywords = {confocal microscopy, fluorescence correlation spectroscopy, fluorescence fluctuation spectroscopy, fluorescence intensity distribu-, förster resonance energy transfer, protease reaction, tion analysis},
	pages = {351--371},
}

@article{biol-_where_2002,
	title = {Where is the {Field} of {Clinical} {Immunology} {Going} to {Go} : {Assays} to {Rapidly} {Detect} and},
	volume = {2},
	number = {5},
	author = {Biol-, Molecular},
	year = {2002},
	pages = {224--227},
}

@article{hell_microscopy_2009,
	title = {Microscopy and its focal switch.},
	volume = {6},
	issn = {1548-7105 (Electronic){\textbackslash}r1548-7091 (Linking)},
	doi = {10.1038/nmeth.1291},
	abstract = {Until not very long ago, it was widely accepted that lens-based (far-field) optical microscopes cannot visualize details much finer than about half the wavelength of light. The advent of viable physical concepts for overcoming the limiting role of diffraction in the early 1990s set off a quest that has led to readily applicable and widely accessible fluorescence microscopes with nanoscale spatial resolution. Here I discuss the principles of these methods together with their differences in implementation and operation. Finally, I outline potential developments.},
	number = {1},
	journal = {Nature methods},
	author = {Hell, Stefan W},
	year = {2009},
	pages = {24--32},
}

@article{vielreicher_dynamic_2007,
	title = {Dynamic interaction between {Src} and {C}-terminal {Src} kinase in integrin α{IIbβ3}-mediated signaling to the cytoskeleton},
	volume = {282},
	doi = {10.1074/jbc.M704107200},
	abstract = {Integrin-bound Src tyrosine kinase mediates alpha(IIb)beta(3) out-side-in signaling to the cytoskeleton required for platelet adhesion and thrombus formation. Src activation (signal initiation) by phosphorylation of Tyr-418 occurs at lamellipodia leading edges. However, little is known about Src inactivation mediated by C-terminal Src kinase (Csk) Tyr-529 phosphorylation. In an established platelet model cell line (A5-Chinese hamster ovary), we studied the inactivation of Src during alpha(IIb)beta(3)-mediated adhesion to fibrinogen with live cell fluorescence resonance energy transfer (FRET) microscopy. Imaging revealed highly dynamic Src-Csk interactions at the leading edges of active lamellipodia. The Src-Csk interaction followed a highly dynamic pattern. Every 2-3 min, Src-Csk complexes moved inward in the cell, reorganized, and formed stable focal adhesions. These accumulations were primarily seen during retraction of lamellipodia, whereas no interaction was observed during protrusions. Western blot analysis during the run time of FRET signaling revealed an increase in Csk-mediated SrcTyr-529 phosphorylation with a parallel decline of tyrosine 418 phosphorylation. Mutation analysis provided additional insights into the role of Src. Although inactivation of Csk (CskK222R) had no effect on cell adhesion and spreading efficiency, cells with constitutively active expressed Src (SrcY529F) exhibited hardly any adhesion and no spreading. The few adherent cells showed weak focal adhesions that were disorganized and oversized. The data clearly demonstrate the important role of tight Src control by Csk for functional cell adhesion and spreading. The novel experimental FRET approach reported here for the inactivation of Src can be readily applied to other integrin and signaling pathways, including closely related Src family kinase members.},
	number = {8},
	journal = {Journal of Biological Chemistry},
	author = {Vielreicher, Martin and Harms, Gregory and Butt, Elke and Walter, Ulrich and Obergfell, Achim},
	year = {2007},
	pages = {33623--33631},
}

@article{guan_adaptive_2008,
	title = {Adaptive correction technique for {3D} reconstruction of fluorescence microscopy images.},
	volume = {71},
	issn = {1059-910X (Print)},
	doi = {10.1002/jemt},
	abstract = {Recent advances in high-resolution imaging have provided valuable novel insights into structural relationships within cells and tissues both in vitro and in vivo. An analysis of this kind is regularly done by optical sectioning using either confocal or deconvolution microscopy. However, the reconstruction of 3D images suffers from light scattering and absorption with increasing depth by finite transparency of the used media. Photobleaching of fluorochromes has been especially troublesome and often the only remedy for loss of signal during optical sectioning is to reduce the number of sections. This causes disparities in the x-y and z dimensions of voxels, which lead to vertical distortion of the original stack of images and necessitates interpolation. Interpolation is necessary to fill up the gaps between consecutive sections in the original image stack to obtain cubic voxels. The present manuscript describes a novel method for adaptive compensation of attenuation of light intensity in stacks of fluorescence microscopy images that is based on a physical model of light attenuation. First, we use a fast interpolation technique to generate a cubic voxel-based volume stack with the aid of a contribution look up table. With the contribution look up table, multiple calculations are avoided, which substantially reduces the computational time without compromising the accuracy of the restoration procedure. Second, each section within the resulting volume is processed to rectify its intensity values that have been altered due to photobleaching and scattering and absorption. The method allows to define the last good section in the stack and the correction is then done automatically.},
	number = {June},
	journal = {Microscopy research and technique},
	author = {Guan, Y Q and Cai, Y Y and Zhang, X and Lee, Y T and Opas, M},
	year = {2008},
	keywords = {microscopy, confocal, fluorescence yield, scan speed, scanning, triplet state},
	pages = {146--157},
}

@article{ve_perspect_2005,
	title = {{PERSPECT} {I} {VE} {A} g u i d e t o c h oo s i n g f l u o r e sc e n t pr o t e i n s},
	number = {December},
	author = {Ve, Perspect I},
	year = {2005},
}

@article{hillesheim_photon_2003,
	title = {The photon counting histogram in fluorescence fluctuation spectroscopy with non-ideal photodetectors.},
	volume = {85},
	issn = {0006-3495},
	doi = {10.1016/S0006-3495(99)76912-2},
	abstract = {Fluorescence fluctuation spectroscopy utilizes the signal fluctuations of single molecules for studying biological processes. Information about the biological system is extracted from the raw data by statistical methods such as used in fluctuation correlation spectroscopy or photon counting histogram (PCH) analysis. Since detectors are never ideal, it is crucial to understand the influence of photodetectors on signal statistics to correctly interpret the experimental data. Here we focus on the effects of afterpulsing and detector dead-time on PCH statistics. We determine the dead-time and afterpulse probability for our detectors experimentally and show that afterpulsing can be neglected for most experiments. Dead-time effects on the PCH are concentration-dependent and become significant when more than one molecule is present in the excitation volume. We develop a new PCH theory that includes dead-time effects and verify it experimentally. Additionally, we derive a simple analytical expression that accurately predicts the effect of dead-time on the molecular brightness. Corrections for non-ideal detector effects extend the useful concentration range of PCH experiments and are crucial for the interpretation of titration and dilution experiments.},
	number = {July},
	journal = {Biophysical journal},
	author = {Hillesheim, Lindsey N and Müller, Joachim D},
	year = {2003},
	pages = {1948--1958},
}

@article{wang_four-wave_2011,
	title = {Four-wave mixing microscopy of nanostructures},
	volume = {3},
	doi = {10.1364/AOP.3.000001},
	abstract = {The basics of four-wave mixing (FWM) and recent advances in FWM micros- copy are reviewed with a particular emphasis on applications in the field of na- nomaterials. The vast progress in nanostructure synthesis has triggered a need for advanced analytical tools suitable to interrogate nanostructures one at a time. The single-nanostructure sensitivity of optical microscopy has solidified the optical approach as a reliable technique for examining the electronic struc- ture of materials at the nanoscale. By zooming in on the individual, optical mi- croscopy has permitted detailed investigations of the linear optical response of nanomaterials such as semiconducting quantum dots and plasmon active nano- metals. Besides studying the linear optical properties of nanostructures, optical microscopy has also been used to probe the nonlinear optical properties of nanoscale materials. FWM microscopy, a coherent third-order optical imaging technique, has shown great potential as a tool for investigating the nonlinear optical response of nanostructures. FWM microscopy not only permits the characterization of the nonlinear susceptibility of individual nanostructures, it also offers a route to explore the time-resolved dynamics of electronic and vi- brational excitations on single structures. In addition, FWM produces strong signals from nanomaterials that are compatible with fast imaging applications, which holds promise for biological imaging studies based on nanoparticle la- bels that are not prone to photobleaching.},
	journal = {Advances in Optics and Photonics},
	author = {Wang, Yong and Lin, Chia-Yu and Nikolaenko, Alexei and Raghunathan, Varun and Potma, Eric O.},
	year = {2011},
	pages = {1--1},
}

@article{noauthor_ncomms3207-s14_nodate,
	title = {ncomms3207-s14},
}

@article{naturwissenschaften_new_nodate,
	title = {New advancements in highly sensitive time-resolved fluorescence two-photon microscopy},
	author = {Naturwissenschaften, Doktorin Der},
}

@article{stiehl_kinetics_2013,
	title = {Kinetics of conformational fluctuations in {DNA} hairpin-loops in crowded fluids},
	volume = {15},
	issn = {0027-8424},
	doi = {10.1088/1367-2630/15/11/113010},
	abstract = {The kinetics of DNA hairpin-loop fluctuations has been investigated by using a combination of fluorescence energy transfer and fluorescence correlation spectroscopy. We measure the chemical rates and the activation energies associated with the opening and the closing of the hairpin for different sizes and sequences of the loop and for various salt concentrations. The rate of unzipping of the hairpin stem is essentially independent of the characteristics of the loop, whereas the rate of closing varies greatly with the loop length and sequence. The closing rate scales with the loop length, with an exponent 2.6 +/- 0.3. The closing rate is increased at higher salt concentrations. For hairpin closing, a loop of adenosine repeats leads to smaller rates and higher activation energies than a loop with thymine repeats.},
	number = {July},
	journal = {New Journal of Physics},
	author = {Stiehl, Olivia and Weidner-Hertrampf, Kathrin and Weiss, Matthias},
	year = {2013},
	pages = {8602--8606},
}

@article{lippincott-schwartz_profile--jennifer_2001,
	title = {Profile--{Jennifer} {Lippincott}-{Schwartz}.},
	volume = {11},
	issn = {0962-8924 (Print)},
	number = {52},
	journal = {Trends in cell biology},
	author = {Lippincott-Schwartz, J},
	year = {2001},
	pages = {275--276},
}

@article{rinia_quantitative_2008,
	title = {Quantitative label-free imaging of lipid composition and packing of individual cellular lipid droplets using multiplex {CARS} microscopy.},
	volume = {95},
	issn = {0006-3495},
	doi = {10.1529/biophysj.108.137737},
	abstract = {Lipid droplets (LDs) are highly dynamic organelles that perform multiple functions, including the regulated storage and release of cholesterol and fatty acids. Information on the molecular composition of individual LDs within their cellular context is crucial in understanding the diverse biological functions of LDs, as well as their involvement in the development of metabolic disorders such as obesity, type II diabetes, and atherosclerosis. Although ensembles of LDs isolated from cells and tissues were analyzed in great detail, quantitative information on the heterogeneity in lipid composition of individual droplets, and possible variations within single lipid droplets, is lacking. Therefore, we used a label-free quantitative method to image lipids within LDs in 3T3-L1 cells. The method combines submicron spatial resolution in three dimensions, using label-free coherent anti-Stokes Raman scattering microscopy, with quantitative analysis based on the maximum entropy method. Our method allows quantitative imaging of the chemistry (level of acyl unsaturation) and physical state (acyl chain order) of individual LDs. Our results reveal variations in lipid composition and physical state between LDs contained in the same cell, and even within a single LD.},
	number = {3},
	journal = {Biophysical journal},
	author = {Rinia, Hilde a and Burger, Koert N J and Bonn, Mischa and Müller, Michiel},
	year = {2008},
	pages = {4908--4914},
}

@article{offterdinger_imaging_2004,
	title = {Imaging phosphorylation dynamics of the epidermal growth factor receptor},
	volume = {279},
	issn = {0021-9258 (Print){\textbackslash}n0021-9258 (Linking)},
	doi = {10.1074/jbc.M405830200},
	abstract = {Epidermal growth factor receptor (EGFR) signaling is initiated by ligand binding followed by homodimerization and rapid receptor autophosphorylation. Monitoring EGFR phosphorylation was achieved by measuring translocation and binding of an enhanced yellow fluorescent protein (EYFP)-labeled phosphotyrosine-binding domain (PTB) to enhanced cyan fluorescent protein (ECFP)-tagged EGFR using fluorescence lifetime imaging microscopy or sensitized emission measurements. To simplify dynamic phosphorylation pattern measurements in cells, FLAME, a ratiometric sensor containing both EGFR-ECFP and PTB-EYFP in one molecule, was designed and examined in COS7 cells. Epidermal growth factor (EGF) treatment demonstrated rapid and reversible changes in the EYFP/ECFP fluorescence emission ratios, due to binding of the PTB domain to its consensus binding sites upon phosphorylation at the cell periphery, whereas perinuclear regions failed to respond to EGF but were responsive to tyrosine kinase inhibition. Long-term EGF treatment resulted in accumulation of dephosphorylated receptor in the perinuclear region due to active dephosphorylation occurring at intracellular sites. This indicates that the sensor closely approaches the true dynamics of tyrosine kinase autophosphorylation and dephosphorylation. Phosphatase inhibition by pervanadate resulted in an irreversible response in all cellular compartments. These data show that EGFR is under tonic phosphatase suppression maintaining the receptor in an unphosphorylated (silent) state and is dephosphorylated at endomembranes after ligand-mediated endocytosis.},
	number = {35},
	journal = {Journal of Biological Chemistry},
	author = {Offterdinger, Martin and Georget, Virginie and Girod, Andreas and Bastiaens, P. I H},
	year = {2004},
	pages = {36972--36981},
}

@article{detection_atto_2001,
	title = {Atto {Labels} – {A} {Challenging} {Range} of {New} {Fluorescent} {Labels} {Contents} : • {Atto} {Labels} – {New} {Fluorescent} {Labels} • {Hemicyanine} {Dyes} optimized for {Excitation} at 633 nm • {Luminecent} {Oxygen} {Probes}},
	author = {Detection, Single-molecule},
	year = {2001},
}

@article{ying_ratiometric_2000,
	title = {Ratiometric {Analysis} of {Single}-{Molecule} {Fluorescence} {Resonance} {Energy} {Transfer} {Using} {Logical} {Combinations} of {Threshold} {Criteria}:  {A} {Study} of 12-mer {DNA}},
	volume = {104},
	url = {http://pubs.acs.org/doi/abs/10.1021/jp993914k},
	doi = {10.1021/jp993914k},
	abstract = {Single-molecule fluorescence resonance energy transfer (FRET) combined with bulk fluorescence lifetimes, anisotropy, and spectra have been used to study a donor-acceptor labeled model DNA system (Cy5-5'-ACCTGCCGACGC-3'-TMR). A general ratiometric analysis method using independent donor and acceptor thresholding has been developed. Use of two logical combinations of thresholding criteria provides more information than either method alone, revealing heterogeneity within this system. Conditions yielding similar bulk fluorescence spectra can be readily distinguished by this single-molecule method. Fluorescence lifetimes and anisotropy measurements also suggest nonnegligible fluorophore-DNA interaction},
	journal = {The Journal of Physical Chemistry B},
	author = {Ying, Liming and Wallace, Mark I. and Balasubramanian, Shankar and Klenerman, David},
	year = {2000},
	pages = {5171--5178},
}

@article{invitrogen_fluorescence_2006,
	title = {Fluorescence {Polarization}},
	abstract = {Fluorescence polarization (FP) is a powerful tool for studying molecular interactions by monitoring changes in the apparent size of fluorescently-labeled or inherently fluorescent molecules, often referred to as the tracer or ligand (Checovich et al., 1995; Heyduk et al., 1996; Jameson and Sawyer, 1995; Nasir and Jolley, 1999). It is unique among methods used to analyze molecular binding because it gives a direct, nearly instantaneous measure of a tracer’s bound/free ratio.  FP enables the researcher to view molecular binding events in solution, allowing true equilibrium analysis into the low picomolar range (i.e., with as little as 10 fmol/mL of sample stoichiometrically labeled with fluorescein). FP measurements do not affect samples, so they can be treated and reanalyzed in order to ascertain the effect on binding by such changes as pH, temperature, and salt concentration. In addition, because FP measurements are taken in “real-time,” experiments are not limited to equilibrium binding studies. Kinetic analysis of association and dissociation reactions are routine with fluorescence polarization. Because FP is a truly homogeneous technique, it does not require the separation of bound and free species. Methods that depend on separation are not only more time-consuming, but they disturb the reaction equilibrium and therefore prevent accurate quantification of binding. Alternative homogeneous fluorescent techniques, such as fluorescence resonance energy transfer (FRET) and time-resolved fluorescence or TR-FRET (Pope et al., 1999) require multiple labeling reactions instead of one as in FP.},
	journal = {Technical Resource Guide},
	author = {{Invitrogen}},
	year = {2006},
	keywords = {fluorescence, fluorescence anisotropy, fluorescence polarization},
}

@article{seiffert_polyacrylamidketten_2004,
	title = {Polyacrylamidketten in konzentrierten {Systemen} {Untersucht} mittels fluorescence recovery after {Diplomarbeit}},
	author = {Seiffert, Sebastian},
	year = {2004},
}

@article{noauthor_spacer_nodate,
	title = {spacer},
}

@article{von_der_hocht_fluorescence_2007,
	title = {Fluorescence correlation spectroscopy in cells: {Confinement} and excluded volume effects},
	volume = {82},
	doi = {10.1016/j.yexmp.2006.12.009},
	abstract = {Fluorescence correlation spectroscopy (FCS) has become an important technique in biophysical research, which is also used for in vivo studies of molecular mobilities in cells. We theoretically study how confinement or exclusion of the diffusing fluorescent molecules by a spherical region influences the measured autocorrelation function in an FCS experiment. It is shown that close to the boundary of the spherical region the diffusion time can be significantly changed due to the geometric restriction of the detection volume. This is important when quantitatively evaluating and interpreting FCS measurements in cells. © 2007 Elsevier Inc. All rights reserved.},
	journal = {Experimental and Molecular Pathology},
	author = {von der Hocht, Iris and Enderlein, Jörg},
	year = {2007},
	pages = {142--146},
}

@article{schaeferling_fluorescence_2005,
	title = {Fluorescence {Lifetime} {Imaging} ( {FLIM} ) of {Sensor} {Membranes} , {Arrays} , and {Micro} {Titer} {Plates}},
	volume = {12},
	author = {Schaeferling, M and Lin, Z and Wolfbeis, O S},
	year = {2005},
	pages = {1--5},
}

@article{noauthor_no_1944,
	title = {No {Title}},
	year = {1944},
}

@article{royer_probing_2006,
	title = {Probing {Protein} {Folding} and {Conformational} {Transitions} with {Fluorescence} {Probing} {Protein} {Folding} and {Conformational} {Transitions} with {Fluorescence}},
	volume = {106},
	doi = {10.1021/cr0404390},
	author = {Royer, Catherine a},
	year = {2006},
	pages = {1769--1784},
}

@article{guan_adaptive_2008-1,
	title = {Adaptive correction technique for {3D} reconstruction of fluorescence microscopy images.},
	volume = {71},
	issn = {1059-910X (Print)},
	doi = {10.1002/jemt},
	abstract = {Recent advances in high-resolution imaging have provided valuable novel insights into structural relationships within cells and tissues both in vitro and in vivo. An analysis of this kind is regularly done by optical sectioning using either confocal or deconvolution microscopy. However, the reconstruction of 3D images suffers from light scattering and absorption with increasing depth by finite transparency of the used media. Photobleaching of fluorochromes has been especially troublesome and often the only remedy for loss of signal during optical sectioning is to reduce the number of sections. This causes disparities in the x-y and z dimensions of voxels, which lead to vertical distortion of the original stack of images and necessitates interpolation. Interpolation is necessary to fill up the gaps between consecutive sections in the original image stack to obtain cubic voxels. The present manuscript describes a novel method for adaptive compensation of attenuation of light intensity in stacks of fluorescence microscopy images that is based on a physical model of light attenuation. First, we use a fast interpolation technique to generate a cubic voxel-based volume stack with the aid of a contribution look up table. With the contribution look up table, multiple calculations are avoided, which substantially reduces the computational time without compromising the accuracy of the restoration procedure. Second, each section within the resulting volume is processed to rectify its intensity values that have been altered due to photobleaching and scattering and absorption. The method allows to define the last good section in the stack and the correction is then done automatically.},
	number = {August},
	journal = {Microscopy research and technique},
	author = {Guan, Y Q and Cai, Y Y and Zhang, X and Lee, Y T and Opas, M},
	year = {2008},
	keywords = {fluorescence lifetime imaging, fluorescence resonance energy transfer, fluorescent proteins, imaging, live cell, protein-protein interaction},
	pages = {146--157},
}

@article{webb_single-molecule_2008,
	title = {Single-molecule imaging and fluorescence lifetime imaging microscopy show different structures for high- and low-affinity epidermal growth factor receptors in {A431} cells.},
	volume = {94},
	doi = {10.1529/biophysj.107.112623},
	abstract = {Epidermal growth factor (EGF) receptor (EGFR) modulates mitosis and apoptosis through signaling by its high-affinity (HA) and low-affinity (LA) EGF-binding states. The prevailing model of EGFR activation-derived from x-ray crystallography-involves the transition from tethered ectodomain monomers to extended back-to-back dimers and cannot explain these EGFR affinities or their different functions. Here, we use single-molecule Förster resonant energy transfer analysis in combination with ensemble fluorescence lifetime imaging microscopy to investigate the three-dimensional architecture of HA and LA EGFR-EGF complexes in cells by measuring the inter-EGF distances within discrete EGF pairs and the vertical distance from EGF to the plasma membrane. Our results show that EGFR ectodomains form interfaces resulting in two inter-EGF distances ( approximately 8 nm and {\textless} 5.5 nm), different from the back-to-back EGFR ectodomain interface ( approximately 11 nm). Distance measurements from EGF to the plasma membrane show that HA EGFR ectodomains are oriented flat on the membrane, whereas LA ectodomains stand proud from it. Their flat orientation confers on HA EGFR ectodomains the exclusive ability to interact via asymmetric interfaces, head-to-head with respect to the EGF-binding site, whereas LA EGFRs must interact only side-by-side. Our results support a structural model in which asymmetric EGFR head-to-head interfaces may be relevant for HA EGFR oligomerization.},
	number = {0},
	journal = {Biophysical journal},
	author = {Webb, Stephen E D and Roberts, Selene K and Needham, Sarah R and Tynan, Christopher J and Rolfe, Daniel J and Winn, Martyn D and Clarke, David T and Barraclough, Roger and Martin-Fernandez, Marisa L},
	year = {2008},
	keywords = {affinity, egfr, fret, signalling, single- molecule},
	pages = {803--819},
}

@article{gosch_fluorescence_2005,
	title = {Fluorescence correlation spectroscopy of molecular motions and kinetics},
	volume = {57},
	doi = {10.1016/j.addr.2004.07.016},
	abstract = {The foundations for fluorescence correlation spectroscopy (FCS) were already laid in the early 1970s, but this technique did not become widely used until single-molecule detection was established almost 20 years later with the use of diffraction-limited confocal volume element. The analysis of molecular noise from the GHz- to the Hz-region facilitates measurements over a large dynamic range covering photophysics, conformational transitions and interactions as well as transport properties of fluorescent biomolecules. From the Poissonian nature of the noise spectrum the absolute number of molecules is obtainable. Originally used for the analysis of molecular interactions in solutions, the strength of FCS lies also in its applicability to molecular processes at either the surface or interior of single cells. Examples for the analysis of surface kinetics including on and off rates of ligand-receptor interactions will be given. The possibility of obtaining this type of information by FCS will be of particular interest for cell-based drug screening. © 2004 Elsevier B.V. All rights reserved.},
	journal = {Advanced Drug Delivery Reviews},
	author = {Gösch, Michael and Rigler, Rudolf},
	year = {2005},
	keywords = {Fluorescence spectroscopy, Autocorrelation, FCS, FRAP, FRET, Parallel detection, PCH, Photophysics, TIR},
	pages = {169--190},
}

@article{bacskai_fluorescence_2003,
	title = {Fluorescence resonance energy transfer determinations using multiphoton fluorescence lifetime imaging microscopy to characterize amyloid-beta plaques.},
	volume = {8},
	issn = {1083-3668 (Print){\textbackslash}r1083-3668 (Linking)},
	doi = {10.1117/1.1584442},
	abstract = {We describe the implementation of a commercial fluorescence lifetime imaging microscopy (FLIM) instrument used in conjunction with a commercial laser scanning multiphoton microscope. The femtosecond-pulsed near-infrared laser is an ideal excitation source for time-domain fluorescence lifetime measurements. With synchronization from the x-y scanners, fluorescence lifetimes can be acquired on a pixel-by-pixel basis, with high spatial resolution. Multiexponential curve fits for each pixel result in two-dimensional fluorescence resonance energy transfer (FRET) measurements that allow the determination of both proximity of fluorescent FRET pairs, as well as the fraction of FRET pairs close enough for FRET to occur. Experiments are described that characterize this system, as well as commonly used reagents valuable for FRET determinations in biological systems. Constructs of CFP and YFP were generated to demonstrate FRET between this pair of green fluorescent protein (GFP) color variants. The lifetime characteristics of the FRET pair fluorescein and rhodamine, commonly used for immunohistochemistry, were also examined. Finally, these fluorophores were used to demonstrate spatially resolved FRET with senile plaques obtained from transgenic mouse brain. Together these results demonstrate that FLIM allows sensitive measurements of protein-protein interactions on a spatial scale less than 10 nm using commercially available components.},
	number = {3},
	journal = {Journal of biomedical optics},
	author = {Bacskai, Brian J and Skoch, Jesse and Hickey, Gregory a and Allen, Racquel and Hyman, Bradley T},
	year = {2003},
	keywords = {microscopy, 13, 2002, 2003, 22, 7, accepted for publication jan, alzheimer, amyloid-, croscopy, fluorescence lifetime, fluorescence lifetime imaging mi-, multiphoton, paper mm-06 received oct, revised manuscript received jan, s disease},
	pages = {368--375},
}

@article{constants_analysis_nodate,
	title = {Analysis of {FP} {Binding} {Data}},
	journal = {Analysis},
	author = {Constants, Binding and Experiments, Competition and Experiments, Kinetic},
	pages = {1--12},
}

@article{herman_frequency-domain_2001,
	title = {Frequency-domain fluorescence microscopy with the {LED} as a light source},
	volume = {203},
	number = {May},
	author = {Herman, P and Herman, P and Maliwal, B P and Maliwal, B P and Lakowicz, J R and Lakowicz, J R},
	year = {2001},
	keywords = {fluorescence, microscopy, frequency-domain, led, lifetime},
	pages = {176--181},
}

@article{eggeling_direct_2009,
	title = {Direct observation of the nanoscale dynamics of membrane lipids in a living cell.},
	volume = {457},
	issn = {0028-0836},
	doi = {10.1038/nature07596},
	abstract = {Cholesterol-mediated lipid interactions are thought to have a functional role in many membrane-associated processes such as signalling events. Although several experiments indicate their existence, lipid nanodomains ('rafts') remain controversial owing to the lack of suitable detection techniques in living cells. The controversy is reflected in their putative size of 5-200 nm, spanning the range between the extent of a protein complex and the resolution limit of optical microscopy. Here we demonstrate the ability of stimulated emission depletion (STED) far-field fluorescence nanoscopy to detect single diffusing (lipid) molecules in nanosized areas in the plasma membrane of living cells. Tuning of the probed area to spot sizes approximately 70-fold below the diffraction barrier reveals that unlike phosphoglycerolipids, sphingolipids and glycosylphosphatidylinositol-anchored proteins are transiently ( approximately 10-20 ms) trapped in cholesterol-mediated molecular complexes dwelling within {\textless}20-nm diameter areas. The non-invasive optical recording of molecular time traces and fluctuation data in tunable nanoscale domains is a powerful new approach to study the dynamics of biomolecules in living cells.},
	number = {February},
	journal = {Nature},
	author = {Eggeling, Christian and Ringemann, Christian and Medda, Rebecca and Schwarzmann, Günter and Sandhoff, Konrad and Polyakova, Svetlana and Belov, Vladimir N and Hein, Birka and von Middendorff, Claas and Schönle, Andreas and Hell, Stefan W},
	year = {2009},
	pages = {1159--1162},
}

@article{rajaram_simucell_2012,
	title = {{SimuCell}: a flexible framework for creating synthetic microscopy images},
	volume = {9},
	issn = {1548-7105 (Electronic){\textbackslash}r1548-7091 (Linking)},
	doi = {10.1038/nmeth.2096},
	number = {7},
	journal = {Nature Methods},
	author = {Rajaram, Satwik and Pavie, Benjamin and Hac, Nicholas E F and Altschuler, Steven J and Wu, Lani F},
	year = {2012},
	pages = {634--635},
}

@article{jung_ultrafast_2005,
	title = {Ultrafast fluorescence depolarisation in the yellow fluorescent protein due to its dimerisation},
	volume = {6},
	doi = {10.1002/cphc.200400653},
	abstract = {Transient absorption spectroscopy with sub-100 fs time resolution was performed to investigate the oligomerisation behaviour of eYFP in solution. A single time constant tau(AD)=2.2+/-0.15 ps is sufficient to describe the time-resolved anisotropy decay up to at least 200 ps. The close contact of two protein barrels is deduced as the exclusive aggregation state in solution. From the final anisotropy r(infinity)=0.28+/-0.02, the underlying quaternary structure can be traced back to the somewhat distorted structure of the dimers of wt-GFP. The use of autofluorescent proteins as rulers in Förster resonance energy transfer (FRET) measurements may demand polarisation-sensitive detection of the fluorescence with high time resolution.},
	journal = {ChemPhysChem},
	author = {Jung, Gregor and Ma, Yingzhong and Prall, Bradley S. and Fleming, Graham R.},
	year = {2005},
	keywords = {Time-resolved spectroscopy, Fluorescence, FRET (fluorescence resonance energy transfer), Proteins, Single-molecule studies},
	pages = {1628--1632},
}

@article{xie_living_2006,
	title = {Living cells as test tubes.},
	volume = {312},
	issn = {1095-9203 (Electronic){\textbackslash}n0036-8075 (Linking)},
	doi = {10.1126/science.1127566},
	abstract = {The combination of specific probes and advanced optical microscopy now allows quantitative probing of biochemical reactions in living cells. On selected systems, one can detect and track a particular protein with single-molecule sensitivity, nanometer spatial precision, and millisecond time resolution. Metabolites, usually difficult to detect, can be imaged and monitored in living cells with coherent anti-Stokes Raman scattering microscopy. Here, we describe the application of these techniques in studying gene expression, active transport, and lipid metabolism.},
	number = {2006},
	journal = {Science (New York, N.Y.)},
	author = {Xie, X Sunney and Yu, Ji and Yang, Wei Yuan},
	year = {2006},
	pages = {228--230},
}

@article{digman_measuring_2005,
	title = {Measuring fast dynamics in solutions and cells with a laser scanning microscope.},
	volume = {89},
	issn = {0006-3495},
	doi = {10.1529/biophysj.105.062836},
	abstract = {Single-point fluorescence correlation spectroscopy (FCS) allows measurements of fast diffusion and dynamic processes in the microsecond-to-millisecond time range. For measurements on living cells, image correlation spectroscopy (ICS) and temporal ICS extend the FCS approach to diffusion times as long as seconds to minutes and simultaneously provide spatially resolved dynamic information. However, ICS is limited to very slow dynamics due to the frame acquisition rate. Here we develop novel extensions to ICS that probe spatial correlations in previously inaccessible temporal windows. We show that using standard laser confocal imaging techniques (raster-scan mode) not only can we reach the temporal scales of single-point FCS, but also have the advantages of ICS in providing spatial information. This novel method, called raster image correlation spectroscopy (RICS), rapidly measures during the scan many focal points within the cell providing the same concentration and dynamic information of FCS as well as information on the spatial correlation between points along the scanning path. Longer time dynamics are recovered from the information in successive lines and frames. We exploit the hidden time structure of the scan method in which adjacent pixels are a few microseconds apart thereby accurately measuring dynamic processes such as molecular diffusion in the microseconds-to-seconds timescale. In conjunction with simulated data, we show that a wide range of diffusion coefficients and concentrations can be measured by RICS. We used RICS to determine for the first time spatially resolved diffusions of paxillin-EGFP stably expressed in CHOK1 cells. This new type of data analysis has a broad application in biology and it provides a powerful tool for measuring fast as well as slower dynamic processes in cellular systems using any standard laser confocal microscope.},
	number = {August},
	journal = {Biophysical journal},
	author = {Digman, Michelle a and Brown, Claire M and Sengupta, Parijat and Wiseman, Paul W and Horwitz, Alan R and Gratton, Enrico},
	year = {2005},
	pages = {1317--1327},
}

@article{wiseman_spatial_2004,
	title = {Spatial mapping of integrin interactions and dynamics during cell migration by image correlation microscopy.},
	volume = {117},
	issn = {0021-9533 (Print)},
	doi = {10.1242/jcs.01416},
	abstract = {Image correlation microscopy methodology was extended and used to determine retrospectively the density, dynamics and interactions of alpha5-integrin in migrating cells. Alpha5-integrin is present in submicroscopic clusters containing 3-4 integrins before it is discernibly organized. The integrin in nascent adhesions, as identified by the presence of paxillin, is approximately 1.4 times more concentrated, approximately 4.5 times more clustered and much less mobile than in surrounding regions. Thus, while integrins are clustered throughout the cell, they differ in nascent adhesions and appear to initiate adhesion formation, despite their lack of visible organization. In more mature adhesions where the integrin is visibly organized there are approximately 900 integrins microm(-2) (about fivefold higher than surrounding regions). Interestingly, alpha5-integrin and alpha-actinin, but not paxillin, reside in a complex throughout the cell, where they diffuse and flow together, even in regions where they are not organized. During adhesion disassembly some integrins diffuse away slowly, alpha-actinin undergoes a directed movement at speeds similar to actin retrograde flow (0.29 microm min(-1)), while all of the paxillin diffuses away rapidly.},
	journal = {Journal of cell science},
	author = {Wiseman, Paul W and Brown, Claire M and Webb, Donna J and Hebert, Benedict and Johnson, Natalie L and Squier, Jeff a and Ellisman, Mark H and Horwitz, a F},
	year = {2004},
	keywords = {adhesions, cell migration, correlation microscopy, cytoskeleton, paxillin, α -actinin, α 5-integrin},
	pages = {5521--5534},
}

@article{dittrich_spatial_2002,
	title = {Spatial two-photon fluorescence cross-correlation spectroscopy for controlling molecular transport in microfluidic structures},
	volume = {74},
	issn = {1357313578},
	doi = {10.1021/ac025625p},
	abstract = {The increasing availability of microfluidic systems of various geometries and materials for the downscaling of chemical or biochemical processes raises a strong demand for adequate techniques to precisely determine flow parameters and to control fluid and particle manipulation. Of all readout parameters, fluorescence analysis of the fluid or suspended particles is particularly attractive, as it can be employed without mechanical interference and with a sensitivity high enough to detect single molecules in aqueous environments. In this study, we present the determination of flow parameters, such as velocity and direction, in microstructured channels by fluorescence correlation spectroscopy (FCS), a method based on single molecule spectroscopy carried out in confocal optical setups. Different modes of FCS, such as auto- and dual-beam cross-correlation techniques by one- and two-photon excitation, are discussed. Known advantages of two-photon excitation, such as highly restricted detection volumes and low scattering background, are shown to be particularly valuable for measurements in tiny channel systems. Although conventional autocorrelation is sufficient for describing the velocity of single molecules, dual-beam cross-correlation allows the separation of isotropic and anisotropic dynamics, for example, to monitor flow directions or to discriminate against photophysical effects that could be mistaken for mobility parameters. It can be shown that time-gated two-photon excitation in the dual-beam mode significantly lowers the undesired cross-talk between the two measurement volumes. Finally, some applications, such as the calibration of microfluidic sorting units and flow profiling, are demonstrated.},
	number = {17},
	journal = {Analytical Chemistry},
	author = {Dittrich, Petra S. and Schwille, Petra},
	year = {2002},
	pages = {4472--4479},
}

@article{wennmalm_conformational_1997,
	title = {Conformational fluctuations in single {DNA} molecules.},
	volume = {94},
	issn = {0027-8424},
	doi = {10.1073/pnas.94.20.10641},
	abstract = {Measurement of fluorescent lifetimes of dye-tagged DNA molecules reveal the existence of different conformations. Conformational fluctuations observed by fluorescence correlation spectroscopy give rise to a relaxation behavior that is described by "stretched" exponentials and indicates the presence of a distribution of transition rates between two conformations. Whether this is an inhomogeneous distribution, where each molecule contributes with its own reaction rate to the overall distribution, or a homogeneous distribution, where the reaction rate of each molecule is time-dependent, is not yet known. We used a tetramethylrhodamine-linked 217-bp DNA oligonucleotide as a probe for conformational fluctuations. Fluorescence fluctuations from single DNA molecules attached to a streptavidin-coated surface directly show the transitions between two conformational states. The conformational fluctuations typical for single molecules are similar to those seen in single ion channels in cell membranes.},
	number = {September},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Wennmalm, S and Edman, L and Rigler, R},
	year = {1997},
	pages = {10641--10646},
}

@article{keese_quantitative_2007,
	title = {Quantitative imaging of apoptosis commitment in colorectal tumor cells},
	volume = {75},
	issn = {0301-4681 (Print){\textbackslash}r0301-4681 (Linking)},
	doi = {10.1111/j.1432-0436.2007.00186.x},
	abstract = {We have studied caspase-3 activation by combined DNA damage induction and EGFR kinase inhibition in order to identify potential EGFR-mediated survival signals conferring resistance to apoptosis in human colorectal tumor cells. The onset of apoptosis was microscopically imaged with a newly developed caspase-3 substrate sensor based on EGFP and tHcred1, enabling us to monitor caspase-3 activation in cells by fluorescence lifetime imaging microscopy or fluorescence correlation spectroscopy. Both optical approaches provide parameters quantitatively reporting the ratio between cleaved and uncleaved sensor, thereby facilitating the comparison of caspase-3 activation between different cells. Using these methods, we show that EGFR kinase inhibitors sensitize colorectal SW-480 tumor cells for 5-fluorouracil-induced apoptosis, indicating that EGFR-mediated survival signaling contributes to apoptosis resistance via its intrinsic kinase activity.},
	journal = {Differentiation},
	author = {Keese, Michael and Offterdinger, Martin and Tischer, Christian and Girod, Andreas and Lommerse, P. H M and Yagublu, Vugar and Magdeburg, Richard and Bastiaens, P. I H},
	year = {2007},
	keywords = {FRET, Apoptosis, Caspase-3, EGFR, Fluorescence correlation spectroscopy (FCS), Fluorescence lifetime imaging microscopy (FLIM)},
	pages = {809--818},
}

@article{kask_fluorescence-intensity_1999,
	title = {Fluorescence-intensity distribution analysis and its application in biomolecular detection technology.},
	volume = {96},
	issn = {0027-8424 (Print){\textbackslash}r0027-8424 (Linking)},
	doi = {10.1073/pnas.96.24.13756},
	abstract = {A methodology, fluorescence-intensity distribution analysis, has been developed for confocal microscopy studies in which the fluorescence intensity of a sample with a heterogeneous brightness profile is monitored. An adjustable formula, modeling the spatial brightness distribution, and the technique of generating functions for calculation of theoretical photon count number distributions serve as the two cornerstones of the methodology. The method permits the simultaneous determination of concentrations and specific brightness values of a number of individual fluorescent species in solution. Accordingly, we present an extremely sensitive tool to monitor the interaction of fluorescently labeled molecules or other microparticles with their respective biological counterparts that should find a wide application in life sciences, medicine, and drug discovery. Its potential is demonstrated by studying the hybridization of 5'-(6-carboxytetramethylrhodamine)-labeled and nonlabeled complementary oligonucleotides and the subsequent cleavage of the DNA hybrids by restriction enzymes.},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Kask, P and Palo, K and Ullmann, D and Gall, K},
	year = {1999},
	pages = {13756--13761},
}

@article{according_brightness_nodate,
	title = {Brightness and {\textless} {N} {\textgreater} button in the {RICS} screen of {SimFCS}},
	number = {0},
	author = {According, Background and Simfcs, The},
}

@article{creager_technical_2009,
	title = {Technical matters: method, knowledge and  infrastructure in twentieth-century life science.},
	volume = {6},
	url = {http://dx.doi.org/10.1038/nmeth1009-701},
	doi = {10.1038/nmeth1009-701},
	abstract = {Conceptual breakthroughs in science tend to garner accolades and attention. But, as the invention of tissue culture and the development of isotopic tracers show, innovative methods open up new fields and enable the solution of longstanding problems.},
	number = {10},
	journal = {Nature methods},
	author = {Creager, Angela N H and Landecker, Hannah},
	year = {2009},
	note = {Publisher: Nature Publishing Group},
	pages = {701--705},
}

@article{kankaanpaa_bioimagexd_2012,
	title = {{BioImageXD}: an open, general-purpose and high-throughput image-processing platform},
	volume = {9},
	issn = {1548-7105 (Electronic){\textbackslash}n1548-7091 (Linking)},
	doi = {10.1038/nmeth.2047},
	abstract = {BioImageXD puts open-source computer science tools for three-dimensional visualization and analysis into the hands of all researchers, through a user-friendly graphical interface tuned to the needs of biologists. BioImageXD has no restrictive licenses or undisclosed algorithms and enables publication of precise, reproducible and modifiable workflows. It allows simple construction of processing pipelines and should enable biologists to perform challenging analyses of complex processes. We demonstrate its performance in a study of integrin clustering in response to selected inhibitors.},
	number = {7},
	journal = {Nature Methods},
	author = {Kankaanpää, Pasi and Paavolainen, Lassi and Tiitta, Silja and Karjalainen, Mikko and Päivärinne, Joacim and Nieminen, Jonna and Marjomäki, Varpu and Heino, Jyrki and White, Daniel J},
	year = {2012},
	pages = {683--689},
}

@article{sutherland_simultaneous_2002,
	title = {Simultaneous measurement of circular dichroism and fluorescence polarization anisotropy},
	volume = {4625},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=872393},
	doi = {10.1117/12.469782},
	abstract = {Circular dichroism and fluorescence polarization anisotropy are important tools for characterizing biomolecular systems. Both are used extensively in kinetic experiments involving stopped - or continuous flow systems as well as titrations and steady-state spectroscopy. This paper presents the theory for determining circular dichroism and fluorescence polarization anisotropy simultaneously, thus insuring the two parameters are recorded under exactly the same conditions and at exactly the same time in kinetic experiments. The approach to measuring circular dichroism is that used in almost all conventional dichrographs. Two arrangements for measuring fluorescence polarization anisotropy are described. One uses a single fluorescence detector and signal processing with a lock-in amplifier that is similar to the measurement of circular dichroism. The second approach uses classic T format detection optics, and thus can be used with conventional photon-counting detection electronics. Simple extensions permit the simultaneous measurement of the absorption and excitation intensity corrected fluorescence intensity.},
	journal = {International Symposium on …},
	author = {Sutherland, John C.},
	year = {2002},
	keywords = {circular dichroism, fluorescence polarization anisotropy, modulator, photoelastic, stopped flow kinetics, ultraviolet absorption spectra},
	pages = {126--136},
}

@article{nadrigny_systematic_2007,
	title = {Systematic colocalization errors between acridine orange and {EGFP} in astrocyte vesicular organelles.},
	volume = {93},
	issn = {3314286422},
	doi = {10.1529/biophysj.106.102673},
	abstract = {Dual-color imaging of acridine orange (AO) and EGFP fused to a vesicular glutamate transporter or the vesicle-associated membrane proteins 2 or 3 has been used to visualize a supposedly well-defined subpopulation of glutamatergic astrocytic secretory vesicles undergoing regulated exocytosis. However, AO metachromasy results in the concomitant emission of green and red fluorescence from AO-stained tissue. Therefore, the question arises whether AO and EGFP fluorescence can be distinguished reliably. We used evanescent-field imaging with spectral fluorescence detection as well as fluorescence lifetime imaging microscopy to demonstrate that green fluorescent AO monomers inevitably coexist with red fluorescing AO dimers, at the level of single astroglial vesicles. The green monomer emission spectrally overlaps with that of EGFP and produces a false apparent colocalization on dual-color images. On fluorophore abundance maps calculated from spectrally resolved and unmixed single-vesicle spectral image stacks, EGFP is obscured by the strong green monomer fluorescence, precluding the detection of EGFP. Hence, extreme caution is required when deriving quantitative colocalization information from images of dim fluorescing EGFP-tagged organelles colabeled with bright and broadly emitting dyes like AO. We finally introduce FM4-64/EGFP dual-color imaging as a remedy for imaging a distinct population of astroglial fusion-competent secretory vesicles.},
	number = {August},
	journal = {Biophysical journal},
	author = {Nadrigny, Fabien and Li, Dongdong and Kemnitz, Klaus and Ropert, Nicole and Koulakoff, Annette and Rudolph, Stephanie and Vitali, Marco and Giaume, Christian and Kirchhoff, Frank and Oheim, Martin},
	year = {2007},
	pages = {969--980},
}

@article{glucokinase_original_2007,
	title = {Original {Article}},
	volume = {56},
	doi = {10.2337/db06-0894.DMEM},
	number = {May},
	journal = {Insulin},
	author = {Glucokinase, Diffusible},
	year = {2007},
	pages = {1305--1315},
}

@article{enderlein_art_2004,
	title = {Art and artefacts of fluorescence correlation spectroscopy.},
	volume = {5},
	issn = {1389-2010 (Print)1389-2010 (Linking)},
	doi = {10.2174/1389201043377020},
	abstract = {Fluorescence correlation spectroscopy (FCS) is an important technique for studying low concentrations of analyte molecules in solution. The core molecular characteristic that can be addressed by FCS is the translational diffusion coefficient of the analyte molecules, which can be used for i.e. studying molecular binding and reactions, or conformational changes of macromolecules. The present paper discusses several possible optical and photophysical effects that can influence the outcome of a FCS measurement and thus can bias the value of the derived diffusion coefficient.},
	journal = {Current pharmaceutical biotechnology},
	author = {Enderlein, Jörg and Gregor, Ingo and Patra, Digambara and Fitter, Jörg},
	year = {2004},
	keywords = {fluorescence correlation spectroscopy, confocal detection, diffusion coefficients},
	pages = {155--161},
}

@article{tertoolen_dimerization_2001,
	title = {Dimerization of receptor protein-tyrosine phosphatase alpha in living cells.},
	volume = {2},
	issn = {1471-2121 (Electronic){\textbackslash}r1471-2121 (Linking)},
	doi = {10.1186/1471-2121-2-8},
	abstract = {BACKGROUND: Dimerization is an important regulatory mechanism of single membrane-spanning receptors. For instance, activation of receptor protein-tyrosine kinases (RPTKs) involves dimerization. Structural, functional and biochemical studies suggested that the enzymatic counterparts of RPTKs, the receptor protein-tyrosine phosphatases (RPTPs), are inhibited by dimerization, but whether RPTPs actually dimerize in living cells remained to be determined. RESULTS: In order to assess RPTP dimerization, we have assayed Fluorescence Resonance Energy Transfer (FRET) between chimeric proteins of cyan- and yellow-emitting derivatives of green fluorescent protein, fused to RPTPalpha, using three different techniques: dual wavelength excitation, spectral imaging and fluorescence lifetime imaging. All three techniques suggested that FRET occurred between RPTPalpha -CFP and -YFP fusion proteins, and thus that RPTPalpha dimerized in living cells. RPTPalpha dimerization was constitutive, extensive and specific. RPTPalpha dimerization was consistent with cross-linking experiments, using a non-cell-permeable chemical cross-linker. Using a panel of deletion mutants, we found that the transmembrane domain was required and sufficient for dimerization. CONCLUSIONS: We demonstrate here that RPTPalpha dimerized constitutively in living cells, which may be mediated by the transmembrane domain, providing strong support for the model that dimerization is involved in regulation of RPTPs.},
	journal = {BMC cell biology},
	author = {Tertoolen, L G and Blanchetot, C and Jiang, G and Overvoorde, J and Gadella, T W and Hunter, T and den Hertog, J},
	year = {2001},
	pages = {8--8},
}

@article{clair_analysis_1997,
	title = {Analysis of highly disfavored processes through pathway-specific correlated fluorescence.},
	volume = {94},
	doi = {10.1073/pnas.94.5.1623},
	abstract = {A new method is described for the detection of disfavored reaction pathways. The approach combines organic synthesis, to independently prepare reactant and product, with the low detection limits of confocally adjusted fluorescence correlation spectroscopy. Selective detection of disfavored products is achieved by designing a system in which the analyte displays a unique absorption and emission of light. This was accomplished through application of a substance-selective intramolecular charge transfer. The power of this technique was demonstrated by monitoring the progress of the thermal retro-[2+2]-cycloaddition. In conjunction with concurrent 1H-NMR monitoring, the relative abundance of major and disfavored reaction products can be determined and used to calculate the energetics of processes disfavored by more than 5 kcal/mol.},
	number = {March},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Clair, J J},
	year = {1997},
	pages = {1623--1628},
}

@article{bundesanstalt_comparison_2008,
	title = {Comparison and accuracy of methods to determine the confocal volume for quantitative fluorescence correlation spectroscopy},
	volume = {232},
	number = {May},
	journal = {Society},
	author = {Bundesanstalt, Physikalisch-technische and Bundesanstalt, Physikalisch-technische},
	year = {2008},
	keywords = {fcs, confocal microscopy, confocal volume},
	pages = {343--352},
}

@article{schwille_molecular_1999,
	title = {Molecular dynamics in living cells observed by fluorescence correlation spectroscopy with one- and two-photon excitation.},
	volume = {77},
	issn = {0006-3495 (Print){\textbackslash}r0006-3495 (Linking)},
	doi = {10.1016/S0006-3495(99)77065-7},
	abstract = {Multiphoton excitation (MPE) of fluorescent probes has become an attractive alternative in biological applications of laser scanning microscopy because many problems encountered in spectroscopic measurements of living tissue such as light scattering, autofluorescence, and photodamage can be reduced. The present study investigates the characteristics of two-photon excitation (2PE) in comparison with confocal one-photon excitation (1PE) for intracellular applications of fluorescence correlation spectroscopy (FCS). FCS is an attractive method of measuring molecular concentrations, mobility parameters, chemical kinetics, and fluorescence photophysics. Several FCS applications in mammalian and plant cells are outlined, to illustrate the capabilities of both 1PE and 2PE. Photophysical properties of fluorophores required for quantitative FCS in tissues are analyzed. Measurements in live cells and on cell membranes are feasible with reasonable signal-to-noise ratios, even with fluorophore concentrations as low as the single-molecule level in the sampling volume. Molecular mobilities can be measured over a wide range of characteristic time constants from approximately 10(-3) to 10(3) ms. While both excitation alternatives work well for intracellular FCS in thin preparations, 2PE can substantially improve signal quality in turbid preparations like plant cells and deep cell layers in tissue. At comparable signal levels, 2PE minimizes photobleaching in spatially restrictive cellular compartments, thereby preserving long-term signal acquisition.},
	number = {October},
	journal = {Biophysical journal},
	author = {Schwille, P and Haupts, U and Maiti, S and Webb, W W},
	year = {1999},
	pages = {2251--2265},
}

@article{royer_approaches_1995,
	title = {Approaches to teaching fluorescence spectroscopy.},
	volume = {68},
	issn = {0006-3495 (Print){\textbackslash}n0006-3495 (Linking)},
	doi = {10.1016/S0006-3495(95)80295-X},
	number = {March},
	journal = {Biophysical journal},
	author = {Royer, C a},
	year = {1995},
	pages = {1191--1195},
}

@article{becker_fret_2001,
	title = {{FRET} {Measurements} by {TCSPC} {Laser} {Scanning} {Microscopy}},
	volume = {4431},
	url = {http://www.becker-hickl.de/pdf/spcfret.pdf},
	doi = {10.1117/12.447406},
	abstract = {We use a two-photon laser scanning microscope with a new Time-Correlated Single Photon Counting (TCSPC) imaging technique to obtain combined intensity-lifetime images for FRET measurements in living cells. Single photon pulses from a photomultiplier and signals from the scanning head are used to record the three-dimensional photon density over the time- and image coordinates. Double exponential decay analysis delivers the lifetime components of the quenched and the unquenched molecules in all pixels of the image. We use the ratio of the intensity coefficients of the fast and slow decay component to create images that show the size of the FRET effects in different parts of the cell.},
	number = {June},
	journal = {Proc. SPIE},
	author = {Becker, Wolfgang and Becker, Wolfgang and Benndorf, Klaus and Benndorf, Klaus and Bergmann, Axel and Bergmann, Axel and Biskup, Christoph and Biskup, Christoph and König, Karsten and König, Karsten and Tirplapur, Uday and Tirplapur, Uday and Zimmer, Thomas and Zimmer, Thomas},
	year = {2001},
	keywords = {spectroscopy, fluorescence microscopy, time-resolved},
	pages = {414--419},
}

@article{digman_paxillin_2008,
	title = {Paxillin dynamics measured during adhesion assembly and disassembly by correlation spectroscopy.},
	volume = {94},
	issn = {00063495},
	doi = {10.1529/biophysj.107.104984},
	abstract = {Paxillin is an adaptor molecule involved in the assembly of focal adhesions. Using different fluorescence fluctuation approaches, we established that paxillin-EGFP is dynamic on many timescales within the cell, ranging from milliseconds to seconds. In the cytoplasmic regions, far from adhesions, paxillin is uniformly distributed and freely diffusing as a monomer, as determined by single-point fluctuation correlation spectroscopy and photon-counting histogram analysis. Near adhesions, paxillin dynamics are reduced drastically, presumably due to binding to protein partners within the adhesions. The photon-counting histogram analysis of the fluctuation amplitudes reveals that this binding equilibrium in new or assembling adhesions is due to paxillin monomers binding to quasi-immobile structures, whereas in disassembling adhesions or regions of adhesions, the equilibrium is due to exchange of large aggregates. Scanning fluctuation correlation spectroscopy and raster-scan image correlation spectroscopy analysis of laser confocal images show that the environments within adhesions are heterogeneous. Relatively large adhesions appear to slide transversally due to a treadmilling mechanism through the addition of monomeric paxillin at one side and removal of relatively large aggregates of proteins from the retracting edge. Total internal reflection microscopy performed with a fast acquisition EM-CCD camera completes the overall dynamic picture and adds details of the heterogeneous dynamics across single adhesions and simultaneous bursts of activity at many adhesions across the cell.},
	number = {April},
	journal = {Biophysical journal},
	author = {Digman, Michelle a and Brown, Claire M and Horwitz, Alan R and Mantulin, William W and Gratton, Enrico},
	year = {2008},
	pages = {2819--2831},
}

@article{digman_phasor_2008,
	title = {The phasor approach to fluorescence lifetime imaging analysis.},
	volume = {94},
	issn = {0006-3495},
	doi = {10.1529/biophysj.107.120154},
	abstract = {Changing the data representation from the classical time delay histogram to the phasor representation provides a global view of the fluorescence decay at each pixel of an image. In the phasor representation we can easily recognize the presence of different molecular species in a pixel or the occurrence of fluorescence resonance energy transfer. The analysis of the fluorescence lifetime imaging microscopy (FLIM) data in the phasor space is done observing clustering of pixels values in specific regions of the phasor plot rather than by fitting the fluorescence decay using exponentials. The analysis is instantaneous since is not based on calculations or nonlinear fitting. The phasor approach has the potential to simplify the way data are analyzed in FLIM, paving the way for the analysis of large data sets and, in general, making the FLIM technique accessible to the nonexpert in spectroscopy and data analysis.},
	number = {D},
	journal = {Biophysical journal},
	author = {Digman, Michelle a and Caiolfa, Valeria R and Zamai, Moreno and Gratton, Enrico},
	year = {2008},
	pages = {L14--L16},
}

@article{zhu_faster_2012,
	title = {Faster {STORM} using compressed sensing},
	volume = {9},
	issn = {1548-7091},
	doi = {10.1038/nmeth.1978},
	abstract = {In super-resolution microscopy methods based on single-molecule switching, the rate of accumulating single-molecule activation events often limits the time resolution. Here we developed a sparse-signal recovery technique using compressed sensing to analyze images with highly overlapping fluorescent spots. This method allows an activated fluorophore density an order of magnitude higher than what conventional single-molecule fitting methods can handle. Using this method, we demonstrated imaging microtubule dynamics in living cells with a time resolution of 3 s.},
	number = {7},
	journal = {Nature Methods},
	author = {Zhu, Lei and Zhang, Wei and Elnatan, Daniel and Huang, Bo},
	year = {2012},
	pages = {721--723},
}

@article{shao_i5s_2008,
	title = {{I5S}: wide-field light microscopy with 100-nm-scale resolution in three dimensions.},
	volume = {94},
	issn = {0006-3495},
	doi = {10.1529/biophysj.107.120352},
	abstract = {A new type of wide-field fluorescence microscopy is described, which produces 100-nm-scale spatial resolution in all three dimensions, by using structured illumination in a microscope that has two opposing objective lenses. Illumination light is split by a grating and a beam splitter into six mutually coherent beams, three of which enter the specimen through each objective lens. The resulting illumination intensity pattern contains high spatial frequency components both axially and laterally. In addition, the emission is collected by both objective lenses coherently, and combined interferometrically on a single camera, resulting in a detection transfer function with axially extended support. These two effects combine to produce near-isotropic resolution. Experimental images of test samples and biological specimens confirm the theoretical predictions.},
	number = {June},
	journal = {Biophysical journal},
	author = {Shao, Lin and Isaac, Berith and Uzawa, Satoru and Agard, David a and Sedat, John W and Gustafsson, Mats G L},
	year = {2008},
	pages = {4971--4983},
}

@article{imanishi_two-photon_2009,
	title = {Two-{Photon} {Microscopy}:shedding light on the chemistry of vision},
	volume = {46},
	issn = {2163681300},
	doi = {10.1021/bi701055g.Two-photon},
	journal = {Biochemistry},
	author = {Imanishi, Kerrie; Koutalos, Yiannis, Yoshikazu; Lodowski},
	year = {2009},
	keywords = {fluorescence, allowed the visualization of, and has been instrumental, biological, eye, for several hundred years, in the, photoreceptor, retina, second harmonic generation, structures and processes invisible, the light microscope has, to the naked eye, two-photon microscopy},
	pages = {9674--9684},
}

@article{stirbet_chlorophyllafluorescence_1998,
	title = {{ChlorophyllaFluorescence} {Induction} in {Higher} {Plants}: {Modelling} and {Numerical} {Simulation}},
	volume = {193},
	url = {http://www.sciencedirect.com/science/article/pii/S0022519398906920},
	doi = {10.1006/jtbi.1998.0692},
	abstract = {Chlorophyllafluorescence induction is extensively used as a probe of photosynthesis, and thus, it has become necessary to quantitatively analyse it to extend its usefulness. We simulate the experimental data of fluorescence transients in strong light through numerical integration, both in dark- and light-adapted plants. In the mathematical model used here we have considered for the first time the redox reactions at both the acceptor and the donor sides of photosystem II, and the non-photochemical quenching by the oxidised plastoquinone molecules from the lipid matrix of the thylakoid membrane. The model is based on assumptions established in the literature and also the values of input parameters used in simulations. The simulated fluorescence induction curves show the characteristics O→J→I→P steps as in the experimental ones and, in specific conditions, the presence of a dip (D) between the I and P steps of the transient. Moreover, it has been shown here how typical patterns of fluorescence kinetics are influenced by the state of the sample by studying the basic effects of the influence of some parameters [i.e. the connectivity between different PS II units, initial QB:QB−ratio and the ratio of the starting states of the oxygen evolving complex (S1:S2), number of plastoquinone molecules in the plastoquinone pool, initial redox state of the plastoquinone pool, and the rate of plastoquinol oxidation]. In this way the information can be drawn from the experimental curves relative to these parameters.},
	journal = {Journal of Theoretical Biology},
	author = {Stirbet, a},
	year = {1998},
	pages = {131--151},
}

@article{digman_measuring_2005-1,
	title = {Measuring fast dynamics in solutions and cells with a laser scanning microscope.},
	volume = {89},
	issn = {0006-3495},
	doi = {10.1529/biophysj.105.062836},
	abstract = {Single-point fluorescence correlation spectroscopy (FCS) allows measurements of fast diffusion and dynamic processes in the microsecond-to-millisecond time range. For measurements on living cells, image correlation spectroscopy (ICS) and temporal ICS extend the FCS approach to diffusion times as long as seconds to minutes and simultaneously provide spatially resolved dynamic information. However, ICS is limited to very slow dynamics due to the frame acquisition rate. Here we develop novel extensions to ICS that probe spatial correlations in previously inaccessible temporal windows. We show that using standard laser confocal imaging techniques (raster-scan mode) not only can we reach the temporal scales of single-point FCS, but also have the advantages of ICS in providing spatial information. This novel method, called raster image correlation spectroscopy (RICS), rapidly measures during the scan many focal points within the cell providing the same concentration and dynamic information of FCS as well as information on the spatial correlation between points along the scanning path. Longer time dynamics are recovered from the information in successive lines and frames. We exploit the hidden time structure of the scan method in which adjacent pixels are a few microseconds apart thereby accurately measuring dynamic processes such as molecular diffusion in the microseconds-to-seconds timescale. In conjunction with simulated data, we show that a wide range of diffusion coefficients and concentrations can be measured by RICS. We used RICS to determine for the first time spatially resolved diffusions of paxillin-EGFP stably expressed in CHOK1 cells. This new type of data analysis has a broad application in biology and it provides a powerful tool for measuring fast as well as slower dynamic processes in cellular systems using any standard laser confocal microscope.},
	number = {August},
	journal = {Biophysical journal},
	author = {Digman, Michelle a and Brown, Claire M and Sengupta, Parijat and Wiseman, Paul W and Horwitz, Alan R and Gratton, Enrico},
	year = {2005},
	pages = {1317--1327},
}

@article{campbell_monomeric_2002-1,
	title = {A monomeric red fluorescent protein.},
	volume = {99},
	issn = {0027-8424 (Print){\textbackslash}r0027-8424 (Linking)},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/12060735},
	doi = {10.1073/pnas.082243699},
	abstract = {All coelenterate fluorescent proteins cloned to date display some form of quaternary structure, including the weak tendency of Aequorea green fluorescent protein (GFP) to dimerize, the obligate dimerization of Renilla GFP, and the obligate tetramerization of the red fluorescent protein from Discosoma (DsRed). Although the weak dimerization of Aequorea GFP has not impeded its acceptance as an indispensable tool of cell biology, the obligate tetramerization of DsRed has greatly hindered its use as a genetically encoded fusion tag. We present here the stepwise evolution of DsRed to a dimer and then either to a genetic fusion of two copies of the protein, i.e., a tandem dimer, or to a true monomer designated mRFP1 (monomeric red fluorescent protein). Each subunit interface was disrupted by insertion of arginines, which initially crippled the resulting protein, but red fluorescence could be rescued by random and directed mutagenesis totaling 17 substitutions in the dimer and 33 in mRFP1. Fusions of the gap junction protein connexin43 to mRFP1 formed fully functional junctions, whereas analogous fusions to the tetramer and dimer failed. Although mRFP1 has somewhat lower extinction coefficient, quantum yield, and photostability than DsRed, mRFP1 matures {\textgreater}10 times faster, so that it shows similar brightness in living cells. In addition, the excitation and emission peaks of mRFP1, 584 and 607 nm, are approximately 25 nm red-shifted from DsRed, which should confer greater tissue penetration and spectral separation from autofluorescence and other fluorescent proteins.},
	number = {12},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Campbell, Robert E and Campbell, Robert E and Tour, Oded and Tour, Oded and Palmer, Amy E and Palmer, Amy E and Steinbach, Paul a and Steinbach, Paul a and Baird, Geoffrey S and Baird, Geoffrey S and Zacharias, David a and Zacharias, David a and Tsien, Roger Y and Tsien, Roger Y},
	year = {2002},
	keywords = {Amino Acid Sequence, Animals, Cell Line, Cnidaria, Cnidaria: metabolism, Dimerization, Kinetics, Luminescent Proteins, Luminescent Proteins: chemistry, Luminescent Proteins: genetics, Luminescent Proteins: metabolism, Mammals, Models, Molecular, Molecular Sequence Data, Mutagenesis, Site-Directed, Protein Structure, Quaternary, Recombinant Proteins, Recombinant Proteins: chemistry, Recombinant Proteins: metabolism, Transfection},
	pages = {7877--82},
}

@article{schneider_nih_2012,
	title = {{NIH} {Image} to {ImageJ}: 25 years of image analysis},
	volume = {9},
	issn = {1548-7091},
	url = {http://dx.doi.org/10.1038/nmeth.2089},
	doi = {10.1038/nmeth.2089},
	abstract = {For the past 25 years NIH Image and ImageJ software have been pioneers as open tools for the analysis of scientific images. We discuss the origins, challenges and solutions of these two programs, and how their history can serve to advise and inform other software projects.},
	number = {7},
	journal = {Nature Methods},
	author = {Schneider, Caroline a and Rasband, Wayne S and Eliceiri, Kevin W},
	year = {2012},
	note = {Publisher: Nature Publishing Group},
	pages = {671--675},
}

@article{probes_alexa_2008,
	title = {Alexa {Fluor} {Dyes} - {Simply} the {Best} and {Brightest} {Fluorescent} {Dyes} and {Conjugates}},
	url = {http://www.google.com/search?client=safari&rls=en-us&q=2008-MolecularProbes-Alexa+Fluor+Dyes+-+Simply+the+Best+and+Brightest+Fluorescent+Dyes+and+Conjugates&ie=UTF-8&oe=UTF-8},
	journal = {Molecular Probes},
	author = {Probes, Molecular},
	year = {2008},
	pages = {36--36},
}

@article{shtengel_interferometric_2009,
	title = {Interferometric fluorescent super-resolution microscopy resolves {3D} cellular ultrastructure.},
	volume = {106},
	issn = {1091-6490 (Electronic){\textbackslash}n0027-8424 (Linking)},
	doi = {10.1073/pnas.0813131106},
	abstract = {Understanding molecular-scale architecture of cells requires determination of 3D locations of specific proteins with accuracy matching their nanometer-length scale. Existing electron and light microscopy techniques are limited either in molecular specificity or resolution. Here, we introduce interferometric photoactivated localization microscopy (iPALM), the combination of photoactivated localization microscopy with single-photon, simultaneous multiphase interferometry that provides sub-20-nm 3D protein localization with optimal molecular specificity. We demonstrate measurement of the 25-nm microtubule diameter, resolve the dorsal and ventral plasma membranes, and visualize the arrangement of integrin receptors within endoplasmic reticulum and adhesion complexes, 3D protein organization previously resolved only by electron microscopy. iPALM thus closes the gap between electron tomography and light microscopy, enabling both molecular specification and resolution of cellular nanoarchitecture.},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Shtengel, Gleb and Galbraith, James a and Galbraith, Catherine G and Lippincott-Schwartz, Jennifer and Gillette, Jennifer M and Manley, Suliana and Sougrat, Rachid and Waterman, Clare M and Kanchanawong, Pakorn and Davidson, Michael W and Fetter, Richard D and Hess, Harald F},
	year = {2009},
	pages = {3125--3130},
}

@article{ronneberger_erratum_2012,
	title = {Erratum: {ViBE}-{Z}: a framework for {3D} virtual colocalization analysis in zebrafish larval brains},
	volume = {9},
	doi = {10.1038/nmeth1012-1031c},
	abstract = {Precise three-dimensional (3D) mapping of a large number of gene expression patterns, neuronal types and connections to an anatomical reference helps us to understand the vertebrate brain and its development. We developed the Virtual Brain Explorer (ViBE-Z), a software that automatically maps gene expression data with cellular resolution to a 3D standard larval zebrafish (Danio rerio) brain. ViBE-Z enhances the data quality through fusion and attenuation correction of multiple confocal microscope stacks per specimen and uses a fluorescent stain of cell nuclei for image registration. It automatically detects 14 predefined anatomical landmarks for aligning new data with the reference brain. ViBE-Z performs colocalization analysis in expression databases for anatomical domains or subdomains defined by any specific pattern; here we demonstrate its utility for mapping neurons of the dopaminergic system. The ViBE-Z database, atlas and software are provided via a web interface.},
	number = {7},
	journal = {Nature Methods},
	author = {Ronneberger, Olaf and Liu, Kun and Rath, Meta and Rueß, Dominik and Mueller, Thomas and Skibbe, Henrik and Drayer, Benjamin and Schmidt, Thorsten and Filippi, Alida and Nitschke, Roland and Brox, Thomas and Burkhardt, Hans and Driever, Wolfgang},
	year = {2012},
	pages = {1031--1031},
}

@article{drepper_reporter_2007,
	title = {Reporter proteins for in vivo fluorescence without oxygen.},
	volume = {25},
	issn = {1087-0156 (Print){\textbackslash}r1087-0156 (Linking)},
	doi = {10.1038/nbt1293},
	abstract = {Fluorescent reporter proteins such as green fluorescent protein are valuable noninvasive molecular tools for in vivo real-time imaging of living specimens. However, their use is generally restricted to aerobic systems, as the formation of their chromophores strictly requires oxygen. Starting with blue-light photoreceptors from Bacillus subtilis and Pseudomonas putida that contain light-oxygen-voltage-sensing domains, we engineered flavin mononucleotide-based fluorescent proteins that can be used as fluorescent reporters in both aerobic and anaerobic biological systems.},
	number = {4},
	journal = {Nature biotechnology},
	author = {Drepper, Thomas and Eggert, Thorsten and Circolone, Franco and Heck, Achim and Krauss, Ulrich and Guterl, Jan-Karl and Wendorff, Marion and Losi, Aba and Gärtner, Wolfgang and Jaeger, Karl-Erich},
	year = {2007},
	pages = {443--445},
}

@article{cardona_current_2012,
	title = {Current challenges in open-source bioimage informatics},
	volume = {9},
	issn = {1548-7105 (Electronic){\textbackslash}r1548-7091 (Linking)},
	url = {http://dx.doi.org/10.1038/nmeth.2082},
	doi = {10.1038/nmeth.2082},
	abstract = {We discuss the advantages and challenges of the open-source strategy in biological image analysis and argue that its full impact will not be realized without better support and recognition of software engineers' contributions to the biological sciences and more support of this development model from funders and institutions.},
	number = {7},
	journal = {Nature Methods},
	author = {Cardona, Albert and Tomancak, Pavel},
	year = {2012},
	note = {Publisher: Nature Publishing Group},
	pages = {661--665},
}

@article{jain_high_2012,
	title = {High {Content} {Screening} in {Neurodegenerative} {Diseases}},
	doi = {10.3791/3452},
	abstract = {The functional annotation of genomes, construction of molecular networks and novel drug target identification, are important challenges that need to be addressed as a matter of great urgency. Multiple complementary 'omics' approaches have provided clues as to the genetic risk factors and pathogenic mechanisms underlying numerous neurodegenerative diseases, but most findings still require functional validation. For example, a recent genome wide association study for Parkinson's Disease (PD), identified many new loci as risk factors for the disease, but the underlying causative variant(s) or pathogenic mechanism is not known. As each associated region can contain several genes, the functional evaluation of each of the genes on phenotypes associated with the disease, using traditional cell biology techniques would take too long. There is also a need to understand the molecular networks that link genetic mutations to the phenotypes they cause. It is expected that disease phenotypes are the result of multiple interactions that have been disrupted. Reconstruction of these networks using traditional molecular methods would be time consuming. Moreover, network predictions from independent studies of individual components, the reductionism approach, will probably underestimate the network complexity. This underestimation could, in part, explain the low success rate of drug approval due to undesirable or toxic side effects. Gaining a network perspective of disease related pathways using HT/HC cellular screening approaches, and identifying key nodes within these pathways, could lead to the identification of targets that are more suited for therapeutic intervention. High-throughput screening (HTS) is an ideal methodology to address these issues. but traditional methods were one dimensional whole-well cell assays, that used simplistic readouts for complex biological processes. They were unable to simultaneously quantify the many phenotypes observed in neurodegenerative diseases such as axonal transport deficits or alterations in morphology properties. This approach could not be used to investigate the dynamic nature of cellular processes or pathogenic events that occur in a subset of cells. To quantify such features one has to move to multi-dimensional phenotypes termed high-content screening (HCS). HCS is the cell-based quantification of several processes simultaneously, which provides a more detailed representation of the cellular response to various perturbations compared to HTS. HCS has many advantages over HTS, but conducting a high-throughput (HT)-high-content (HC) screen in neuronal models is problematic due to high cost, environmental variation and human error. In order to detect cellular responses on a 'phenomics' scale using HC imaging one has to reduce variation and error, while increasing sensitivity and reproducibility. Herein we describe a method to accurately and reliably conduct shRNA screens using automated cell culturing and HC imaging in neuronal cellular models. We describe how we have used this methodology to identify modulators for one particular protein, DJ1, which when mutated causes autosomal recessive parkinsonism. Combining the versatility of HC imaging with HT methods, it is possible to accurately quantify a plethora of phenotypes. This could subsequently be utilized to advance our understanding of the genome, the pathways involved in disease pathogenesis as well as identify potential therapeutic targets.},
	number = {January},
	journal = {Journal of Visualized Experiments},
	author = {Jain, Shushant and van Kesteren, Ronald E. and Heutink, Peter},
	year = {2012},
	keywords = {s disease, high-content screening, 1, 2012, 6, article distributed under the, automated cell culturing, commons attribution non-commercial license, date published, high-throughput screening, issue 59, medicine, neurodegeneration, parkinson, terms of the creative, this is an open-access, which permits, x2019},
	pages = {1--8},
}

@article{eder_einzelmolekulspektroskopie_2007,
	title = {Einzelmolekülspektroskopie ( {Fluoreszenzmethoden} ) {Bedeutung}},
	author = {Eder, Wolfgang and Sp, Michael},
	year = {2007},
}

@article{elson_time-domain_2004,
	title = {Time-domain fluorescence lifetime imaging applied to biological tissue.},
	volume = {3},
	issn = {1474-905X (Print){\textbackslash}n1474-905X (Linking)},
	doi = {10.1039/b316456j},
	abstract = {Fluorescence lifetime imaging (FLIM) is a functional imaging methodology that can provide information, not only concerning the localisation of specific fluorophores, but also about the local fluorophore environment. It may be implemented in scanning confocal or multi-photon microscopes, or in wide-field microscopes and endoscopes. When applied to tissue autofluorescence, it reveals intrinsic excellent contrast between different types and states of tissue. This article aims to review our recent progress in developing time-domain FLIM technology for microscopy and endoscopy and applying it to biological tissue.},
	journal = {Photochemical \& photobiological sciences : Official journal of the European Photochemistry Association and the European Society for Photobiology},
	author = {Elson, Dan and Requejo-Isidro, Jose and Munro, Ian and Reavell, Fred and Siegel, Jan and Suhling, Klaus and Tadrous, Paul and Benninger, Richard and Lanigan, Peter and McGinty, James and Talbot, Clifford and Treanor, Bebhinn and Webb, Stephen and Sandison, Ann and Wallace, Andrew and Davis, Dan and Lever, John and Neil, Mark and Phillips, David and Stamp, Gordon and French, Paul},
	year = {2004},
	pages = {795--801},
}

@article{noauthor_application-flim-fret_nodate,
	title = {application-{FLIM}-{FRET}},
}

@article{kolin_k-space_2006,
	title = {k-{Space} image correlation spectroscopy: a method for accurate transport measurements independent of fluorophore photophysics.},
	volume = {91},
	issn = {0006-3495},
	url = {http://dx.doi.org/10.1529/biophysj.106.082768},
	doi = {10.1529/biophysj.106.082768},
	abstract = {We present the theory and application of reciprocal space image correlation spectroscopy (kICS). This technique measures the number density, diffusion coefficient, and velocity of fluorescently labeled macromolecules in a cell membrane imaged on a confocal, two-photon, or total internal reflection fluorescence microscope. In contrast to r-space correlation techniques, we show kICS can recover accurate dynamics even in the presence of complex fluorophore photobleaching and/or "blinking". Furthermore, these quantities can be calculated without nonlinear curve fitting, or any knowledge of the beam radius of the exciting laser. The number densities calculated by kICS are less sensitive to spatial inhomogeneity of the fluorophore distribution than densities measured using image correlation spectroscopy. We use simulations as a proof-of-principle to show that number densities and transport coefficients can be extracted using this technique. We present calibration measurements with fluorescent microspheres imaged on a confocal microscope, which recover Stokes-Einstein diffusion coefficients, and flow velocities that agree with single particle tracking measurements. We also show the application of kICS to measurements of the transport dynamics of alpha5-integrin/enhanced green fluorescent protein constructs in a transfected CHO cell imaged on a total internal reflection fluorescence microscope using charge-coupled device area detection.},
	number = {8},
	journal = {Biophysical journal},
	author = {Kolin, David L and Ronis, David and Wiseman, Paul W},
	year = {2006},
	note = {Publisher: Elsevier},
	pages = {3061--3075},
}

@article{brown_raster_2008,
	title = {Raster image correlation spectroscopy ({RICS}) for measuring fast protein dynamics and concentrations with a commercial laser scanning confocal microscope},
	volume = {229},
	issn = {1365-2818},
	doi = {10.1111/j.1365-2818.2007.01871.x},
	abstract = {Raster image correlation spectroscopy (RICS) is a new and novel technique for measuring molecular dynamics and concentrations from fluorescence confocal images. The RICS technique extracts information about molecular dynamics and concentrations from images of living cells taken on commercial confocal systems. Here we develop guidelines for performing the RICS analysis on an analogue commercial laser scanning confocal microscope. Guidelines for typical instrument settings, image acquisition settings and analogue detector characterization are presented. Using appropriate instrument/acquisition parameters, diffusion coefficients and concentrations can be determined, even for highly dynamic dye molecules in solution. Standard curves presented herein demonstrate the ability to detect protein concentrations as low as approximately 2 nM. Additionally, cellular measurements give accurate values for the diffusion of paxillin-enhanced-green fluorescent protein (EGFP), an adhesion adaptor molecule, in the cytosol of the cell and also show slower paxillin dynamics near adhesions where paxillin interacts with immobile adhesion components. Methods are presented to account for bright immobile structures within the cell that dominate spatial correlation functions; allowing the extraction of fast protein dynamics within and near these structures. A running average algorithm is also presented to address slow cellular movement or movement of cellular features such as adhesions. Finally, methods to determine protein concentration in the presence of immobile structures within the cell are presented. A table is presented giving guidelines for instrument and imaging setting when performing RICS on the Olympus FV300 confocal and these guidelines are a starting point for performing the analysis on other commercial confocal systems.},
	number = {August 2007},
	journal = {Journal of Microscopy},
	author = {Brown, C. M. and Dalal, R. B. and Hebert, B. and Digman, M. a. and Horwitz, a. R. and Gratton, E.},
	year = {2008},
	keywords = {FCS, Correlation spectroscopy, ICS, RICS},
	pages = {78--91},
}

@article{noauthor_home_nodate,
	title = {home},
}

@article{biskup_interaction_2004,
	title = {Interaction of {PSD}-95 with potassium channels visualized by fluorescence lifetime-based resonance energy transfer imaging.},
	volume = {9},
	issn = {1083-3668 (Print)},
	doi = {10.1117/1.1755721},
	abstract = {Resonance energy transfer (RET) has been extensively used to estimate the distance between two different fluorophores. This study demonstrates how protein-protein interactions can be visualized and quantified in living cells by time-correlated single-photon counting (TCSPC) imaging techniques that exploit the RET between appropriate fluorescent labels. We used this method to investigate the association of the potassium inward rectifier channel Kir2.1 and the neuronal PDZ protein PSD-95, which has been implicated in subcellular targeting and clustering of ion channels. Our data show that the two proteins not only colocalize within clusters but also interact with each other. Moreover, the data allow a spatially resolved quantification of this protein-protein interaction with respect to the relative number and the proximity between interacting molecules. Depending on the subcellular localization, a fraction of 20 to 60\% of PSD-95 molecules interacted with Kir2.1 channels, approximating their fluorescent labels by less than 5 nm.},
	number = {4},
	journal = {Journal of biomedical optics},
	author = {Biskup, Christoph and Kelbauskas, Laimonas and Zimmer, Thomas and Benndorf, Klaus and Bergmann, Axel and Becker, Wolfgang and Ruppersberg, J Peter and Stockklausner, Clemens and Klöcker, Nikolaj},
	year = {2004},
	keywords = {flim, fluorescence lifetime imaging, 2003, resonance energy transfer, 23, 29, 8, accepted for publication oct, inwardly rectifying potassium channels, paper 03091 received jul, postsynaptic density protein, psd-95, revised manuscript received sep},
	pages = {753--759},
}

@article{draviam_meeting_2013,
	title = {Meeting report of the {International} {Workshop} on {Quantitative} {Biology} 2012: mesoscopic and microscopic worlds meet},
	volume = {3},
	url = {http://journal.frontiersin.org/article/10.3389/fphys.2012.00479/abstract},
	doi = {10.3389/fphys.2012.00479},
	number = {January},
	journal = {Frontiers in Physiology},
	author = {Draviam, Viji M. and Funahashi, Akira and Hiroi, Noriko and Kimura, Akatsuki and Kobayashi, Tetsuya J.},
	year = {2013},
	pages = {2012--2013},
}

@article{gadella_visualization_1994,
	title = {Visualization and quantification of glycolipid polarity dynamics in the plasma membrane of the mammalian spermatozoon.},
	volume = {107 ( Pt 8},
	issn = {0021-9533 (Print)},
	abstract = {Seminolipid (sulphogalactosylalkylacylglycerol), the glycolipid that is specific for mammalian germ cells, is located exclusively in the outer leaflet of the sperm plasma membrane. In this study the lateral distribution of seminolipid on sperm heads has been investigated by indirect immunofluorescence labelling and detection with digital imaging fluorescence microscopy. In freshly ejaculated sperm cells this glycolipid was present primarily at the apical ridge subdomain of the plasma membrane of the sperm head. After binding the sperm cells to zona-coated coverslips seminolipid migrated, in 40 minutes, from the apical ridge to the equatorial subdomain of the plasma membrane. A similar redistribution of seminolipid was observed during capacitation of sperm cells in vitro induced by Ca2+ or bovine serum albumin. Comparable migration of seminolipid was also found after prolonged storage of ejaculated sperm cells, albeit at a much slower rate. Addition of arylsulphatase A, an enzyme present in seminal plasma that desulphates seminolipid, significantly enhanced the migration of seminolipid during storage of sperm cells. Its breakdown product desulphoseminolipid (galactosylalkylacylglycerol) appeared highly specifically at the equatorial segment. The measured fluorescence intensity over the sperm head surface correlated linearly with the spatial probe distribution as was checked by fluorescence lifetime imaging microscopy. This paper demonstrates and quantifies for the first time the polarity of seminolipid on the surface of the sperm cell and the dynamic alterations that occur in this polarity during post-ejaculatory events.},
	journal = {Journal of cell science},
	author = {Gadella, B M and Gadella, T W and Colenbrander, B and van Golde, L M and Lopes-Cardozo, M},
	year = {1994},
	keywords = {plasma membrane, fluorescence lifetime imaging microscopy, immunolocalization, lipid polarity, seminolipid, sperm},
	pages = {2151--2163},
}

@article{noauthor_turbogfp_spectra_nodate,
	title = {{TurboGFP}\_spectra},
}

@article{goda_high-throughput_2012,
	title = {High-throughput single-microparticle imaging flow analyzer},
	volume = {109},
	issn = {0027-8424},
	doi = {10.1073/pnas.1204718109},
	abstract = {Optical microscopy is one of the most widely used diagnostic methods in scientific, industrial, and biomedical applications. However, while useful for detailed examination of a small number ({\textless} 10,000) of microscopic entities, conventional optical microscopy is incapable of statistically relevant screening of large populations ({\textgreater} 100,000,000) with high precision due to its low throughput and limited digital memory size. We present an automated flow-through single-particle optical microscope that overcomes this limitation by performing sensitive blur-free image acquisition and nonstop real-time image-recording and classification of microparticles during high-speed flow. This is made possible by integrating ultrafast optical imaging technology, self-focusing microfluidic technology, optoelectronic communication technology, and information technology. To show the system's utility, we demonstrate high-throughput image-based screening of budding yeast and rare breast cancer cells in blood with an unprecedented throughput of 100,000 particles/s and a record false positive rate of one in a million.},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Goda, K. and Ayazi, a. and Gossett, D. R. and Sadasivam, J. and Lonappan, C. K. and Sollier, E. and Fard, a. M. and Hur, S. C. and Adam, J. and Murray, C. and Wang, C. and Brackbill, N. and Di Carlo, D. and Jalali, B.},
	year = {2012},
	pages = {11630--11635},
}

@article{gaits_shedding_2003,
	title = {Shedding light on cell signaling: interpretation of {FRET} biosensors.},
	volume = {2003},
	issn = {6195548766},
	doi = {10.1126/stke.2003.165.pe3},
	abstract = {This Perspective compares recently developed approaches for studying live-cell signaling dynamics. It is now possible not only to study the changing localizations of proteins within living cells but to correlate these dynamics with quantitation of protein activities, including ligand interactions and phosphorylation. Initial applications of an increasingly varied toolchest of techniques have revealed strengths and weaknesses in each approach, clarifying which tool is best for which job.},
	number = {January},
	journal = {Science's STKE : signal transduction knowledge environment},
	author = {Gaits, Frédérique and Hahn, Klaus},
	year = {2003},
	pages = {PE3--PE3},
}

@article{patterson_superresolution_2010,
	title = {Superresolution imaging using single-molecule localization.},
	volume = {61},
	issn = {1545-1593 (Electronic){\textbackslash}n0066-426X (Linking)},
	doi = {10.1146/annurev.physchem.012809.103444},
	abstract = {Superresolution imaging is a rapidly emerging new field of microscopy that dramatically improves the spatial resolution of light microscopy by over an order of magnitude (approximately 10-20-nm resolution), allowing biological processes to be described at the molecular scale. Here, we discuss a form of superresolution microscopy based on the controlled activation and sampling of sparse subsets of photoconvertible fluorescent molecules. In this single-molecule-based imaging approach, a wide variety of probes have proved valuable, ranging from genetically encodable photoactivatable fluorescent proteins to photoswitchable cyanine dyes. These have been used in diverse applications of superresolution imaging: from three-dimensional, multicolor molecule localization to tracking of nanometric structures and molecules in living cells. Single-molecule-based superresolution imaging thus offers exciting possibilities for obtaining molecular-scale information on biological events occurring at variable timescales.},
	number = {December},
	journal = {Annual review of physical chemistry},
	author = {Patterson, George and Davidson, Michael and Manley, Suliana and Lippincott-Schwartz, Jennifer},
	year = {2010},
	keywords = {single molecule, diffraction limit, fpalm, palm, photoactivatable fluorescent protein, photoactivation, storm, superresolution microscopy},
	pages = {345--367},
}

@article{noauthor_home_nodate-1,
	title = {home},
}

@article{korlach_characterization_1999,
	title = {Characterization of lipid bilayer phases by confocal microscopy and fluorescence correlation spectroscopy.},
	volume = {96},
	issn = {0027-8424},
	doi = {10.1073/pnas.96.15.8461},
	abstract = {We report the application of confocal imaging and fluorescence correlation spectroscopy (FCS) to characterize chemically well-defined lipid bilayer models for biomembranes. Giant unilamellar vesicles of dilauroyl phosphatidylcholine/dipalmitoyl phosphatidylcholine (DLPC/DPPC)/cholesterol were imaged by confocal fluorescence microscopy with two fluorescent probes, 1, 1'-dieicosanyl-3,3,3',3'-tetramethylindocarbocyanine perchlorate (DiI-C(20)) and 2-(4,4-difluoro-5,7-dimethyl-4-bora-3a, 4a-diaza-s-indacene-3-pentanoyl)-1-hexadecanoyl-sn-glycero-3 -phosphoc holine (Bodipy-PC). Phase separation was visualized by differential probe partition into the coexisting phases. Three-dimensional image reconstructions of confocal z-scans through giant unilamellar vesicles reveal the anisotropic morphology of coexisting phase domains on the surface of these vesicles with full two-dimensional resolution. This method demonstrates by direct visualization the exact superposition of like phase domains in apposing monolayers, thus answering a long-standing open question. Cholesterol was found to induce a marked change in the phase boundary shapes of the coexisting phase domains. To further characterize the phases, the translational diffusion coefficient, D(T), of the DiI-C(20) was measured by FCS. D(T) values at approximately 25 degrees C ranged from approximately 3 x 10(-8) cm(2)/s in the fluid phase, to approximately 2 x 10(-9) cm(2)/s in high-cholesterol-content phases, to approximately 2 x 10(-10) cm(2)/s in the spatially ordered phases that coexist with fluid phases. In favorable cases, FCS could distinguish two different values of D(T) in a region of two-phase coexistence on a single vesicle.},
	number = {July},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Korlach, J and Schwille, P and Webb, W W and Feigenson, G W},
	year = {1999},
	pages = {8461--8466},
}

@article{noauthor_ncomms3207-s9_nodate,
	title = {ncomms3207-s9},
}

@article{rusu_fluorescence_2004,
	title = {Fluorescence correlation spectroscopy studies of {Peptide} and protein binding to phospholipid vesicles.},
	volume = {87},
	doi = {10.1529/biophysj.104.039958},
	abstract = {We used fluorescence correlation spectroscopy (FCS) to analyze the binding of fluorescently labeled peptides to lipid vesicles and compared the deduced binding constants to those obtained using other techniques. We used a well-characterized peptide corresponding to the basic effector domain of myristoylated alanine-rich C kinase substrate, MARCKS(151-175), that was fluorescently labeled with Alexa488, and measured its binding to large unilamellar vesicles (diameter approximately 100 nm) composed of phosphatidylcholine and phosphatidylserine or phosphatidylinositol 4,5-bisphosphate. Because the large unilamellar vesicles are significantly larger than the peptide, the correlation times for the free and bound peptide could be distinguished using single color autocorrelation measurements. The molar partition coefficients calculated from the FCS measurements were comparable to those obtained from binding measurements of radioactively labeled MARCKS(151-175) using a centrifugation technique. Moreover, FCS can measure binding of peptides present at very low concentrations (1-10 nmolar), which is difficult or impossible with most other techniques. Our data indicate FCS can be an accurate and valuable tool for studying the interaction of peptides and proteins with lipid membranes.},
	number = {August},
	journal = {Biophysical journal},
	author = {Rusu, Laura and Gambhir, Alok and McLaughlin, Stuart and Rädler, Joachim},
	year = {2004},
	pages = {1044--1053},
}

@article{shaner_improved_2004-1,
	title = {Improved monomeric red, orange and yellow fluorescent proteins derived from {Discosoma} sp. red fluorescent protein.},
	volume = {22},
	issn = {1087-0156 (Print){\textbackslash}r1087-0156 (Linking)},
	doi = {10.1038/nbt1037},
	abstract = {Fluorescent proteins are genetically encoded, easily imaged reporters crucial in biology and biotechnology. When a protein is tagged by fusion to a fluorescent protein, interactions between fluorescent proteins can undesirably disturb targeting or function. Unfortunately, all wild-type yellow-to-red fluorescent proteins reported so far are obligately tetrameric and often toxic or disruptive. The first true monomer was mRFP1, derived from the Discosoma sp. fluorescent protein "DsRed" by directed evolution first to increase the speed of maturation, then to break each subunit interface while restoring fluorescence, which cumulatively required 33 substitutions. Although mRFP1 has already proven widely useful, several properties could bear improvement and more colors would be welcome. We report the next generation of monomers. The latest red version matures more completely, is more tolerant of N-terminal fusions and is over tenfold more photostable than mRFP1. Three monomers with distinguishable hues from yellow-orange to red-orange have higher quantum efficiencies.},
	number = {12},
	journal = {Nature biotechnology},
	author = {Shaner, Nathan C and Campbell, Robert E and Steinbach, Paul a and Giepmans, Ben N G and Palmer, Amy E and Tsien, Roger Y},
	year = {2004},
	pages = {1567--1572},
}

@article{noauthor_ncomms3207-s5_nodate,
	title = {ncomms3207-s5},
}

@article{hayden_microscopic_2009,
	title = {Microscopic marvels: {Microscope} for the masses.},
	volume = {459},
	issn = {1476-4687 (Electronic){\textbackslash}n0028-0836 (Linking)},
	doi = {10.1038/459632a},
	abstract = {1: Nature. 2009 Jun 4;459(7247):632-3. Microscopic marvels: . Hayden EC. Publication Types: News. Mesh Terms},
	number = {June},
	journal = {Nature},
	author = {Hayden, Erika Check},
	year = {2009},
	pages = {632--633},
}

@article{ulrich_tropical-parameter_2006,
	title = {Tropical-parameter estimation and simulation of reaction-diffusion models based on spatio-temporal microscopy images},
	volume = {22},
	abstract = {Tropical is a software for simulation and parameter estimation of reaction-diffusion models. Based on spatio-temporal microscopy images, Tropical estimates reaction and diffusion coefficients for user-defined models. Tropical allows the investigation of systems with an inhomogeneous distribution of molecules, making it well suited for quantitative analyses of microscopy experiments such as fluorescence recovery after photobleaching (FRAP). AVAILABILITY: Tropical is available free of charge for academic use at http://www.dkfz.de/tbi/projects/modellingAndSimulationOfCelluarSystems/tropical.jsp after signing a material transfer agreement.},
	number = {21},
	journal = {Bioinformatics},
	author = {Ulrich, Markus and Kappel, Constantin and Beaudouin, Joel and Hezel, Stefan and Ulrich, Jochen and Eils, Roland},
	year = {2006},
	pages = {2709--2710},
}

@article{sekar_fluorescence_2003,
	title = {Fluorescence resonance energy transfer ({FRET}) microscopy imaging of live cell protein localizations},
	volume = {160},
	doi = {10.1083/jcb.200210140},
	abstract = {The current advances in fluorescence microscopy, coupled with the development of new fluorescent probes, make fluorescence resonance energy transfer (FRET) a powerful technique for studying molecular interactions inside living cells with improved spatial (angstrom) and temporal (nanosecond) resolution, distance range, and sensitivity and a broader range of biological applications.},
	journal = {Journal of Cell Biology},
	author = {Sekar, Rajesh Babu and Periasamy, Ammasi},
	year = {2003},
	keywords = {Dimerization, Cameleons, Data analysis, FRET assays, FRET microscopy},
	pages = {629--633},
}

@article{benda_how_2003,
	title = {How to determine diffusion coefficients in planar phospholipid systems by confocal fluorescence correlation spectroscopy},
	volume = {19},
	issn = {0743-7463},
	doi = {10.1021/la0270136},
	abstract = {Confocal fluorescence correlation spectroscopy (FCS) allows for the determination of lateral diffusion coefficients and surface densities in planar phospholipid systems. The determination of the vertical (z-) position of the laser focus relative to the phospholipid surface plane is of crucial importance for the accuracy of the confocal FCS experiment. In this work we determine for the first time this vertical (z-) position of the laser focus by a so-called “Z-scan”, which is based on the determination of diffusion times and particle numbers in 0.2 µm steps along the vertical (z-) axis. Experiments on supported phospholipid bilayers composed of dioleoylphosphatidylcholine (DOPC) and small amounts of Rhodamine Red-X 1,2-dihexa- decanoyl-sn-glycero-3-phosphoethanolamine, triethylammonium salt(RhodamineRed-XDHPE)adsorbed onto atomically flat mica and borosilicate glass demonstrate that results obtained by the Z-scan approach are significantly more precise than those results obtained when the fluorescence intensity maximum is used as an indicator in the determination of the vertical (z-) position of the sample. In addition to this basic contribution for the investigation of planar bilayer systemsbyconfocalFCS,the lateral diffusion coefficients of Rhodamine Red-XDHPEin supported phospholipid bilayers composed ofDOPCand cholesterol as well as in DOPC or dipalmitoylphosphatidylcholine (DPPC) monolayers adsorbed at a liquid-liquid interface were determined.},
	number = {14},
	journal = {Langmuir},
	author = {Benda, a. and Beneš, M. and Mareček, V. and Lhotský, a. and Hermens, W. Th and Hof, M.},
	year = {2003},
	pages = {4120--4126},
}

@article{chen_live_2008,
	title = {Live cell dynamics of promyelocytic leukemia nuclear bodies upon entry into and exit from mitosis.},
	volume = {19},
	abstract = {Promyelocytic leukemia nuclear bodies (PML NBs) have been proposed to be involved in tumor suppression, viral defense, DNA repair, and/or transcriptional regulation. To study the dynamics of PML NBs during mitosis, we developed several U2OS cell lines stably coexpressing PML-enhanced cyan fluorescent protein with other individual marker proteins. Using three-dimensional time-lapse live cell imaging and four-dimensional particle tracking, we quantitatively demonstrated that PML NBs exhibit a high percentage of directed movement when cells progressed from prophase to prometaphase. The timing of this increased dynamic movement occurred just before or upon nuclear entry of cyclin B1, but before nuclear envelope breakdown. Our data suggest that entry into prophase leads to a loss of tethering between regions of chromatin and PML NBs, resulting in their increased dynamics. On exit from mitosis, Sp100 and Fas death domain-associated protein (Daxx) entered the daughter nuclei after a functional nuclear membrane was reformed. However, the recruitment of these proteins to PML NBs was delayed and correlated with the timing of de novo PML NB formation. Together, these results provide insight into the dynamic changes associated with PML NBs during mitosis.},
	number = {7},
	journal = {Molecular biology of the cell},
	author = {Chen, Yi-Chun M and Kappel, Constantin and Beaudouin, Joel and Eils, Roland and Spector, David L},
	year = {2008},
	pages = {3147--3162},
}

@article{eggeling_direct_2009-1,
	title = {Direct observation of the nanoscale dynamics of membrane lipids in a living cell.},
	volume = {457},
	issn = {0028-0836},
	doi = {10.1038/nature07596},
	abstract = {Cholesterol-mediated lipid interactions are thought to have a functional role in many membrane-associated processes such as signalling events. Although several experiments indicate their existence, lipid nanodomains ('rafts') remain controversial owing to the lack of suitable detection techniques in living cells. The controversy is reflected in their putative size of 5-200 nm, spanning the range between the extent of a protein complex and the resolution limit of optical microscopy. Here we demonstrate the ability of stimulated emission depletion (STED) far-field fluorescence nanoscopy to detect single diffusing (lipid) molecules in nanosized areas in the plasma membrane of living cells. Tuning of the probed area to spot sizes approximately 70-fold below the diffraction barrier reveals that unlike phosphoglycerolipids, sphingolipids and glycosylphosphatidylinositol-anchored proteins are transiently ( approximately 10-20 ms) trapped in cholesterol-mediated molecular complexes dwelling within {\textless}20-nm diameter areas. The non-invasive optical recording of molecular time traces and fluctuation data in tunable nanoscale domains is a powerful new approach to study the dynamics of biomolecules in living cells.},
	number = {February},
	journal = {Nature},
	author = {Eggeling, Christian and Ringemann, Christian and Medda, Rebecca and Schwarzmann, Günter and Sandhoff, Konrad and Polyakova, Svetlana and Belov, Vladimir N and Hein, Birka and von Middendorff, Claas and Schönle, Andreas and Hell, Stefan W},
	year = {2009},
	pages = {1159--1162},
}

@article{noauthor_peter_nodate,
	title = {peter biophys {J} 88(2)\_ 2005.pdf},
}

@article{noauthor_1471-2121-2-8-3_nodate,
	title = {1471-2121-2-8-3},
}

@article{verveer_global_2000,
	title = {Global analysis of fluorescence lifetime imaging microscopy data.},
	volume = {78},
	doi = {10.1016/S0006-3495(00)76759-2},
	abstract = {Global analysis techniques are described for frequency domain fluorescence lifetime imaging microscopy (FLIM) data. These algorithms exploit the prior knowledge that only a limited number of fluorescent molecule species whose lifetimes do not vary spatially are present in the sample. Two approaches to implementing the lifetime invariance constraint are described. In the lifetime invariant fit method, each image in the lifetime image sequence is spatially averaged to obtain an improved signal-to-noise ratio. The lifetime estimations from these averaged data are used to recover the fractional contribution to the steady-state fluorescence on a pixel-by-pixel basis for each species. The second, superior, approach uses a global analysis technique that simultaneously fits the fractional contributions in all pixels and the spatially invariant lifetimes. In frequency domain FLIM the maximum number of lifetimes that can be fit with the global analysis method is twice the number of lifetimes that can be fit with conventional approaches. As a result, it is possible to discern two lifetimes with a single-frequency FLIM setup. The algorithms were tested on simulated data and then applied to separate the cellular distributions of coexpressed green fluorescent proteins in living cells.},
	number = {April},
	journal = {Biophysical journal},
	author = {Verveer, P J and Squire, a and Bastiaens, P I},
	year = {2000},
	pages = {2127--2137},
}

@article{kapusta_absolute_2010,
	title = {Absolute diffusion coefficients: compilation of reference data for {FCS} calibration},
	url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Absolute+Diffusion+Coefficients+:+Compilation+of+Reference+Data+for+FCS+Calibration#0},
	number = {December},
	journal = {Application note},
	author = {Kapusta, Peter},
	year = {2010},
	pages = {0--1},
}

@article{korlach_characterization_1999-1,
	title = {Characterization of lipid bilayer phases by confocal microscopy and fluorescence correlation spectroscopy.},
	volume = {96},
	issn = {0027-8424},
	doi = {10.1073/pnas.96.15.8461},
	abstract = {We report the application of confocal imaging and fluorescence correlation spectroscopy (FCS) to characterize chemically well-defined lipid bilayer models for biomembranes. Giant unilamellar vesicles of dilauroyl phosphatidylcholine/dipalmitoyl phosphatidylcholine (DLPC/DPPC)/cholesterol were imaged by confocal fluorescence microscopy with two fluorescent probes, 1, 1'-dieicosanyl-3,3,3',3'-tetramethylindocarbocyanine perchlorate (DiI-C(20)) and 2-(4,4-difluoro-5,7-dimethyl-4-bora-3a, 4a-diaza-s-indacene-3-pentanoyl)-1-hexadecanoyl-sn-glycero-3 -phosphoc holine (Bodipy-PC). Phase separation was visualized by differential probe partition into the coexisting phases. Three-dimensional image reconstructions of confocal z-scans through giant unilamellar vesicles reveal the anisotropic morphology of coexisting phase domains on the surface of these vesicles with full two-dimensional resolution. This method demonstrates by direct visualization the exact superposition of like phase domains in apposing monolayers, thus answering a long-standing open question. Cholesterol was found to induce a marked change in the phase boundary shapes of the coexisting phase domains. To further characterize the phases, the translational diffusion coefficient, D(T), of the DiI-C(20) was measured by FCS. D(T) values at approximately 25 degrees C ranged from approximately 3 x 10(-8) cm(2)/s in the fluid phase, to approximately 2 x 10(-9) cm(2)/s in high-cholesterol-content phases, to approximately 2 x 10(-10) cm(2)/s in the spatially ordered phases that coexist with fluid phases. In favorable cases, FCS could distinguish two different values of D(T) in a region of two-phase coexistence on a single vesicle.},
	number = {July},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Korlach, J and Schwille, P and Webb, W W and Feigenson, G W},
	year = {1999},
	pages = {8461--8466},
}

@article{noauthor_test_nodate,
	title = {Test specimen for fluorescence imaging},
}

@article{noauthor_tcspc6_nodate,
	title = {tcspc6},
}

@article{hom_analysis_2002,
	title = {Analysis of coupled bimolecular reaction kinetics and diffusion by two-color fluorescence correlation spectroscopy: enhanced resolution of kinetics by resonance energy transfer.},
	volume = {83},
	issn = {4154768530},
	doi = {10.1016/S0006-3495(02)75189-8},
	abstract = {In two-color fluorescence correlation spectroscopy (TCFCS), the fluorescence intensities of two fluorescently-labeled species are cross-correlated over time and can be used to identify static and dynamic interactions. Generally, fluorophore labels are chosen that do not undergo Förster resonance energy transfer (FRET). Here, a general TCFCS theory is presented that accounts for the possibility of FRET between reactants in the reversible bimolecular reaction, [reaction: see text] where k(f) and k(b) are forward and reverse rate constants, respectively (dissociation constant K(d) = k(b)/k(f)). Using this theory, we systematically investigated the influence on the correlation function of FRET, reaction rates, reactant concentrations, diffusion, and component visibility. For reactants of comparable size and an energy-transfer efficiency of approximately 90\%, experimentally measurable cross-correlation functions should be sensitive to reaction kinetics for K(d) {\textgreater} 10(-8) M and k(f) {\textgreater}or= approximately 10(7) M(-1)s(-1). Measured auto-correlation functions corresponding to donor and acceptor labels are generally less sensitive to reaction kinetics, although for the acceptor, this sensitivity increases as the visibility of the donor increases relative to the acceptor. In the absence of FRET or a significant hydrodynamic difference between reactant species, there is little effect of reaction kinetics on the shape of auto- and cross-correlation functions. Our results suggest that a subset of biologically relevant association-dissociation kinetics can be measured by TCFCS and that FRET can be advantageous in enhancing these effects.},
	number = {July},
	journal = {Biophysical journal},
	author = {Hom, Erik F Y and Verkman, a S},
	year = {2002},
	pages = {533--546},
}

@article{qian_analysis_1990,
	title = {On the analysis of high order moments of fluorescence fluctuations.},
	volume = {57},
	issn = {0006-3495},
	doi = {10.1016/S0006-3495(90)82539-X},
	abstract = {A simple, straightforward analysis to characterize the distribution of aggregate sizes in a reversible aggregation system at equilibrium is presented. The method, an extension of fluorescence correlation spectroscopy (FCS), is based on measurements of higher order moments of spontaneous fluctuations of fluorescence intensity emitted from a defined open region of the sample. These fluctuations indicate fluctuations of the numbers of the fluorescent molecules in the observation region. Shot noise resulting from the random character of fluorescence emission and from the photoelectric detection system is modeled as a Poisson distribution and is subtracted from the measured photon count fluctuation moments to yield the desired fluorescence fluctuation moments. This analysis can also be used to estimate the fraction of immobile fluorophores in FCS measurements.},
	number = {February},
	journal = {Biophysical journal},
	author = {Qian, H and Elson, E L},
	year = {1990},
	pages = {375--380},
}

@article{lidke_role_2005,
	title = {The role of photon statistics in fluorescence anisotropy imaging},
	volume = {14},
	issn = {1057-7149 (Print){\textbackslash}r1057-7149 (Linking)},
	doi = {10.1109/TIP.2005.852458},
	abstract = {Anisotropy imaging can be used to image resonance energy transfer between pairs of identical fluorophores and, thus, constitutes a powerful tool for monitoring protein homo-association in living single cells. The requirement for only a single fluorophore significantly simplifies biological preparation and interpretation. We use quantitative methods for the acquisition and image processing of anisotropy data that return the expected error of the anisotropy per pixel based on photon statistics. The analysis methods include calibration procedures and allow for a balance in spatial, anisotropy, and temporal resolution. They are featured here with anisotropy images of fluorescent calibration beads and enhanced green fluorescent protein complexes in live cells.},
	number = {9},
	journal = {IEEE Transactions on Image Processing},
	author = {Lidke, Keith a. and Rieger, Bernd and Lidke, Diane S. and Jovin, Thomas M.},
	year = {2005},
	keywords = {Confocal microscopy, Energy migration fluorescence resonance energy tra, Enhanced green fluorescent protein (eGFP), Scale selective filtering},
	pages = {1237--1245},
}

@article{saffarian_statistical_2003,
	title = {Statistical analysis of fluorescence correlation spectroscopy: the standard deviation and bias.},
	volume = {84},
	doi = {10.1016/S0006-3495(03)75011-5},
	abstract = {We present a detailed statistical analysis of fluorescence correlation spectroscopy for a wide range of timescales. The derivation is completely analytical and can provide an excellent tool for planning and analysis of FCS experiments. The dependence of the signal-to-noise ratio on different measurement conditions is extensively studied. We find that in addition to the shot noise and the noise associated with correlated molecular dynamics there is another source of noise that appears at very large lag times. We call this the "particle noise," as its behavior is governed by the number of particles that have entered and left the laser beam sample volume during large dwell times. The standard deviations of all the points on the correlation function are calculated analytically and shown to be in good agreement with experiments. We have also investigated the bias associated with experimental correlation function measurements. A "phase diagram" for FCS experiments is constructed that demonstrates the significance of the bias for any given experiment. We demonstrate that the value of the bias can be calculated and added back as a first-order correction to the experimental correlation function.},
	number = {March},
	journal = {Biophysical journal},
	author = {Saffarian, Saveez and Elson, Elliot L},
	year = {2003},
	pages = {2030--2042},
}

@article{noauthor_helpact_nodate,
	title = {{helpACT}},
}

@article{kask_two-dimensional_2000,
	title = {Two-{Dimensional} {Fluorescence} {Intensity} {Distribution} {Analysis}: {Theory} and {Applications}},
	volume = {78},
	doi = {10.1016/S0006-3495(00)76722-1},
	number = {January},
	journal = {Biophysical Journal},
	author = {Kask, Peet and Palo, Kaupo and Fay, Nicolas and Brand, Leif and Mets, Ülo and Ullmann, Dirk and Jungmann, Joern and Pschorr, Johannes and Gall, Karsten},
	year = {2000},
	pages = {1703--1713},
}

@article{grecco_situ_2010,
	title = {In situ analysis of tyrosine phosphorylation networks by {FLIM} on cell arrays.},
	volume = {7},
	issn = {1548-7105 (Electronic){\textbackslash}r1548-7091 (Linking)},
	url = {http://dx.doi.org/10.1038/nmeth.1458},
	doi = {10.1038/nmeth.1458},
	abstract = {Extracellular stimuli are transduced inside the cell by posttranslational modifications (PTMs), such as phosphorylation, of proteins in signaling networks. Insight into the structure of these networks requires quantification of PTM levels in individual cells. Fluorescence resonance energy transfer (FRET) measured by fluorescence lifetime imaging microscopy (FLIM) is a powerful tool to image PTM levels in situ. FLIM on cell arrays that express fluorescent protein fusions can quantify tyrosine phosphorylation patterns in large networks in individual cells. We identified tyrosine kinase substrates by imaging their phosphorylation levels after inhibition of protein tyrosine phosphatases. Analysis of the correlation between protein phosphorylation and expression levels at single cell resolution allowed us to identify positive feedback motifs. Using FLIM on cell arrays (CA-FLIM), we uncovered components that transduce signals from epidermal growth factor receptor.},
	number = {may},
	journal = {Nature methods},
	author = {Grecco, Hernán E and Roda-Navarro, Pedro and Girod, Andreas and Hou, Jian and Frahm, Thomas and Truxius, Dina C and Pepperkok, Rainer and Squire, Anthony and Bastiaens, Philippe I H},
	year = {2010},
	note = {Publisher: Nature Publishing Group},
	pages = {467--472},
}

@article{mies_epithelial_2007,
	title = {Epithelial {Na}+ channel stimulation by n-3 fatty acids requires proximity to a membrane-bound {A}-kinase-anchoring protein complexed with protein kinase {A} and phosphodiesterase},
	volume = {282},
	issn = {0021-9258},
	doi = {10.1074/jbc.M611160200},
	abstract = {Essential polyunsatured fatty acids have been shown to modulate enzymes, channels and transporters, to interact with lipid bilayers and to affect metabolic pathways. We have previously shown that eicosapentanoic acid (EPA, C20:5, n-3) activates epithelial sodium channels (ENaCs) in a cAMP-dependent manner involving stimulation of cAMP-dependent protein kinase (PKA). In the present study, we explored further the mechanism of EPA stimulation of ENaC in A6 cells. Fluorescence resonance energy transfer experiments confirmed activation of PKA by EPA. Consistent with our previous studies, EPA had no further stimulatory effect on amiloride-sensitive transepithelial current (INa) in the presence of CPT-cAMP. Thus, we investigated the effect of EPA on cellular pathways which produce cAMP. EPA did not stimulate adenylate cyclase activity or total cellular cAMP accumulation. However, membrane-bound phosphodiesterase activity was inhibited by EPA from 2.46 pmol/mg of protein/min to 1.3 pmol/mg of protein/min. To investigate the potential role of an A-kinase-anchoring protein (AKAP), we used HT31, an inhibitor of the binding between PKA and AKAPs as well as cerulenin, an inhibitor of myristoylation and palmitoylation. Both agents prevented the stimulatory effect of EPA and CPT-cAMP on INa and drastically decreased the amount of PKA in the apical membrane. Colocalization experiments in A6 cells cotransfected with fluorescently labeled ENaC beta subunit and PKA regulatory subunit confirmed the close proximity of the two proteins and the membrane anchorage of PKA. Last, in A6 cells transfected with a dead mutant of Sgk, an enzyme which up-regulates ENaCs, EPA did not stimulate Na+ current. Our results suggest that stimulation of ENaCs by EPA occurs via SGK in membrane-bound compartments containing an AKAP, activated PKA, and a phosphodiesterase.},
	journal = {Journal of Biological Chemistry},
	author = {Mies, Frédérique and Spriet, Corentin and Héliot, Laurent and Sariban-Sohraby, Sarah},
	year = {2007},
	pages = {18339--18347},
}

@article{wollman_high_2007,
	title = {High throughput microscopy: from raw images to discoveries.},
	volume = {120},
	issn = {0021-9533 (Print){\textbackslash}n0021-9533 (Linking)},
	doi = {10.1242/jcs.013623},
	abstract = {Technological advances in automated microscopy now allow rapid acquisition of many images without human intervention, images that can be used for large-scale screens. The main challenge in such screens is the conversion of the raw images into interpretable information and hence discoveries. This post-acquisition component of image-based screens requires computational steps to identify cells, choose the cells of interest, assess their phenotype, and identify statistically significant 'hits'. Designing such an analysis pipeline requires careful consideration of the necessary hardware and software components, image analysis, statistical analysis and data presentation tools. Given the increasing availability of such hardware and software, these types of experiments have come within the reach of individual labs, heralding many interesting new ways of acquiring biological knowledge.},
	journal = {Journal of cell science},
	author = {Wollman, Roy and Stuurman, Nico},
	year = {2007},
	keywords = {image analysis, are part of a, bigger genomic revolution, contributors to the development, genome-wide screen, high-throughput microscopy, htm, image-based screens, in particular, of htm, rnai, the},
	pages = {3715--3722},
}

@article{eliceiri_biological_2012,
	title = {Biological imaging software tools},
	volume = {9},
	issn = {1548-7105 (Electronic){\textbackslash}n1548-7091 (Linking)},
	doi = {10.1038/nmeth.2084},
	abstract = {Few technologies are more widespread in modern biological laboratories than imaging. Recent advances in optical technologies and instrumentation are providing hitherto unimagined capabilities. Almost all these advances have required the development of software to enable the acquisition, management, analysis and visualization of the imaging data. We review each computational step that biologists encounter when dealing with digital images, the inherent challenges and the overall status of available software for bioimage informatics, focusing on open-source options.},
	number = {7},
	journal = {Nature Methods},
	author = {Eliceiri, Kevin W and Berthold, Michael R and Goldberg, Ilya G and Ibáñez, Luis and Manjunath, B S and Martone, Maryann E and Murphy, Robert F and Peng, Hanchuan and Plant, Anne L and Roysam, Badrinath and Stuurmann, Nico and Swedlow, Jason R and Tomancak, Pavel and Carpenter, Anne E},
	year = {2012},
	pages = {697--710},
}

@article{deuschle_rapid_2006,
	title = {Rapid metabolism of glucose detected with {FRET} glucose nanosensors in epidermal cells and intact roots of {Arabidopsis} {RNA}-silencing mutants.},
	volume = {18},
	issn = {1040-4651},
	doi = {10.1105/tpc.106.044073},
	abstract = {Genetically encoded glucose nanosensors have been used to measure steady state glucose levels in mammalian cytosol, nuclei, and endoplasmic reticulum. Unfortunately, the same nanosensors in Arabidopsis thaliana transformants manifested transgene silencing and undetectable fluorescence resonance energy transfer changes. Expressing nanosensors in sgs3 and rdr6 transgene silencing mutants eliminated silencing and resulted in high fluorescence levels. To measure glucose changes over a wide range (nanomolar to millimolar), nanosensors with higher signal-to-noise ratios were expressed in these mutants. Perfusion of leaf epidermis with glucose led to concentration-dependent ratio changes for nanosensors with in vitro K(d) values of 600 microM (FLIPglu-600 microDelta13) and 3.2 mM (FLIPglu-3.2 mDelta13), but one with 170 nM K(d) (FLIPglu-170 nDelta13) showed no response. In intact roots, FLIPglu-3.2 mDelta13 gave no response, whereas FLIPglu-600 microDelta13, FLIPglu-2 microDelta13, and FLIPglu-170 nDelta13 all responded to glucose. These results demonstrate that cytosolic steady state glucose levels depend on external supply in both leaves and roots, but under the conditions tested they are lower in root versus epidermal and guard cells. Without photosynthesis and external supply, cytosolic glucose can decrease to {\textless}90 nM in root cells. Thus, observed gradients are steeper than expected, and steady state levels do not appear subject to tight homeostatic control. Nanosensor-expressing plants can be used to assess glucose flux differences between cells, invertase-mediated sucrose hydrolysis in vivo, delivery of assimilates to roots, and glucose flux in mutants affected in sugar transport, metabolism, and signaling.},
	number = {September},
	journal = {The Plant cell},
	author = {Deuschle, Karen and Chaudhuri, Bhavna and Okumoto, Sakiko and Lager, Ida and Lalonde, Sylvie and Frommer, Wolf B},
	year = {2006},
	pages = {2314--2325},
}

@article{digman_mapping_2008,
	title = {Mapping the number of molecules and brightness in the laser scanning microscope.},
	volume = {94},
	issn = {9498242992},
	doi = {10.1529/biophysj.107.114645},
	abstract = {We describe a technique based on moment-analysis for the measurement of the average number of molecules and brightness in each pixel in fluorescence microscopy images. The average brightness of the particle is obtained from the ratio of the variance to the average intensity at each pixel. To obtain the average number of fluctuating particles, we divide the average intensity at one pixel by the brightness. This analysis can be used in a wide range of concentrations. In cells, the intensity at any given pixel may be due to bright immobile structures, dim fast diffusing particles, and to autofluorescence or scattering. The total variance is given by the variance of each of the above components in addition to the variance due to detector noise. Assuming that all sources of variance are independent, the total variance is the sum of the variances of the individual components. The variance due to the particles fluctuating in the observation volume is proportional to the square of the particle brightness while the variance of the immobile fraction, the autofluorescence, scattering, and that of the detector is proportional to the intensity of these components. Only the fluctuations that depend on the square of the brightness (the mobile particles) will have a ratio of the variance to the intensity {\textgreater}1. Furthermore, changing the fluorescence intensity by increasing the illumination power, distinguishes between these possible contributions. We show maps of molecular brightness and number of cell migration proteins obtained using a two-photon scanning microscope operating with a photon-counting detector. These brightness maps reveal binding dynamics at the focal adhesions with pixel resolution and provide a picture of the binding and unbinding process in which dim molecules attach to the adhesions or large molecular aggregates dissociate from adhesion.},
	number = {March},
	journal = {Biophysical journal},
	author = {Digman, Michelle a and Dalal, Rooshin and Horwitz, Alan F and Gratton, Enrico},
	year = {2008},
	pages = {2320--2332},
}

@article{clayton_dynamic_2002,
	title = {Dynamic fluorescence anisotropy imaging microscopy in the frequency domain ({rFLIM}).},
	volume = {83},
	issn = {4955120113},
	doi = {10.1016/S0006-3495(02)73932-5},
	abstract = {We describe a novel variant of fluorescence lifetime imaging microscopy (FLIM), denoted anisotropy-FLIM or rFLIM, which enables the wide-field measurement of the anisotropy decay of fluorophores on a pixel-by-pixel basis. We adapted existing frequency-domain FLIM technology for rFLIM by introducing linear polarizers in the excitation and emission paths. The phase delay and intensity ratios (AC and DC) between the polarized components of the fluorescence signal are recorded, leading to estimations of rotational correlation times and limiting anisotropies. Theory is developed that allows all the parameters of the hindered rotator model to be extracted from measurements carried out at a single modulation frequency. Two-dimensional image detection with a sensitive CCD camera provides wide-field imaging of dynamic depolarization with parallel interrogation of different compartments of a complex biological structure such as a cell. The concepts and technique of rFLIM are illustrated with a fluorophore-solvent (fluorescein-glycerol) system as a model for isotropic rotational dynamics and with bacteria expressing enhanced green fluorescent protein (EGFP) exhibiting depolarization due to homotransfer of electronic excitation energy (emFRET). The frequency-domain formalism was extended to cover the phenomenon of emFRET and yielded data consistent with a concentration depolarization mechanism resulting from the high intracellular concentration of EGFP. These investigations establish rFLIM as a powerful tool for cellular imaging based on rotational dynamics and molecular proximity.},
	number = {September 2002},
	journal = {Biophysical journal},
	author = {Clayton, Andrew H a and Hanley, Quentin S and Arndt-Jovin, Donna J and Subramaniam, Vinod and Jovin, Thomas M},
	year = {2002},
	pages = {1631--1649},
}

@article{bohmer_time-resolved_2001,
	title = {Time-resolved confocal scanning device for ultrasensitive fluorescence detection},
	volume = {72},
	issn = {0034-6748},
	doi = {10.1063/1.1406926},
	abstract = {A confocal laser-scanning microscope for ultrasensitive fluorescence lifetime imaging on surfaces is presented. The system employs a compact electronics for time-correlated single-photon counting (TCSPC), allowing for measuring fluorescence lifetime with 40 ps time resolution, and for continuously recording photon arrival times with 100 ns time resolution. Additionally developed driver electronics serve for synchronization of scanning and data acquisition, which is significant for achieving high spatial image resolution. The capabilities of the measurement system are demonstrated on imaging single molecules immobilized on glass substrates. Finally, it is shown how the TCSPC capabilities of the system can be used not only for lifetime imaging but also for multichannel measurements. (C) 2001 American Institute of Physics.},
	number = {11},
	journal = {Review of Scientific Instruments},
	author = {Böhmer, Martin and Pampaloni, Francesco and Wahl, Michael and Rahn, Hans Jürgen and Erdmann, Rainer and Enderlein, Jörg},
	year = {2001},
	pages = {4145--4145},
}

@article{meissner_lateral_2003,
	title = {Lateral mobility and specific binding to {GABA}({A}) receptors on hippocampal neurons monitored by fluorescence correlation spectroscopy},
	volume = {42},
	url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Citation&list_uids=12578381},
	abstract = {The binding behavior of a fluorescently labeled muscimol derivative to the GABA(A) receptor was analyzed at rat hippocampal neurons by fluorescence correlation spectroscopy. After muscimol had been labeled with the fluorophore Alexa Fluor 532, specific binding constants for binding of the dye-labeled ligand (Mu-Alexa) to the GABA(A) receptor were determined. We found a high specific binding affinity of Mu-Alexa with a K(D) value of 3.4 +/- 0.5 nM and a rate constant of ligand-receptor dissociation (k(diss)) of (5.37 +/- 0.95) x 10(-2) s(-1). A rate constant of ligand-receptor association (k(ass)) of (1.57 +/- 0.28) x 10(7) L mol(-1) s(-1) was calculated. The following diffusion coefficients were observed: D(free) = 233 +/- 20 microm(2)/s (n = 66) for free diffusing Mu-Alexa, D(bound1) = 2.8 +/- 0.9 microm(2)/s (n = 64) for the lateral mobility, and D(bound2) = 0.14 +/- 0.05 microm(2)/s (n = 56) for the hindered mobility of the GABA(A) receptor-ligand complex in the cell membrane. Saturation of Mu-Alexa binding was observed at a concentration of 50 nM. A maximum number of binding sites [B(max) = 18.4 +/- -0.4 nM (n = 5)] was found. Similar K(i) values of 4.5 +/- 1.0 nM for nonlabeled muscimol and 8.8 +/- 1.8 nM for Mu-Alexa were found by RRAs using [(3)H]muscimol as a radioligand. A concentration-dependent increase in the level of specific Mu-Alexa binding was demonstrated by the positive cooperative activity of co-incubated midazolam, which was selectively found in GABA(A) receptor-ligand complexes with hindered mobility.},
	journal = {Biochemistry},
	author = {Meissner, O and Haberlein, H},
	year = {2003},
	keywords = {Animals, Kinetics, Allosteric Site/drug effects, Cells, Cultured, Esters, Fluorescent Dyes/metabolism, Hippocampus/cytology/drug effects/*metabolism, Midazolam/pharmacology, Muscimol/metabolism, Neurons/drug effects/*metabolism, Protein Binding/drug effects, Radioligand Assay, Rats, Rats, Sprague-Dawley, Receptors, GABA-A/agonists/*metabolism, Research Support, Non-U.S. Gov't, Spectrometry, Fluorescence/instrumentation/methods, Succinimides/metabolism},
	pages = {1667--1672},
}

@article{jankevics_diffusion-time_2005,
	title = {Diffusion-time distribution analysis reveals characteristic ligand-dependent interaction patterns of nuclear receptors in living cells},
	volume = {44},
	issn = {1167611683},
	doi = {10.1021/bi050744v},
	abstract = {Nuclear receptors initiate transcription, interact with regulatory proteins, and are influenced by hormones, drugs, and pollutants. Herein, we discover ligand-specific mobility patterns of human estrogen receptor-alpha (ER) in living cells using diffusion-time distribution analysis (DDA). This novel method, based on fluorescence correlation spectroscopy (FCS), is especially suited to unraveling multiple protein interactions in vivo at native expression levels. We found that ER forms a limited number of distinct complexes with a varying population by dynamic interaction with other nuclear components. Dose-response curves of different ligands could be obtained for each receptor interaction. The potential to identify interacting proteins was demonstrated by comparing DDA of the ER cofactor SRC-3 attached to yellow fluorescent protein (YFP) with those of YFP-ER. Our findings open up new routes to elucidating transcription regulation and to detecting and distinguishing pharmacologically and toxicologically active compounds in vivo. Moreover, DDA provides a general approach to monitoring biochemical networks in individual living cells.},
	journal = {Biochemistry},
	author = {Jankevics, Hanna and Prummer, Michael and Izewska, Paulina and Pick, Horst and Leufgen, Kirsten and Vogel, Horst},
	year = {2005},
	pages = {11676--11683},
}

@article{asai_fluorescence_1985-2,
	title = {Fluorescence correlation spectroscopy},
	volume = {83},
	abstract = {Fluorescence correlation spectroscopy (FCS) is a technique that allows for an extremely sensitive determination of molecular diffusion properties, down to the level of single molecules. It thus provides an attractive alternative to FRAP, requiring much less laser power and lower concentrations of fluorophores. FCS has recently been applied on live cells, and in comparison on domain-forming model membrane systems, to systematically study the influence of cholesterol on local membrane structure by investigating the mobility of selected lipid probes. The findings demonstrate the ability of FCS to sensitively distinguish between different local lipid structures, and emphasize the value of model systems for understanding membrane dynamics in general.},
	number = {October},
	journal = {Tanpakushitsu kakusan koso. Protein, nucleic acid, enzyme},
	author = {Asai, H},
	year = {1985},
	pages = {246--248},
}

@article{noauthor_phys_rev_lett_29_705_webb_et_al_fcs_seminal_paper_1972pdf_nodate,
	title = {Phys\_Rev\_Lett\_29\_705\_Webb\_et\_al\_FCS\_seminal\_paper\_1972.pdf},
}

@article{thompson_maf_2003,
	title = {{MAF} 2003, {Prague}, {August} 24 – 27, 2003},
	author = {Thompson, Nancy L and Lieto, Alena M and Starr, Tammy E and Cush, Randall C},
	year = {2003},
	pages = {2003--2003},
}

@article{digman_paxillin_2008-1,
	title = {Paxillin dynamics measured during adhesion assembly and disassembly by correlation spectroscopy.},
	volume = {94},
	issn = {00063495},
	doi = {10.1529/biophysj.107.104984},
	abstract = {Paxillin is an adaptor molecule involved in the assembly of focal adhesions. Using different fluorescence fluctuation approaches, we established that paxillin-EGFP is dynamic on many timescales within the cell, ranging from milliseconds to seconds. In the cytoplasmic regions, far from adhesions, paxillin is uniformly distributed and freely diffusing as a monomer, as determined by single-point fluctuation correlation spectroscopy and photon-counting histogram analysis. Near adhesions, paxillin dynamics are reduced drastically, presumably due to binding to protein partners within the adhesions. The photon-counting histogram analysis of the fluctuation amplitudes reveals that this binding equilibrium in new or assembling adhesions is due to paxillin monomers binding to quasi-immobile structures, whereas in disassembling adhesions or regions of adhesions, the equilibrium is due to exchange of large aggregates. Scanning fluctuation correlation spectroscopy and raster-scan image correlation spectroscopy analysis of laser confocal images show that the environments within adhesions are heterogeneous. Relatively large adhesions appear to slide transversally due to a treadmilling mechanism through the addition of monomeric paxillin at one side and removal of relatively large aggregates of proteins from the retracting edge. Total internal reflection microscopy performed with a fast acquisition EM-CCD camera completes the overall dynamic picture and adds details of the heterogeneous dynamics across single adhesions and simultaneous bursts of activity at many adhesions across the cell.},
	number = {April},
	journal = {Biophysical journal},
	author = {Digman, Michelle a and Brown, Claire M and Horwitz, Alan R and Mantulin, William W and Gratton, Enrico},
	year = {2008},
	pages = {2819--2831},
}

@article{digman_stoichiometry_2009,
	title = {Stoichiometry of molecular complexes at adhesions in living cells.},
	volume = {106},
	issn = {1091-6490 (Electronic){\textbackslash}n0027-8424 (Linking)},
	doi = {10.1073/pnas.0806036106},
	abstract = {We describe a method to detect molecular complexes and measure their stoichiometry in living cells from simultaneous fluctuations of the fluorescence intensity in two image channels, each detecting a different kind of protein. The number and brightness (N\&B) analysis, namely, the use of the ratio between the variance and the average intensity to obtain the brightness of molecules, is extended to the cross-variance of the intensity fluctuations in two channels. We apply the cross-variance method to determine the stoichiometry of complexes containing paxillin and vinculin or focal adhesion kinase (FAK) in disassembling adhesions in mouse embryo fibroblasts expressing FAK, vinculin, and paxillin-tagged with EGFP and mCherry. We found no complexes of these proteins in the cytoplasm away from the adhesions. However, at the adhesions, large aggregates leave, forming a hole, during their disassembly. This hole shows cross-correlation between FAK and paxillin and vinculin and paxillin. From the amplitude of the correlated fluctuations we determine the composition of the aggregates leaving the adhesions. These aggregates disassemble rapidly in the cytoplasm because large complexes are found only in very close proximity to the adhesions or at their borders.},
	number = {7},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Digman, Michelle a and Wiseman, Paul W and Choi, Colin and Horwitz, Alan R and Gratton, Enrico},
	year = {2009},
	pages = {2170--2175},
}

@article{petersen_quantitation_1993,
	title = {Quantitation of membrane receptor distributions by image correlation spectroscopy: concept and application.},
	volume = {65},
	issn = {0006-3495 (Print){\textbackslash}r0006-3495 (Linking)},
	doi = {10.1016/S0006-3495(93)81173-1},
	abstract = {Measurement of receptor distributions on cell surfaces is one important aspect of understanding the mechanism whereby receptors function. In recent years, scanning fluorescence correlation spectroscopy has emerged as an excellent tool for making quantitative measurements of cluster sizes and densities. However, the measurements are slow and usually require fixed preparations. Moreover, while the precision is good, the accuracy is limited by the relatively small amount of information in each measurement, such that many are required. Here we present a novel extension of the scanning correlation spectroscopy that solves a number of the present problems. The new technique, which we call image correlation spectroscopy, is based on quantitative analysis of confocal scanning laser microscopy images. Since these can be generated in a matter of a second or so, the measurements become more rapid. The image is collected over a large cell area so that more sampling is done, improving the accuracy. The sacrifice is a lower resolution in the sampling, which leads to a lower precision. This compromise of precision in favor of speed and accuracy still provides an enormous advantage for image correlation spectroscopy over scanning correlation spectroscopy. The present work demonstrates the underlying theory, showing how the principles can be applied to measurements on standard fluorescent beads and changes in distribution of receptors for platelet-derived growth factor on human foreskin fibroblasts.},
	number = {September},
	journal = {Biophysical journal},
	author = {Petersen, N O and Höddelius, P L and Wiseman, P W and Seger, O and Magnusson, K E},
	year = {1993},
	pages = {1135--1146},
}

@article{camuzeaux_imaging_2005,
	title = {Imaging {Erg} and {Jun} transcription factor interaction in living cells using fluorescence resonance energy transfer analyses},
	volume = {332},
	issn = {0006-291X (Print)},
	doi = {10.1016/j.bbrc.2005.05.057},
	abstract = {Physical interactions between transcription factors play important roles in modulating gene expression. Previous in vitro studies have shown a transcriptional synergy between Erg protein, an Ets family member, and Jun/Fos heterodimer, members of the bZip family, which requires direct Erg-Jun protein interactions. Visualization of protein interactions in living cells is a new challenge in biology. For this purpose, we generated fusion proteins of Erg, Fos, and Jun with yellow and cyan fluorescent proteins, YFP and CFP, respectively. After transient expression in HeLa cells, interactions of the resulting fusion proteins were explored by fluorescence resonance energy transfer microscopy (FRET) in fixed and living cells. FRET between YFP-Erg and CFP-Jun was monitored by using photobleaching FRET and fluorescence lifetime imaging microscopy. Both techniques revealed the occurrence of intermolecular FRET between YFP-Erg and CFP-Jun. This is stressed by loss of FRET with an YFP-Erg version carrying a point mutation in its ETS domain. These results provide evidence for the interaction of Erg and Jun proteins in living cells as a critical prerequisite of their transcriptional synergy, but also for the essential role of the Y371 residue, conserved in most Ets proteins, in this interaction. © 2005 Elsevier Inc. All rights reserved.},
	journal = {Biochemical and Biophysical Research Communications},
	author = {Camuzeaux, Barbara and Spriet, Corentin and Héliot, Laurent and Coll, Jean and Duterque-Coquillaud, Martine},
	year = {2005},
	keywords = {FRET, FLIM, Ets and AP1 families, Protein interactions, Transcriptional regulation},
	pages = {1107--1114},
}

@article{hebert_spatiotemporal_2005,
	title = {Spatiotemporal image correlation spectroscopy ({STICS}) theory, verification, and application to protein velocity mapping in living {CHO} cells.},
	volume = {88},
	issn = {0006-3495 (Print)},
	url = {http://dx.doi.org/10.1529/biophysj.104.054874},
	doi = {10.1529/biophysj.104.054874},
	abstract = {We introduce a new extension of image correlation spectroscopy (ICS) and image cross-correlation spectroscopy (ICCS) that relies on complete analysis of both the temporal and spatial correlation lags for intensity fluctuations from a laser-scanning microscopy image series. This new approach allows measurement of both diffusion coefficients and velocity vectors (magnitude and direction) for fluorescently labeled membrane proteins in living cells through monitoring of the time evolution of the full space-time correlation function. By using filtering in Fourier space to remove frequencies associated with immobile components, we are able to measure the protein transport even in the presence of a large fraction ({\textgreater}90\%) of immobile species. We present the background theory, computer simulations, and analysis of measurements on fluorescent microspheres to demonstrate proof of principle, capabilities, and limitations of the method. We demonstrate mapping of flow vectors for mixed samples containing fluorescent microspheres with different emission wavelengths using space time image cross-correlation. We also present results from two-photon laser-scanning microscopy studies of alpha-actinin/enhanced green fluorescent protein fusion constructs at the basal membrane of living CHO cells. Using space-time image correlation spectroscopy (STICS), we are able to measure protein fluxes with magnitudes of mum/min from retracting lamellar regions and protrusions for adherent cells. We also demonstrate the measurement of correlated directed flows (magnitudes of mum/min) and diffusion of interacting alpha5 integrin/enhanced cyan fluorescent protein and alpha-actinin/enhanced yellow fluorescent protein within living CHO cells. The STICS method permits us to generate complete transport maps of proteins within subregions of the basal membrane even if the protein concentration is too high to perform single particle tracking measurements.},
	number = {5},
	journal = {Biophysical journal},
	author = {Hebert, Benedict and Costantino, Santiago and Wiseman, Paul W},
	year = {2005},
	note = {Publisher: Elsevier},
	pages = {3601--3614},
}

@article{schneckenburger_fluorescence_2004,
	title = {Fluorescence lifetime imaging ({FLIM}) of rhodamine 123 in living cells.},
	volume = {3},
	issn = {1474-905X (Print){\textbackslash}n1474-905X (Linking)},
	doi = {10.1039/b306129a},
	abstract = {A novel setup for fluorescence intensity and lifetime imaging (FLIM) of living cells is reported. Time-resolving techniques are combined with total internal reflection fluorescence microscopy (TIRFM), which permits optical excitation of either plasma membranes or whole cells depending on whether the angle of incidence of the excitation light is greater or smaller than the critical angle for total internal reflection. The method is applied to BKEz-7 endothelial cells incubated with various concentrations of the well established mitochondrial marker rhodamine 123(R123). Measurements show that only at low concentrations this dye is mainly located within the mitochondria, whereas at higher concentrations an accumulation within the plasma membrane occurs as well. Concomitantly, fluorescence quenching in the mitochondria is observed at high concentrations, probably due to aggregation of the R123 molecules. Therefore, for diagnostic applications the concentration of R123 in the incubation medium should not be above 25 microM.},
	journal = {Photochemical \& photobiological sciences : Official journal of the European Photochemistry Association and the European Society for Photobiology},
	author = {Schneckenburger, Herbert and Stock, Karl and Lyttek, Marco and Strauss, Wolfgang S L and Sailer, Reinhard},
	year = {2004},
	pages = {127--131},
}

@article{saalfeld_elastic_2012,
	title = {Elastic volume reconstruction from series of ultra-thin microscopy sections},
	volume = {9},
	issn = {1548-7091},
	doi = {10.1038/nmeth.2072},
	abstract = {Anatomy of large biological specimens is often reconstructed from serially sectioned volumes imaged by high-resolution microscopy. We developed a method to reassemble a continuous volume from such large section series that explicitly minimizes artificial deformation by applying a global elastic constraint. We demonstrate our method on a series of transmission electron microscopy sections covering the entire 558-cell Caenorhabditis elegans embryo and a segment of the Drosophila melanogaster larval ventral nerve cord.},
	number = {7},
	journal = {Nature Methods},
	author = {Saalfeld, Stephan and Fetter, Richard and Cardona, Albert and Tomancak, Pavel},
	year = {2012},
	pages = {717--720},
}

@article{bastiaens_microspectroscopic_1996,
	title = {Microspectroscopic imaging tracks the intracellular processing of a signal transduction protein: fluorescent-labeled protein kinase {C} beta {I}.},
	volume = {93},
	issn = {0027-8424 (Print)},
	doi = {10.1073/pnas.93.16.8407},
	abstract = {We have devised a microspectroscopic strategy for assessing the intracellular (re)distribution and the integrity of the primary structure of proteins involved in signal transduction. The purified proteins are fluorescent-labeled in vitro and reintroduced into the living cell. The localization and molecular state of fluorescent-labeled protein kinase C beta I isozyme were assessed by a combination of quantitative confocal laser scanning microscopy, fluorescence lifetime imaging microscopy, and novel determinations of fluorescence resonance energy transfer based on photobleaching digital imaging microscopy. The intensity and fluorescence resonance energy transfer efficiency images demonstrate the rapid nuclear translocation and ensuing fragmentation of protein kinase C beta I in BALB/c3T3 fibroblasts upon phorbol ester stimulation, and suggest distinct, compartmentalized roles for the regulatory and catalytic fragments.},
	number = {August},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Bastiaens, P I and Jovin, T M},
	year = {1996},
	pages = {8407--8412},
}

@article{duncan_multi-dimensional_2004,
	title = {Multi-dimensional time-correlated single photon counting ({TCSPC}) fluorescence lifetime imaging microscopy ({FLIM}) to detect {FRET} in cells},
	volume = {215},
	issn = {0022-2720},
	doi = {10.1111/j.0022-2720.2004.01343.x},
	abstract = {We present a novel, multi-dimensional, time-correlated single photon counting (TCSPC) technique to perform fluorescence lifetime imaging with a laser-scanning microscope operated at a pixel dwell-time in the microsecond range. The unsurpassed temporal accuracy of this approach combined with a high detection efficiency was applied to measure the fluorescent lifetimes of enhanced cyan fluorescent protein (ECFP) in isolation and in tandem with EYFP (enhanced yellow fluorescent protein). This technique enables multi-exponential decay analysis in a scanning microscope with high intrinsic time resolution, accuracy and counting efficiency, particularly at the low excitation levels required to maintain cell viability and avoid photobleaching. Using a construct encoding the two fluorescent proteins separated by a fixed-distance amino acid spacer, we were able to measure the fluorescence resonance energy transfer (FRET) efficiency determined by the interchromophore distance. These data revealed that ECFP exhibits complex exponential fluorescence decays under both FRET and non-FRET conditions, as previously reported. Two approaches to calculate the distance between donor and acceptor from the lifetime delivered values within a 10\% error range. To confirm that this method can be used also to quantify intermolecular FRET, we labelled cultured neurones with the styryl dye FM1-43, quantified the fluorescence lifetime, then quenched its fluorescence using FM4-64, an efficient energy acceptor for FM1-43 emission. These experiments confirmed directly for the first time that FRET occurs between these two chromophores, characterized the lifetimes of these probes, determined the interchromophore distance in the plasma membrane and provided high-resolution two-dimensional images of lifetime distributions in living neurones.},
	number = {December 2003},
	journal = {Journal of Microscopy},
	author = {Duncan, R. R. and Bergmann, a. and Cousin, M. a. and Apps, D. K. and Shipston, M. J.},
	year = {2004},
	keywords = {FM1-43, GFP, Multi-photon, Two-photon},
	pages = {1--12},
}

@article{erdel_quantifying_2012,
	title = {Quantifying transient binding of {ISWI} chromatin remodelers in living cells by pixel-wise photobleaching profile evolution analysis.},
	volume = {109},
	issn = {1091-6490 (Electronic){\textbackslash}r0027-8424 (Linking)},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3511136&tool=pmcentrez&rendertype=abstract},
	doi = {10.1073/pnas.1209579109},
	abstract = {Interactions between nuclear proteins and chromatin frequently occur on the time scale of seconds and below. These transient binding events are important for the fast identification of target sites as concluded from our previous analysis of the human chromatin remodelers Snf2H and Snf2L from the imitation switch (ISWI) family. Both ATP-driven molecular motor proteins are able to translocate nucleosomes along the DNA and appear to exert this activity only on a small number of nucleosomes to which they bind more tightly. For mechanistic studies, one needs to distinguish such translocation reactions or other long-lived interactions associated with conformational changes and/or ATP hydrolysis from nonproductive chromatin sampling during target search. These processes can be separated by measuring the duration of nucleosome binding with subsecond time resolution. To reach this goal, we have developed a fluorescence bleaching technique termed pixel-wise photobleaching profile evolution analysis (3PEA). It exploits the inherent time structure of confocal microscopy images and yields millisecond resolution. 3PEA represents a generally applicable approach to quantitate transient chromatin interactions in the 2- to 500-ms time regime within only ∼1 s needed for a measurement. The green autofluorescent protein (GFP)-tagged Snf2H and Snf2L and the inactive Snf2L+13 splice variant were studied by 3PEA in comparison to the isolated GFP or red autofluorescent protein and a GFP pentamer. Our results reveal that the residence time for transient chromatin binding of Snf2H and Snf2L is {\textless}2 ms, and strongly support the view that ISWI-type remodelers are only rarely active in unperturbed cells during G1 phase.},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Erdel, Fabian and Rippe, Karsten},
	year = {2012},
	keywords = {Humans, Cell Line, Kinetics, Luminescent Proteins, Luminescent Proteins: metabolism, Photobleaching, Adenosine Triphosphatases, Adenosine Triphosphatases: metabolism, Cell Survival, Chromatin Assembly and Disassembly, Chromosomal Proteins, Non-Histone, Chromosomal Proteins, Non-Histone: metabolism, Diffusion, Fluorescence Recovery After Photobleaching, Green Fluorescent Proteins, Green Fluorescent Proteins: metabolism, Protein Binding, Protein Transport, Transcription Factors, Transcription Factors: metabolism},
	pages = {E3221--30},
}

@article{ng_imaging_1999,
	title = {Imaging protein kinase {C} alpha activation in cells},
	volume = {283},
	url = {http://www.sciencemag.org/cgi/content/full/283/5410/2085\nfile://d/MORE DOCS/pdf Papers/Imaging PKC activation.pdf},
	doi = {10.1126/science.283.5410.2085},
	abstract = {Spatially resolved fluorescence resonance energy transfer (FRET) measured by fluorescence lifetime imaging microscopy (FLIM), provides a method for tracing the catalytic activity of fluorescently tagged proteins inside live cell cultures and enables determination of the functional state of proteins in fixed cells and tissues. Here, a dynamic marker of protein kinase C alpha (PKC alpha) activation is identified and exploited. Activation of PKC alpha is detected through the binding of fluorescently tagged phosphorylation site-specific antibodies; the consequent FRET is measured through the donor fluorophore on PKC alpha by FLIM. This approach enabled the imaging of PKC alpha activation in live and fixed cultured cells and was also applied to pathological samples},
	number = {March},
	journal = {Science},
	author = {Ng, T and Squire, a and Hansra, G and Bornancin, F and Prevostel, C and Hanby, a and Harris, W and Barnes, D and Schmidt, S and Mellor, H and Bastiaens, P I H and Parker, P J},
	year = {1999},
	keywords = {microscopy, fluorescence resonance energy transfer, imaging, FRET, antibody, cell culture, culture, cultured, kinase, marker, method, phosphorylation, PKC, PKC-alpha, protein kinase C},
	pages = {2085--2089},
}

@article{berezhna_r_2001,
	title = {r 2 ( ∆ t ) ,},
	volume = {1464},
	number = {c},
	author = {Berezhna, Svitlana and Heintzmann, Rainer and Kahya, Nicoletta and Schaefer, Stephan and Gordon, Sean and Schwille, Petra},
	year = {2001},
	keywords = {imaging, single molecule, a, be a suitable and, diic 18, giant unilamellar vesicle, guvs, membrane behavior and function, molecules were imaged and, moreover, of different aspects of, reliable, testing bed for studies, that have proven to, tracked in the giant, unilamellar vesicles, we present experiments in, which single carbocyanine},
	pages = {2001--2001},
}

@article{perroud_effect_2005,
	title = {Effect of bin time on the photon counting histogram for one-photon excitation},
	volume = {6},
	issn = {1439-4235 (Print)},
	doi = {10.1002/cphc.200400547},
	abstract = {We have demonstrated that our photon counting histogram (PCH) model with the correction for one-photon excitation is valid at multiple bin times. The fitted apparent brightness and concentration follow the three-dimensional diffusion model. More importantly, the semi-empirical parameter, F, introduced in the PCH model for one-photon excitation to correct for the non-Gaussian shape of the observation volume, shows small variations with different bin times. These variations are consistent with the physical interpretation of F, and they do not affect the resolving power of the PCH model for one-photon excitation. Based on these findings, we extend the time-independent PCH analysis to time-dependent photon counting multiple histograms (PCMH). This model considers the effect of bin time on the PCH parameters in a way that is similar to fluorescence intensity multiple distribution analysis (FIMDA). From the same set of data, PCMH extracts time-dependent parameters (diffusion time and triplet-state relaxation time) as well as time-independent parameters (true specific brightness and true average number of molecules). Given a three- to fourfold experimental difference in molecular brightness, we find that PCMH can resolve each species in a two-species sample and extract their respective diffusion times even when fluorescence correlation spectroscopy cannot.},
	journal = {ChemPhysChem},
	author = {Perroud, Thomas D. and Huang, Bo and Zare, Richard N.},
	year = {2005},
	keywords = {Single-molecule studies, Confocal microscopy, Photon counting histogram, Few-molecule specctroscopy, Fluorescence specctroscopy},
	pages = {905--912},
}

@article{zucker_statistical_2001,
	title = {Statistical evaluation of confocal microscopy images.},
	volume = {44},
	issn = {0196-4763 (Print)},
	abstract = {BACKGROUND: The coefficient of variation (CV) is defined as the standard deviation (sigma) of the fluorescent intensity of a population of beads or pixels expressed as a proportion or percentage of the mean (mu) intensity (CV = sigma/mu). The field of flow cytometry has used the CV of a population of bead intensities to determine if the flow cytometer is aligned correctly and performing properly. In a similar manner, the analysis of CV has been applied to the confocal laser scanning microscope (CLSM) to determine machine performance and sensitivity. METHODS: Instead of measuring 10,000 beads using a flow cytometer and determining the CV of this distribution of intensities, thousands of pixels are measured from within one homogeneous Spherotech 10-microm bead. Similar to a typical flow cytometry population that consists of 10,000 beads, a CLSM scanned image consists of a distribution of pixel intensities representing a population of approximately 100,000 pixels. In order to perform this test properly, it is important to have a population of homogeneous particles. A biological particle usually has heterogeneous pixel intensities that correspond to the details in the biological image and thus shows more variability as a test particle. RESULTS: The bead CV consisting of a population of pixel intensities is dependent on a number of machine variables that include frame averaging, photomultiplier tube (PMT) voltage, PMT noise, and laser power. The relationship among these variables suggests that the machine should be operated with lower PMT values in order to generate superior image quality. If this cannot be achieved, frame averaging will be necessary to reduce the CV and improve image quality. There is more image noise at higher PMT settings, making it is necessary to average more frames to reduce the CV values and improve image quality. The sensitivity of a system is related to system noise, laser light efficiency, and proper system alignment. It is possible to compare different systems for system performance and sensitivity if the laser power is maintained at a constant value. Using this bead CV test, 1 mW of 488 nm laser light measured on the scan head yielded a CV value of 4\% with a Leica TCS-SP1 (75-mW argon-krypton laser) and a CV value of 1.3\% with a Zeiss 510 (25-mW argon laser). A biological particle shows the same relationship between laser power, averaging, PMT voltage, and CV as do the beads. However, because the biological particle has heterogeneous pixel intensities, there is more particle variability, which does not make as useful as a test particle. CONCLUSIONS: This CV analysis of a 10-microm Spherotech fluorescent bead can help determine the sensitivity in a confocal microscope and the system performance. The relationship among the factors that influence image quality is explained from a statistical endpoint. The data obtained from this test provides a systematic method of reducing noise and increasing image clarity. Many components of a CLSM, including laser power, laser stability, PMT functionality, and alignment, influence the CV and determine if the equipment is performing properly. Preliminary results have shown that the bead CV can be used to compare different confocal microscopy systems with regard to performance and sensitivity. The test appears to be analogous to CV tests made on the flow cytometer to assess instrument performance and sensitivity. Published 2001 Wiley-Liss, Inc.},
	journal = {Cytometry},
	author = {Zucker, R M and Price, O T},
	year = {2001},
	pages = {295--308},
}

@article{isothiocyanate_environmental_1995,
	title = {Environmental {Factors} {Recorded} with a {Confocal} {Laser} {Scanning} {Microscope} ’ {I} {\textbackslash} {TexanRed}},
	volume = {43},
	number = {7},
	author = {Isothiocyanate, Tetramethylrhodamine and Brismar, Hjalmar and Trepte, Oliver and Ulfhake, Brun},
	year = {1995},
	keywords = {antibody conjuga-, cence lifetime, confocal laser microscopy, emission, excitation, fluores-, fluorophore, ph, spectrum, tion, tissue embedding},
	pages = {699--707},
}

@article{at_s_2001,
	title = {S m f s a t : d a t -r c m s -m s},
	author = {At, Pectroscopy and With, Icroscope},
	year = {2001},
}

@article{noauthor_cy2_shift_nodate,
	title = {Cy2\_shift},
}

@article{noauthor_1471-2121-2-8-2_nodate,
	title = {1471-2121-2-8-2},
}

@article{noauthor_umassmed_nodate,
	title = {umassmed},
}

@article{pinaud_bioactivation_2004,
	title = {Bioactivation and {Cell} {Targeting} of {Semiconductor} {CdSe} / {ZnS} {Nanocrystals} with {Phytochelatin}-{Related} {Peptides} {Bioactivation} and {Cell} {Targeting} of {Semiconductor} {CdSe} / {ZnS} {Nanocrystals} with {Phytochelatin}-{Related} {Peptides}},
	doi = {10.1021/ja031691c},
	number = {7},
	journal = {IUBMB Life},
	author = {Pinaud, Fabien and King, David and Moore, Hsiao-ping and Weiss, Shimon},
	year = {2004},
	pages = {6115--6123},
}

@article{billinton_seeing_2001,
	title = {Seeing the wood through the trees: a review of techniques for distinguishing green fluorescent protein from endogenous autofluorescence.},
	volume = {291},
	issn = {00032697},
	doi = {10.1006/abio.2000.5006},
	abstract = {The green fluorescent protein (GFP)2 from the jelly-fish Aequorea{\textbackslash}nvictoria has recently leapt to prominence within numerous biological{\textbackslash}nfields. Interest in the use of GFP has grown enormously over the{\textbackslash}npast 5 years. The number of papers concerning GFP held in the Web{\textbackslash}nof Science database, which abstracts the scientific literature (http://wos.mimas.ac.uk),{\textbackslash}nrose from 13 in the period 1981-1994, to over 3400 in the period{\textbackslash}n1995 to October 2000. This is due to the ability to clone and heterologously{\textbackslash}nexpress GFP genes in a diverse range of cells and organisms, from{\textbackslash}nbacteria and yeast, to plants and mammals, coupled with favorable{\textbackslash}nproperties such as high stability, minimal toxicity, noninvasive{\textbackslash}ndetection, and the ability to generate the highly visible fluorophore{\textbackslash}nin vivo in the absence of external cofactors. Thus, GFP has become{\textbackslash}na truly versatile marker for visualizing physiological processes,{\textbackslash}nmonitoring subcellular protein localization, distinguishing successful{\textbackslash}ntransfection, or reporting on gene expression. As such a powerful{\textbackslash}ntool, GFP now impinges on almost every area of biological research.},
	journal = {Analytical biochemistry},
	author = {Billinton, N and Knight, a W},
	year = {2001},
	pages = {175--197},
}

@article{noauthor_lsm5_bdl_probes_kaede_nodate,
	title = {{LSM5}\_BDL\_Probes\_Kaede},
}

@article{de_chaumont_icy_2012,
	title = {Icy: an open bioimage informatics platform for extended reproducible research},
	volume = {9},
	issn = {1548-7105 (Electronic){\textbackslash}n1548-7091 (Linking)},
	doi = {10.1038/nmeth.2075},
	abstract = {Current research in biology uses evermore complex computational and imaging tools. Here we describe Icy, a collaborative bioimage informatics platform that combines a community website for contributing and sharing tools and material, and software with a high-end visual programming framework for seamless development of sophisticated imaging workflows. Icy extends the reproducible research principles, by encouraging and facilitating the reusability, modularity, standardization and management of algorithms and protocols. Icy is free, open-source and available at http://icy.bioimageanalysis.org/.},
	number = {7},
	journal = {Nature Methods},
	author = {de Chaumont, Fabrice and Dallongeville, Stéphane and Chenouard, Nicolas and Hervé, Nicolas and Pop, Sorin and Provoost, Thomas and Meas-Yedid, Vannary and Pankajakshan, Praveen and Lecomte, Timothée and Le Montagner, Yoann and Lagache, Thibault and Dufour, Alexandre and Olivo-Marin, Jean-Christophe},
	year = {2012},
	pages = {690--696},
}

@article{heikal_molecular_2000,
	title = {Molecular spectroscopy and dynamics of intrinsically fluorescent proteins: coral red ({dsRed}) and yellow ({Citrine}).},
	volume = {97},
	issn = {0027-8424 (Print){\textbackslash}r0027-8424 (Linking)},
	doi = {10.1073/pnas.97.22.11996},
	abstract = {Gene expression of intrinsically fluorescent proteins in biological systems offers new noninvasive windows into cellular function, but optimization of these probes relies on understanding their molecular spectroscopy, dynamics, and structure. Here, the photophysics of red fluorescent protein (dsRed) from discosoma (coral), providing desired longer emission/absorption wavelengths, and an improved yellow fluorescent protein mutant (Citrine) (S65G/V68L/Q69 M/S72A/T203Y) for significant comparison, are characterized by using fluorescence correlation spectroscopy and time-correlated single-photon counting. dsRed fluorescence decays as a single exponential with a 3.65 +/- 0.07-ns time constant, indicating a single emitting state/species independent of pH 4.4-9.0, in contrast with Citrine. However, laser excitation drives reversible fluorescence flicker at 10(3)-10(4) Hz between dark and bright states with a constant partition fraction f(1) = 0.42 +/- 0.06 and quantum yield of approximately 3 x 10(-3). Unlike Citrine (pKa approximately 5.7), pH-dependent proton binding is negligible (pH 3. 9-11) in dsRed. Time-resolved anisotropy of dsRed reveals rapid depolarization (211 +/- 6 ps) plus slow rotational motion (53 +/- 8 ns), in contrast with a single rotational time (16 +/- 2 ns) for Citrine. The molecular dimensions, calculated from rotational and translational diffusion, indicate that dsRed is hydrodynamically 3.8 +/- 0.4 times larger than predicted for a monomer, which suggests an oligomer (possibly a tetramer) configuration even at approximately 10(-9) M. The fast depolarization is attributed to intraoligomer energy transfer between mobile nonparallel chromophores with the initial anisotropy implying a 24 +/- 3 degrees depolarization angle. Large two-photon excitation cross sections ( approximately 100 GM at 990 nm for dsRed and approximately 50 GM at 970 nm for Citrine), advantageous for two-photon-fluorescence imaging in cells, are measured.},
	number = {22},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Heikal, a a and Hess, S T and Baird, G S and Tsien, R Y and Webb, W W},
	year = {2000},
	pages = {11996--12001},
}

@article{noauthor_logon_nodate,
	title = {logon},
}

@article{enderlein_statistical_2005,
	title = {Statistical analysis of diffusion coefficient determination by fluorescence correlation spectroscopy},
	volume = {15},
	doi = {10.1007/s10895-005-2633-0},
	abstract = {Fluorescence correlation spectroscopy (FCS) has become an important and widely used technique for many applications in physics, chemistry, and biology. The parameter most frequently addressed by FCS is the diffusion of molecules in solution. Due to the highly non-linear connection between the diffusion coefficient and a measured autocorrelation function, it is extremely difficult to analyse the accuracy of the diffusion-coefficient determination in a FCS experiment. Here, we present a simplified analysis based on some general maximum-likelihood considerations, and numerical result are given for the dependence of the accuracy of the diffusion-coefficient determination on sample concentration, brightness, and measurement time. Optimal concentration values for performing FCS are found.},
	number = {3},
	journal = {Journal of Fluorescence},
	author = {Enderlein, Jörg and Gregor, Ingo and Patra, Digambara and Fitter, Jörg},
	year = {2005},
	keywords = {Fluorescence correlation spectroscopy, Diffusion coefficient, Statistical accuracy},
	pages = {415--422},
}

@article{shu_novel_2006,
	title = {Novel chromophores and buried charges control color in {mFruits}},
	volume = {45},
	issn = {0006-2960},
	doi = {10.1021/bi060773l},
	abstract = {mFruits are second-generation monomeric red fluorescent proteins (mRFPs) that have improved brightness and photostability compared to the first-generation mRFP1. The emission and excitation maxima are distributed over the remarkably large ranges of about 550-650 and 540-590 nm, respectively; however, the variations in the spectra can be traced to a few key amino acids. Spectroscopic and atomic resolution crystallographic analyses of three representatives, mOrange, mStrawberry, and mCherry, reveal that different mechanisms operate to establish the excitation and emission maxima. Evidently, they all undergo the second oxidation step to produce an acylimine linkage in the polypeptide backbone. In comparison to the progenitor DsRed, direct covalent modification to this linkage (mOrange) and indirect modification of the chromophore environment (mStrawberry and mCherry) produce strong blue- and red-shifted variants. The blue shift of mOrange is induced by an unprecedented covalent modification of the protein backbone. The electron-density map indicates the formation of a third heterocycle, 2-hydroxy-dihydrooxazole, upon the reaction of Thr 66 Ogamma with the polypeptide backbone, which in turn reduces the conjugation of the carbonyl at position 65 with the rest of the chromophore. In mStrawberry and mCherry, the movement of charged Lys 70 and protonation of Glu 215 are proposed to modify the chromophore electron-density distribution, inducing the red shift. pH-dependent spectral shifts of mCherry and mStrawberry appear to result from the titration of Glu 215, although, for mStrawberry, partial cyclization of Thr 66 may contribute at high pH.},
	number = {32},
	journal = {Biochemistry},
	author = {Shu, Xiaokun and Shaner, Nathan C. and Yarbrough, Corinne a. and Tsien, Roger Y. and Remington, S. James},
	year = {2006},
	pages = {9639--9647},
}

@article{barzda_fluorescence_2001,
	title = {Fluorescence lifetime heterogeneity in aggregates of {LHCII} revealed by time-resolved microscopy.},
	volume = {81},
	issn = {8585340290},
	doi = {10.1016/S0006-3495(01)75720-7},
	abstract = {Two-photon excitation, time-resolved fluorescence microscopy was used to investigate the fluorescence quenching mechanisms in aggregates of light-harvesting chlorophyll a/b pigment protein complexes of photosystem II from green plants (LHCII). Time-gated microscopy images show the presence of large heterogeneity in fluorescence lifetimes not only for different LHCII aggregates, but also within a single aggregate. Thus, the fluorescence decay traces obtained from macroscopic measurements reflect an average over a large distribution of local fluorescence kinetics. This opens the possibility to resolve spatially different structural/functional units in chloroplasts and other heterogeneous photosynthetic systems in vivo, and gives the opportunity to investigate individually the excited states dynamics of each unit. We show that the lifetime distribution is sensitive to the concentration of quenchers contained in the system. Triplets, which are generated at high pulse repetition rates of excitation ({\textgreater}1 MHz), preferentially quench domains with initially shorter fluorescence lifetimes. This proves our previous prediction from singlet-singlet annihilation investigations (Barzda, V., V. Gulbinas, R. Kananavicius, V. Cervinskas, H. van Amerongen, R. van Grondelle, and L. Valkunas. 2001. Biophys. J. 80:2409-2421) that shorter fluorescence lifetimes originate from larger domains in LHCII aggregates. We found that singlet-singlet annihilation has a strong effect in time-resolved fluorescence microscopy of connective systems and has to be taken into consideration. Despite that, clear differences in fluorescence decays can be detected that can also qualitatively be understood.},
	number = {July},
	journal = {Biophysical journal},
	author = {Barzda, V and de Grauw, C J and Vroom, J and Kleima, F J and van Grondelle, R and van Amerongen, H and Gerritsen, H C},
	year = {2001},
	pages = {538--546},
}

@article{elsner_spatiotemporal_2003,
	title = {Spatiotemporal dynamics of the {COPI} vesicle machinery.},
	volume = {4},
	doi = {10.1038/sj.embor.embor942},
	abstract = {Assembly of the coat protein I (COPI) vesicle coat is controlled by the small GTPase ADP ribosylation factor 1 (ARF1) and its GTPase-activating protein, ARFGAP1. Here, we investigate the diffusional behaviours of coatomer, the main component of the coat, and also those of ARF1 and ARFGAP1. Using fluorescence-correlation spectroscopy, we found that most ARF1 and ARFGAP1 molecules are highly mobile in the cytosol (diffusion constant D approximately equal to 15 microm(2) s(-1)), whereas coatomer diffuses 5-10 times more slowly than expected (D approximately equal to 1 microm(2) s(-1)). This slow diffusion causes diffusion-limited binding kinetics to Golgi membranes, which, in FRAP (fluorescence recovery after photobleaching) experiments, translates into a twofold slower binding rate. The addition of aluminium fluoride locks coatomer onto Golgi membranes and also decreases the binding kinetics of both ARF1 and ARFGAP1, suggesting that these proteins function in concert to mediate sorting and vesicle formation.},
	number = {10},
	journal = {EMBO reports},
	author = {Elsner, Markus and Hashimoto, Hitoshi and Simpson, Jeremy C and Cassel, Dan and Nilsson, Tommy and Weiss, Matthias},
	year = {2003},
	pages = {1000--1004},
}

@article{noauthor_fluctuation_2008,
	title = {Fluctuation information extraction from digital images ( {N} \& {B} approach )},
	number = {April},
	year = {2008},
	pages = {2008--2008},
}

@article{bacia_probing_2002,
	title = {Probing the endocytic pathway in live cells using dual-color fluorescence cross-correlation analysis.},
	volume = {83},
	issn = {0006-3495 (Print){\textbackslash}n0006-3495 (Linking)},
	doi = {10.1016/S0006-3495(02)75242-9},
	abstract = {Fluorescence (auto)correlation spectroscopy (FCS) has developed into a widely used method for investigating molecular dynamics and mobility of molecules in vitro and in vivo. Dual-color cross-correlation, an extension of this technique, also assesses the concomitant movement of two spectrally distinguishable fluorescent molecules and has therefore proven superior to autocorrelation analysis to study interactions between different molecular species in solution. Here we explore the benefits of cross-correlation analysis when applied to live cells, by demonstrating its potential in analyzing endocytic processes. Bacterial cholera toxin (CTX) was labeled with Cy2 and Cy5 dyes on different subunits of the same holotoxin. Along the endocytic pathway, positive cross-correlation between the A and B subunits was first preserved, later followed by a loss in cross-correlation upon their separation in the Golgi. Furthermore, endocytosis of a mixture of only Cy2- and only Cy5-labeled holotoxins also gave rise to cross-correlation. Our results suggest that cross-correlation may be used to recognize whether different cargoes use the same endocytic pathway. Additionally, we show that cross-correlation is applicable to two-dimensional membrane diffusion. CTX bound to GM1-containing artificial giant unilamellar vesicles was diffusible, whereas CTX bound to the plasma membrane was immobile on the FCS time-scale, possibly because of raft-association of GM1.},
	number = {August},
	journal = {Biophysical journal},
	author = {Bacia, Kirsten and Majoul, Irina V and Schwille, Petra},
	year = {2002},
	pages = {1184--1193},
}

@article{craenenbroeck_heuristic_2001,
	title = {Heuristic {Statistical} {Analysis} of {Fluorescence} {Fluctuation} {Data} with {Bright} {Spikes}: {Application} to {Ligand} {Binding} to the {Human} {Serotonin} {Receptor} {Expressed} in {Escherichia} coli {Cells}},
	volume = {382},
	doi = {10.1515/BC.2001.043},
	number = {March},
	journal = {Biological Chemistry},
	author = {Craenenbroeck, Elke Van and Vercammen, Jo and Matthys, Gunther and Beirlant, Jan and Marot, Christophe and Hoebeke, Johan and Strobbe, Rik and Engelborghs, Yves},
	year = {2001},
	keywords = {fluorescence, competition, fluorescein},
	pages = {355--361},
}

@article{sinha_development_2004,
	title = {Development of single-molecule tracking confocal microscope combined with force spectroscopy for gene-expression analysis},
	volume = {87},
	abstract = {We have constructed a confocal fluorescence microscope{\textbackslash}n{\textbackslash}ncombined with force measurements. Our method{\textbackslash}n{\textbackslash}nallows for simultaneous measurements of fluorescence{\textbackslash}n{\textbackslash}nanisotropy, energy transfer and correlation. The methodology{\textbackslash}n{\textbackslash}nand the sensitivity of the set-up using enhanced{\textbackslash}n{\textbackslash}ngreen fluorescent protein are described. We present{\textbackslash}n{\textbackslash}nresults on (a) the detection of mRNA polymerization{\textbackslash}n{\textbackslash}nduring in vitro transcription using fluorescence correlation{\textbackslash}n{\textbackslash}nspectroscopy and anisotropy, (b) detection of in{\textbackslash}n{\textbackslash}nvivo protein-DNA interactions using fluorescence anisotropy{\textbackslash}n{\textbackslash}nand (c) nanomanipulation of polytene chromosomes{\textbackslash}n{\textbackslash}nusing the micropippete force sensor. Such a{\textbackslash}n{\textbackslash}ncombined method allows for probing novel structure–{\textbackslash}n{\textbackslash}nfunction relationship underlying gene-expression.},
	journal = {Current Science},
	author = {Sinha, Deepak Kumar and Bhattacharya, Dipanjan and Banerjee, Bidisha and Hameed, Feroz Meeran and Shivashankar, G. V.},
	year = {2004},
	pages = {239--244},
}

@article{altan-bonnet_bubble_2003,
	title = {Bubble dynamics in double-stranded {DNA}.},
	volume = {90},
	doi = {10.1103/PhysRevLett.90.138101},
	abstract = {We report the first measurement of the dynamics of bubble formation in double-stranded DNA. Fluctuations of fluorescence of a synthetic DNA construct, internally tagged with a fluorophore and a quencher, are monitored by fluorescence correlation spectroscopy. The relaxation dynamics follow a multistate relaxation kinetics, with a characteristic time scale of 50 microseconds. A simple model of bubble dynamics based on constant zipping-unzipping rates is proposed to account for our experimental data. The role of different secondary structures stabilizing the open bubble is tested.},
	number = {April},
	journal = {Physical review letters},
	author = {Altan-Bonnet, Grégoire and Libchaber, Albert and Krichevsky, Oleg},
	year = {2003},
	pages = {138101--138101},
}

@article{kim_fully_2007-2,
	title = {Fully automated segmentation and morphometrical analysis of muscle fiber images.},
	volume = {71},
	doi = {10.1002/cyto.a},
	abstract = {BACKGROUND: Measurement of muscle fiber size and determination of size distribution is important in the assessment of neuromuscular disease. Fiber size estimation by simple inspection is inaccurate and subjective. Manual segmentation and measurement are time-consuming and tedious. We therefore propose an automated image analysis method for objective, reproducible, and time-saving measurement of muscle fibers in routinely hematoxylin-eosin stained cryostat sections. METHODS: The proposed segmentation technique makes use of recent advances in level set based segmentation, where classical edge based active contours are extended by region based cues, such as color and texture. Segmentation and measurement are performed fully automatically. Multiple morphometric parameters, i.e., cross sectional area, lesser diameter, and perimeter are assessed in a single pass. The performance of the computed method was compared to results obtained by manual measurement by experts. RESULTS: The correct classification rate of the computed method was high (98\%). Segmentation and measurement results obtained manually or automatically did not reveal any significant differences. CONCLUSIONS: The presented region based active contour approach has been proven to accurately segment and measure muscle fibers. Complete automation minimizes user interaction, thus, batch processing, as well as objective and reproducible muscle fiber morphometry are provided.},
	number = {June},
	journal = {Cytometry. Part A : the journal of the International Society for Analytical Cytology},
	author = {Kim, Yoo-Jin and Brox, Thomas and Feiden, Wolfgang and Weickert, Joachim},
	year = {2007},
	pages = {8--15},
}

@article{ayling_automated_2009,
	title = {Automated light-based mapping of motor cortex by photoactivation of channelrhodopsin-2 transgenic mice.},
	volume = {6},
	issn = {1548-7105},
	doi = {10.1038/nmeth.1303},
	abstract = {Traditionally, mapping the motor cortex requires electrodes to stimulate the brain and define motor output pathways. Although effective, electrode-based methods are labor-intensive, potentially damaging to the cortex and can have off-target effects. As an alternative method of motor mapping, we photostimulated transgenic mice expressing the light-sensitive ion channel channelrhodopsin-2 in predominantly layer-5 output cortical neurons. We report that optical stimulation of these neurons in vivo using a stage scanning laser system resulted in muscle excitation within 10-20 ms, which can be recorded using implanted electromyogram electrodes or by a noninvasive motion sensor. This approach allowed us to make highly reproducible automated maps of the mouse forelimb and hindlimb motor cortex much faster than with previous methods. We anticipate that the approach will facilitate the study of changes in the location and properties of motor maps after skilled training or damage to the nervous system.},
	journal = {Nature methods},
	author = {Ayling, Oliver G S and Harrison, Thomas C and Boyd, Jamie D and Goroshkov, Alexander and Murphy, Timothy H},
	year = {2009},
	pages = {219--224},
}

@article{feature_naked_2009,
	title = {The naked microscope},
	volume = {459},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/19494892},
	abstract = {1: Nature. 2009 Jun 4;459(7247):636-7. Microscopic marvels: . Ledford H. Publication Types: News. Mesh Terms: Microscopy},
	number = {June},
	journal = {Nature},
	author = {Feature, News and Marvels, Microscopic},
	year = {2009},
	pages = {1--2},
}

@article{krausz_high-content_2007,
	title = {High-content {siRNA} screening.},
	volume = {3},
	issn = {1742-206X (Print)},
	doi = {10.1039/b616187c},
	abstract = {Very recent developments in instrumentation and image analysis have made microscopy applicable to high-throughput screening (HTS). For 'High-Content Screening' modern automated microscopy systems provide a throughput of up to 100,000 (confocal) images, with amazingly high resolution, of cells fluorescently stained using multiple colours that are imaged simultaneously during the screen. Image analysis tools provide multi-parametric pattern extraction and quantification on-the-fly. Big pharmaceutical companies have presented image-based screens of more than 100,000 compounds, while academia has published data on large RNA interference screens for functional genomics. Numerous whole-genome sequencing projects have been completed and published. Gene annotation is still in flux. Nevertheless, about 23,000 human genes have been reliably annotated. Additionally, gene expression array technologies and proteomics have added further data on molecules present in cells and tissues. The major challenge of the present and future is to unravel the detailed function of all these gene products and their interaction. One way to gain insight, is to design oligonucleotides that induce lack-of-function phenotypes by specifically inhibiting protein production.},
	journal = {Molecular bioSystems},
	author = {Krausz, Eberhard},
	year = {2007},
	pages = {232--240},
}

@article{guan_adaptive_2008-2,
	title = {Adaptive correction technique for {3D} reconstruction of fluorescence microscopy images.},
	volume = {71},
	issn = {1059-910X (Print)},
	doi = {10.1002/jemt},
	abstract = {Recent advances in high-resolution imaging have provided valuable novel insights into structural relationships within cells and tissues both in vitro and in vivo. An analysis of this kind is regularly done by optical sectioning using either confocal or deconvolution microscopy. However, the reconstruction of 3D images suffers from light scattering and absorption with increasing depth by finite transparency of the used media. Photobleaching of fluorochromes has been especially troublesome and often the only remedy for loss of signal during optical sectioning is to reduce the number of sections. This causes disparities in the x-y and z dimensions of voxels, which lead to vertical distortion of the original stack of images and necessitates interpolation. Interpolation is necessary to fill up the gaps between consecutive sections in the original image stack to obtain cubic voxels. The present manuscript describes a novel method for adaptive compensation of attenuation of light intensity in stacks of fluorescence microscopy images that is based on a physical model of light attenuation. First, we use a fast interpolation technique to generate a cubic voxel-based volume stack with the aid of a contribution look up table. With the contribution look up table, multiple calculations are avoided, which substantially reduces the computational time without compromising the accuracy of the restoration procedure. Second, each section within the resulting volume is processed to rectify its intensity values that have been altered due to photobleaching and scattering and absorption. The method allows to define the last good section in the stack and the correction is then done automatically.},
	number = {September 2006},
	journal = {Microscopy research and technique},
	author = {Guan, Y Q and Cai, Y Y and Zhang, X and Lee, Y T and Opas, M},
	year = {2008},
	keywords = {autofluorescence, imaging, eye, emission spectra of endogenous, excitation and, fluorophores, in vivo fluorescence lifetime},
	pages = {146--157},
}

@article{noauthor_korlach_correction_nodate,
	title = {Korlach\_correction},
}

@article{heinze_simultaneous_2000-1,
	title = {Simultaneous two-photon excitation of distinct labels for dual-color fluorescence crosscorrelation analysis.},
	volume = {97},
	issn = {0027-8424 (Print){\textbackslash}r0027-8424 (Linking)},
	doi = {10.1073/pnas.180317197},
	abstract = {Confocal fluorescence correlation spectroscopy as a time-averaging fluctuation analysis combining maximum sensitivity with high statistical confidence has proved to be a very versatile and powerful tool for detection and temporal investigation of biomolecules at ultralow concentrations on surfaces, in solutions, and in living cells. To probe the interaction of different molecular species for a detailed understanding of biologically relevant mechanisms, crosscorrelation studies on dual or multiple fluorophore assays with spectrally distinct excitation and emission are particularly promising. Despite the considerable improvement of detection specificity provided by fluorescence crosscorrelation analysis, few applications have so far been reported, presumably because of the practical challenges of properly aligning and controlling the stability of the experimental setup. In this work, we demonstrate that two-photon excitation combined with dual-color fluorescence correlation spectroscopy can be the key to simplifying simultaneous investigations of multiple fluorescent species significantly on a single-molecule scale. Two-photon excitation allows accession of common fluorophores of largely distinct emission by the same excitation wavelength, because differences in selection rules and vibronic coupling can induce considerable shifts between the one-photon and two-photon excitation spectra. The concept of dual-color two-photon fluorescence crosscorrelation analysis is introduced and experimentally demonstrated with an established assay probing the selective cleavage of dual-labeled DNA substrates by restriction endonuclease EcoRI.},
	number = {19},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Heinze, K G and Koltermann, a and Schwille, P},
	year = {2000},
	pages = {10377--10382},
}

@article{behne_nhe1_2002,
	title = {{NHE1} regulates the stratum corneum permeability barrier homeostasis: {Microenvironment} acidification assessed with fluorescence lifetime imaging},
	volume = {277},
	issn = {0021-9258 (Print)},
	doi = {10.1074/jbc.M204759200},
	abstract = {The outermost epidermal layer, the stratum corneum (SC), exhibits an acidic surface pH, whereas the pH at its base approaches neutrality. NHE1 is the only Na(+)/H(+) antiporter isoform in keratinocytes and epidermis, and has been shown to regulate intracellular pH. We now demonstrate a novel function for NHE1, as we find that it also controls acidification of extracellular "microdomains" in the SC that are essential for activation of pH-sensitive enzymes and the formation of the epidermal permeability barrier. NHE1 expression in epidermis is most pronounced in granular cell layers, and although the surface pH of NHE1 knockout mice is only slightly more alkaline than normal using conventional pH measurements, a more sensitive method, fluorescence lifetime imaging, demonstrates that the acidic intercellular domains at the surface and of the lower SC disappear in NHE1 -/- animals. Fluorescence lifetime imaging studies also reveal that SC acidification does not occur through a uniform gradient, but through the progressive accumulation of acidic microdomains. These findings not only visualize the spatial distribution of the SC pH gradient, but also demonstrate a role for NHE1 in the generation of acidic extracellular domains of the lower SC, thus providing the acidification of deep SC interstices necessary for lipid processing and barrier homeostasis.},
	number = {49},
	journal = {Journal of Biological Chemistry},
	author = {Behne, Martin J. and Meyer, Jamie W. and Hanson, Kerry M. and Barry, Nicholas P. and Murata, Satoru and Crumrine, Debra and Clegg, Robert W. and Gratton, Enrico and Holleran, Walter M. and Elias, Peter M. and Mauro, Theodora M.},
	year = {2002},
	pages = {47399--47406},
}

@article{bader_protein_nodate,
	title = {Protein cluster size imaging by time-resolved fluorescence anisotropy microscopy},
	author = {Bader, Arjen and Hofman, Erik and Bergen, Paul Van and Gerritsen, Hans},
	pages = {4--4},
}

@article{wawrezinieck_fluorescence_2004,
	title = {Fluorescence correlation spectroscopy to determine diffusion laws: application to live cell membranes},
	volume = {5462},
	url = {http://link.aip.org/link/?PSI/5462/92/1},
	doi = {10.1117/12.545014},
	abstract = {Fluorescencecorrelation spectroscopy (FCS) is a mature and powerful technique formeasuring diffusion coefficients. In a standard experiment, it measures thespontaneous fluorescence fluctuations arising from a single observation volume definedby confocal optics. However, the study becomes uneasy as soonas the diffusion is impeded by obstacles or specific mechanisms,as it is the case for the cell membrane componentsin live cells. In this paper, we show that doingFCS measurements at different sizes of observation volumes gives accessto the diffusion laws without a priori knowledge of thelandscape in which molecules are diffusing. Using this strategy, ameasurement of diffusion laws of lipids in monophasic Giant UnilamellarVesicles and in the plasma membrane of live cells iscarried out. 2004 COPYRIGHT SPIE-The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.},
	journal = {Biophotonics Micro and NanoImaging},
	author = {Wawrezinieck, Laure and Lenne, Pierre-Francois and Marguet, Didier and Rigneault, Herve},
	year = {2004},
	keywords = {fluorescence correlation spectroscopy, live cell, anomalous diffusion, cell membrane, diffusion, diffusion law, giant unilamellar, vesicle},
	pages = {92--102},
}

@article{mcguinness_selective_2005,
	title = {Selective excitation of tryptophan fluorescence decay in proteins using a subnanosecond 295 nm light-emitting diode and time-correlated single-photon counting},
	volume = {86},
	issn = {00036951},
	doi = {10.1063/1.1984088},
	abstract = {We demonstrate an AlGaN light-emitting diode (LED) giving pulses of {\textasciitilde}600 ps full width half maximum, 0.35 W average power, 0.6 mW peak power, and {\textasciitilde}12 nm bandwidth at 295 nm. This source is ideal for protein intrinsic tryptophan fluorescence decay research without the unwanted excitation of tyrosine and paves the way to lab-on-a-chip protein assays using fluorescence decay times. Fluorescence decay and anisotropy decay measurements of human serum albumin are reported and the usefulness of the 295 nm LED demonstrated in comparisons with a nanosecond flashlamp and LEDs with nominal wavelength emission of 280 nm.},
	journal = {Applied Physics Letters},
	author = {McGuinness, Colin D. and Sagoo, Kulwinder and McLoskey, David and Birch, D. J S},
	year = {2005},
	pages = {1--3},
}

@article{noauthor_flim_2008,
	title = {{FLIM} analysis made easy and more significant ( {Phasor} approach )},
	number = {April},
	year = {2008},
	pages = {2008--2008},
}

@article{rigler_lecture_2009,
	title = {Lecture 10 “ {Fluorescence} {Correlation} {Spectroscopy} in {Single} {Molecule} {Analysis} ”},
	doi = {10.1529/biophysj.104.053884.L01},
	number = {1994},
	author = {Rigler, Rudolf},
	year = {2009},
	pages = {211--213},
}

@article{schwille_fluorescence_2001,
	title = {Fluorescence correlation spectroscopy and its potential for intracellular applications.},
	volume = {34},
	issn = {1085-9195},
	doi = {10.1385/CBB:34:3:383},
	abstract = {Fluorescence correlation spectroscopy (FCS) is a time-averaging fluctuation analysis of small molecular ensembles, combining maximum sensitivity with high statistical confidence. Among a multitude of physical parameters that are, in principle, accessible by FCS, it most conveniently allows to determine local concentrations, mobility coefficients, and characteristic rate constants of fast-reversible and slow-irreversible reactions of fluorescently labeled biomolecules at very low (nanomolar) concentrations, under equilibrium conditions and without physical separation. Its presently most popular instrumentation by confocal-microscope setups allows for a spatial resolution of fractions of femtoliters for the measurement volumes, containing sparse or even single molecules at any time, and encourages the adaptation of the solution-based technique for cellular applications. The scope of this review is thus, to introduce the FCS technique in particular to the reader with biological background, searching for new methods for a precise quantification of physical parameters governing cellular mechanisms and dynamics, especially if high sensitivity and fast dynamic resolution are required. After a short theoretical introduction, examples are given for the so far most important experimental applications, with respect to their implementation in cellular systems. As an interesting alternative to the confocal instrumentation, two-photon excitation will be introduced, offering a number of important advantages especially in cellular systems with high-noise and low-signal levels.},
	journal = {Cell biochemistry and biophysics},
	author = {Schwille, P},
	year = {2001},
	pages = {383--408},
}

@article{noauthor_timo_zimmermann_personal_communication_resonant_scanner_photoprotectionpdf_nodate,
	title = {Timo\_Zimmermann\_personal\_communication\_Resonant\_scanner\_photoprotection.pdf},
}

@article{noauthor___utm_nodate,
	title = {\_\_utm},
}

@article{van_kuppeveld_homomultimerization_2002,
	title = {Homomultimerization of the coxsackievirus {2B} protein in living cells visualized by fluorescence resonance energy transfer microscopy.},
	volume = {76},
	issn = {0022-538X (Print). 0022-538X (Linking)},
	doi = {10.1128/JVI.76.18.9446-9456.2002},
	abstract = {The 2B protein of enteroviruses is the viral membrane-active protein that is responsible for the modifications in host cell membrane permeability that take place in enterovirus-infected cells. The 2B protein shows structural similarities to the group of lytic polypeptides, polypeptides that permeate membranes either by forming multimeric membrane-integral pores or, alternatively, by lying parallel to the lipid bilayer and disturbing the curvature and symmetry of the membrane. Our aim is to gain more insight into the molecular architecture of the 2B protein in vivo. In this study, the possible existence of multimers of the coxsackie B3 virus 2B protein in single living cells was explored by fluorescence resonance energy transfer (FRET) microscopy. FRET between fusion proteins 2B-ECFP and 2B-EYFP (enhanced cyan and yellow fluorescent variants of green fluorescent protein) was monitored by using spectral imaging microscopy (SPIM) and fluorescence lifetime imaging microscopy (FLIM). Both techniques revealed the occurrence of intermolecular FRET between 2B-ECFP and 2B-EYFP, providing evidence for the formation of protein 2B homomultimers. Putative models for the mode of action of the membrane-active 2B protein and the formation of membrane-integral pores by 2B multimers are discussed.},
	number = {18},
	journal = {Journal of virology},
	author = {van Kuppeveld, Frank J M and Melchers, Willem J G and Willems, Peter H G M and Gadella, Theodorus W J},
	year = {2002},
	pages = {9446--9456},
}

@article{noauthor_ncomms3207-s12_nodate,
	title = {ncomms3207-s12},
}

@article{sun_characterization_2009,
	title = {Characterization of an orange acceptor fluorescent protein for sensitized spectral fluorescence resonance energy transfer microscopy using a white-light laser.},
	volume = {14},
	issn = {1083-3668},
	doi = {10.1117/1.3227036},
	abstract = {Orange fluorescent proteins (FPs) are attractive candidates as Forster resonance energy transfer (FRET) partners, bridging the gap between green and red/far-red FPs, but they pose significant challenges using common fixed laser wavelengths. We investigated monomeric Kusabira orange 2 (mKO2) FP as a FRET acceptor for monomeric teal FP (mTFP) as donor on a FRET standard construct using a fixed-distance amino acid linker, expressed in live cells. We quantified the apparent FRET efficiency (E\%) of this construct, using sensitized spectral FRET microscopy on the Leica TCS SP5 X imaging system equipped with a white-light laser that allows choosing any excitation wavelength from 470 to 670 nm in 1-nm increments. The E\% obtained in sensitized spectral FRET microscopy was then confirmed with fluorescence lifetime measurements. Our results demonstrate that mKO2 and mTFP are good FRET partners given proper imaging setups. mTFP was optimally excited by the Argon 458 laser line, and the 540-nm wavelength excitation for mKO2 was chosen from the white-light laser. The white-light laser generally extends the usage of orange and red/far-red FPs in sensitized FRET microscopy assays by tailoring excitation and emission precisely to the needs of the FRET pair.},
	number = {October},
	journal = {Journal of biomedical optics},
	author = {Sun, Yuansheng and Booker, Cynthia F and Kumari, Sangeeta and Day, Richard N and Davidson, Mike and Periasamy, Ammasi},
	year = {2009},
	keywords = {förster resonance energy transfer, protein, protein interactions, fluores-, ͒, ͑ flim-fret ͒, ͑ fret ͒, ͑ mtfp ͒, cence lifetime imaging fret, function ͑ irf ͒, instrument response, monomeric kusabira, monomeric teal fluorescent protein, orange 2 ͑ mko2, spectral fret ͑ sfret, white-light laser},
	pages = {054009--054009},
}

@article{noauthor_ncomms3207-s6_nodate,
	title = {ncomms3207-s6},
}

@article{verveer_quantitative_2000,
	title = {Quantitative imaging of lateral {ErbB1} receptor signal propagation in the plasma membrane.},
	volume = {290},
	issn = {0036-8075 (Print){\textbackslash}r0036-8075 (Linking)},
	doi = {10.1126/science.290.5496.1567},
	abstract = {Evidence for a new signaling mechanism consisting of ligand-independent lateral propagation of receptor activation in the plasma membrane is presented. We visualized the phosphorylation of green fluorescent protein (GFP)-tagged ErbB1 (ErbB1-GFP) receptors in cells focally stimulated with epidermal growth factor (EGF) covalently attached to beads. This was achieved by quantitative imaging of protein reaction states in cells by fluorescence resonance energy transfer (FRET) with global analysis of fluorescence lifetime imaging microscopy (FLIM) data. The rapid and extensive propagation of receptor phosphorylation over the entire cell after focal stimulation demonstrates a signaling wave at the plasma membrane resulting in full activation of all receptors.},
	number = {November},
	journal = {Science (New York, N.Y.)},
	author = {Verveer, P J and Wouters, F S and Reynolds, a R and Bastiaens, P I},
	year = {2000},
	pages = {1567--1570},
}

@article{chen_protein_2003,
	title = {Protein localization in living cells and tissues using {FRET} and {FLIM}.},
	volume = {71},
	issn = {0301-4681},
	doi = {10.1111/j.1432-0436.2003.07109007.x},
	abstract = {Interacting proteins assemble into molecular machines that control cellular homeostasis in living cells. While the in vitro screening methods have the advantage of providing direct access to the genetic information encoding unknown protein partners, they do not allow direct access to interactions of these protein partners in their natural environment inside the living cell. Using wide-field, confocal, or two-photon (2p) fluorescence resonance energy transfer (FRET) microscopy, this information can be obtained from living cells and tissues with nanometer resolution. One of the important conditions for FRET to occur is the overlap of the emission spectrum of the donor with the absorption spectrum of the acceptor. As a result of spectral overlap, the FRET signal is always contaminated by donor emission into the acceptor channel and by the excitation of acceptor molecules by the donor excitation wavelength. Mathematical algorithms are required to correct the spectral bleed-through signal in wide-field, confocal, and two-photon FRET microscopy. In contrast, spectral bleed-through is not an issue in FRET/FLIM imaging because only the donor fluorophore lifetime is measured; also, fluorescence lifetime imaging microscopy (FLIM) measurements are independent of excitation intensity or fluorophore concentration. The combination of FRET and FLIM provides high spatial (nanometer) and temporal (nanosecond) resolution when compared to intensity-based FRET imaging. In this paper, we describe various FRET microscopy techniques and its application to protein-protein interactions.},
	journal = {Differentiation; research in biological diversity},
	author = {Chen, Ye and Mills, James D and Periasamy, Ammasi},
	year = {2003},
	keywords = {microscopy, flim, fluorescence resonance energy transfer, fret, 2p-fret, á confocal fret, á flim-fret á ccaat, á fluorescence lifetime imaging, á two-photon, c-fret},
	pages = {528--541},
}

@article{schwille_fluorescence_1999,
	title = {Fluorescence correlation spectroscopy with single-molecule sensitivity on cell and model membranes.},
	volume = {36},
	issn = {0196-4763},
	doi = {10.1002/(SICI)1097-0320(19990701)36:3<176::AID-CYTO5>3.0.CO;2-F},
	abstract = {We report on the successful application of fluorescence correlation spectroscopy (FCS) to the analysis of single fluorescently labeled lipid analogue molecules diffusing laterally in lipid bilayers, as exemplified by time traces of fluorescence bursts of individual molecules entering and leaving the excitation area. FCS measurements performed on lipid probes in rat basophilic leukemia cell membranes showed deviations from two-dimensional Brownian motion with a single uniform diffusion constant. Giant unilamellar vesicles were employed as model systems to characterize diffusion of fluorescent lipid analogues in both homogeneous and mixed lipid phases with diffusion heterogeneity. Comparing the results of cell membrane diffusion with the findings on the model systems suggests possible explanations for the observations: (a) anomalous subdiffusion in which evanescent attractive interactions with disparate mobile molecules modifies the diffusion statistics; (b) alternatively, probe molecules are localized in microdomains of submicroscopic size, possibly in heterogeneous membrane phases.},
	journal = {Cytometry},
	author = {Schwille, P and Korlach, J and Webb, W W},
	year = {1999},
	pages = {176--182},
}

@article{lucas_towards_2005,
	title = {Towards a better understanding of the dissociation behavior of liposome-oligonucleotide complexes in the cytosol of cells},
	volume = {103},
	doi = {10.1016/j.jconrel.2004.12.017},
	abstract = {To obtain real breakthroughs in antisense therapy, it is necessary to understand the cellular behavior of antisense delivery systems. Fluorescence fluctuation spectroscopy (FFS), which measures in time fluorescence fluctuations in the excitation volume of a microscope and which can thus be applied on a cellular scale, shows potential for this purpose. In this study dual color FFS was explored to characterize the complexation (association and dissociation) between Cy5-labeled oligonucleotides (Cy5-ONs) and FITC-labeled cationic liposomes (FITC-liposomes) in respectively buffer, cell lysate and the cytosol of Vero cells. In Hepes buffer the association of the Cy5-ONs to the FITC-liposomes could be clearly observed from the high peaks of Cy5- and FITC-fluorescence, which appeared simultaneously in the excitation volume. This was explained by the fact that in the complexed state many Cy5-ONs and FITC-liposomes are bound to each other and thus move together through the excitation volume thereby resulting in high fluorescence 'FITC/Cy5-peaks'. FFS measurements on FITC-liposome/Cy5-ONs complexes in cell lysate revealed that a minor part of the Cy5-ONs was released from the complexes. The major part of the Cy5-ONs remained in the complexes, which also seemed to aggregate in cell lysate. In agreement with the measurements in cell lysate, after microinjection of FITC-liposome/Cy5-ONs complexes in the cytosol of Vero cells a part of the Cy5-ONs was released (as Cy-ONs were detected by FFS in the nuclei) while the other part remained bound (as Cy5-peaks were frequently observed in the cytosol). As will be explained, the Cy5-peaks could be due both to Cy5-ONs clustered with cytosol components and Cy5-ONs still complexed to FITC-liposomes with quenched FITC-fluorescence. ?? 2004 Elsevier B.V. All rights reserved.},
	journal = {Journal of Controlled Release},
	author = {Lucas, B. and Remaut, K. and Sanders, N. N. and Braeckmans, K. and De Smedt, S. C. and Demeester, J.},
	year = {2005},
	keywords = {Fluorescence correlation spectroscopy (FCS), Antisense oligonucleotides, Gene therapy, Intracellular trafficking, Lipoplexes},
	pages = {435--450},
}

@article{weiss_fluorescence_1999,
	title = {Fluorescence spectroscopy of single biomolecules.},
	volume = {283},
	issn = {0036-8075},
	doi = {10.1126/science.283.5408.1676},
	abstract = {Recent advances in single-molecule detection and single-molecule spectroscopy at room temperature by laser-induced fluorescence offer new tools for the study of individual macromolecules under physiological conditions. These tools relay conformational states, conformational dynamics, and activity of single biological molecules to physical observables, unmasked by ensemble averaging. Distributions and time trajectories of these observables can therefore be measured during a reaction without the impossible need to synchronize all the molecules in the ensemble. The progress in applying these tools to biological studies with the use of fluorophores that are site-specifically attached to macromolecules is reviewed.},
	number = {1999},
	journal = {Science (New York, N.Y.)},
	author = {Weiss, S},
	year = {1999},
	pages = {1676--1683},
}

@article{winkler_confocal_1999,
	title = {Confocal fluorescence coincidence analysis: an approach to ultra high-throughput screening.},
	volume = {96},
	doi = {10.1073/pnas.96.4.1375},
	abstract = {Fluorescence-based assay technologies play an increasing role in high-throughput screening. They can be classified into different categories: fluorescence polarization, time-resolved fluorescence, fluorescence resonance energy transfer, and fluorescence correlation spectroscopy. In this work we present an alternative analytical technique for high-throughput screening, which we call confocal fluorescence coincidence analysis. Confocal fluorescence coincidence analysis extracts fluorescence fluctuations that occur coincidently in two different spectral ranges from a tiny observation volume of below 1 fl. This procedure makes it possible to monitor whether an association between molecular fragments that are labeled with different fluorophores is established or broken. Therefore, it provides access to the characterization of a variety of cleavage and ligation reactions in biochemistry. Confocal fluorescence coincidence analysis is a very sensitive and ultrafast technique with readout times of 100 ms and below. This feature is demonstrated by means of a homogeneous assay for restriction endonuclease EcoRI. The presented achievements break ground for throughput rates as high as 10(6) samples per day with using only small amounts of sample substance and therefore constitute a solid base for screening applications in drug discovery and evolutionary biotechnology.},
	number = {February},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Winkler, T and Kettling, U and Koltermann, a and Eigen, M},
	year = {1999},
	pages = {1375--1378},
}

@article{kohler_active_2000,
	title = {Active protein transport through plastid tubules: velocity quantified by fluorescence correlation spectroscopy.},
	volume = {113 ( Pt 2},
	issn = {0021-9533},
	abstract = {Dynamic tubular projections emanate from plastids in certain cells of vascular plants and are especially prevalent in non-photosynthetic cells. Tubules sometimes connect two or more different plastids and can extend over long distances within a cell, observations that suggest that the tubules may function in distribution of molecules within, to and from plastids. In a new application of two-photon excitation (2PE) fluorescence correlation spectroscopy (FCS), we separated diffusion of fluorescent molecules from active transport in vivo. We quantified the velocities of diffusion versus active transport of green fluorescent protein (GFP) within plastid tubules and in the cytosol in vivo. GFP moves by 3-dimensional (3-D) diffusion both in the cytosol and plastid tubules, but diffusion in tubules is about 50 times and 100 times slower than in the cytosol and an aqueous solution, respectively. Unexpectedly larger GFP units within plastid tubules exhibited active transport with a velocity of about 0.12 microm/second. Active transport might play an important role in the long-distance distribution of large numbers of molecules within the highly viscous stroma of plastid tubules.},
	journal = {Journal of cell science},
	author = {Köhler, R H and Schwille, P and Webb, W W and Hanson, M R},
	year = {2000},
	keywords = {fluorescence correlation, spectroscopy, green fluorescent protein, plastid tubule, diffusion, active transport},
	pages = {3921--3930},
}

@article{dertinger_optics_2008,
	title = {The optics and performance of dual-focus fluorescence correlation spectroscopy.},
	volume = {16},
	issn = {1094-4087 (Electronic){\textbackslash}r1094-4087 (Linking)},
	doi = {10.1364/OE.16.014353},
	abstract = {Fluorescence correlation spectroscopy (FCS) is an important spectroscopic technique which can be used for measuring the diffusion and thus size of fluorescing molecules at pico- to nanomolar concentrations. Recently, we introduced an extension of conventional FCS, which is called dual-focus FCS (2fFCS) and allows absolute diffusion measurements with high precision and repeatability. It was shown experimentally that the method is robust against most optical and sample artefacts which are troubling conventional FCS measurements, and is furthermore able to yield absolute values of diffusion coefficients without referencing against known standards. However, a thorough theoretical treatment of the performance of 2fFCS is still missing. The present paper aims at filling this gap. Here, we have systematically studied the performance of 2fFCS with respect to the most important optical and photophysical factors such as cover slide thick-ness, refractive index of the sample, laser beam geometry, and optical satu-ration. We show that 2fFCS has indeed a superior performance when com-pared with conventional FCS, being mostly insensitive to most potential ab-errations when working under optimized conditions.},
	number = {19},
	journal = {Optics express},
	author = {Dertinger, Thomas and Loman, Anastasia and Ewers, Benjamin and Müller, Claus B and Krämer, Benedikt and Enderlein, Jörg},
	year = {2008},
	pages = {14353--14368},
}

@article{matzinger_fluorescent_1998,
	title = {Fluorescent probe solubilization in the headgroup and core regions of micelles: {Fluorescence} lifetime and orientational relaxation measurements},
	volume = {102},
	doi = {10.1021/jp981860t},
	abstract = {Experimental results demonstrate that the fluorescent probes 2- (N- hexadecylamino)-naphthalene-6-sulfonate (HANS) and 2-(N- decylamino)- naphthalene-6-sulfonate (DANS) are solubilized in two distinct regions, that is, the headgroup and core, within micelles of cetyltrimethylammoniumbromide (CTAB), tetradecyltrimethylammoniumbromide (TTAB), dodecyltrimethylammoniumbromide (DTAB), cetyltrimethylammoniumchloride (CTAC), and tetradecyltrimethylammoniumchloride (TTAC). The fluorescence lifetime decays for both chromophores are biexponential in all the different micelles. The population associated with the shorter lifetime (tau(1) congruent to 4-5 ns) is located in the Stern layer, where reduction of the fluorescence lifetime occurs because of quenching induced by the bromide counterions, The second population of chromophores is located in},
	number = {98},
	journal = {Journal of Physical Chemistry B},
	author = {Matzinger, S and Hussey, D M and Fayer, M D},
	year = {1998},
	keywords = {737-1998, ANTHRACENE, CHROMOPHORE, CTAB, ENG, FLUORESCENCE, LIFETIME, M:PRO, MICELLE, O:C, PYRENE, QUENCHING, REDUCTION, RELAXATION, S},
	pages = {7216--7224},
}

@article{hirtz_fluorescence_2012,
	title = {Fluorescence resonance energy transfer},
	url = {http://www.roempp.com},
	journal = {Römpp Chemie Lexikon Online, Version 3.29},
	author = {Hirtz, Michael and Kalinowski, Jörn},
	year = {2012},
}

@article{sengupta_measuring_2003,
	title = {Measuring size distribution in highly heterogeneous systems with fluorescence correlation spectroscopy.},
	volume = {84},
	doi = {10.1016/S0006-3495(03)75006-1},
	abstract = {Fluorescence correlation spectroscopy (FCS) is a sensitive and widely used technique for measuring diffusion. FCS data are conventionally modeled with a finite number of diffusing components and fit with a least-square fitting algorithm. This approach is inadequate for analyzing data obtained from highly heterogeneous systems. We introduce a Maximum Entropy Method based fitting routine (MEMFCS) that analyzes FCS data in terms of a quasicontinuous distribution of diffusing components, and also guarantees a maximally wide distribution that is consistent with the data. We verify that for a homogeneous specimen (green fluorescent protein in dilute aqueous solution), both MEMFCS and conventional fitting yield similar results. Further, we incorporate an appropriate goodness of fit criterion in MEMFCS. We show that for errors estimated from a large number of repeated measurements, the reduced chi(2) value in MEMFCS analysis does approach unity. We find that the theoretical prediction for errors in FCS experiments overestimates the actual error, but can be empirically modified to serve as a guide for estimating the goodness of the fit where reliable error estimates are unavailable. Finally, we compare the performance of MEMFCS with that of a conventional fitting routine for analyzing simulated data describing a highly heterogeneous distribution containing 41 diffusing species. Both methods fit the data well. However, the conventional fit fails to reproduce the essential features of the input distribution, whereas MEMFCS yields a distribution close to the actual input.},
	number = {March},
	journal = {Biophysical journal},
	author = {Sengupta, Parijat and Garai, K and Balaji, J and Periasamy, N and Maiti, S},
	year = {2003},
	pages = {1977--1984},
}

@article{myers_why_2012,
	title = {Why bioimage informatics matters},
	volume = {9},
	issn = {1548-7105 (Electronic){\textbackslash}r1548-7091 (Linking)},
	url = {http://dx.doi.org/10.1038/nmeth.2024},
	doi = {10.1038/nmeth.2024},
	abstract = {Driven by the importance of spatial and physical factors in cellular processes and the size and complexity of modern image data, computational analysis of biological imagery has become a vital emerging sub-discipline of bioinformatics and computer vision.},
	number = {7},
	journal = {Nature Methods},
	author = {Myers, Gene},
	year = {2012},
	note = {Publisher: Nature Publishing Group},
	pages = {659--660},
}

@article{bacia_dynamic_2003,
	title = {A dynamic view of cellular processes by in vivo fluorescence auto- and cross-correlation spectroscopy},
	volume = {29},
	issn = {1046-2023 (Print)},
	doi = {10.1016/S1046-2023(02)00291-8},
	abstract = {Fluorescence correlation spectroscopy (FCS) is becoming increasingly popular as a technique that aims at complementing live cell images with biophysical information. This article provides both a short overview over recent intracellular FCS applications and a practical guide for investigators, who are seeking to integrate FCS into live cell imaging to obtain information on particle mobility, local concentrations, and molecular interactions. A brief introduction to the principles of FCS is provided, particularly emphasizing practical aspects such as the choice of appropriate dyes and positioning of the measurement volume in the sample. Possibilities and limitations in extracting parameters from autocorrelation curves are discussed, and attention is drawn to potential artifacts, such as photobleaching and probe aggregation. The principle of dual-color cross-correlation is reviewed along with considerations for proper setup and adjustment. Practical implications of nonideal conditions including incomplete focus overlap and spectral cross-talk are considered. Recent examples of both auto- and cross-correlation applications demonstrate the potential of FCS for cell biology. ?? 2002 Elsevier Science (USA). All rights reserved.},
	journal = {Methods},
	author = {Bacia, Kirsten and Schwille, Petra},
	year = {2003},
	keywords = {Fluorescence correlation spectroscopy, Confocal microscopy, Fluorescence recovery after photobleaching, Fluorescence resonance energy transfer, Colocalization, In vivo, Photobleaching, Single-molecule analysis},
	pages = {74--85},
}

@article{becker_high_2001,
	title = {High count rate multichannel {TCSPC} for optical tomography},
	url = {http://www.opticsinfobase.org/viewmedia.cfm?id=117633&seq=0},
	doi = {10.1117/12.447430},
	number = {June},
	journal = {Proc. SPIE},
	author = {Becker, Wolfgang and Bergmann, Axel and Wabnitz, Heidrun},
	year = {2001},
	keywords = {170, 3660, 3830, 6960, light propagation in tissue, mammography, tomography},
	pages = {4--9},
}

@article{maiti_fluorescence_1997,
	title = {Fluorescence correlation spectroscopy: diagnostics for sparse molecules.},
	volume = {94},
	issn = {0027-8424 (Print){\textbackslash}r0027-8424 (Linking)},
	doi = {10.1073/pnas.94.22.11753},
	abstract = {The robust glow of molecular fluorescence renders even sparse molecules detectable and susceptible to analysis for concentration, mobility, chemistry, and photophysics. Correlation spectroscopy, a statistical-physics-based tool, gleans quantitative information from the spontaneously fluctuating fluorescence signals obtained from small molecular ensembles. This analytical power is available for studying molecules present at minuscule concentrations in liquid solutions (less than one nanomolar), or even on the surfaces of living cells at less than one macromolecule per square micrometer. Indeed, routines are becoming common to detect, locate, and examine individual molecules under favorable conditions.},
	number = {October},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Maiti, S and Haupts, U and Webb, W W},
	year = {1997},
	pages = {11753--11757},
}

@article{razvi_long_2013,
	title = {Long {Non}-coding {Research} {Market} {Trends}},
	author = {Razvi, Enal and Ph, D},
	year = {2013},
}

@article{noauthor_ncomms3207-s10_nodate,
	title = {ncomms3207-s10},
}

@article{ii_absorptions_nodate,
	title = {Absorptions - und {Fluoreszenzspektroskopie} {Prinzipien} ( {Quantenmechanik} )},
	author = {Ii, Biochemie},
}

@article{gould_new_2009,
	title = {New roles for endosomes: from vesicular carriers to multi-purpose platforms.},
	volume = {10},
	issn = {1471-0080 (Electronic){\textbackslash}r1471-0072 (Linking)},
	doi = {10.1038/nrm2652},
	abstract = {The careful sorting and recycling of membranes and cargo and the intracellular delivery of proteins, toxins and viruses by endocytosis are well-established roles for the endocytic apparatus, which is present in all eukaryotic cells. Recently, it has become clear that endosomes have key roles in such diverse processes as cytokinesis, polarization and migration, in which their functions might be distinct from those classically associated with endosomes. We speculate that endosomes function as multifunctional platforms on which unique sets of molecular machines are assembled to suit different cellular roles.},
	number = {April},
	journal = {Nature reviews. Molecular cell biology},
	author = {Gould, Gwyn W and Lippincott-Schwartz, Jennifer},
	year = {2009},
	pages = {287--292},
}

@article{starr_total_2001,
	title = {Total internal reflection with fluorescence correlation spectroscopy: combined surface reaction and solution diffusion.},
	volume = {80},
	issn = {0006-3495 (Print){\textbackslash}n0006-3495 (Linking)},
	doi = {10.1016/S0006-3495(01)76130-9},
	abstract = {Total internal reflection with fluorescence correlation spectroscopy (TIR-FCS) is a method for measuring the surface association/dissociation rates and absolute densities of fluorescent molecules at the interface of solution and a planar substrate. This method can also report the apparent diffusion coefficient and absolute concentration of fluorescent molecules very close to the surface. An expression for the fluorescence fluctuation autocorrelation function in the absence of contributions from diffusion through the evanescent wave, in solution, has been published previously (N. L. Thompson, T. P. Burghardt, and D. Axelrod. 1981, Biophys. J. 33:435-454). This work describes the nature of the TIR-FCS autocorrelation function when both surface association/dissociation kinetics and diffusion through the evanescent wave contribute to the fluorescence fluctuations. The fluorescence fluctuation autocorrelation function depends in general on the kinetic association and dissociation rate constants, the surface site density, the concentration of fluorescent molecules in solution, the solution diffusion coefficient, and the depth of the evanescent field. Both general and approximate expressions are presented.},
	number = {July 2000},
	journal = {Biophysical journal},
	author = {Starr, T E and Thompson, N L},
	year = {2001},
	pages = {1575--1584},
}

@article{widengren_strategies_2007,
	title = {Strategies to improve photostabilities in ultrasensitive fluorescence spectroscopy},
	volume = {111},
	issn = {1089-5639},
	doi = {10.1021/jp0646325},
	abstract = {Given the particular importance of dye photostability for single-molecule and fluorescence fluctuation spectroscopy investigations, refined strategies were explored for how to chemically retard dye photobleaching. These strategies will be useful for fluorescence correlation spectroscopy (FCS), fluorescence-based confocal single-molecule detection (SMD) and related techniques. In particular, the effects on the addition of two main categories of antifading compounds, antioxidants (n-propyl gallate, nPG, ascorbic acid, AA) and triplet state quenchers (mercaptoethylamine, MEA, cyclo-octatetraene, COT), were investigated, and the relevant rate parameters involved were determined for the dye Rhodamine 6G. Addition of each of the compound categories resulted in significant improvements in the fluorescence brightness of the monitored fluorescent molecules in FCS measurements. For antioxidants, we identify the balance between reduction of photoionized fluorophores on the one hand and that of intact fluorophores on the other as an important guideline for what concentrations to be added for optimal fluorescence generation in FCS and SMD experiments. For nPG/AA, this optimal concentration was found to be in the lower micromolar range, which is considerably less than what has previously been suggested. Also, for MEA, which is a compound known as a triplet state quencher, it is eventually its antioxidative properties and the balance between reduction of fluorophore cation radicals and that of intact fluorophores that defines the optimal added concentration. Interestingly, in this optimal concentration range the triplet state quenching is still far from sufficient to fully minimize the triplet populations. We identify photoionization as the main mechanism of photobleaching within typical transit times of fluorescent molecules through the detection volume in a confocal FCS or SMD instrument ({\textless}1-20 ms), and demonstrate its generation via both one- and multistep excitation processes. Apart from reflecting a major pathway for photobleaching, our results also suggest the exploitation of the photoinduced ionization and the subsequent reduction by antioxidants for biomolecular monitoring purposes and as a possible switching mechanism with applications in high-resolution microscopy.},
	journal = {Journal of Physical Chemistry A},
	author = {Widengren, Jerker and Chmyrov, Andriy and Eggeling, Christian and Löfdahl, P. Å and Seidel, C. a M},
	year = {2007},
	pages = {429--440},
}

@article{rossow_raster_2010,
	title = {Raster image correlation spectroscopy in live cells},
	volume = {5},
	doi = {10.1038/nprot.2010.122.Raster},
	abstract = {Raster image correlation spectroscopy (RICS) is a noninvasive technique to detect and quantify events in a live cell, including concentration of molecules and diffusion coefficients of molecules; in addition, by measuring changes in diffusion coefficients, RICS can indirectly detect binding. Any specimen containing fluorophores that can be imaged with a laser scanning microscope can be analyzed using RICS. There are other techniques to measure diffusion coefficients and binding; however, RICS fills a unique niche. It provides spatial information and can be performed in live cells using a conventional confocal microscope. It can measure a range of diffusion coefficients that is not accessible with any other single optical correlation–based technique. In this article we describe a protocol to obtain raster scanned images with an Olympus FluoView FV1000 confocal laser scanning microscope using Olympus FluoView software to acquire data and SimFCS software to perform RICS analysis. Each RICS measurement takes several minutes. The entire procedure can be completed in {\textasciitilde}2 h. This procedure includes focal volume calibration using a solution of fluorophores with a known diffusion coefficient and measurement of the diffusion coefficients of cytosolic enhanced green fluorescent protein (EGFP) and EGFP-paxillin.},
	number = {11},
	journal = {Nat Protoc},
	author = {Rossow, Molly J and Sasaki, Jennifer M and Digman, Michelle a and Gratton, Enrico},
	year = {2010},
	pages = {1761--1774},
}

@article{bundesanstalt_comparison_2008-1,
	title = {Comparison and accuracy of methods to determine the confocal volume for quantitative fluorescence correlation spectroscopy},
	volume = {232},
	number = {May},
	journal = {Society},
	author = {Bundesanstalt, Physikalisch-technische and Bundesanstalt, Physikalisch-technische},
	year = {2008},
	keywords = {fcs, confocal microscopy, confocal volume},
	pages = {343--352},
}

@article{hein_stimulated_2008,
	title = {Stimulated emission depletion ({STED}) nanoscopy of a fluorescent protein-labeled organelle inside a living cell.},
	volume = {105},
	issn = {1091-6490 (Electronic){\textbackslash}r0027-8424 (Linking)},
	doi = {10.1073/pnas.0807705105},
	abstract = {We demonstrate far-field optical imaging with subdiffraction resolution of the endoplasmic reticulum (ER) in the interior of a living mammalian cell. The diffraction barrier is overcome by applying stimulated emission depletion (STED) on a yellow fluorescent protein tag. Imaging individual structural elements of the ER revealed a focal plane (x, y) resolution of {\textless}50 nm inside the living cell, corresponding to a 4-fold improvement over that of a confocal microscope and a 16-fold reduction in the focal-spot cross-sectional area. A similar gain in resolution is realized with both pulsed- and continuous-wave laser illumination. Images of highly convoluted parts of the ER reveal a similar resolution improvement in 3D optical sectioning by a factor of 3 along the optic axis (z). Time-lapse STED recordings document morphological changes of the ER over time. Thus, nanoscale 3D imaging of organelles in the interior of living cells greatly expands the scope of light microscopy in cell biology.},
	number = {38},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Hein, Birka and Willig, Katrin I and Hell, Stefan W},
	year = {2008},
	pages = {14271--14276},
}

@article{noauthor_l_nodate,
	title = {l},
}

@article{lippincott-schwartz_putting_2009,
	title = {Putting super-resolution fluorescence microscopy to work.},
	volume = {6},
	issn = {1548-7105 (Electronic){\textbackslash}r1548-7091 (Linking)},
	doi = {10.1038/nmeth.f.233},
	abstract = {Super-resolution microscopy is poised to revolutionize our understanding of the workings of the cell. But the technology still has some limitations, and these must be taken into consideration if widespread application is to yield biological insight.},
	number = {1},
	journal = {Nature methods},
	author = {Lippincott-Schwartz, Jennifer and Manley, Suliana},
	year = {2009},
	pages = {21--23},
}

@article{eggeling_monitoring_1998-1,
	title = {Monitoring conformational dynamics of a single molecule by selective fluorescence spectroscopy.},
	volume = {95},
	doi = {10.1073/pnas.95.4.1556},
	abstract = {A recently developed, real-time spectroscopic technique, burst-integrated fluorescence lifetime (BIFL), is shown to be well suited for monitoring the individual molecular conformational dynamics of a single molecule diffusing through the microscopic, open measurement volume (approximately 10 fl) of a confocal epi-illuminated set-up. In a highly diluted aqueous solution of 20-mer oligonucleotide strand of DNA duplex labeled with the environment-sensitive fluorescent dye tetramethylrhodamine (TMR), fluorescence bursts indicating traces of individual molecules are registered and further subjected to selective burst analysis. The two-dimensional BIFL data allow the identification and detection of different temporally resolved conformational states. A complementary autocorrelation analysis was performed on the time-dependent fluctuations in fluorescence lifetime and intensity. The consistent results strongly support the hypothesized three-state model of the conformational dynamics of the TMR-DNA duplex with a polar, a nonpolar, and a quenching environment of TMR.},
	number = {February},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Eggeling, C and Fries, J R and Brand, L and Günther, R and Seidel, C a},
	year = {1998},
	pages = {1556--1561},
}

@article{harpur_imaging_2001,
	title = {Imaging {FRET} between spectrally similar {GFP} molecules in single cells.},
	volume = {19},
	issn = {1087-0156 (Print){\textbackslash}r1087-0156 (Linking)},
	doi = {10.1038/84443},
	abstract = {Fluorescence resonance energy transfer (FRET) detection in fusion constructs consisting of green fluorescent protein (GFP) variants linked by a sequence that changes conformation upon modification by enzymes or binding of ligands has enabled detection of physiological processes such as Ca(2+) ion release, and protease and kinase activity. Current FRET microscopy techniques are limited to the use of spectrally distinct GFPs such as blue or cyan donors in combination with green or yellow acceptors. The blue or cyan GFPs have the disadvantages of less brightness and of autofluorescence. Here a FRET imaging method is presented that circumvents the need for spectral separation of the GFPs by determination of the fluorescence lifetime of the combined donor/acceptor emission by fluorescence lifetime imaging microscopy (FLIM). This technique gives a sensitive, reproducible, and intrinsically calibrated FRET measurement that can be used with the spectrally similar and bright yellow and green fluorescent proteins (EYFP/EGFP), a pair previously unusable for FRET applications. We demonstrate the benefits of this approach in the analysis of single-cell signaling by monitoring caspase activity in individual cells during apoptosis.},
	number = {February},
	journal = {Nature biotechnology},
	author = {Harpur, a G and Wouters, F S and Bastiaens, P I},
	year = {2001},
	pages = {167--169},
}

@article{noauthor_tirf-fcs_nodate,
	title = {{TIRF}-{FCS}},
}

@article{weyermann_physicochemical_2004,
	title = {Physicochemical characterisation of cationic polybutylcyanoacrylate-nanoparticles by fluorescence correlation spectroscopy.},
	volume = {58},
	doi = {10.1016/j.ejpb.2004.02.011},
	abstract = {The aim of this study was to compare different physical and chemical methods with fluorescence correlation spectroscopy (FCS) in order to characterise cationic acrylate nanoparticles (NP), which can deliver oligonucleotides (ON) into mammalian cells. These positively charged nanoparticles were prepared from diethylaminoethyl dextran (DEAE-dextran) and poly(n-butyl-2-cyanoacrylate) (PBCA). NP consists of PBCA oligochains with an average size of PBCA 9 mer and were formed by entrapping DEAE-dextran and dextran 70,000 in high amounts into the particle matrix. The oligochain length of PBCA was investigated by mass-spectroscopy (MALDI TOF). The molecular weight of a particle with d = 108 nm was estimated to be approximately 3.6 x 10(8) Da. The mean size of the nanoparticles were in a range of dh = 130-140 nm, as determined independently by FCS and dynamic light scattering. Atomic force microscopy and scanning electron microscopy images confirm this size range. Furthermore, the particle mass of the PBCA-NP was estimated by FCS measurements. For this approach two new methods for fluorescence labelling of cationic particles were developed. Fluorescent labelled dextran 70,000 was entrapped into the particle matrix; in addition, the derivatisation of hydroxyl groups of the NP was achieved with 5-([4,6-dichlorotriazin-2-yl]amino) fluorescein (DTAF). ON can be localised in a complex with the NP by dual-colour fluorescence cross correlation spectroscopy measurements. The zetapotential of the unloaded NP was positively charged with about +39 mV and decreased down to -40 mV on addition of excess ON. After centrifugation quantification of the ON loading onto the particles by strong anion exchange high performance liquid chromatography (SAX HPLC) and FCS showed that approximately 20 microg ON per 100 g NP was adsorbed. The FCS measurements of the ON adsorption in situ was found to be much higher with approximately 95 microg ON per 100 g NP.},
	journal = {European journal of pharmaceutics and biopharmaceutics : official journal of Arbeitsgemeinschaft fur Pharmazeutische Verfahrenstechnik e.V},
	author = {Weyermann, Jörg and Lochmann, Dirk and Georgens, Christiane and Rais, Isam and Kreuter, Jörg and Karas, Michael and Wolkenhauer, Markus and Zimmer, Andreas},
	year = {2004},
	keywords = {fluorescence correlation spectroscopy, acrylate, deae-dextran, dna, drug delivery, n -butyl-2-cyanoacrylate, nanoparticles, oligonucleotides, phosphorothioate, poly, system},
	pages = {25--35},
}

@article{boens_fluorescence_2007,
	title = {Fluorescence lifetime standards for time and frequency domain fluorescence spectroscopy.},
	volume = {79},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/17269654},
	doi = {10.1021/ac062160k},
	abstract = {A series of fluorophores with single-exponential fluorescence decays in liquid solution at 20 degrees C were measured independently by nine laboratories using single-photon timing and multifrequency phase and modulation fluorometry instruments with lasers as excitation source. The dyes that can serve as fluorescence lifetime standards for time-domain and frequency-domain measurements are all commercially available, are photostable under the conditions of the measurements, and are soluble in solvents of spectroscopic quality (methanol, cyclohexane, water). These lifetime standards are anthracene, 9-cyanoanthracene, 9,10-diphenylanthracene, N-methylcarbazole, coumarin 153, erythrosin B, N-acetyl-l-tryptophanamide, 1,4-bis(5-phenyloxazol-2-yl)benzene, 2,5-diphenyloxazole, rhodamine B, rubrene, N-(3-sulfopropyl)acridinium, and 1,4-diphenylbenzene. At 20 degrees C, the fluorescence lifetimes vary from 89 ps to 31.2 ns, depending on fluorescent dye and solvent, which is a useful range for modern pico- and nanosecond time-domain or mega- to gigahertz frequency-domain instrumentation. The decay times are independent of the excitation and emission wavelengths. Dependent on the structure of the dye and the solvent, the excitation wavelengths used range from 284 to 575 nm, the emission from 330 to 630 nm. These lifetime standards may be used to either calibrate or test the resolution of time- and frequency-domain instrumentation or as reference compounds to eliminate the color effect in photomultiplier tubes. Statistical analyses by means of two-sample charts indicate that there is no laboratory bias in the lifetime determinations. Moreover, statistical tests show that there is an excellent correlation between the lifetimes estimated by the time-domain and frequency-domain fluorometries. Comprehensive tables compiling the results for 20 (fluorescence lifetime standard/solvent) combinations are given.},
	number = {5},
	journal = {Analytical chemistry},
	author = {Boens, Noël Noel and Qin, Wenwu and Basarić, Nikola and Hofkens, Johan and Ameloot, Marcel and Pouget, Jacques and Lefèvre, Jean-Pierre and Valeur, Bernard and Gratton, Enrico and vandeVen, Martin and Silva, Norberto D and Engelborghs, Yves and Willaert, Katrien and Sillen, Alain and Rumbles, Garry and Phillips, David and Visser, Antonie J W G and van Hoek, Arie and Lakowicz, Joseph R and Malak, Henryk and Gryczynski, Ignacy and Szabo, Arthur G and Krajcarski, Don T and Tamai, Naoto and Miura, Atsushi and Basaric, Nikola and Hasselt, Universiteit and D, Building and Lefe, Jean-pierre and Cedex, Cachan and Building, I I and Hoek, Arie Van and Basari, Nikola and Wilson, Président},
	year = {2007},
	keywords = {Fluorescence, Fluorescence: standards, Fluorescent Dyes, Fluorescent Dyes: chemistry, Luminescent Measurements, Luminescent Measurements: standards, Solvents, Solvents: chemistry, Spectrometry, Time Factors},
	pages = {2137--2149},
}

@article{weiss_anomalous_2003,
	title = {Anomalous protein diffusion in living cells as seen by fluorescence correlation spectroscopy.},
	volume = {84},
	issn = {0006-3495 (Print)},
	doi = {10.1016/S0006-3495(03)75130-3},
	abstract = {We investigate the challenges and limitations that are encountered when studying membrane protein dynamics in vivo by means of fluorescence correlation spectroscopy (FCS). Based on theoretical arguments and computer simulations, we show that, in general, the fluctuating fluorescence has a fractal dimension D(0) {\textgreater}or= 1.5, which is determined by the anomality alpha of the diffusional motion of the labeled particles, i.e., by the growth of their mean square displacement as (Deltax)(2) approximately t(alpha). The fractality enforces an initial power-law behavior of the autocorrelation function and related quantities for small times. Using this information, we show by FCS that Golgi resident membrane proteins move subdiffusively in the endoplasmic reticulum and the Golgi apparatus in vivo. Based on Monte Carlo simulations for FCS on curved surfaces, we can rule out that the observed anomalous diffusion is a result of the complex topology of the membrane. The apparent mobility of particles as determined by FCS, however, is shown to depend crucially on the shape of the membrane and its motion in time. Due to this fact, the hydrodynamic radius of the tracked particles can be easily overestimated by an order of magnitude.},
	number = {June},
	journal = {Biophysical journal},
	author = {Weiss, Matthias and Hashimoto, Hitoshi and Nilsson, Tommy},
	year = {2003},
	pages = {4043--4052},
}

@article{jakobs_egfp_2000,
	title = {{EGFP} and {DsRed} expressing cultures of {Escherichia} coli imaged by confocal, two-photon and fluorescence lifetime microscopy},
	volume = {479},
	issn = {0014-5793 (Print)},
	doi = {10.1016/S0014-5793(00)01896-2},
	abstract = {The green fluorescent protein (GFP) has become an invaluable marker for monitoring protein localization and gene expression in vivo. Recently a new red fluorescent protein (drFP583 or DsRed), isolated from tropical corals, has been described [Matz, M.V. et al. (1999) Nature Biotech. 17, 969-973]. With emission maxima at 509 and 583 nm respectively, EGFP and DsRed are suited for almost crossover free dual color labeling upon simultaneous excitation. We imaged mixed populations of Escherichia coli expressing either EGFP or DsRed by one-photon confocal and by two-photon microscopy. Both excitation modes proved to be suitable for imaging cells expressing either of the fluorescent proteins. DsRed had an extended maturation time and E. coli expressing this fluorescent protein were significantly smaller than those expressing EGFP. In aging bacterial cultures DsRed appeared to aggregate within the cells, accompanied by a strong reduction in its fluorescence lifetime as determined by fluorescence lifetime imaging microscopy. Copyright (C) 2000 Federation of European Biochemical Societies.},
	journal = {FEBS Letters},
	author = {Jakobs, Stefan and Subramaniam, Vinod and Schönle, Andreas and Jovin, Thomas M. and Hell, Stefan W.},
	year = {2000},
	keywords = {Two-photon, Confocal, Escherichia coli, Fluorescence lifetime microscopy, Green fluorescent protein, Red fluorescent protein},
	pages = {131--135},
}

@article{becker_fluorescence_nodate,
	title = {Fluorescence {Lifetime} {Images} and {Correlation} {Spectra} {Obtained} by {Multi}-{Dimensional} {TCSPC} {Advanced} {Features} of {Time}-{Correlated} {Single} {Photon} {Counting} applied to {Laser} {Scanning} {Fluctuation} of the fluorescence intensity and lifetime delivers additional infor},
	author = {Becker, Wolfgang and Bergmann, Axel},
}

@article{digman_phasor_2008-1,
	title = {The phasor approach to fluorescence lifetime imaging analysis-{SI}},
	volume = {94},
	number = {1},
	journal = {Biophysical journal},
	author = {Digman, Michelle a and Caiolfa, Valeria R and Zamai, Moreno and Gratton, Enrico},
	year = {2008},
	pages = {1--3},
}

@article{noauthor_gadella_95apdf_nodate,
	title = {gadella\_95a.pdf},
}

@article{wallrabe_microscopy_2014,
	title = {Microscopy {Core} {Facilities} : {Results} of an {International} {Survey}},
	doi = {10.1017/S1551929514000091},
	author = {Wallrabe, Horst and Periasamy, Ammasi and Elangovan, Masilamani},
	year = {2014},
}

@article{benda_tcspc_2005,
	title = {{TCSPC} upgrade of a confocal {FCS} microscope},
	volume = {76},
	issn = {00346748},
	doi = {10.1063/1.1866814},
	abstract = {We extended the measurement capabilities of the Carl Zeiss ConfoCor 1 FCS microscope by (a) using pulsed picosecond diode lasers instead of a continuous wave (CW) laser excitation, (b) introducing a fast single photon avalanche diode detector, and (c) exploiting the capabilities of the PicoQuant TimeHarp 200 board. When the time-tagged time-resolved (TTTR) mode of the TimeHarp is utilized, the complete fluorescence dynamics are recorded. That is, the time-evolution of the fluctuations and the fluorescence decay kinetics are captured simultaneously. Recording individual photon events (without on-the-fly data reduction like in hardware correlators) preserves the full information content of the measurement for virtually unlimited data analysis tasks and provides a much more detailed view of processes happening in the detection volume. For example, autocorrelation functions of dyes in a mixture can be separated and/or their cross-correlation can be investigated. These virtual two-channel measurements are performed utilizing a single detection channel setup. The time-resolved FCS is a powerful tool in biological studies and is demonstrated here on unilamellar vesicles giving clear evidence for Bodipy dye exchange between them. The described upgrade scenario is applicable to other confocal microscopes as well. In principle, any FCS system so far utilizing conventional CW lasers can benefit from pulsed excitation and the original functionality of the setup is fully preserved. (C) 2005 American Institute of Physics.},
	journal = {Review of Scientific Instruments},
	author = {Benda, Aleš and Hof, Martin and Wahl, Michael and Patting, Matthias and Erdmann, Rainer and Kapusta, Peter},
	year = {2005},
	pages = {1--4},
}

@article{noauthor_email-ca_nodate,
	title = {email-ca},
}

@article{wiedenmann_eosfp_2004,
	title = {{EosFP}, a fluorescent marker protein with {UV}-inducible green-to-red fluorescence conversion.},
	volume = {101},
	issn = {0027-8424},
	doi = {10.1073/pnas.0403668101},
	abstract = {A gene encoding a fluorescent protein from the stony coral Lobophyllia hemprichii has been cloned in Escherichia coli and characterized by biochemical and biophysical methods. The protein, which we named EosFP, emits strong green fluorescence (516 nm) that changes to red (581 nm) upon near-UV irradiation at approximately 390 nm because of a photo-induced modification involving a break in the peptide backbone next to the chromophore. Single-molecule fluorescence spectroscopy shows that the wild type of EosFP is tetrameric, with strong Forster resonance coupling among the individual fluorophores. We succeeded in breaking up the tetramer into AB and AC subunit dimers by introducing the single point mutations V123T and T158H, respectively, and the combination of both mutations yielded functional monomers. Fusion constructs with a variety of proteins were prepared and expressed in human cells, showing that normal biological functions were retained. The possibility to locally change the emission wavelength by focused UV light makes EosFP a superb marker for experiments aimed at tracking the movements of biomolecules within the living cell.},
	number = {45},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Wiedenmann, Jörg and Ivanchenko, Sergey and Oswald, Franz and Schmitt, Florian and Röcker, Carlheinz and Salih, Anya and Spindler, Klaus-Dieter and Nienhaus, G Ulrich},
	year = {2004},
	pages = {15905--15910},
}

@article{hanley_fluorescence_2001,
	title = {Fluorescence {Lifetime} {Imaging}: {Multi}-point {Calibration}, {Minimum} {Resolvable} {Differences}, and {Artifact} {Suppression}},
	volume = {43},
	issn = {0196-4763},
	doi = {10.1002/1097-0320(20010401)43:4<248::AID-CYTO1057>3.0.CO;2-Y},
	abstract = {BACKGROUND: Frequency-domain fluorescence lifetime imaging microscopy (FLIM) is finding increasing use in the analysis of biological systems. However, the calibration, determination of resolvable lifetime differences, and evaluation of artifacts have not been extensively treated. We describe a multi-point method for calibrating a frequency-domain FLIM system, characterize the minimum detectable heterogeneity and intra- and inter-image lifetime differences, discuss the statistical treatment of FLIM data, and suggest methods for minimizing artifacts. METHODS: A set of solutions exhibiting single-component lifetimes suffice for accurately calibrating a reference material with a single-component lifetime, even in the absence of accurate data on the lifetimes of the individual solutions or the reference material. We used a set of rhodamine 6G solutions quenched with varying concentrations of iodide, leading to lifetimes of 0.5--4.0 ns, to calibrate a 1 microM reference solution of rhodamine 6G in water. RESULTS: We measured a value of 4.11 ns with an estimated absolute error of +/-0.05 ns for the rhodamine 6G reference solution. With 57.7 MHz modulation, the minimum detectable inter-image lifetime difference was 0.1--0.15 ns and the minimum detectable intra-image lifetime difference was 4--5 ps, allowing solutions differing in lifetime by 40 and 70 ps to be easily distinguished. The minimum detectable lifetime heterogeneity was 50--80 ps. Evaluation of replicate measurements of single solutions demonstrated that inter-image instrument errors exceeded those predicted from intra-image statistics by more than an order of magnitude. We also measured lifetimes and heterogeneity in 4 GFP variants (WTGFP, EGFP, S65T, and EYFP) with the technique. CONCLUSION: The multi-point calibration method is applicable to any system consisting of single-component lifetimes. Applying the method in our FLIM microscope allowed us to demonstrate a previously unreported degree of lifetime resolution in a FLIM microscope. Cytometry 43:248-260;2001.},
	journal = {Cytometry},
	author = {Hanley, Quentin S. and Subramaniam, Vinod and Arndt-Jovin, Donna J. and Jovin, Thomas M.},
	year = {2001},
	keywords = {FLIM, Green fluorescent protein (GFP), Iodide quenching, Lifetime standards: Rhodamine 6G},
	pages = {248--260},
}

@article{ganesan_dark_2006,
	title = {A dark yellow fluorescent protein ({YFP})-based {Resonance} {Energy}-{Accepting} {Chromoprotein} ({REACh}) for {Förster} resonance energy transfer with {GFP}.},
	volume = {103},
	issn = {0027-8424},
	doi = {10.1073/pnas.0509922103},
	abstract = {Förster resonance energy transfer (FRET) microscopy is a powerful technique that enables the visualization of signaling intermediates, protein interactions, and protein conformational and biochemical status. With the availability of an ever-increasing collection of fluorescent proteins, pairs of spectrally different variants have been used for the study of FRET in living cells. However, suitable spectral overlap, necessary for efficient FRET, is limited by the requirement for proper emission separation. Currently used FRET pairs represent compromises between these opposing spectral demands that reduce the maximally attainable FRET sensitivity. We present a previously undescribed FRET acceptor, a nonfluorescent yellow fluorescent protein (YFP) mutant called REACh (for Resonance Energy-Accepting Chromoprotein). REACh allows the use of the photophysically superior FRET donor EGFP, with which it exhibits optimal spectral overlap, which obviates the need for narrow spectral filtering and allows additional fluorescent labels to be used within the same cell. The latter allows the generation of sophisticated bioassays for complex biological questions. We show that this dark acceptor is ideally suited for donor fluorescence lifetime imaging microscopy (FLIM) and confirm these measurements with an independent intensity-based donor fluorescence quenching resonance energy transfer (FqRET) assay. REACh also can be used in donor photobleaching kinetics-based FRET studies. By detecting FRET between a GFP-tagged ubiquitination substrate and REACh-labeled ubiquitin, we imaged the active ubiquitination machinery inside cells. This assay therefore can be used to study proteins whose function is regulated by ubiquitination.},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Ganesan, Sundar and Ameer-Beg, Simon M and Ng, Tony T C and Vojnovic, Borivoj and Wouters, Fred S},
	year = {2006},
	pages = {4089--4094},
}

@article{noauthor_jcs_117_55xx_figs1_nodate,
	title = {{JCS}\_117\_55xx\_figS1},
}

@article{murata_fluorescence_2000-1,
	title = {Fluorescence lifetime imaging of nuclear {DNA}: effect of fluorescence resonance energy transfer.},
	volume = {41},
	doi = {10.1002/1097-0320(20001101)41:3<178::AID-CYTO4>3.0.CO;2-N},
	abstract = {BACKGROUND: DNA fluorescence dyes have been used to study DNA dynamics, chromatin structure, and cell cycle analysis. However, most microscopic fluorescence studies of DNA use only steady-state measurements and do not take advantage of the additional information content of the time-resolved fluorescence. In this paper, we combine fluorescence imaging of DNA with time-resolved measurements to examine the proximity of donors and acceptors bound to chromatin. METHODS: We used frequency-domain fluorescence lifetime imaging microscopy to study the spatial distribution of DNA-bound donors and acceptors in fixed 3T3 nuclei. Over 50 cell nuclei were imaged in the presence of an AT-specific donor, Hoechst 33258 (Ho), and a GC-specific acceptor, 7-aminoactinomycin D (7-AAD). RESULTS: The intensity images of Ho alone showed a spatially irregular distribution due to the various concentrations of DNA or AT-rich DNA throughout the nuclei. The lifetime imaging of the Ho-stained nuclei was typically flat. Addition of 7-AAD decreased the fluorescence intensity and lifetime of the Ho-stained DNA. The spatially dependent phase and modulation values of Ho in the presence of 7-AAD showed that the Ho decay becomes nonexponential, as is expected for a resonance energy transfer (RET) with multiple acceptors located over a range of distances. In approximately 40 nuclei, the intensity and lifetime decrease was spatially homogeneous. In approximately 10 nuclei, addition of 7-AAD resulted in a spatially nonhomogeneous decrease in intensity and lifetime. The RET efficiency was higher in G(2)/M than in G(0/1) phase cells. CONCLUSIONS: Because RET efficiency depends on the average distance between Ho and 7-AAD, data suggest that the heterogeneity of lifetimes and spatial variation of the RET efficiency are caused by the presence of highly condensed regions of DNA in nuclei.},
	journal = {Cytometry},
	author = {Murata, S and Herman, P and Lin, H J and Lakowicz, J R},
	year = {2000},
	pages = {178--185},
}

@article{bub_temporal_2010,
	title = {Temporal pixel multiplexing for simultaneous high-speed, high-resolution imaging.},
	volume = {7},
	issn = {1548-7091},
	url = {http://dx.doi.org/10.1038/nmeth.1429},
	doi = {10.1038/nmeth.1429},
	abstract = {We introduce an imaging modality that, by offsetting pixel-exposure times during capture of a single image frame, embeds temporal information in each frame. This allows simultaneous acquisition of full-resolution images at native detector frame rates and high-speed image sequences at reduced resolution, without increasing bandwidth requirements. We demonstrate this method using macroscopic and microscopic examples, including imaging calcium transients in heart cells at 250 Hz using a 10-Hz megapixel camera.},
	number = {October 2009},
	journal = {Nature methods},
	author = {Bub, Gil and Tecza, Matthias and Helmes, Michiel and Lee, Peter and Kohl, Peter},
	year = {2010},
	note = {Publisher: Nature Publishing Group},
	pages = {209--211},
}

@article{noauthor_calibration_cc_nodate,
	title = {calibration\_cc},
}

@article{eggeling_rapid_2005,
	title = {Rapid analysis of {Forster} resonance energy transfer by two-color global fluorescence correlation spectroscopy: trypsin proteinase reaction.},
	volume = {89},
	issn = {4955120113},
	doi = {10.1529/biophysj.104.052753},
	abstract = {In this study we introduce the combination of two-color global fluorescence correlation spectroscopy (2CG-FCS) and Förster resonance energy transfer (FRET) as a very powerful combination for monitoring biochemical reactions on the basis of single molecule events. 2CG-FCS, which is a new variation emerging from the family of fluorescence correlation spectroscopy, globally analyzes the simultaneously recorded auto- and cross-correlation data from two photon detectors monitoring the fluorescence emission of different colors. Overcoming the limitations inherent in mere auto- and cross-correlation analysis, 2CG-FCS is sensitive in resolving and quantifying fluorescent species that differ in their diffusion characteristics and/or their molecular brightness either in one or both detection channels. It is able to account for effects that have often been considered as sources of severe artifacts in two-color and FRET measurements, the most prominent artifacts comprising photobleaching, cross talk, or concentration variations in sample preparation. Because of its very high statistical accuracy, the combination of FRET and 2CG-FCS is suited for high-throughput applications such as drug screening. Employing beam scanning during data acquisition even further enhances this capability and allows measurement times of {\textless}2 s. The improved performance in monitoring a FRET sample was verified by following the protease cleavage reaction of a FRET-active peptide. The FRET-inactive subpopulation of uncleaved substrate could be correctly assigned, revealing a substantial portion of inactive or missing acceptor label. The results were compared to those obtained by two-dimensional fluorescence intensity distribution analysis.},
	number = {July},
	journal = {Biophysical journal},
	author = {Eggeling, Christian and Kask, Peet and Winkler, Dirk and Jäger, Stefan},
	year = {2005},
	pages = {605--618},
}

@article{noauthor_no_nodate,
	title = {No {Title}},
}

@article{humpolickova_probing_2006,
	title = {Probing diffusion laws within cellular membranes by {Z}-scan fluorescence correlation spectroscopy.},
	volume = {91},
	issn = {0006-3495 (Print){\textbackslash}n0006-3495 (Linking)},
	doi = {10.1529/biophysj.106.089474},
	abstract = {The plasma membrane of various mammalian cell types is heterogeneous in structure and may contain microdomains, which can impose constraints on the lateral diffusion of its constituents. Fluorescence correlation spectroscopy (FCS) can be used to investigate the dynamic properties of the plasma membrane of living cells. Very recently, Wawrezinieck et al. (Wawrezinieck, L., H. Rigneault, D. Marguet, and P. F. Lenne. 2005. Biophys. J. 89:4029-4042) described a method to probe the nature of the lateral microheterogeneities of the membrane by varying the beam size in the FCS instrument. The dependence of the width of the autocorrelation function at half-maximum, i.e., the diffusion time, on the transverse area of the confocal volume gives information on the nature of the imposed confinement. We describe an alternative approach that yields essentially the same information, and can readily be applied on commercial FCS instruments by measuring the diffusion time and the particle number at various relative positions of the cell membrane with respect to the waist of the laser beam, i.e., by performing a Z-scan.},
	journal = {Biophysical journal},
	author = {Humpolícková, Jana and Gielen, Ellen and Benda, Ales and Fagulova, Veronika and Vercammen, Jo and Vandeven, Martin and Hof, Martin and Ameloot, Marcel and Engelborghs, Yves},
	year = {2006},
	pages = {L23--L25},
}

@article{ng_ezrin_2001,
	title = {Ezrin is a downstream effector of traf ® cking {PKC} ± integrin complexes involved in the control of cell motility},
	volume = {20},
	number = {11},
	author = {Ng, Tony and Parsons, Maddy and Hughes, William E and Monypenny, James and Zicha, Daniel and Gautreau, Alexis and Arpin, Monique and Gschmeissner, Steve and Verveer, Peter J and Bastiaens, Philippe I H and Parker, Peter J},
	year = {2001},
}

@article{shaner_guide_2005,
	title = {A guide to choosing fluorescent proteins.},
	volume = {2},
	issn = {1548-7091 (Print){\textbackslash}n1548-7091 (Linking)},
	doi = {10.1038/nmeth819},
	abstract = {The recent explosion in the diversity of available fluorescent proteins (FPs) promises a wide variety of new tools for biological imaging. With no unified standard for assessing these tools, however, a researcher is faced with difficult questions. Which FPs are best for general use? Which are the brightest? What additional factors determine which are best for a given experiment? Although in many cases, a trial-and-error approach may still be necessary in determining the answers to these questions, a unified characterization of the best available FPs provides a useful guide in narrowing down the options.},
	number = {12},
	journal = {Nature methods},
	author = {Shaner, Nathan C and Steinbach, Paul a and Tsien, Roger Y},
	year = {2005},
	pages = {905--909},
}

@article{boukari_stability_2003,
	title = {Stability of drug-induced tubulin rings by fluorescence correlation spectroscopy},
	volume = {42},
	issn = {0006-2960},
	doi = {10.1021/bi026751q},
	abstract = {Fluorescence correlation spectroscopy (FCS) was applied to investigate the stability of tubulin rings that result from the interaction of alpha beta-tubulin dimers with three vinca domain-binding peptides--cryptophycin 1, hemiasterlin, and dolastatin 10. These peptides inhibit tubulin polymerization into microtubules and, instead, induce the formation of single-walled tubulin rings of 23.8 nm mean diameter for cryptophycin and 44.6 nm mean diameter for hemiasterlin and dolastatin, as revealed by electron microscopy on micromolar drug-tubulin samples. However, the hydrodynamic diameter and the apparent number of fluorescent particles, determined from analysis of FCS measurements obtained from nanomolar drug-tubulin samples, indicate variation in the stability of the rings depending on the drug and the tubulin concentration. Cryptophycin-tubulin rings appear to be the most stable even with tubulin concentration as low as 1 nM, whereas hemiasterlin-tubulin rings are the least, depolymerizing even at relatively high concentrations (100 nM). In contrast, the dolastatin-tubulin rings demonstrate an intermediate level of stability, depolymerizing significantly only at tubulin concentrations below 10 nM. We also compare the stability results with those of cytotoxicity measurements taken on several cell lines and note a rough correlation between the cytotoxicity of the drugs in cell cultures and the stability of the corresponding drug-induced rings.},
	journal = {Biochemistry},
	author = {Boukari, Hacène and Nossal, Ralph and Sackett, Dan L.},
	year = {2003},
	pages = {1292--1300},
}

@article{gromova_visualizing_2007,
	title = {Visualizing {Smad1}/4 signaling response to bone morphogenetic {Protein}-4 activation by {FRET} biosensors},
	volume = {1773},
	issn = {0006-3002 (Print)},
	doi = {10.1016/j.bbamcr.2007.09.007},
	abstract = {Smad proteins are the major signal transducers for the Transforming Growth Factor superfamily of cytokines and their serine/threonine kinase receptors. Smads mediate the signal from the membrane into the nucleus. Bone Morphogenetic Protein-4 stimulates phosphorylation of Smad1, which interacts with Smad4. This complex translocates into the nucleus and regulates transcription of target genes. Here, we report our development of cellular fluorescence biosensors for direct visualization of Smad signaling in live mammalian cells. Fluorescence resonance energy transfer between cyan and yellow fluorescent proteins fused to the Smad1 and Smad4 proteins was used to unravel the temporal aspects of BMP/Smad signaling. A rate-limiting delay of 2-5??min occurred between BMP activation and Smad1 activity. A similar delay was observed in the Smad1/Smad4 complexation. Further experimentation indicated that the delay is dependent on the MH1 domain and linker of Smad1. These results give new insights into the dynamics of the BMP receptor -Smad1/4 signaling process and provide a new tool for studying Smads. ?? 2007 Elsevier B.V. All rights reserved.},
	journal = {Biochimica et Biophysica Acta - Molecular Cell Research},
	author = {Gromova, Kira V. and Friedrich, Mike and Noskov, Andrey and Harms, Gregory S.},
	year = {2007},
	keywords = {FRET, GFP, Biosensor, BMP, Smad},
	pages = {1759--1773},
}

@article{rigler_specific_1999,
	title = {Specific binding of proinsulin {C}-peptide to human cell membranes.},
	volume = {96},
	issn = {0027-8424 (Print){\textbackslash}r0027-8424 (Linking)},
	doi = {10.1073/pnas.96.23.13318},
	abstract = {Recent reports have demonstrated beneficial effects of proinsulin C-peptide in the diabetic state, including improvements of kidney and nerve function. To examine the background to these effects, C-peptide binding to cell membranes has been studied by using fluorescence correlation spectroscopy. Measurements of ligand-membrane interactions at single-molecule detection sensitivity in 0.2-fl confocal volume elements show specific binding of fluorescently labeled C-peptide to several human cell types. Full saturation of the C-peptide binding to the cell surface is obtained at low nanomolar concentrations. Scatchard analysis of binding to renal tubular cells indicates the existence of a high-affinity binding process with K(ass) {\textgreater} 3.3 x 10(9) M(-1). Addition of excess unlabeled C-peptide is accompanied by competitive displacement, yielding a dissociation rate constant of 4.5 x 10(-4) s(-1). The C-terminal pentapeptide also displaces C-peptide bound to cell membranes, indicating that the binding occurs at this segment of the ligand. Nonnative D-C-peptide and a randomly scrambled C-peptide do not compete for binding with the labeled C-peptide, nor were crossreactions observed with insulin, insulin-like growth factor (IGF)-I, IGF-II, or proinsulin. Pretreatment of cells with pertussis toxin, known to modify receptor-coupled G proteins, abolishes the binding. It is concluded that C-peptide binds to specific G protein-coupled receptors on human cell membranes, thus providing a molecular basis for its biological effects.},
	number = {23},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Rigler, R and Pramanik, a and Jonasson, P and Kratz, G and Jansson, O T and Nygren, P and Stâhl, S and Ekberg, K and Johansson, B and Uhlén, S and Uhlén, M and Jörnvall, H and Wahren, J},
	year = {1999},
	pages = {13318--13323},
}

@article{mayor_for_2001,
	title = {For the article ‘‘{Protein} folding and unfolding in microseconds to nanoseconds by experiment and simulation’’ by {Ugo} {Mayor}, {Christopher} {M}. {Johnson}, {Valerie} {Daggett}, and {Alan} {R}. {Fersht}, which appeared in number 25, {December} 5, 2000, of},
	volume = {98},
	number = {25},
	author = {Mayor, Ugo and Johnson, Christopher M and Daggett, Valerie and Herrlinger, Ulrich and Ourednik, Vaclav and Black, Peter Mcl},
	year = {2001},
	pages = {2001--2001},
}

@article{doose_single_2003,
	title = {Single molecule characterization of photophysical and colloidal properties of biocompatible quantum dots},
	author = {Doose, Soren},
	year = {2003},
}

@article{state_pmh-100_nodate,
	title = {{PMH}-100 {Internal} {GHz} {Preamplifier} : {High} {Output} {Amplitude} {PMH}-100},
	author = {State, Steady and Counting, Gated Photon},
}

@article{xu_fluorescence_2005,
	title = {The fluorescence resonance energy transfer ({FRET}) gate: a time-resolved study.},
	volume = {102},
	doi = {10.1073/pnas.0408568102},
	abstract = {The two-step energy-transfer process in a self-assembled complex comprising a cationic conjugated polymer (CCP) and a dsDNA is investigated by using pump-dump-emission spectroscopy and time-correlated single-photon counting; energy is transferred from the CCP to an ethidium bromide (EB) molecule intercalated into the dsDNA through a fluorescein molecule linked to one terminus of the DNA. Time-dependent anisotropy measurements indicate that the inefficient direct energy transfer from the CCP to the intercalated EB results from the near orthogonality of their transition moments. These measurements also show that the transition moment of the fluorescein spans a range of angular distributions and lies between that of the CCP and EB. Consequently, the fluorescein acts as a fluorescence resonance energy-transfer gate to relay the excitation energy from the CCP to the EB.},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Xu, Qing-Hua and Wang, Shu and Korystov, Dmitry and Mikhailovsky, Alexander and Bazan, Guillermo C and Moses, Daniel and Heeger, Alan J},
	year = {2005},
	pages = {530--535},
}

@article{gmbh_lifetime_nodate,
	title = {Lifetime {Upgrade} {Kit} for {Leica} {TCS} {SP2} {FLIM} {Upgrade} {Kit} - {Basis} {Version}},
	number = {800},
	author = {Gmbh, Hickl and Corp, Boston Electronics},
}

@article{foldes-papp_new_2002,
	title = {A new dimension for the development of fluorescence-based assays in solution: from physical principles of {FCS} detection to biological applications.},
	volume = {227},
	abstract = {Ultrasensitive detection methods such as laser-induced fluorescence represent the current state-of-the-art in analytics. Single-molecule detection in solution has received a remarkable amount of attention in the last few years because of its applicability to life sciences. Studies have been performed on the fundamentals of the detection processes themselves and on some biological systems. Fluorescence correlation spectroscopy (FCS) is the link for ultrasensitive multicomponent analysis, showing possibilities for experiments on molecular interactions. Based on the theoretical background of FCS, this article gives full explanation of FCS and an update of highlights in experimental biology and medicine studied by FCS. We focus on a repertoire of diverse immunoglobulin specificities, a ribosome display system, single-molecule DNA sequencing, and a mutant enzyme generated by random mutagenesis of amino acids. We describe the usefulness and the enormous potential of the methodology. Further, this contribution clearly indicates that FCS is a valuable tool for solution-phase single-molecule (SPSM) experiments in immunobiology and medicine. In experiments with the Goodpasture autoantibody, we worked out conditions for the design of experiments on a complex single molecule in solution. The possibility to use SPSM-FCS as a quantitation methodology opens up other important applications beyond the scope of this article. Original results extending the published studies are presented for the rational foundation of SPSM-FCS. In this original contribution, we deal with experimental systems for biology and medicine where the number of molecules in solution is very small. This article is mandatory for gaining confidence in the interpretation of experimental SPSM-FCS results on the selfsame, individual single molecule in solution.},
	journal = {Experimental biology and medicine (Maywood, N.J.)},
	author = {Földes-Papp, Zeno and Demel, Ulrike and Domej, Wolfgang and Tilz, Gernot P},
	year = {2002},
	keywords = {fcs, -phase single-molecule, con-, conventional fluorescence correlation spectros-, conventional two-color cross-correlation fcs, copy, focal microscopy, interactions, single, single molecules in, solution, spsm, ultrasensitive detection of molecular},
	pages = {291--300},
}

@article{westphal_video-rate_2008,
	title = {Video-rate far-field optical nanoscopy dissects synaptic vesicle movement.},
	volume = {320},
	issn = {1095-9203 (Electronic){\textbackslash}n0036-8075 (Linking)},
	doi = {10.1126/science.1154228},
	abstract = {We present video-rate (28 frames per second) far-field optical imaging with a focal spot size of 62 nanometers in living cells. Fluorescently labeled synaptic vesicles inside the axons of cultured neurons were recorded with stimulated emission depletion (STED) microscopy in a 2.5-micrometer by 1.8-micrometer field of view. By reducing the cross-sectional area of the focal spot by about a factor of 18 below the diffraction limit (260 nanometers), STED allowed us to map and describe the vesicle mobility within the highly confined space of synaptic boutons. Although restricted within boutons, the vesicle movement was substantially faster in nonbouton areas, consistent with the observation that a sizable vesicle pool continuously transits through the axons. Our study demonstrates the emerging ability of optical microscopy to investigate intracellular physiological processes on the nanoscale in real time.},
	number = {February},
	journal = {Science (New York, N.Y.)},
	author = {Westphal, Volker and Rizzoli, Silvio O and Lauterbach, Marcel a and Kamin, Dirk and Jahn, Reinhard and Hell, Stefan W},
	year = {2008},
	pages = {246--249},
}

@article{noauthor_biophy_nodate,
	title = {biophy meeting poster},
}

@article{noauthor_cy3_solvent0_nodate,
	title = {Cy3\_solvent0},
}

@article{noauthor_1652_001pdf_nodate,
	title = {1652\_001.pdf},
}

@article{palo_fluorescence_2000-1,
	title = {Fluorescence intensity multiple distributions analysis: concurrent determination of diffusion times and molecular brightness.},
	volume = {79},
	issn = {0006-3495},
	doi = {10.1016/S0006-3495(00)76523-4},
	abstract = {Fluorescence correlation spectroscopy (FCS) has proven to be a powerful technique with single-molecule sensitivity. Recently, it has found a complement in the form of fluorescence intensity distribution analysis (FIDA). Here we introduce a fluorescence fluctuation method that combines the features of both techniques. It is based on the global analysis of a set of photon count number histograms, recorded with multiple widths of counting time intervals simultaneously. This fluorescence intensity multiple distributions analysis (FIMDA) distinguishes fluorescent species on the basis of both the specific molecular brightness and the translational diffusion time. The combined information, extracted from a single measurement, increases the readout effectively by one dimension and thus breaks the individual limits of FCS and FIDA. In this paper a theory is introduced that describes the dependence of photon count number distributions on diffusion coefficients. The theory is applied to a series of photon count number histograms corresponding to different widths of counting time intervals. Although the ability of the method to determine specific brightness values, diffusion times, and concentrations from mixtures is demonstrated on simulated data, its experimental utilization is shown by the determination of the binding constant of a protein-ligand interaction exemplifying its broad applicability in the life sciences.},
	number = {December},
	journal = {Biophysical journal},
	author = {Palo, K and Mets, U and Jäger, S and Kask, P and Gall, K},
	year = {2000},
	pages = {2858--2866},
}

@article{noauthor_ncomms3207-s15_nodate,
	title = {ncomms3207-s15},
}

@article{digman_mapping_2008-1,
	title = {Mapping the number of molecules and brightness in the laser scanning microscope.},
	volume = {94},
	issn = {9498242992},
	doi = {10.1529/biophysj.107.114645},
	abstract = {We describe a technique based on moment-analysis for the measurement of the average number of molecules and brightness in each pixel in fluorescence microscopy images. The average brightness of the particle is obtained from the ratio of the variance to the average intensity at each pixel. To obtain the average number of fluctuating particles, we divide the average intensity at one pixel by the brightness. This analysis can be used in a wide range of concentrations. In cells, the intensity at any given pixel may be due to bright immobile structures, dim fast diffusing particles, and to autofluorescence or scattering. The total variance is given by the variance of each of the above components in addition to the variance due to detector noise. Assuming that all sources of variance are independent, the total variance is the sum of the variances of the individual components. The variance due to the particles fluctuating in the observation volume is proportional to the square of the particle brightness while the variance of the immobile fraction, the autofluorescence, scattering, and that of the detector is proportional to the intensity of these components. Only the fluctuations that depend on the square of the brightness (the mobile particles) will have a ratio of the variance to the intensity {\textgreater}1. Furthermore, changing the fluorescence intensity by increasing the illumination power, distinguishes between these possible contributions. We show maps of molecular brightness and number of cell migration proteins obtained using a two-photon scanning microscope operating with a photon-counting detector. These brightness maps reveal binding dynamics at the focal adhesions with pixel resolution and provide a picture of the binding and unbinding process in which dim molecules attach to the adhesions or large molecular aggregates dissociate from adhesion.},
	number = {March},
	journal = {Biophysical journal},
	author = {Digman, Michelle a and Dalal, Rooshin and Horwitz, Alan F and Gratton, Enrico},
	year = {2008},
	pages = {2320--2332},
}

@article{kahya_lipid_2004,
	title = {Lipid domain formation and dynamics in giant unilamellar vesicles explored by fluorescence correlation spectroscopy},
	volume = {147},
	issn = {1047-8477},
	doi = {10.1016/j.jsb.2003.09.021},
	abstract = {Lipids in eukaryotic cell membranes have been shown to cluster in "rafts" with different lipid/protein compositions and molecular packing. Model membranes such as giant unilamellar vesicles (GUVs) provide a key system to elucidate the physical mechanisms of raft assembly. Despite the large amount of work devoted to the detection and characterization of rafts, one of the most important pieces of information still missing in the picture of the cell membrane is dynamics: how lipids organize and move in rafts and how they modulate membrane fluidity. This missing element is of crucial importance for the trafficking at and from the periphery of the cell regulated by endo- and exocytosis and, in general, for the constant turnover which redistributes membrane components. Here, we review studies of combined confocal fluorescence microscopy and fluorescence correlation spectroscopy on lipid dynamics and organization in rafts assembled in GUVs prepared from various lipid mixtures which are relevant to the problem of raft formation. ?? 2003 Elsevier Inc. All rights reserved.},
	journal = {Journal of Structural Biology},
	author = {Kahya, Nicoletta and Scherfeld, Dag and Bacia, Kirsten and Schwille, Petra},
	year = {2004},
	keywords = {Fluorescence correlation spectroscopy, Cholesterol, Confocal fluorescence microscopy, GUVs, Lipid rafts, Phosphatidylcholine, Sphingomyelin},
	pages = {77--89},
}

@article{shaner_correction_2008,
	title = {Correction {A} guide to choosing fluorescent proteins {Supplementary} {Table} 2},
	volume = {909},
	number = {December 2005},
	author = {Shaner, Nathan C and Steinbach, Paul a and Tsien, Roger Y},
	year = {2008},
	pages = {1--2},
}

@article{noauthor_1923073447topbottomright1right2top_nodate,
	title = {1923073447@{Top},{Bottom},{Right1},{Right2}!{Top}},
}

@article{saito_situ_2003,
	title = {In situ observation of mobility and anchoring of {PKC}??{I} in plasma membrane},
	volume = {541},
	doi = {10.1016/S0014-5793(03)00324-7},
	abstract = {We employed fluorescence correlation spectroscopy (FCS) to analyze the characteristics of biomolecules in living cells. Protein kinase C (PKC) changes its subcellular localization from cytosol to the plasma membrane by its ligand. Using FCS, we found PKC??I labeled with enhanced green fluorescent protein freely diffusing in cytosol. Upon 12-O-tetradecanoylphorbol-13-acetate activation, a large part of PKC??I is anchored in the plasma membrane but some PKC??I still moves freely near the plasma membrane. These results indicate that a diffusion-driven transport mechanism is appropriate for the molecular mechanism of the PKC??I localization change. ?? 2003 Federation of European Biochemical Societies. Published by Elsevier Science B.V. All rights reserved.},
	journal = {FEBS Letters},
	author = {Saito, Kenta and Ito, Eiko and Takakuwa, Yuichi and Tamura, Mamoru and Kinjo, Masataka},
	year = {2003},
	keywords = {Fluorescence correlation spectroscopy, Green fluorescent protein, Diffusion constant, Plasma membrane, Protein kinase C, Signal transduction},
	pages = {126--131},
}

@article{noauthor_fcs_nodate,
	title = {{FCS} query 1997 to 2001},
}

@article{schwille_fluorescence_2009,
	title = {Fluorescence {Correlation} {Spectroscopy}: {An} {Introduction} to its {Concepts} and {Applications}},
	volume = {94},
	doi = {10.1002/lpor.200910041},
	abstract = {An alternative version of fluorescence correlation spectroscopy is presented, where the signal from a medium surrounding the particles of interest is analyzed, as opposed to a signal from the particles themselves. This allows for analysis of unlabeled particles and potentially of biomolecules. Here, the concept together with principal experiments on polystyrene beads of 100, 200, 400, and 800 nm diameter in an aqueous solution of alexa 488-fluorophores are presented. The use of photo detectors allowing higher photon fluxes, or of reduced detection volumes, should enable analysis of significantly smaller particles or even biomolecules.},
	journal = {Spectroscopy},
	author = {Schwille, Petra and Haustein, Elke},
	year = {2009},
	pages = {1--33},
}

@article{wawrezinieck_fluorescence_2004-1,
	title = {Fluorescence correlation spectroscopy to determine diffusion laws: application to live cell membranes},
	volume = {5462},
	url = {http://link.aip.org/link/?PSI/5462/92/1},
	doi = {10.1117/12.545014},
	abstract = {Fluorescencecorrelation spectroscopy (FCS) is a mature and powerful technique formeasuring diffusion coefficients. In a standard experiment, it measures thespontaneous fluorescence fluctuations arising from a single observation volume definedby confocal optics. However, the study becomes uneasy as soonas the diffusion is impeded by obstacles or specific mechanisms,as it is the case for the cell membrane componentsin live cells. In this paper, we show that doingFCS measurements at different sizes of observation volumes gives accessto the diffusion laws without a priori knowledge of thelandscape in which molecules are diffusing. Using this strategy, ameasurement of diffusion laws of lipids in monophasic Giant UnilamellarVesicles and in the plasma membrane of live cells iscarried out. 2004 COPYRIGHT SPIE-The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.},
	journal = {Biophotonics Micro and NanoImaging},
	author = {Wawrezinieck, Laure and Lenne, Pierre-Francois and Marguet, Didier and Rigneault, Herve},
	year = {2004},
	keywords = {fluorescence correlation spectroscopy, live cell, anomalous diffusion, cell membrane, diffusion, diffusion law, giant unilamellar, vesicle},
	pages = {92--102},
}

@article{hanson_gfp_2001,
	title = {{GFP} imaging: methodology and application to investigate cellular compartmentation in plants.},
	volume = {52},
	issn = {0022-0957 (Print)},
	doi = {10.1093/jexbot/52.356.529},
	abstract = {The cloning of the jellyfish gfp (green fluorescent protein) gene and its alteration for expression in subcellular locations in transformed plant cells have resulted in new views of intracellular organization and dynamics. Fusions of GFP with entire proteins of known or unknown function have shown where the proteins are located and whether the proteins move from one compartment to another. GFP and variants with different spectral properties have been deliberately targeted to separate compartments to determine their size, shape, mobility, and dynamic changes during development or environmental response. Fluorescence Resonance Energy Transfer (FRET) between GFP variants can discern protein/ protein interactions. GFP has been used as a sensor to detect changes or differences in calcium, pH, voltage, metal, and enzyme activity. Photobleaching and photoactivation of GFP as well as fluorescence correlation spectroscopy can measure rates of diffusion and movement of GFP within or between compartments. This review covers past applications of these methods as well as promising developments in GFP imaging for understanding the functional organization of plant cells.},
	number = {356},
	journal = {Journal of experimental botany},
	author = {Hanson, M R and Köhler, R H},
	year = {2001},
	keywords = {microscopy, confocal, green fluorescent protein, localization, organelle, photobleaching, plastid tubule},
	pages = {529--539},
}

@article{kahya_lipid_2004-1,
	title = {Lipid domain formation and dynamics in giant unilamellar vesicles explored by fluorescence correlation spectroscopy},
	volume = {147},
	issn = {1047-8477},
	doi = {10.1016/j.jsb.2003.09.021},
	abstract = {Lipids in eukaryotic cell membranes have been shown to cluster in "rafts" with different lipid/protein compositions and molecular packing. Model membranes such as giant unilamellar vesicles (GUVs) provide a key system to elucidate the physical mechanisms of raft assembly. Despite the large amount of work devoted to the detection and characterization of rafts, one of the most important pieces of information still missing in the picture of the cell membrane is dynamics: how lipids organize and move in rafts and how they modulate membrane fluidity. This missing element is of crucial importance for the trafficking at and from the periphery of the cell regulated by endo- and exocytosis and, in general, for the constant turnover which redistributes membrane components. Here, we review studies of combined confocal fluorescence microscopy and fluorescence correlation spectroscopy on lipid dynamics and organization in rafts assembled in GUVs prepared from various lipid mixtures which are relevant to the problem of raft formation. ?? 2003 Elsevier Inc. All rights reserved.},
	journal = {Journal of Structural Biology},
	author = {Kahya, Nicoletta and Scherfeld, Dag and Bacia, Kirsten and Schwille, Petra},
	year = {2004},
	keywords = {Fluorescence correlation spectroscopy, Cholesterol, Confocal fluorescence microscopy, GUVs, Lipid rafts, Phosphatidylcholine, Sphingomyelin},
	pages = {77--89},
}

@article{noauthor_rarrowsm_nodate,
	title = {rarrowsm},
}

@article{becker_fluorescence_2005,
	title = {Fluorescence lifetime images and correlation spectra obtained by multidimensional {TCSPC}},
	volume = {5700},
	url = {http://link.aip.org/link/?PSI/5700/144/1&Agg=doi},
	doi = {10.1117/12.588990},
	journal = {Proceedings of SPIE},
	author = {Becker, Wolfgang},
	year = {2005},
	keywords = {flim, tcspc, fcs, fret, bifl, fida, filda},
	pages = {144--151},
}

@article{dorsey_funding_2010,
	title = {Funding of {US} biomedical research, 2003-2008.},
	volume = {303},
	issn = {1538-3598},
	doi = {10.1001/jama.2009.1987},
	abstract = {With the exception of the American Recovery and Reinvestment Act, funding support for biomedical research in the United States has slowed after a decade of doubling. However, the extent and scope of slowing are largely unknown.},
	number = {2},
	journal = {JAMA : the journal of the American Medical Association},
	author = {Dorsey, E Ray and de Roulet, Jason and Thompson, Joel P and Reminick, Jason I and Thai, Ashley and White-Stellato, Zachary and Beck, Christopher a and George, Benjamin P and Moses, Hamilton},
	year = {2010},
	pages = {137--143},
}

@article{hess_biological_2002,
	title = {Biological and chemical applications of fluorescence correlation spectroscopy: {A} review},
	volume = {41},
	issn = {0006-2960},
	doi = {10.1021/bi0118512},
	abstract = {... The excitation intensity profile and detection optics (in particular, the confocal aperture) define the ... rapidly with the axial distance (roughly as z - 4 ) from the focal plane. ... excitation confinement provides intrinsic 3D spatial resolution and a nearly 3D Gaussian observation volume . ...},
	number = {3},
	journal = {Biochemistry},
	author = {Hess, Samuel T. and Huang, Shaohui and Heikal, Ahmed a. and Webb, Watt W.},
	year = {2002},
	pages = {697--705},
}

@article{huang_photon_2004,
	title = {Photon counting histogram: {One}-photon excitation},
	volume = {5},
	issn = {1439-4235},
	doi = {10.1002/cphc.200400176},
	abstract = {The photon counting histogram (PCH) analysis is a fluorescence fluctuation method that is able to characterize the brightness and concentration of different fluorescent species present in a liquid sample. We find that the PCH model using a three-dimensional Gaussian observation volume profile is inadequate for fitting experimental data obtained from a confocal setup with one-photon excitation. We propose an imoroved model, which is based on the correction to the observation volume profile for the out-of-focus emission. We demonstrate that this model is able to resolve different species present under a wide range of conditions. Attention is given to how this model allows the examination of the effects of different instrumental setups on the resolvability.},
	journal = {ChemPhysChem},
	author = {Huang, Bo and Perroud, Thomas D. and Zare, Richard N.},
	year = {2004},
	keywords = {Fluorescence spectroscopy, Confocal microscopy, One-photon excitation, Photon counting histogram},
	pages = {1523--1531},
}

@article{suhling_imaging_2002,
	title = {Imaging the environment of green fluorescent protein.},
	volume = {83},
	issn = {0006-3495 (Print){\textbackslash}r0006-3495 (Linking)},
	doi = {10.1016/S0006-3495(02)75359-9},
	abstract = {An emerging theme in cell biology is that cell surface receptors need to be considered as part of supramolecular complexes of proteins and lipids facilitating specific receptor conformations and distinct distributions, e.g., at the immunological synapse. Thus, a new goal is to develop bioimaging that not only locates proteins in live cells but can also probe their environment. Such a technique is demonstrated here using fluorescence lifetime imaging of green fluorescent protein (GFP). We first show, by time-correlated single-photon counting, that the fluorescence decay of GFP depends on the local refractive index. This is in agreement with the Strickler Berg formula, relating the Einstein A and B coefficients for absorption and spontaneous emission in molecules. We then quantitatively image, by wide-field time-gated fluorescence lifetime imaging, the refractive index of the environment of GFP. This novel approach paves the way for imaging the biophysical environment of specific GFP-tagged proteins in live cells.},
	number = {December},
	journal = {Biophysical journal},
	author = {Suhling, Klaus and Siegel, Jan and Phillips, David and French, Paul M W and Lévêque-Fort, Sandrine and Webb, Stephen E D and Davis, Daniel M},
	year = {2002},
	pages = {3589--3595},
}

@article{cole_whole-field_2000,
	title = {Whole-field optically sectioned fluorescence lifetime imaging.},
	volume = {25},
	doi = {10.1364/OL.25.001361},
	abstract = {We describe a novel three-dimensional fluorescence lifetime imaging microscope that exploits structured illumination to achieve whole-field sectioned fluorescence lifetime images with a spatial resolution of a few micrometers.},
	number = {18},
	journal = {Optics letters},
	author = {Cole, M J and Siegel, J and Webb, S E and Jones, R and Dowling, K and French, P M and Lever, M J and Sucharov, L O and Neil, M a and Juskaitis, R and Wilson, T},
	year = {2000},
	pages = {1361--1363},
}

@article{kahya_probing_2003,
	title = {Probing lipid mobility of raft-exhibiting model membranes by fluorescence correlation spectroscopy},
	volume = {278},
	issn = {0021-9258 (Print){\textbackslash}r0021-9258 (Linking)},
	doi = {10.1074/jbc.M302969200},
	abstract = {Confocal fluorescence microscopy and fluorescence correlation spectroscopy (FCS) have been employed to investigate the lipid spatial and dynamic organization in giant unilamellar vesicles (GUVs) prepared from ternary mixtures of dioleoyl-phosphatidylcholine/sphingomyelin/cholesterol. For a certain range of cholesterol concentration, formation of domains with raft-like properties was observed. Strikingly, the lipophilic probe 1,1'-dioctadecyl-3,3,3',3'-tetramethylindocarbocyanine perchlorate (DiI-C18) was excluded from sphingomyelin-enriched regions, where the raft marker ganglioside GM1 was localized. Cholesterol was shown to promote lipid segregation in dioleoyl-phosphatidylcholine-enriched, liquid-disordered, and sphingomyelin-enriched, liquid-ordered phases. Most importantly, the lipid mobility in sphingomyelin-enriched regions significantly increased by increasing the cholesterol concentration. These results pinpoint the key role, played by cholesterol in tuning lipid dynamics in membranes. At cholesterol concentrations {\textgreater}50 mol\%, domains vanished and the lipid diffusion slowed down upon further addition of cholesterol. By taking the molecular diffusion coefficients as a fingerprint of membrane phase compositions, FCS is proven to evaluate domain lipid compositions. Moreover, FCS data from ternary and binary mixtures have been used to build a ternary phase diagram, which shows areas of phase coexistence, transition points, and, importantly, how lipid dynamics varies between and within phase regions.},
	number = {30},
	journal = {Journal of Biological Chemistry},
	author = {Kahya, Nicoletta and Scherfeld, Dag and Bacia, Kirsten and Poolman, Bert and Schwille, Petra},
	year = {2003},
	pages = {28109--28115},
}

@article{core_raster_nodate,
	title = {Raster {Image} {Correlation} {Spectroscopy} {RICS}},
	author = {Core, Optical Biology},
}

@article{ruan_cellular_2002,
	title = {Cellular characterization of adenylate kinase and its isoform: two-photon excitation fluorescence imaging and fluorescence correlation spectroscopy.},
	volume = {83},
	doi = {10.1016/S0006-3495(02)75320-4},
	abstract = {Adenylate kinase (AK) is a ubiquitous enzyme that regulates the homeostasis of adenine nucleotides in the cell. AK1beta (long form) from murine cells shares the same protein sequence as AK1 (short form) except for the addition of 18 amino acid residues at its N-terminus. It is hypothesized that these residues serve as a signal for protein lipid modification and targeting of the protein to the plasma membrane. To better understand the cellular function of these AK isoforms, we have used several modern fluorescence techniques to characterize these two isoforms of AK enzyme. We fused cytosolic adenylate kinase (AK1) and its isoform (AK1beta) with enhanced green fluorescence protein (EGFP) and expressed the chimera proteins in HeLa cells. Using two-photon excitation scanning fluorescence imaging, we were able to directly visualize the localization of AK1-EGFP and AK1beta-EGFP in live cells. AK1beta-EGFP mainly localized on the plasma membrane, whereas AK1-EGFP distributed throughout the cell except for trace amounts in the nuclear membrane and some vesicles. We performed fluorescence correlation spectroscopy measurements and photon-counting histogram analysis in specific domains of live cells. For AK1-EGFP, we observed only one diffusion component in the cytoplasm. For AK1beta-EGFP, we observed two distinct diffusion components on the plasma membrane. One corresponded to the free diffusing protein, whereas the other represented the membrane-bound AK1beta-EGFP. The diffusion rate of AK1-EGFP was slowed by a factor of 1.8 with respect to that of EGFP, which was 50\% more than what we would expect for a free diffusing AK1-EGFP. To rule out the possibility of oligomer formation, we performed photon-counting histogram analysis to direct analyze the brightness difference between AK1-EGFP and EGFP. From our analysis, we concluded that cytoplasmic AK1-EGFP is monomeric. fluorescence correlation spectroscopy proved to be a powerful technique for quantitatively studying the mobility of the target protein in live cells. This technology offers advantages in studying protein interactions and function in the cell.},
	number = {December},
	journal = {Biophysical journal},
	author = {Ruan, Qiaoqiao and Chen, Yan and Gratton, Enrico and Glaser, Michael and Mantulin, William W},
	year = {2002},
	pages = {3177--3187},
}

@article{noauthor_ncomms3207-s11_nodate,
	title = {ncomms3207-s11},
}

@article{gerritsen_fluorescence_1997,
	title = {Fluorescence lifetime imaging of oxygen in living cells},
	volume = {7},
	doi = {10.1007/BF02764572},
	number = {1},
	journal = {Journal of Fluorescence},
	author = {Gerritsen, H. C. and Sanders, R. and Draaijer, a. and Ince, C. and Levine, Y. K.},
	year = {1997},
	keywords = {fluorescence lifetime imaging, confocal microscopy, macrophages, oxygen imaging},
	pages = {11--15},
}

@article{noauthor_ncomms3207-s4_nodate,
	title = {ncomms3207-s4},
}

@article{halbsguth_positive_2003,
	title = {Positive cooperation of protoberberine type 2 alkaloids from {Corydalis} cava on the {GABAA} binding site},
	volume = {69},
	doi = {10.1055/s-2003-38869},
	abstract = {Protoberberine alkaloids from the rhizomes of Corydalis cava were investigated with regard to their influence on the GABA A receptor using radioreceptor assays. Whereas the protoberberine type 2 alkaloids, isoapocavidine, corydaline, tetrahydropalmatine, scoulerine and isocorypalmine, increased the specific [(3)H]BMC-binding in a range of 21 - 49 \%, the protoberberine type 1 alkaloids, palmatine, coptisine, dehydroapocavidine, and dehydrocorydaline, had no influence on the binding behaviour of the GABA A receptor. To confirm the modulatory activity of the protoberberine type 2 alkaloids on living cells, GABA A receptor binding studies were performed by fluorescence correlation spectroscopy (FCS) using hippocampal neurons and the fluorescently labelled ligand, muscimol-Alexa (Mu-Alexa). The incubation of hippocampal neurons with 7.5 nM Mu-Alexa showed a specific binding of 5.25 nM (70 \%). The evaluation of the autocorrelation curve revealed two different mobilities of receptor ligand complexes, D bound1 = (2.8 +/- 0.91) microm 2/s for the free lateral mobility and D bound2 = (0.14 +/- 0.05) microm 2/s for the hindered mobility. An incubation of hippocampal neurons with 7.5 nM Mu-Alexa and 7.5 nM scoulerine showed a maximal increase of the specific Mu-Alexa binding of approximately 27 \% by selectively modulating the amount of receptor-ligand complexes with a hindered mobility (9 \% to 27 \%).},
	journal = {Planta Medica},
	author = {Halbsguth, Christiane and Meißner, Oliver and Häberlein, Hanns},
	year = {2003},
	keywords = {Fluorescence correlation spectroscopy, Alkaloids, Corydalis cava, Fumariaceae, GABAA receptor, Radioreceptor assay},
	pages = {305--309},
}

@article{rolda_does_2006,
	title = {Does {Green} {Light} {In} uence the {Fluorescence} {Properties} and {Structure} of {Phototrophic} {Bio} lms?},
	volume = {72},
	doi = {10.1128/AEM.72.4.3026},
	number = {4},
	journal = {Society},
	author = {Rolda, M and Oliva, F and Go, M a and Herna, M},
	year = {2006},
	pages = {3026--3031},
}

@article{sengupta_measuring_2003-1,
	title = {Measuring size distribution in highly heterogeneous systems with fluorescence correlation spectroscopy.},
	volume = {84},
	url = {http://dx.doi.org/10.1016/S0006-3495(03)75006-1},
	doi = {10.1016/S0006-3495(03)75006-1},
	abstract = {Fluorescence correlation spectroscopy (FCS) is a sensitive and widely used technique for measuring diffusion. FCS data are conventionally modeled with a finite number of diffusing components and fit with a least-square fitting algorithm. This approach is inadequate for analyzing data obtained from highly heterogeneous systems. We introduce a Maximum Entropy Method based fitting routine (MEMFCS) that analyzes FCS data in terms of a quasicontinuous distribution of diffusing components, and also guarantees a maximally wide distribution that is consistent with the data. We verify that for a homogeneous specimen (green fluorescent protein in dilute aqueous solution), both MEMFCS and conventional fitting yield similar results. Further, we incorporate an appropriate goodness of fit criterion in MEMFCS. We show that for errors estimated from a large number of repeated measurements, the reduced chi(2) value in MEMFCS analysis does approach unity. We find that the theoretical prediction for errors in FCS experiments overestimates the actual error, but can be empirically modified to serve as a guide for estimating the goodness of the fit where reliable error estimates are unavailable. Finally, we compare the performance of MEMFCS with that of a conventional fitting routine for analyzing simulated data describing a highly heterogeneous distribution containing 41 diffusing species. Both methods fit the data well. However, the conventional fit fails to reproduce the essential features of the input distribution, whereas MEMFCS yields a distribution close to the actual input.},
	number = {3},
	journal = {Biophysical journal},
	author = {Sengupta, Parijat and Garai, K and Balaji, J and Periasamy, N and Maiti, S},
	year = {2003},
	note = {Publisher: Elsevier},
	pages = {1977--1984},
}

@article{kim_fully_2007-3,
	title = {Fully automated segmentation and morphometrical analysis of muscle fiber images.},
	volume = {71},
	doi = {10.1002/cyto.a},
	abstract = {BACKGROUND: Measurement of muscle fiber size and determination of size distribution is important in the assessment of neuromuscular disease. Fiber size estimation by simple inspection is inaccurate and subjective. Manual segmentation and measurement are time-consuming and tedious. We therefore propose an automated image analysis method for objective, reproducible, and time-saving measurement of muscle fibers in routinely hematoxylin-eosin stained cryostat sections. METHODS: The proposed segmentation technique makes use of recent advances in level set based segmentation, where classical edge based active contours are extended by region based cues, such as color and texture. Segmentation and measurement are performed fully automatically. Multiple morphometric parameters, i.e., cross sectional area, lesser diameter, and perimeter are assessed in a single pass. The performance of the computed method was compared to results obtained by manual measurement by experts. RESULTS: The correct classification rate of the computed method was high (98\%). Segmentation and measurement results obtained manually or automatically did not reveal any significant differences. CONCLUSIONS: The presented region based active contour approach has been proven to accurately segment and measure muscle fibers. Complete automation minimizes user interaction, thus, batch processing, as well as objective and reproducible muscle fiber morphometry are provided.},
	number = {June},
	journal = {Cytometry. Part A : the journal of the International Society for Analytical Cytology},
	author = {Kim, Yoo-Jin and Brox, Thomas and Feiden, Wolfgang and Weickert, Joachim},
	year = {2007},
	pages = {8--15},
}

@article{gmbh_correlation_2002,
	title = {Correlation {Measurements} by {Advanced} {TCSPC} {Techniques}},
	number = {April},
	author = {Gmbh, Hickl},
	year = {2002},
	pages = {1--14},
}

@article{katiliene_single_2003,
	title = {Single molecule detection of {DNA} looping by {NgoMIV} restriction endonuclease.},
	volume = {84},
	issn = {0006-3495},
	doi = {10.1016/S0006-3495(03)75131-5},
	abstract = {Single molecule fluorescence resonance energy transfer (FRET) and fluorescence correlation spectroscopy were used to investigate DNA looping by NgoMIV restriction endonuclease. Using a linear double-stranded DNA (dsDNA) molecule labeled with a fluorescence donor molecule, Cy3, and fluorescence acceptor molecule, Cy5, and by varying the concentration of NgoMIV endonuclease from 0 to 3 x 10(-6) M, it was possible to detect and determine diffusion properties of looped DNA/protein complexes. FRET efficiency distributions revealed a subpopulation of complexes with an energy transfer efficiency of 30\%, which appeared upon addition of enzyme in the picomolar to nanomolar concentration range (using 10(-11) M dsDNA). The concentration dependence, fluorescence burst size analysis, and fluorescence correlation analysis were all consistent with this subpopulation arising from a sequence specific interaction between an individual enzyme and a DNA molecule. A 30\% FRET efficiency corresponds to a distance of approximately 65 A, which correlates well with the distance between the ends of the dsDNA molecule when bound to NgoMIV according to the crystal structure of this complex. Formation of the looped complexes was also evident in measurements of the diffusion times of freely diffusing DNA molecules with and without NgoMIV. At very high protein concentrations compared to the DNA concentration, FRET and fluorescence correlation spectroscopy results revealed the formation of larger DNA/protein complexes.},
	number = {June},
	journal = {Biophysical journal},
	author = {Katiliene, Zivile and Katilius, Evaldas and Woodbury, Neal W},
	year = {2003},
	pages = {4053--4061},
}

@article{noauthor_feedbackact_nodate,
	title = {{feedbackACT}},
}

@article{rimon_getting_2011,
	title = {Getting the whole picture: combining throughput with content in microscopy},
	volume = {124},
	issn = {1477-9137 (Electronic){\textbackslash}r0021-9533 (Linking)},
	doi = {10.1242/jcs.087486},
	abstract = {The increasing availability and performance of automated scientific equipment in the past decades have brought about a revolution in the biological sciences. The ease with which data can now be generated has led to a new culture of high-throughput science, in which new types of biological questions can be asked and tackled in a systematic and unbiased manner. High-throughput microscopy, also often referred to as high-content screening (HCS), allows acquisition of systematic data at the single-cell level. Moreover, it allows the visualization of an enormous array of cellular features and provides tools to quantify a large number of parameters for each cell. These features make HCS a powerful method to create data that is rich and biologically meaningful without compromising systematic capabilities. In this Commentary, we will discuss recent work, which has used HCS, to demonstrate the diversity of applications and technological solutions that are evolving in this field. Such advances are placing HCS methodologies at the frontier of high-throughput science and enable scientists to combine throughput with content to address a variety of cell biological questions.},
	journal = {Journal of Cell Science},
	author = {Rimon, N. and Schuldiner, M.},
	year = {2011},
	keywords = {high-content screening, automated microscopy, fluorescent labeling, functional genomics, genetically encoded probes, high-throughput biology},
	pages = {3743--3751},
}

@article{roth_imaging_2007,
	title = {Imaging diffusion in living cells using time-correlated single-photon counting},
	volume = {79},
	issn = {0003-2700},
	doi = {10.1021/ac071039q},
	abstract = {Current efforts to monitor the diffusion of proteins in living cells are based on either fluorescence correlation spectroscopy (FCS), fluorescence recovery after photobleaching, or image correlation spectroscopy. However, these methods cannot generate a map of diffusion times. Here, we introduce a new method termed diffusion imaging microscopy that combines scanning confocal microscopy, time-correlated single-photon counting, and FCS and thus allows us to measure spatially resolved diffusion times. In our approach, we record scan images with time-resolved photon streams within each individual pixel. By extending the pixel dwell time to 25-100 ms, a software correlation of individual photons within each pixel yields the average diffusion time. Additionally, information on fluorescence intensity (number of photons) and fluorescence lifetime is available and can be used to sort fluorescence photons and to discriminate from autofluorescence. We evaluated our method by measuring diffusion times of dT20-TMR in solutions of different viscosity. We further demonstrate the applicability of the method to living cells and recorded a diffusion map of a living 3T3 mouse fibroblast incubated with dT20-ATTO488.},
	number = {19},
	journal = {Analytical Chemistry},
	author = {Roth, Christian M. and Heinlein, Pia I. and Heilemann, Mike and Herten, Dirk Peter},
	year = {2007},
	pages = {7340--7345},
}

@article{kumbhakar_single-molecule_2004,
	title = {Single-molecule detection in exploring nanoenvironments: {An} overview},
	volume = {5},
	issn = {1389-5567},
	doi = {10.1016/j.jphotochemrev.2004.07.004},
	abstract = {In the last one decade or so, a variety of optical experiments have been designed and performed that are capable of exploring down to the regime of single-molecule detection and measurements in all different environments, including solids, surfaces, and liquids. Single-molecule detection in condensed phases has many important chemical and biological applications. A few to list are: rapid DNA sequencing, DNA fragment sizing, medical diagnosis, forensic analysis, understanding of chemical dynamics and mechanisms, etc. Single-molecule spectroscopy allows us to observe the individual molecules hidden in a condensed phase sample, by using a tunable laser light. This technique has the ability to detect and monitor systems with an ultimate sensitivity level of ???1.66 ?? 10 
-24 moles (1/N
0). Measurement at the single-molecule level can completely remove the complicacy associated with ensemble-averaged macroscopic measurements. It allows us to construct a frequency histogram of the distribution of values for a parameter of interest following a large number of measurements on many individual molecules. Such a distribution carries much more information than the average value of the parameter obtained from a macroscopic measurement. As there is no ensemble averaging involved, only measurements at the single-molecule level can give an appropriate test for microscopic dynamical theories. Using single-molecule spectroscopy one can, in principle, follow the temporal evolution of any complex reaction path. As the field is still emerging, with newer methodologies of detecting single molecules with improved signal-to-noise ratios, it is expected that many new physical and chemical phenomena will certainly be explored using this technique. In the present article, our endeavor is to give an overview of the different aspects of single-molecule detection, along with some of its important applications in the areas of bioscience and chemical physics. ?? 2004 Japanese Photochemistry Association. Published by Elsevier B.V. All rights reserved.},
	journal = {Journal of Photochemistry and Photobiology C: Photochemistry Reviews},
	author = {Kumbhakar, Manoj and Nath, Sukhendu and Mukherjee, Tulsi and Mittal, Jai Pal and Pal, Haridas},
	year = {2004},
	keywords = {Fluorescence correlation spectroscopy, Confocal microscopy, Fluorescence excitation spectroscopy, Frequency modulation spectroscopy, Near-field optical microscopy, Single-molecule detection, Single-molecule spectroscopy, Total internal reflection microscopy},
	pages = {113--137},
}

@article{heinze_simultaneous_2000-2,
	title = {Simultaneous two-photon excitation of distinct labels for dual-color fluorescence crosscorrelation analysis.},
	volume = {97},
	issn = {0027-8424 (Print){\textbackslash}r0027-8424 (Linking)},
	doi = {10.1073/pnas.180317197},
	abstract = {Confocal fluorescence correlation spectroscopy as a time-averaging fluctuation analysis combining maximum sensitivity with high statistical confidence has proved to be a very versatile and powerful tool for detection and temporal investigation of biomolecules at ultralow concentrations on surfaces, in solutions, and in living cells. To probe the interaction of different molecular species for a detailed understanding of biologically relevant mechanisms, crosscorrelation studies on dual or multiple fluorophore assays with spectrally distinct excitation and emission are particularly promising. Despite the considerable improvement of detection specificity provided by fluorescence crosscorrelation analysis, few applications have so far been reported, presumably because of the practical challenges of properly aligning and controlling the stability of the experimental setup. In this work, we demonstrate that two-photon excitation combined with dual-color fluorescence correlation spectroscopy can be the key to simplifying simultaneous investigations of multiple fluorescent species significantly on a single-molecule scale. Two-photon excitation allows accession of common fluorophores of largely distinct emission by the same excitation wavelength, because differences in selection rules and vibronic coupling can induce considerable shifts between the one-photon and two-photon excitation spectra. The concept of dual-color two-photon fluorescence crosscorrelation analysis is introduced and experimentally demonstrated with an established assay probing the selective cleavage of dual-labeled DNA substrates by restriction endonuclease EcoRI.},
	number = {19},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Heinze, K G and Koltermann, a and Schwille, P},
	year = {2000},
	pages = {10377--10382},
}

@article{guan_adaptive_2008-3,
	title = {Adaptive correction technique for {3D} reconstruction of fluorescence microscopy images.},
	volume = {71},
	issn = {1059-910X (Print)},
	doi = {10.1002/jemt},
	abstract = {Recent advances in high-resolution imaging have provided valuable novel insights into structural relationships within cells and tissues both in vitro and in vivo. An analysis of this kind is regularly done by optical sectioning using either confocal or deconvolution microscopy. However, the reconstruction of 3D images suffers from light scattering and absorption with increasing depth by finite transparency of the used media. Photobleaching of fluorochromes has been especially troublesome and often the only remedy for loss of signal during optical sectioning is to reduce the number of sections. This causes disparities in the x-y and z dimensions of voxels, which lead to vertical distortion of the original stack of images and necessitates interpolation. Interpolation is necessary to fill up the gaps between consecutive sections in the original image stack to obtain cubic voxels. The present manuscript describes a novel method for adaptive compensation of attenuation of light intensity in stacks of fluorescence microscopy images that is based on a physical model of light attenuation. First, we use a fast interpolation technique to generate a cubic voxel-based volume stack with the aid of a contribution look up table. With the contribution look up table, multiple calculations are avoided, which substantially reduces the computational time without compromising the accuracy of the restoration procedure. Second, each section within the resulting volume is processed to rectify its intensity values that have been altered due to photobleaching and scattering and absorption. The method allows to define the last good section in the stack and the correction is then done automatically.},
	number = {December 2006},
	journal = {Microscopy research and technique},
	author = {Guan, Y Q and Cai, Y Y and Zhang, X and Lee, Y T and Opas, M},
	year = {2008},
	keywords = {flim, tcspc, fret, protein, protein interactions, slim, two-photon excitation},
	pages = {146--157},
}

@article{nakashima_two-dimensional_2004,
	title = {Two-dimensional fluorescence correlation spectroscopy {II}: {Spectral} analysis of derivatives of anthracene and pyrene in micellar solutions},
	volume = {60},
	issn = {1386-1425},
	doi = {10.1016/j.saa.2003.08.028},
	abstract = {Generalized two-dimensional (2D) correlation spectroscopy has been applied to the analysis of fluorescence spectra in two micellar systems: (1) a mixture of pyrene and 1,3,6,8-pyrenetetrasulfonic acid in the cationic micellar solutions of cetyltrimethylammonium chloride (CTAC) and (2) a mixture of pyrene and 9-anthracencepropionic acid in anionic micellar solutions of sodium dodecyl sulfate (SDS). Fluorescence quenching is employed as a perturbation mode for causing intensity changes in fluorescence bands (quenching perturbation). Iodide ion (I-) is used as a quencher in the former system, and cetyl pridinium chloride (CPC) is used in the latter. Vibronic bands in the complicated fluorescence spectra of the mixture of the analytes were successfully resolved. It is shown that asynchronous maps are especially useful for spectral resolution enhancement when the quenching perturbation is employed in 2D fluorescence correlation spectroscopy. Furthermore, the information about the order of response of the bands to quenching is obtained by comparing the signs of synchronous and asynchronous cross-peaks. ?? 2003 Elsevier B.V. All rights reserved.},
	journal = {Spectrochimica Acta - Part A: Molecular and Biomolecular Spectroscopy},
	author = {Nakashima, Kenichi and Yuda, Kazuki and Ozaki, Yukihiro and Noda, Isao},
	year = {2004},
	keywords = {Fluorescence, Anthracene, Micelle, Pyrene, Quenching, Two-dimensional correlation spectroscopy},
	pages = {1783--1791},
}

@article{weidemann_analysis_2002,
	title = {Analysis of ligand binding by two-colour fluorescence cross-correlation spectroscopy},
	volume = {3},
	issn = {4962214252339},
	doi = {10.1002/1438-5171(200204)3:1<49::AID-SIMO49>3.0.CO;2-T},
	abstract = {Fluorescence correlation spectroscopy (FCS) is a well-established method for the analysis of freely diffusing fluorescent particles in solution. In a two-colour setup, simultaneous detection of two different dyes allows the acquisition of both the autocorrelation of the signal of each channel and the cross-correlation of the two channels (fluorescence cross-correlation spectroscopy, FCCS). The cross-correlation function is related to the amount of diffusing particles carrying both dyes and can be used for monitoring a binding reaction. Here we develop a formalism for a quantitative analysis of ligand binding from a combination of the auto- and the cross-correlation amplitudes. Technical constraints, like the focal geometry, background signal and cross-talk between the detection channels as well as photophysical and biochemical effects which modulate the brightness of the particles are included in the analysis. Based on this framework a comprehensive treatment for the determination of two-component binding equilibria by FCS/FCCS is presented.},
	journal = {Single Molecules},
	author = {Weidemann, Thomas and Wachsmuth, Malte and Tewes, Michael and Rippe, Karsten and Langowski, Jörg},
	year = {2002},
	keywords = {FCS, FCCS, Protein-DNA interactions, Receptor-ligand binding},
	pages = {49--61},
}

@article{pan_improved_2012,
	title = {Improved {Li}-storage performance of {Li} {4Ti} {5O} 12 coated with {C}-{N} compounds derived from pyrolysis of urea through a low-temperature approach},
	volume = {5},
	issn = {0024929719},
	doi = {10.1002/cssc.201100629},
	abstract = {A uniform and thin amorphous layer of a CN compound was coated on porous Li(4) Ti(5) O(12)  by pyrolysis of urea on its surface at a rather low temperature of 400 °C in an Ar atmosphere. Such a CN coating layer greatly improved the electrochemical performance of Li(4) Ti(5) O(12) . After coating, Li(4) Ti(5) O(12)  showed good rate and excellent cycling performance. Reversible capacities for the coated sample of 134 and 105 mAh g(-1)  were obtained at current rates of 5C and 10C, respectively, in the voltage range of 1-2.2 V, which is approximately two and five times higher than those of pristine Li(4) Ti(5) O(12)  at the same current rates. Excellent capacity retention of 95.8 \% was achieved for the coated sample after 2000 cycles in a half cell at a 2C rate.},
	number = {June},
	journal = {ChemSusChem},
	author = {Pan, Huilin and Zhao, Liang and Hu, Yong Sheng and Li, Hong and Chen, Liquan},
	year = {2012},
	keywords = {batteries, lithium, surface modification, titanates, urea},
	pages = {526--529},
}

@article{noauthor_ref_nodate,
	title = {{REF} - {Fluorophore} {Spectra}},
}

@article{bur_real-time_2004,
	title = {Real-time monitoring of fluorescence anisotropy and temperature during processing of biaxially stretched polypropylene film},
	volume = {44},
	doi = {10.1002/pen.20072},
	number = {4},
	journal = {Polymer Engineering and Science},
	author = {Bur, Anthony J. and Roth, Steven C.},
	year = {2004},
	keywords = {fluorescence anisotropy, biaxial orientation, fluo},
	pages = {805--813},
}

@article{haustein_single-molecule_2004,
	title = {Single-molecule spectroscopic methods},
	volume = {14},
	issn = {0959-440X},
	doi = {10.1016/j.sbi.2004.09.004},
	abstract = {Being praised for the mere fact of enabling the detection of individual fluorophores a dozen years ago, single-molecule techniques nowadays represent standard methods for the elucidation of the structural rearrangements of biologically relevant macromolecules. Single-molecule-sensitive techniques, such as fluorescence correlation spectroscopy, allow real-time access to a multitude of molecular parameters (e.g. diffusion coefficients, concentration and molecular interactions). As a result of various recent advances, this technique shows promise even for intracellular applications. Fluorescence imaging can reveal the spatial localization of fluorophores on nanometer length scales, whereas fluorescence resonance energy transfer supports a wide range of different applications, including real-time monitoring of conformational rearrangements (as in protein folding). Still in their infancy, single-molecule spectroscopic methods thus provide unprecedented insights into basic molecular mechanisms.},
	journal = {Current Opinion in Structural Biology},
	author = {Haustein, Elke and Schwille, Petra},
	year = {2004},
	pages = {531--540},
}

@article{wallace_non-arrhenius_2001,
	title = {Non-{Arrhenius} kinetics for the loop closure of a {DNA} hairpin.},
	volume = {98},
	issn = {1091-6490},
	doi = {10.1073/pnas.101523498},
	abstract = {Intramolecular chain diffusion is an elementary process in the conformational fluctuations of the DNA hairpin-loop. We have studied the temperature and viscosity dependence of a model DNA hairpin-loop by FRET (fluorescence resonance energy transfer) fluctuation spectroscopy (FRETfs). Apparent thermodynamic parameters were obtained by analyzing the correlation amplitude through a two-state model and are consistent with steady-state fluorescence measurements. The kinetics of closing the loop show non-Arrhenius behavior, in agreement with theoretical prediction and other experimental measurements on peptide folding. The fluctuation rates show a fractional power dependence (beta = 0.83) on the solution viscosity. A much slower intrachain diffusion coefficient in comparison to that of polypeptides was derived based on the first passage time theory of SSS [Szabo, A., Schulten, K. \& Schulten, Z. (1980) J. Chem. Phys. 72, 4350-4357], suggesting that intrachain interactions, especially stacking interaction in the loop, might increase the roughness of the free energy surface of the DNA hairpin-loop.},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Wallace, M I and Ying, L and Balasubramanian, S and Klenerman, D},
	year = {2001},
	pages = {5584--5589},
}

@article{hebert_spatiotemporal_2005-1,
	title = {Spatiotemporal image correlation spectroscopy ({STICS}) theory, verification, and application to protein velocity mapping in living {CHO} cells.},
	volume = {88},
	issn = {0006-3495 (Print)},
	url = {http://dx.doi.org/10.1529/biophysj.104.054874},
	doi = {10.1529/biophysj.104.054874},
	abstract = {We introduce a new extension of image correlation spectroscopy (ICS) and image cross-correlation spectroscopy (ICCS) that relies on complete analysis of both the temporal and spatial correlation lags for intensity fluctuations from a laser-scanning microscopy image series. This new approach allows measurement of both diffusion coefficients and velocity vectors (magnitude and direction) for fluorescently labeled membrane proteins in living cells through monitoring of the time evolution of the full space-time correlation function. By using filtering in Fourier space to remove frequencies associated with immobile components, we are able to measure the protein transport even in the presence of a large fraction ({\textgreater}90\%) of immobile species. We present the background theory, computer simulations, and analysis of measurements on fluorescent microspheres to demonstrate proof of principle, capabilities, and limitations of the method. We demonstrate mapping of flow vectors for mixed samples containing fluorescent microspheres with different emission wavelengths using space time image cross-correlation. We also present results from two-photon laser-scanning microscopy studies of alpha-actinin/enhanced green fluorescent protein fusion constructs at the basal membrane of living CHO cells. Using space-time image correlation spectroscopy (STICS), we are able to measure protein fluxes with magnitudes of mum/min from retracting lamellar regions and protrusions for adherent cells. We also demonstrate the measurement of correlated directed flows (magnitudes of mum/min) and diffusion of interacting alpha5 integrin/enhanced cyan fluorescent protein and alpha-actinin/enhanced yellow fluorescent protein within living CHO cells. The STICS method permits us to generate complete transport maps of proteins within subregions of the basal membrane even if the protein concentration is too high to perform single particle tracking measurements.},
	number = {5},
	journal = {Biophysical journal},
	author = {Hebert, Benedict and Costantino, Santiago and Wiseman, Paul W},
	year = {2005},
	note = {Publisher: Elsevier},
	pages = {3601--3614},
}

@article{habenicht_two-photon_2002,
	title = {Two-photon excitation and time-resolved fluorescence: {I}. {The} proper response function for analysing single-photon counting experiments},
	volume = {354},
	doi = {10.1016/S0009-2614(02)00141-0},
	abstract = {An accurate instrumental response function is needed to deconvolute fluorescence data obtained by time-correlated single-photon counting (TCSPC) upon multi-photon excitation. Hitherto the response function was obtained by measuring Rayleigh scattering (RS) from colloidal solutions, as is also used in one-photon excited fluorescence. We show that hyper Rayleigh scattering (HRS) provides a better choice for deconvolution of fluorescence decays, as obtained by TCSPC and two-photon excitation (TPE). The one- and two-photon response functions were monitored as RS and HRS from colloidal gold particles at 800 and 400 nm, respectively. © 2002 Published by Elsevier Science B.V.},
	number = {March},
	journal = {Chemical Physics Letters},
	author = {Habenicht, Anja and Hjelm, Johan and Mukhtar, Emad and Bergström, Fredrik and Johansson, Lennart B Å},
	year = {2002},
	pages = {367--375},
}

@article{noauthor_ncomms3207-s17_nodate,
	title = {ncomms3207-s17},
}

@article{ying_green_2007,
	title = {Green autofluorescence in dinoflagellates, diatoms, and other microalgae and its implications for vital staining and morphological studies},
	volume = {73},
	issn = {0099-2240 (Print)},
	doi = {10.1128/AEM.01741-06},
	abstract = {Green autofluorescence (GAF) has been described in the short flagellum of golden and brown algae, the stigma of Euglenophyceae, and cytoplasm of different life stages of dinoflagellates and is considered by some researchers a valuable taxonomic feature for dinoflagellates. In addition, green fluorescence staining has been widely proposed or adopted to measure cell viability (or physiological state) in areas such as apoptosis of phytoplankton, pollutant stresses on algae, metabolic activity of algae, and testing treatment technologies for ships' ballast water. This paper reports our epifluorescence microscopic observations and quantitative spectrometric measurements of GAF in a broad phylogenetic range of microalgae. Our results demonstrate GAF is a common feature of dinoflagellates, diatoms, green algae, cyanobacteria, and raphidophytes, occurs in the cytoplasm and particularly in eyespots, accumulation bodies, spines, and aerotopes, and is caused by molecules other than chlorophyll. GAF intensity increased with time after cell death or fixation and with excitation by blue or UV light and was affected by pH. GAF of microalgae may be only of limited value in taxonomy. It can be strong enough to interfere with the results of green fluorescence staining, particularly when stained samples are observed microscopically. GAF is useful, however, for microscopic study of algal morphology, especially to visualize cellular components such as eyespots, nucleus, aerotopes, spines, and chloroplasts. Furthermore, GAF can be used to visualize and enumerate dinoflagellate cysts in marine and estuarine sediments in the context of anticipating and monitoring harmful algal blooms and in tracking potentially harmful dinoflagellates transported in ships' ballast tanks.},
	number = {7},
	journal = {Applied and Environmental Microbiology},
	author = {Ying, Zhong Tang and Dobbs, Fred C.},
	year = {2007},
	pages = {2306--2313},
}

@article{ryder_timeharp_nodate,
	title = {{TimeHarp} 100 / {PDL} {800B} {Measurement} {Example}: {Measurement} of {Fluorescence} {Lifetime} {Variation} with {pH}},
	author = {Ryder, Courtesy A},
}

@article{becker_lifetime_2002,
	title = {Lifetime imaging with the {Zeiss} {LSM}-510},
	volume = {4620},
	doi = {10.1117/12.470706},
	journal = {Photonics West - BIOS},
	author = {Becker, W and Bergmann, a and Weiss, G},
	year = {2002},
}

@article{static_supporting_nodate,
	title = {Supporting {Appendix}},
	author = {Static, Why and Methods, Sampling and Analyses, Time Course},
	pages = {1--27},
}

@article{padilla-parra_quantitative_2008,
	title = {Quantitative {FRET} analysis by fast acquisition time domain {FLIM} at high spatial resolution in living cells.},
	volume = {95},
	issn = {1542-0086 (Electronic){\textbackslash}n0006-3495 (Linking)},
	doi = {10.1529/biophysj.108.131276},
	abstract = {Quantitative analysis in Förster resonance energy transfer (FRET) experiments in live cells for protein interaction studies is still a challenging issue. In a two-component system (FRET and no FRET donor species), fitting of fluorescence lifetime imaging microscopy (FLIM) data gives the fraction of donor molecules involved in FRET (f(D)) and the intrinsic transfer efficiency. But when fast FLIM acquisitions are used to monitor dynamic changes in protein-protein interactions at high spatial and temporal resolutions in living cells, photon statistics and time resolution are limited. In this case, fitting procedures are not reliable, even for single lifetime donors. We introduce the new concept of a minimal fraction of donor molecules involved in FRET (mf(D)), coming from the mathematical minimization of f(D). We find particular advantage in the use of mf(D) because it can be obtained without fitting procedures and it is derived directly from FLIM data. mf(D) constitutes an interesting quantitative parameter for live cell studies because it is related to the minimal relative concentration of interacting proteins. For multi-lifetime donors, the process of fitting complex fluorescence decays to find at least four reliable lifetimes is a near impossible task. Here, mf(D) extension for multi-lifetime donors is the only quantitative determinant. We applied this methodology for imaging the interaction between the bromodomains of TAF(II250) and acetylated histones H4 in living cells at high resolution. We show the existence of discrete acetylated chromatin domains where the minimal fraction of bromodomain interacting with acetylated H4 oscillates from 0.26 to 0.36 and whose size is smaller than half of one micron cube. We demonstrate that mf(D) by itself is a useful tool to investigate quantitatively protein interactions in live cells, especially when using fast FRET-FLIM acquisition times.},
	number = {September},
	journal = {Biophysical journal},
	author = {Padilla-Parra, Sergi and Audugé, Nicolas and Coppey-Moisan, Maïté and Tramier, Marc},
	year = {2008},
	pages = {2976--2988},
}

@article{chin_fluorescence_2004,
	title = {Fluorescence {Anisotropy} {Assays} for {Analysis} of {ISWI}-{DNA} and {ISWI}-{Nucleosome} {Interactions}},
	volume = {376},
	issn = {0076-6879},
	doi = {10.1016/S0076-6879(03)76001-7},
	number = {2001},
	journal = {Methods in Enzymology},
	author = {Chin, J. and Längst, G. and Becker, P. B. and Widom, J.},
	year = {2004},
	pages = {3--16},
}

@article{biophysiker_untersuchung_2002,
	title = {Untersuchung zur {Mobilität} von {Replikationsproteinen} in lebenden {Zellen}},
	number = {November},
	author = {Biophysiker, Diplom},
	year = {2002},
}

@article{sectioning_super-resolution_nodate,
	title = {Super-{Resolution} {Microscopy}},
	author = {Sectioning, Optical},
	pages = {1--3},
}

@article{biebricher_tracking_2009,
	title = {Tracking of single quantum dot labeled {EcoRV} sliding along {DNA} manipulated by double optical tweezers},
	volume = {96},
	issn = {1542-0086 (Electronic){\textbackslash}n0006-3495 (Linking)},
	doi = {10.1016/j.bpj.2009.01.035},
	abstract = {Fluorescence microscopy provides a powerful method to directly observe single enzymes moving along a DNA held in an extended conformation. In this work, we present results from single EcoRV enzymes labeled with quantum dots which interact with DNA manipulated by double optical tweezers. The application of quantum dots facilitated accurate enzyme tracking without photobleaching whereas the tweezers allowed us to precisely control the DNA extension. The labeling did not affect the biochemical activity of EcoRV checked by directly observing DNA digestion on the single molecule level. We used this system to demonstrate that during sliding, the enzyme stays in close contact with the DNA. Additionally, slight overstretching of the DNA resulted in a significant decrease of the 1D diffusion constant, which suggests that the deformation changes the energy landscape of the sliding interaction. Together with the simplicity of the setup, these results demonstrate that the combination of optical tweezers with fluorescence tracking is a powerful tool for the study of enzyme translocation along DNA. © 2009 by the Biophysical Society.},
	number = {April},
	journal = {Biophysical Journal},
	author = {Biebricher, Andreas and Wende, Wolfgang and Escudé, Christophe and Pingoud, Alfred and Desbiolles, Pierre},
	year = {2009},
	pages = {50--52},
}

@article{heintzmann_correcting_2010,
	title = {Correcting distorted optics: back to the basics.},
	volume = {7},
	issn = {1548-7091},
	url = {http://dx.doi.org/10.1038/nmeth0210-108},
	doi = {10.1038/nmeth0210-108},
	abstract = {A surprisingly simple method provides an effective way of correcting  optical distortions in two-photon fluorescence microscopy and  recovers nearly ideal images of inhomogeneous thick samples.},
	number = {2},
	journal = {Nature methods},
	author = {Heintzmann, Rainer},
	year = {2010},
	note = {Publisher: Nature Publishing Group},
	pages = {108--110},
}

@article{ljosa_annotated_2012,
	title = {Annotated high-throughput microscopy image sets for validation},
	volume = {9},
	issn = {1548-7091},
	url = {http://dx.doi.org/10.1038/nmeth.2083},
	doi = {10.1038/nmeth.2083},
	abstract = {... Metrics for: Annotated  high - throughput  microscopy  image  sets for validation . ... {\textbackslash}n},
	number = {7},
	journal = {Nature Methods},
	author = {Ljosa, Vebjorn and Sokolnicki, Katherine L and Carpenter, Anne E},
	year = {2012},
	note = {Publisher: Nature Publishing Group},
	pages = {637--637},
}

@article{borst_effects_2005,
	title = {Effects of refractive index and viscosity on fluorescence and anisotropy decays of enhanced cyan and yellow fluorescent proteins},
	volume = {15},
	issn = {1053-0509 (Print){\textbackslash}r1053-0509 (Linking)},
	doi = {10.1007/s10895-005-2523-5},
	abstract = {The fluorescence lifetime strongly depends on the immediate environment of the fluorophore. Time-resolved fluorescence measurements of the enhanced forms of ECFP and EYFP in water-glycerol mixtures were performed to quantify the effects of the refractive index and viscosity on the fluorescence lifetimes of these proteins. The experimental data show for ECFP and EYFP two fluorescence lifetime components: one short lifetime of about 1 ns and a longer lifetime of about 3.7 ns of ECFP and for EYFP 3.4. The fluorescence of ECFP is very heterogeneous, which can be explained by the presence of two populations: a conformation (67\% present) where the fluorophore is less quenched than in the other conformation (33\% present). The fluorescence decay of EYFP is much more homogeneous and the amplitude of the short fluorescence lifetime is about 5\%. The fluorescence anisotropy decays show that the rotational correlation time of both proteins scales with increasing viscosity of the solvent similarly as shown earlier for GFP. The rotational correlation times are identical for ECFP and EYFP, which can be expected since both proteins have the same shape and size. The only difference observed is the slightly lower initial anisotropy for ECFP as compared to the one of EYFP.},
	number = {2},
	journal = {Journal of Fluorescence},
	author = {Borst, Jan Willem and Hink, Mark a. and Van Hoek, Arie and Visser, a. J W G},
	year = {2005},
	keywords = {FRET, FLIM, Refractive index, Fluorescence lifetime, ECFP, EYFP, Rotational correlation time, Viscosity},
	pages = {153--160},
}

@article{kirstein_diffusion_2007,
	title = {Diffusion of single molecules in nanoporous mesostructured materials},
	author = {Kirstein, Johanna},
	year = {2007},
}

@article{suhling_imaging_2002-1,
	title = {Imaging the environment of green fluorescent protein.},
	volume = {83},
	issn = {0006-3495 (Print){\textbackslash}r0006-3495 (Linking)},
	doi = {10.1016/S0006-3495(02)75359-9},
	abstract = {An emerging theme in cell biology is that cell surface receptors need to be considered as part of supramolecular complexes of proteins and lipids facilitating specific receptor conformations and distinct distributions, e.g., at the immunological synapse. Thus, a new goal is to develop bioimaging that not only locates proteins in live cells but can also probe their environment. Such a technique is demonstrated here using fluorescence lifetime imaging of green fluorescent protein (GFP). We first show, by time-correlated single-photon counting, that the fluorescence decay of GFP depends on the local refractive index. This is in agreement with the Strickler Berg formula, relating the Einstein A and B coefficients for absorption and spontaneous emission in molecules. We then quantitatively image, by wide-field time-gated fluorescence lifetime imaging, the refractive index of the environment of GFP. This novel approach paves the way for imaging the biophysical environment of specific GFP-tagged proteins in live cells.},
	number = {December},
	journal = {Biophysical journal},
	author = {Suhling, Klaus and Siegel, Jan and Phillips, David and French, Paul M W and Lévêque-Fort, Sandrine and Webb, Stephen E D and Davis, Daniel M},
	year = {2002},
	pages = {3589--3595},
}

@article{noauthor_glucose_2005,
	title = {Glucose {Sensing} and {Glucose} {Determination} {Using} {Glucose} {Oxidase} or {Molecular} {Receptors} , {A}. {Duerkop}, {M}. {Schaeferling}, {O}. {S}. {Wolfbeis},},
	volume = {11},
	number = {c},
	year = {2005},
	pages = {2005--2005},
}

@article{noauthor_petersen-fcs_nodate,
	title = {Petersen-{FCS}},
}

@article{noauthor_screen_nodate,
	title = {screen},
}

@article{lineage_melanocyte_nodate,
	title = {Melanocyte {Lineage} {Specification} {Oncogenic} {Signaling} {Patient} prognosis p g},
	author = {Lineage, Melanocyte},
}

@article{raicu_determination_2009,
	title = {Determination of supramolecular structure and spatial distribution of protein complexes in living cells},
	volume = {3},
	doi = {10.1038/nphoton.2008.291},
	abstract = {Resonant energy transfer from an optically excited donor molecule to a non-excited acceptor molecule residing nearby is widely used to detect molecular interactions in living cells. To date, resonant energy transfer has been used to obtain stoichiometric information, such as the number of proteins forming a complex, for a handful of proteins, but only after performing sequential scans of the emission wavelengths, excitation wavelengths, or sometimes both. During this lengthy process of measurement, the molecular makeup of a cellular region may change, limiting the applicability of resonant energy transfer to the determination of cellular averages. Here, we demonstrate a method for the determination of protein complex size, configuration, and spatial distribution in single living cells. It relies on a spectrally resolved two-photon microscope, a simple but competent theory, and a judicious selection of fluorescent tags. This approach eventually may lead to tracking the dynamics of individual molecular complexes inside living cells},
	number = {February},
	journal = {Nature Photonics},
	author = {Raicu, Valerica and Stoneman, Michael R. and Fung, Russell and Melnichuk, Mike and Jansma, David B. and Pisterzi, Luca F. and Rath, Sasmita and Fox, Michael and Wells, James W. and Saldin, Dilano K.},
	year = {2009},
	pages = {107--113},
}

@article{gratton_fluorescence_2003,
	title = {Fluorescence lifetime imaging for the two-photon microscope: time-domain and frequency-domain methods.},
	volume = {8},
	issn = {1083-3668 (Print){\textbackslash}r1083-3668 (Linking)},
	doi = {10.1117/1.1586704},
	abstract = {Fluorescence lifetime images are obtained with the laser scanning microscope using two methods: the time-correlated single-photon counting method and the frequency-domain method. In the same microscope system, we implement both methods. We perform a comparison of the performance of the two approaches in terms of signal-to-noise ratio (SNR) and the speed of data acquisition. While in our practical implementation the time-correlated single-photon counting technique provides a better SNR for low-intensity images, the frequency-domain method is faster and provides less distortion for bright samples.},
	number = {3},
	journal = {Journal of biomedical optics},
	author = {Gratton, Enrico and Breusegem, Sophie and Sutin, Jason and Ruan, Qiaoqiao and Barry, Nicholas},
	year = {2003},
	keywords = {fluorescence lifetime imaging, frequency domain, time domain, two-},
	pages = {381--390},
}

@article{dertinger_measuring_2006,
	title = {Measuring precise diffusion coefficients with two-focus fluorescence correlation spectroscopy},
	volume = {6092},
	url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Measuring+precise+diffusion+coefficients+with+Two-Focus+Fluorescence+Correlation+Spectroscopy#0},
	doi = {10.1117/12.651053},
	journal = {Ultrasensitive and Single-Molecule Detection Technologies. Edited by Enderlein, Jörg; Gryczynski, Zygmunt K. Proceedings of the SPIE},
	author = {Dertinger, T. and Dertinger, T. and Gregor, I. and Gregor, I. and von Der Hocht, I. and von Der Hocht, I. and Erdmann, R. and Erdmann, R. and Kr{\textbackslash}{\textbackslash}"amer, B. and Kr{\textbackslash}{\textbackslash}"amer, B. and Koberling, F. and Koberling, F. and Hartmann, R. and Hartmann, R. and Enderlein, J. and Enderlein, J.},
	year = {2006},
	keywords = {fcs, diffusion coefficient, two focus fcs},
	pages = {6--10--6--10},
}

@article{bourgeois_femtosecond_2007,
	title = {Femtosecond laser nanoaxotomy properties and their effect on axonal recovery in {C}. elegans.},
	volume = {15},
	issn = {1094-4087 (Electronic)},
	doi = {10.1364/OE.16.005963},
	abstract = {We present a study characterizing the properties of femtosecond laser nanosurgery applied to individual axons in live Caenorhabditis elegans (C. elegans) using nano-Joule laser pulses at 1 kHz repetition rate. Emphasis is placed on the characterization of the damage threshold, the extent of damage, and the statistical rates of axonal recovery as a function of laser parameters. The ablation threshold decreases with increasing number of pulses applied during nanoaxotomy. This dependency suggests the existence of an incubation effect. In terms of extent of damage, the energy per pulse is found to be a more critical parameter than the number of pulses. Axonal recovery improves when surgery is performed using a large number of low energy pulses.},
	journal = {Optics express},
	author = {Bourgeois, Frederic and Ben-Yakar, Adela},
	year = {2007},
	pages = {8521--8531},
}

@article{warr_scientific_2012,
	title = {Scientific workflow systems: {Pipeline} {Pilot} and {KNIME}},
	volume = {26},
	issn = {0920-654X},
	doi = {10.1007/s10822-012-9577-7},
	journal = {Journal of Computer-Aided Molecular Design},
	author = {Warr, Wendy a.},
	year = {2012},
	pages = {801--804},
}

@article{owen_fluorescence_2006,
	title = {Fluorescence lifetime imaging provides enhanced contrast when imaging the phase-sensitive dye di-4-{ANEPPDHQ} in model membranes and live cells.},
	volume = {90},
	issn = {0006-3495},
	doi = {10.1529/biophysj.106.084673},
	abstract = {We apply fluorescence lifetime imaging to the membrane phase-sensing dye di-4-ANEPPDHQ in model membranes and live cells. We show that the 1700 ps lifetime shift between liquid-disordered and liquid-ordered phases offers greater contrast than the 60 nm spectral shift previously reported. Detection of cholesterol-rich membrane microdomains is confirmed by observation of the temperature dependence of membrane order and by cholesterol depletion using methyl-beta-cyclodextrin.},
	journal = {Biophysical journal},
	author = {Owen, Dylan M and Lanigan, Peter M P and Dunsby, Christopher and Munro, Ian and Grant, David and Neil, Mark a a and French, Paul M W and Magee, Anthony I},
	year = {2006},
	pages = {L80--L82},
}

@article{noauthor_ncomms3207-s3_nodate,
	title = {ncomms3207-s3},
}

@article{antonik_separating_2006,
	title = {Separating structural heterogeneities from stochastic variations in fluorescence resonance energy transfer distributions via photon distribution analysis},
	volume = {110},
	issn = {1520-6106},
	doi = {10.1021/jp057257+},
	abstract = {We establish a probability distribution analysis (PDA) method for the analysis of fluorescence resonance energy transfer (FRET) signals to determine with high precision the originating value of a shot-noise-limited signal distribution. PDA theoretical distributions are calculated explicitly including crosstalk, stochastic variations, and background and represent the minimum width that a FRET distribution must have. In this way an unambiguous distinction is made between shot-noise distributions and distributions broadened by heterogeneities. This method simultaneously and effectively extracts highly resolved information from FRET distributions. The theoretical histograms match the exact profile of histograms generated from constant transfer efficiency experimental data with a chi2 near unity. The chi2 surface suggests an ultimate level of precision with FRET of {\textless} 1\% of the Förster radius. Distributions of FRET signals in donor-acceptor-labeled DNA were unambiguously identified as being broader than shot-noise variations could explain. A model describing a Gaussian distribution of distances was tested with the PDA method and demonstrated 5 A inhomogeneities due to dye motions. The capability of this method to recover quantitative information from FRET distributions has potential applications for studying molecular conformations and dynamics. Potential sources for artifacts such as acceptor photobleaching, spectrally different observation volumes, and fluctuations of the Förster radius are ruled out.},
	journal = {Journal of Physical Chemistry B},
	author = {Antonik, Matthew and Felekyan, Suren and Gaiduk, Alexander and Seidel, C. a M},
	year = {2006},
	pages = {6970--6978},
}

@article{peterman_fluorescence_1998,
	title = {Fluorescence and absorption spectroscopy of the weakly fluorescent chlorophyll a in cytochrome b6f of {Synechocystis} {PCC6803}.},
	volume = {75},
	issn = {0006-3495 (Print)},
	doi = {10.1016/S0006-3495(98)77523-X},
	abstract = {A spectroscopic characterization of the chlorophyll a (Chl) molecule in the monomeric cytochrome b6f complex (Cytb6f) isolated from the cyanobacterium Synechocystis PCC6803 is presented. The fluorescence lifetime and quantum yield have been determined, and it is shown that Chl in Cytb6f has an excited-state lifetime that is 20 times smaller than that of Chl in methanol. This shortening of the Chl excited state lifetime is not caused by an increased rate of intersystem crossing. Most probably it is due to quenching by a nearby amino acid. It is suggested that this quenching is a mechanism for preventing the formation of Chl triplets, which can lead to the formation of harmful singlet oxygen. Using site-selected fluorescence spectroscopy, detailed information on vibrational frequencies in both the ground and Qy excited states has been obtained. The vibrational frequencies indicate that the Chl molecule has one axial ligand bound to its central magnesium and accepts a hydrogen bond to its 13(1)-keto carbonyl. The results show that the Chl binds to a well-defined pocket of the protein and experiences several close contacts with nearby amino acids. From the site-selected fluorescence spectra, it is further concluded that the electron-phonon coupling is moderately strong. Simulations of both the site-selected fluorescence spectra and the temperature dependence of absorption and fluorescence spectra are presented. These simulations indicate that the Huang-Rhys factor characterizing the electron-phonon coupling strength is between 0.6 and 0.9. The width of the Gaussian inhomogeneous distribution function is 210 +/- 10 cm-1.},
	number = {July},
	journal = {Biophysical journal},
	author = {Peterman, E J and Wenk, S O and Pullerits, T and Pâlsson, L O and van Grondelle, R and Dekker, J P and Rögner, M and van Amerongen, H},
	year = {1998},
	pages = {389--398},
}

@article{buschmann_spectroscopic_2003,
	title = {Spectroscopic study and evaluation of red-absorbing fluorescent dyes.},
	volume = {14},
	issn = {4962215442},
	doi = {10.1021/bc025600x},
	abstract = {The spectroscopic characteristics (absorption, emission, and fluorescence lifetime) of 13 commercially available red-absorbing fluorescent dyes were studied under a variety of conditions. The dyes included in this study are Alexa647, ATTO655, ATTO680, Bodipy630/650, Cy5, Cy5.5, DiD, DY-630, DY-635, DY-640, DY-650, DY-655, and EVOblue30. The thorough characterization of this class of dyes will facilitate selection of the appropriate red-absorbing fluorescent labels for applications in fluorescence assays. The influences of polarity, viscosity, and the addition of detergent (Tween20) on the spectroscopic properties were investigated, and fluorescence correlation spectroscopy (FCS) was utilized to assess the photophysical properties of the dyes under high excitation conditions. The dyes can be classified into groups based on the results presented. For example, while the fluorescence quantum yield of ATTO655, ATTO680, and EVOblue30 is primarily controlled by the polarity of the surrounding medium, more hydrophobic and structurally flexible dyes of the DY-family are strongly influenced by the viscosity of the medium and the addition of detergents. Covalent binding of the dyes to biotin and subsequent addition of streptavidin results in reversible fluorescence quenching or changes in the relaxation time of other photophysical processes of some dyes, most likely due to interactions with tryptophan residues in the streptavin binding site.},
	journal = {Bioconjugate chemistry},
	author = {Buschmann, Volker and Weston, Kenneth D and Sauer, Markus},
	year = {2003},
	pages = {195--204},
}

@article{noauthor_archiveact_nodate,
	title = {{archiveACT}},
}

@article{gadella_glycolipid_1995,
	title = {Glycolipid migration from the apical to the equatorial subdomains of the sperm head plasma membrane precedes the acrosome reaction. {Evidence} for a primary capacitation event in boar spermatozoa.},
	volume = {108 ( Pt 3},
	issn = {0021-9533 (Print). 0021-9533 (Linking)},
	abstract = {In order to extend the static information of immunolabelling sulphogalactolipids in fixed boar spermatozoa, a fluorescent sulphogalactolipid analogue, galactose(3-sulphate)-beta 1-1'[(N-lissamine rhodaminyl)-12-aminodode-canoyl]-sphingosine, was incorporated into plasma membranes of living spermatozoa and its lateral distribution over the sperm head was studied. The fluorescent lipid was enriched in the apical ridge subdomain of freshly ejaculated sperm cells. After sperm binding to the zona pellucida the lipid redistributed to the equatorial segment of the sperm surface. A similar shift occurred during capacitation in vitro with 2 mM CaCl2 or with 4\% (w/v) bovine serum albumin. The desulphated derivative galactose-beta 1-1'[(N-lissamine rhodaminyl)-12-aminododecanoyl]-sphingosine was also incorporated into the plasma membrane of freshly ejaculated sperm cells and clearly stained the apical ridge subdomain and the (pre)-equatorial subdomains of the sperm heads. The desulphogalactolipid analogue showed a slightly faster migration to the equatorial segment of the sperm plasma membrane than did its sulphated counterpart. The measured fluorescence intensity distributions correlated linearly with the spatial probe distribution, which was checked by fluorescence lifetime imaging microscopy. The observed migration of the incorporated glycolipids precedes the acrosome reaction and is one of the underlying molecular events likely to be important in the process of sperm capacitation. The results of this study suggest that lipid phase segregation is an important driving force for the organization of the sperm head plasma membrane into subdomains.},
	journal = {Journal of cell science},
	author = {Gadella, B M and Lopes-Cardozo, M and van Golde, L M and Colenbrander, B and Gadella, T W},
	year = {1995},
	keywords = {microscopy, fluorescence lifetime imaging, acrosome reaction, dynamics, fluorescent galactolipid, plasma membrane, polarity, sperm capacitation},
	pages = {935--946},
}

@article{noauthor_flim_nodate,
	title = {{FLIM}},
}

@article{rolda_noninvasive_2004,
	title = {Noninvasive {Pigment} {Identi} cation in {Single} {Cells} from {Living} {Phototrophic} {Bio} lms by {Confocal} {Imaging} {Spectro} uorometry},
	volume = {70},
	doi = {10.1128/AEM.70.6.3745},
	number = {6},
	journal = {Microbiology},
	author = {Rolda, M and Thomas, F and Castel, S and Quesada, a and Herna, M},
	year = {2004},
	pages = {3745--3750},
}

@article{haustein_fluorescence_2007,
	title = {Fluorescence correlation spectroscopy: novel variations of an established technique.},
	volume = {36},
	issn = {1056-8700{\textbackslash}r1545-4266},
	doi = {10.1146/annurev.biophys.36.040306.132612},
	abstract = {Fluorescence correlation spectroscopy (FCS) is one of the major biophysical techniques used for unraveling molecular interactions in vitro and in vivo. It allows minimally invasive study of dynamic processes in biological specimens with extremely high temporal and spatial resolution. By recording and correlating the fluorescence fluctuations of single labeled molecules through the exciting laser beam, FCS gives information on molecular mobility and photophysical and photochemical reactions. By using dual-color fluorescence cross-correlation, highly specific binding studies can be performed. These have been extended to four reaction partners accessible by multicolor applications. Alternative detection schemes shift accessible time frames to slower processes (e.g., scanning FCS) or higher concentrations (e.g., TIR-FCS). Despite its long tradition, FCS is by no means dated. Rather, it has proven to be a highly versatile technique that can easily be adapted to solve specific biological questions, and it continues to find exciting applications in biology and medicine.},
	journal = {Annual review of biophysics and biomolecular structure},
	author = {Haustein, Elke and Schwille, Petra},
	year = {2007},
	keywords = {single molecule, excitation, autocorrelation, cross-correlation, fluctuation, multiphoton excitation, one-photon},
	pages = {151--169},
}

@article{wiedenmann_far-red_2002,
	title = {A far-red fluorescent protein with fast maturation and reduced oligomerization tendency from {Entacmaea} quadricolor ({Anthozoa}, {Actinaria}).},
	volume = {99},
	issn = {0027-8424 (Print){\textbackslash}n0027-8424 (Linking)},
	doi = {10.1073/pnas.182157199},
	abstract = {We performed the biochemical and biophysical characterization of a red fluorescent protein, eqFP611, from the sea anemone Entacmaea quadricolor cloned in Escherichia coli. With an excitation maximum at 559 nm and an emission maximum at 611 nm, the recombinant protein shows the most red-shifted emission and the largest Stokes shift of all nonmodified proteins in the green fluorescent protein family. The protein fluoresces with a high quantum yield of 0.45, although it resembles the nonfluorescent members of this protein class, as inferred from the absence of the key amino acid serine at position 143. Fluorescence is constant within the range pH 4-10. Red fluorophore maturation reaches a level of 90\% after approximately 12 h by passing through a green intermediate. After complete maturation, only a small fraction of the green species (less than 1\%) persists. The protein has a reduced tendency to oligomerize, as shown by its monomeric appearance in SDS/PAGE analysis and single-molecule experiments. However, it forms tetramers at higher concentrations in the absence of detergent. Fluorescence correlation spectroscopy reveals light-driven transitions between bright and dark states on submillisecond and millisecond time scales. Applicability of eqFP611 for in vivo labeling in eukaryotic systems was shown by expression in a mammalian cell culture.},
	number = {18},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Wiedenmann, Jörg and Schenk, Andreas and Röcker, Carlheinz and Girod, Andreas and Spindler, Klaus-Dieter and Nienhaus, G Ulrich},
	year = {2002},
	pages = {11646--11651},
}

@article{static_supporting_nodate-1,
	title = {Supporting {Appendix}},
	author = {Static, Why and Methods, Sampling and Analyses, Time Course},
	pages = {1--27},
}

@article{noauthor_ncomms3207-s2_nodate,
	title = {ncomms3207-s2},
}

@article{digman_measuring_2005-2,
	title = {Measuring fast dynamics in solutions and cells with a laser scanning microscope.},
	volume = {89},
	issn = {0006-3495},
	doi = {10.1529/biophysj.105.062836},
	abstract = {Single-point fluorescence correlation spectroscopy (FCS) allows measurements of fast diffusion and dynamic processes in the microsecond-to-millisecond time range. For measurements on living cells, image correlation spectroscopy (ICS) and temporal ICS extend the FCS approach to diffusion times as long as seconds to minutes and simultaneously provide spatially resolved dynamic information. However, ICS is limited to very slow dynamics due to the frame acquisition rate. Here we develop novel extensions to ICS that probe spatial correlations in previously inaccessible temporal windows. We show that using standard laser confocal imaging techniques (raster-scan mode) not only can we reach the temporal scales of single-point FCS, but also have the advantages of ICS in providing spatial information. This novel method, called raster image correlation spectroscopy (RICS), rapidly measures during the scan many focal points within the cell providing the same concentration and dynamic information of FCS as well as information on the spatial correlation between points along the scanning path. Longer time dynamics are recovered from the information in successive lines and frames. We exploit the hidden time structure of the scan method in which adjacent pixels are a few microseconds apart thereby accurately measuring dynamic processes such as molecular diffusion in the microseconds-to-seconds timescale. In conjunction with simulated data, we show that a wide range of diffusion coefficients and concentrations can be measured by RICS. We used RICS to determine for the first time spatially resolved diffusions of paxillin-EGFP stably expressed in CHOK1 cells. This new type of data analysis has a broad application in biology and it provides a powerful tool for measuring fast as well as slower dynamic processes in cellular systems using any standard laser confocal microscope.},
	number = {August},
	journal = {Biophysical journal},
	author = {Digman, Michelle a and Brown, Claire M and Sengupta, Parijat and Wiseman, Paul W and Horwitz, Alan R and Gratton, Enrico},
	year = {2005},
	pages = {1317--1327},
}

@article{kapusta_time-resolved_2003,
	title = {Time-{Resolved} {Fluorescence} {Anisotropy} {Measurements} {Made} {Simple}},
	volume = {13},
	doi = {10.1023/A:1022995327718},
	abstract = {Time-resolved anisotropy measurements were performed using simple instrumentation with the aim to demonstrate the speed and ease of the experiment. Subsequent data analysis examples involved common, easily adaptable procedures.},
	number = {September},
	journal = {Journal of Fluorescence},
	author = {Kapusta, Peter and Erdmann, Rainer and Ortmann, Uwe and Wahl, Michael},
	year = {2003},
	keywords = {Anisotropy, Light-emitting diode (LED), Polarization, Time-correlated single photon counting (TCSPC), Time-resolved},
	pages = {179--183},
}

@article{sud_wide-field_2008,
	title = {Wide-field time-domain fluorescence lifetime imaging microscopy ({FLIM}): {Molecular} snapshots of metabolic function in biological systems},
	url = {http://books.google.com/books?hl=en&lr=&id=3W8ACzVlneYC&oi=fnd&pg=PR2&dq=WIDE-FIELD+TIME-DOMAIN+FLUORESCENCE+LIFETIME+IMAGING+MICROSCOPY+(FLIM):+MOLECULAR+SNAPSHOTS+OF+METABOLIC+FUNCTION+IN+BIOLOGICAL+SYSTEMS&ots=DpR7XnciMG&sig=stSKD4qre6vqscEJwUenCoK92To},
	author = {Sud, D},
	year = {2008},
}

@article{palo_fluorescence_2002,
	title = {Fluorescence intensity and lifetime distribution analysis: toward higher accuracy in fluorescence fluctuation spectroscopy.},
	volume = {83},
	issn = {0006-3495},
	doi = {10.1016/S0006-3495(02)75195-3},
	abstract = {Fluorescence fluctuation methods such as fluorescence correlation spectroscopy and fluorescence intensity distribution analysis (FIDA) have proven to be versatile tools for studying molecular interactions with single molecule sensitivity. Another well-known fluorescence technique is the measurement of the fluorescence lifetime. Here, we introduce a method that combines the benefits of both FIDA and fluorescence lifetime analysis. It is based on fitting the two-dimensional histogram of the number of photons detected in counting time intervals of given width and the sum of excitation to detection delay times of these photons. Referred to as fluorescence intensity and lifetime distribution analysis (FILDA), the technique distinguishes fluorescence species on the basis of both their specific molecular brightness and the lifetime of the excited state and is also able to determine absolute fluorophore concentrations. The combined information yielded by FILDA results in significantly increased accuracy compared to that of FIDA or fluorescence lifetime analysis alone. In this paper, the theory of FILDA is elaborated and applied to both simulated and experimental data. The outstanding power of this technique in resolving different species is shown by quantifying the binding of calmodulin to a peptide ligand, thus indicating the potential for application of FILDA to similar problems in the life sciences.},
	number = {August},
	journal = {Biophysical journal},
	author = {Palo, Kaupo and Brand, Leif and Eggeling, Christian and Jäger, Stefan and Kask, Peet and Gall, Karsten},
	year = {2002},
	pages = {605--618},
}

@article{cheong_high-content_2010,
	title = {High-content screening in microfluidic devices},
	volume = {5},
	issn = {1746-045X (Electronic){\textbackslash}r1746-0441 (Linking)},
	doi = {10.1517/17460441.2010.495116},
	abstract = {IMPORTANCE OF THE FIELD: Miniaturization is the key to advancing the state of the art in high-content screening (HCS) in order to enable dramatic cost savings through reduced usage of expensive biochemical reagents and to enable large-scale screening on primary cells. Microfluidic technology offers the potential to enable HCS to be performed with an unprecedented degree of miniaturization. AREAS COVERED IN THIS REVIEW: This perspective highlights a real-world example from the authors’ work of HCS assays implemented in a highly miniaturized microfluidic format. The advantages of this technology are discussed, including cost savings, high-throughput screening on primary cells, improved accuracy, the ability to study complex time-varying stimuli, and ease of automation, integration and scaling. WHAT THE READER WILL GAIN: The reader will understand the capabilities of anew microfluidics-based platform for HCS and the advantages it provides over conventional plate-based HCS. TAKE HOME MESSAGE: Microfluidics technology will drive significant advancements and broader usage and applicability of HCS in drug discovery.},
	number = {8},
	journal = {Expert Opinion on Drug Discovery},
	author = {Cheong, Raymond and Paliwal, Saurabh and Levchenko, Andre},
	year = {2010},
	pages = {715--720},
}

@article{ji_adaptive_2010,
	title = {Adaptive optics via pupil segmentation for high-resolution imaging in biological tissues.},
	volume = {7},
	issn = {1548-7105 (Electronic){\textbackslash}n1548-7091 (Linking)},
	doi = {10.1038/nmeth.1411},
	abstract = {Biological specimens are rife with optical inhomogeneities that seriously degrade imaging performance under all but the most ideal conditions. Measuring and then correcting for these inhomogeneities is the province of adaptive optics. Here we introduce an approach to adaptive optics in microscopy wherein the rear pupil of an objective lens is segmented into subregions, and light is directed individually to each subregion to measure, by image shift, the deflection faced by each group of rays as they emerge from the objective and travel through the specimen toward the focus. Applying our method to two-photon microscopy, we could recover near-diffraction-limited performance from a variety of biological and nonbiological samples exhibiting aberrations large or small and smoothly varying or abruptly changing. In particular, results from fixed mouse cortical slices illustrate our ability to improve signal and resolution to depths of 400 microm.},
	journal = {Nature methods},
	author = {Ji, Na and Milkie, Daniel E and Betzig, Eric},
	year = {2010},
	pages = {141--147},
}

@article{granderath_glia_1999,
	title = {Glia development in the embryonic {CNS} of {Drosophila}},
	volume = {9},
	issn = {0959-4388 (Print)},
	doi = {10.1016/S0959-4388(99)00008-2},
	abstract = {Glial cells are pivotal players during the development and function of complex nervous systems. In Drosophila, recent genetic analyses have revealed several genes that control differentiation and function of CNS glial cells and their interactions with neurons can be studied in detail at the CNS midline, where it is essential for the correct establishment of the commissural axon pattern.},
	journal = {Current Opinion in Neurobiology},
	author = {Granderath, Sebastian and Klämbt, Christian},
	year = {1999},
	pages = {531--536},
}

@article{plc_fluorescence_nodate,
	title = {Fluorescence {Lifetime} {Imaging} and {Fluorescence} {Correlation} for {Laser} {Scanning} {Microscopes}},
	journal = {Time},
	author = {Plc, Photonic Solutions and Plc, Photonic Solutions},
}

@article{becker_high_2003-1,
	title = {High resolution {TCSPC} lifetime imaging},
	volume = {4963},
	url = {http://spiedigitallibrary.org/data/Conferences/SPIEP/32137/175_1.pdf},
	doi = {10.1117/12.472866},
	journal = {Proc. …},
	author = {Becker, Wolfgang and Bergmann, Axel and Biskup, Christoph},
	year = {2003},
	pages = {1--10},
}

@article{kettling_real-time_1998,
	title = {Real-time enzyme kinetics monitored by dual-color fluorescence cross-correlation spectroscopy.},
	volume = {95},
	issn = {0027-8424 (Print)},
	doi = {10.1073/pnas.95.4.1416},
	abstract = {A method for sensitively monitoring enzyme kinetics and activities by using dual-color fluorescence cross-correlation spectroscopy is described. This universal method enables the development of highly sensitive and precise assays for real-time kinetic analyses of any catalyzed cleavage or addition reaction, where a chemical linkage is formed or cleaved through an enzyme's action between two fluorophores that can be discriminated spectrally. In this work, a homogeneous assay with restriction endonuclease EcoRI and a 66-bp double-stranded DNA containing the GAATTC recognition site and fluorophores at each 5' end is described. The enzyme activity can be quantified down to the low picomolar range ({\textgreater}1.6 pM) where the rate constants are linearly dependent on the enzyme concentrations over two orders of magnitude. Furthermore, the reactions were monitored on-line at various initial substrate concentrations in the nanomolar range, and the reaction rates were clearly represented by the Michaelis-Menten equation with a KM of 14 +/- 1 nM and a kcat of 4.6 +/- 0.2 min-1. In addition to kinetic studies and activity determinations, it is proposed that enzyme assays based on the dual-color fluorescence cross-correlation spectroscopy will be very useful for high-throughput screening and evolutionary biotechnology.},
	number = {February},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Kettling, U and Koltermann, a and Schwille, P and Eigen, M},
	year = {1998},
	pages = {1416--1420},
}

@article{noauthor_homeact_nodate,
	title = {{homeACT}},
}

@article{goda_high-throughput_2012-1,
	title = {High-throughput single-microparticle imaging flow analyzer},
	volume = {109},
	issn = {0027-8424},
	doi = {10.1073/pnas.1204718109},
	abstract = {Optical microscopy is one of the most widely used diagnostic methods in scientific, industrial, and biomedical applications. However, while useful for detailed examination of a small number ({\textless} 10,000) of microscopic entities, conventional optical microscopy is incapable of statistically relevant screening of large populations ({\textgreater} 100,000,000) with high precision due to its low throughput and limited digital memory size. We present an automated flow-through single-particle optical microscope that overcomes this limitation by performing sensitive blur-free image acquisition and nonstop real-time image-recording and classification of microparticles during high-speed flow. This is made possible by integrating ultrafast optical imaging technology, self-focusing microfluidic technology, optoelectronic communication technology, and information technology. To show the system's utility, we demonstrate high-throughput image-based screening of budding yeast and rare breast cancer cells in blood with an unprecedented throughput of 100,000 particles/s and a record false positive rate of one in a million.},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Goda, K. and Ayazi, a. and Gossett, D. R. and Sadasivam, J. and Lonappan, C. K. and Sollier, E. and Fard, a. M. and Hur, S. C. and Adam, J. and Murray, C. and Wang, C. and Brackbill, N. and Di Carlo, D. and Jalali, B.},
	year = {2012},
	pages = {11630--11635},
}

@article{benninger_fluorescence_2005,
	title = {Fluorescence imaging of two-photon linear dichroism: cholesterol depletion disrupts molecular orientation in cell membranes.},
	volume = {88},
	issn = {0006-3495 (Print)},
	doi = {10.1529/biophysj.104.050096},
	abstract = {The plasma membrane of cells is an ordered environment, giving rise to anisotropic orientation and restricted motion of molecules and proteins residing in the membrane. At the same time as being an organized matrix of defined structure, the cell membrane is heterogeneous and dynamic. Here we present a method where we use fluorescence imaging of linear dichroism to measure the orientation of molecules relative to the cell membrane. By detecting linear dichroism as well as fluorescence anisotropy, the orientation parameters are separated from dynamic properties such as rotational diffusion and homo energy transfer (energy migration). The sensitivity of the technique is enhanced by using two-photon excitation for higher photo-selection compared to single photon excitation. We show here that we can accurately image lipid organization in whole cell membranes and in delicate structures such as membrane nanotubes connecting two cells. The speed of our wide-field imaging system makes it possible to image changes in orientation and anisotropy occurring on a subsecond timescale. This is demonstrated by time-lapse studies showing that cholesterol depletion rapidly disrupts the orientation of a fluorophore located within the hydrophobic region of the cell membrane but not of a surface bound probe. This is consistent with cholesterol having an important role in stabilizing and ordering the lipid tails within the plasma membrane.},
	number = {January},
	journal = {Biophysical journal},
	author = {Benninger, Richard K P and Onfelt, Björn and Neil, Mark a a and Davis, Daniel M and French, Paul M W},
	year = {2005},
	pages = {609--622},
}

@article{noauthor_flim_2010,
	title = {Flim, fret and fcs},
	year = {2010},
}

@article{bowen_single-molecule_2003,
	title = {Single-molecule fluorescence spectroscopy of {TOTO} on poly-{AT} and poly-{GC} {DNA}.},
	volume = {78},
	doi = {10.1562/0031-8655(2003)078<0576:SFSOTO>2.0.CO;2},
	abstract = {Excited state lifetime and amplitude measurements were made on thiazole orange dimer (TOTO), a dimeric DNA-intercalating fluorophore, at single-molecule concentrations. As expected from previous study, the excited state lifetime of TOTO intercalated in DNA is dependent on the sequence of the double-stranded DNA, having values of 2.2 ns in poly-GC DNA and 1.8 ns in poly-AT DNA. The distribution of excited state lifetimes of single molecules of TOTO intercalated into oligonucleotides having varying proportions of poly-GC sequences relative to poly-AT sequences were analyzed as a function of the fraction of poly-GC. By using excited state lifetime distributions from the purely GC and purely AT oligonucleotides as a basis set, it was possible to estimate the GC content of oligonucleotides with intermediate GC composition to within a few percent error. This serves as a model for the analysis of equilibrium binding distributions in DNA using single-molecule methods.},
	journal = {Photochemistry and photobiology},
	author = {Bowen, Benjamin P and Enderlein, Jörg and Woodbury, Neal W},
	year = {2003},
	pages = {576--581},
}

@article{dowling_two-dimensional_1997,
	title = {Two-dimensional fluorescence-lifetime imaging using a 5-{kHz}/110-ps gated image intensifier},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=1027860},
	doi = {10.1117/12.273522},
	abstract = {We report the demonstration of a high temporal resolution fluorescence lifetime imaging (FLIM) system using a time-gated image intensifier to provide whole field FLIM images. The gate width has been optimised to 110 ps, and changes in the environment of a fluorescent phantom, causing lifetime differences of 20 ps, have been detected. Envi- ronmental changes of the fluorescent indicator, Lucifer Yellow, have been sensed by measuring changes in its fluores- cence lifetime when unbound and when bound to the protein albumin.},
	journal = {Proc. SPIE 2980, Advances in Fluorescence Sensing Technology III, 20 (May 7, 1997)},
	author = {Dowling, Keith and Hyde, Sam C. W. and Barry, Nicholas P. and Dainty, Christopher and French, Paul M. W. and Hughes, Alun J. and Lever, M. J. and Dymoke-Bradshaw, Anthony K. L. and Hares, Jonathan D. and Kellett, Paul a.},
	year = {1997},
	keywords = {medical imaging, fluorescence lifetime imaging, all solid state lasers, functional imaging, gated image},
	pages = {20--23},
}

@article{kask_two-dimensional_2000-1,
	title = {Two-{Dimensional} {Fluorescence} {Intensity} {Distribution} {Analysis}: {Theory} and {Applications}},
	volume = {78},
	doi = {10.1016/S0006-3495(00)76722-1},
	number = {January},
	journal = {Biophysical Journal},
	author = {Kask, Peet and Palo, Kaupo and Fay, Nicolas and Brand, Leif and Mets, Ülo and Ullmann, Dirk and Jungmann, Joern and Pschorr, Johannes and Gall, Karsten},
	year = {2000},
	pages = {1703--1713},
}

@article{cai_improved_2013-1,
	title = {Improved tools for the {Brainbow} toolbox.},
	volume = {10},
	issn = {1548-7105 (Electronic){\textbackslash}r1548-7091 (Linking)},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3713494&tool=pmcentrez&rendertype=abstract},
	doi = {10.1038/nmeth.2450},
	abstract = {In the transgenic multicolor labeling strategy called 'Brainbow', Cre-loxP recombination is used to create a stochastic choice of expression among fluorescent proteins, resulting in the indelible marking of mouse neurons with multiple distinct colors. This method has been adapted to non-neuronal cells in mice and to neurons in fish and flies, but its full potential has yet to be realized in the mouse brain. Here we present several lines of mice that overcome limitations of the initial lines, and we report an adaptation of the method for use in adeno-associated viral vectors. We also provide technical advice about how best to image Brainbow-expressing tissue.},
	number = {6},
	journal = {Nature methods},
	author = {Cai, Dawen and Cohen, Kimberly B and Luo, Tuanlian and Lichtman, Jeff W and Sanes, Joshua R},
	year = {2013},
	pages = {540--7},
}

@article{bayle_combination_2008,
	title = {Combination of novel green fluorescent protein mutant {TSapphire} and {DsRed} variant {mOrange} to set up a versatile in planta {FRET}-{FLIM} assay.},
	volume = {148},
	issn = {0032-0889},
	doi = {10.1104/pp.108.117358},
	abstract = {Förster resonance energy transfer (FRET) measurements based on fluorescence lifetime imaging microscopy (FLIM) are increasingly being used to assess molecular conformations and associations in living systems. Reduction in the excited-state lifetime of the donor fluorophore in the presence of an appropriately positioned acceptor is taken as strong evidence of FRET. Traditionally, cyan fluorescent protein has been widely used as a donor fluorophore in FRET experiments. However, given its photolabile nature, low quantum yield, and multiexponential lifetime, cyan fluorescent protein is far from an ideal donor in FRET imaging. Here, we report the application and use of the TSapphire mutant of green fluorescent protein as an efficient donor to mOrange in FLIM-based FRET imaging in intact plant cells. Using time-correlated single photon counting-FLIM, we show that TSapphire expressed in living plant cells decays with lifetime of 2.93 +/- 0.09 ns. Chimerically linked TSapphire and mOrange (with 16-amino acid linker in between) exhibit substantial energy transfer based on the reduction in the lifetime of TSapphire in the presence of the acceptor mOrange. Experiments performed with various genetically and/or biochemically known interacting plant proteins demonstrate the versatility of the FRET-FLIM system presented here in different subcellular compartments tested (cytosol, nucleus, and at plasma membrane). The better spectral overlap with red monomers, higher photostability, and monoexponential lifetime of TSapphire makes it an ideal FRET-FLIM donor to study protein-protein interactions in diverse eukaryotic systems overcoming, in particular, many technical challenges encountered (like autofluorescence of cell walls and fluorescence of pigments associated with photosynthetic apparatus) while studying plant protein dynamics and interactions.},
	number = {September},
	journal = {Plant physiology},
	author = {Bayle, Vincent and Nussaume, Laurent and Bhat, Riyaz a},
	year = {2008},
	pages = {51--60},
}

@article{noauthor_1471-2121-2-8-i1_nodate,
	title = {1471-2121-2-8-i1},
}

@article{nyborg_signal_2006,
	title = {Signal peptide peptidase ({SPP}) dimer formation as assessed by fluorescence lifetime imaging microscopy ({FLIM}) in intact cells.},
	volume = {1},
	issn = {1750-1326},
	doi = {10.1186/1750-1326-1-16},
	abstract = {BACKGROUND: Signal peptide peptidase (SPP) is an intramembrane cleaving protease identified by its cleavage of several type II membrane signal peptides. Conservation of intramembrane active site residues demonstrates that SPP, SPP family members, and presenilins (PSs) make up a family of intramembrane cleaving proteases. Because SPP appears to function without additional protein cofactors, the study of SPP may provide structural insights into the mechanism of intramembrane proteolysis by this biomedically important family of proteins. Previous studies have shown that SPP isolated from cells appears to be a homodimer, but some evidence exists that in vitro SPP may be active as a monomer. We have conducted additional experiments to determine if SPP exists as a monomer or dimer in vivo. RESULTS: Fluorescence lifetime imaging microscopy (FLIM) can be is used to determine intra- or intermolecular interactions by fluorescently labeling epitopes on one or two different molecules. If the donor and acceptor fluorophores are less than 10 nm apart, the donor fluorophore lifetime shortens proportionally to the distance between the fluorophores. In this study, we used two types of fluorescence energy transfer (FRET) pairs; cyan fluorescent protein (CFP) with yellow fluorescent protein (YFP) or Alexa 488 with Cy3 to differentially label the NH2- or COOH-termini of SPP molecules. A cell based SPP activity assay was used to show that all tagged SPP proteins are proteolytically active. Using FLIM we were able to show that the donor fluorophore lifetime of the CFP tagged SPP construct in living cells significantly decreases when either a NH2- or COOH-terminally YFP tagged SPP construct is co-transfected, indicating close proximity between two different SPP molecules. These data were then confirmed in cell lines stably co-expressing V5- and FLAG-tagged SPP constructs. CONCLUSION: Our FLIM data strongly suggest dimer formation between two separate SPP proteins. Although the tagged SPP constructs are expressed throughout the cell, SPP dimer detection by FLIM is seen predominantly at or near the plasma membrane.},
	journal = {Molecular neurodegeneration},
	author = {Nyborg, Andrew C and Herl, Lauren and Berezovska, Oksana and Thomas, Anne V and Ladd, Thomas B and Jansen, Karen and Hyman, Bradley T and Golde, Todd E},
	year = {2006},
	pages = {16--16},
}

@article{petrasek_precise_2008,
	title = {Precise measurement of diffusion coefficients using scanning fluorescence correlation spectroscopy.},
	volume = {94},
	issn = {1542-0086 (Electronic)},
	doi = {10.1529/biophysj.107.108811},
	abstract = {We have implemented scanning fluorescence correlation spectroscopy (sFCS) for precise determination of diffusion coefficients of fluorescent molecules in solution. The measurement volume where the molecules are excited, and from which the fluorescence is detected, was scanned in a circle with radius comparable to its size at frequencies 0.5-2 kHz. The scan radius R, determined with high accuracy by careful calibration, provides the spatial measure required for the determination of the diffusion coefficient D, without the need to know the exact size of the measurement volume. The difficulties in the determination of the measurement volume size have limited the application of standard FCS with fixed measurement volume to relative measurements, where the diffusion coefficient is determined by comparison with a standard. We demonstrate, on examples of several common fluorescent dyes, that sFCS can be used to measure D with high precision without a need for a standard. The correct value of D can be determined in the presence of weak photobleaching, and when the measurement volume size is modified, indicating the robustness of the method. The applicability of the presented implementation of sFCS to biological systems in demonstrated on the measurement of the diffusion coefficient of eGFP in the cytoplasm of HeLa cells. With the help of simulations, we find the optimal value of the scan radius R for the experiment.},
	journal = {Biophysical journal},
	author = {Petrásek, Zdenek and Schwille, Petra},
	year = {2008},
	keywords = {fluorescence correlation spectroscopy, scanning, diffusion coefficient},
	pages = {1437--1448},
}

@article{pick_monitoring_2003,
	title = {Monitoring expression and clustering of the ionotropic {5HT3} receptor in plasma membranes of live biological cells},
	volume = {42},
	issn = {0006-2960},
	doi = {10.1021/bi026576d},
	abstract = {The ionotropic 5HT(3) receptor was expressed in transiently transfected mammalian cells, yielding an unprecedented high concentration of up to 12 million receptors per cell. Receptor traffic in the plasma membrane of live cells was observed continuously over 24 h by fluorescence scanning confocal microscopy. This was possible by using 5HT(3) receptor-specific fluorescent ligands with high binding affinity and low off-rate to pulse label receptors at any time after appearance on the cell surface, and label subsequently those receptors expressed later by another, spectrally distinguishable, high-affinity fluorescent ligand. Having reached a critical cell surface concentration of approximately 3000 receptors/microm(2), the receptors started to aggregate in patches with a 4-fold increased surface concentration. The clusters were constantly delivered from a pool of freshly expressed receptors isotropically distributed within the basolateral region of the cell membrane. From there, they migrated to and accumulated on the apical cell surface approximately 9 h after transfection. Individual clusters grew until they reached a critical size of 1-2 microm when they merged to form with 3-5 microm large macroclusters. Clustered receptors were immobile on the minute time scale but always coexisted with monomeric receptors in the regions surrounding the clusters as revealed by fluorescence correlation spectroscopy. Because the receptor density of 12 000 receptors/microm(2) in the patches is as high as that found in two-dimensional crystals of certain membrane proteins, such patches might be a proper source for direct crystallization of membrane proteins without prior purification.},
	journal = {Biochemistry},
	author = {Pick, Horst and Preuss, Axel K. and Mayer, Michael and Wohland, Thorsten and Hovius, Ruud and Vogel, Horst},
	year = {2003},
	pages = {877--884},
}

@article{zimmermann_spectral_2002,
	title = {Spectral imaging and linear un-mixing enables improved {FRET} efficiency with a novel {GFP2}-{YFP} {FRET} pair},
	volume = {531},
	issn = {0014-5793 (Print){\textbackslash}r0014-5793 (Linking)},
	doi = {10.1016/S0014-5793(02)03508-1},
	abstract = {Spectral variants of the green fluorescent protein (GFP) have been extensively used as reporters to image molecular interactions in living cells by fluorescence resonance energy transfer (FRET). However, those GFP variants which are the most efficient donor acceptor pairs for FRET measurements show a high degree of spectral overlap which has hampered in the past their use in FRET applications. Here we use spectral imaging and subsequent un-mixing to quantitatively separate highly overlapping donor and acceptor emissions in FRET measurements. We demonstrate the method in fixed and living cells using a novel GFP based FRET pair (GFP2-YFP (yellow)), which has an increased FRET efficiency compared to the most commonly used FRET pair consisting of cyan fluorescent protein and YFP. Moreover, GFP2 has its excitation maximum at 396 nm at which the YFP acceptor is excited only below the detection level and thus this FRET pair is ideal for applications involving sensitized emission. © 2002 Federation of European Biochemical Societies. Published by Elsevier Science B.V. All rights reserved.},
	journal = {FEBS Letters},
	author = {Zimmermann, Timo and Rietdorf, Jens and Girod, Andreas and Georget, Virginie and Pepperkok, Rainer},
	year = {2002},
	keywords = {Fluorescence resonance energy transfer, Green fluorescent protein, Protein-protein interaction},
	pages = {245--249},
}

@article{noauthor_cy3_nodate,
	title = {Cy3},
}

@article{sopkova_ca2_2002,
	title = {Ca(2+) and membrane binding to annexin 3 modulate the structure and dynamics of its {N} terminus and domain {III}.},
	volume = {11},
	issn = {0961-8368 (Print){\textbackslash}r0961-8368 (Linking)},
	doi = {10.1110/ps.4230102},
	abstract = {Annexin 3 (ANX A3) represents approximately 1\% of the total protein of human neutrophils and promotes tight contact between membranes of isolated specific granules in vitro leading to their aggregation. Like for other annexins, the primary molecular events of the action of this protein is likely its binding to negatively charged phospholipid membranes in a Ca(2+)-dependent manner, via Ca(2+)-binding sites located on the convex side of the highly conserved core of the molecule. The conformation and dynamics of domain III can be affected by this process, as it was shown for other members of the family. The 20 amino-acid, N-terminal segment of the protein also could be affected and also might play a role in the modulation of its binding to the membranes. The structure and dynamics of these two regions were investigated by fluorescence of the two tryptophan residues of the protein (respectively, W190 in domain III and W5 in the N-terminal segment) in the wild type and in single-tryptophan mutants. By contrast to ANX A5, which shows a closed conformation and a buried W187 residue in the absence of Ca(2+), domain III of ANX A3 exhibits an open conformation and a widely solvent-accessible W190 residue in the same conditions. This is in agreement with the three-dimensional structure of the ANX A3-E231A mutant lacking the bidentate Ca(2+) ligand in domain III. Ca(2+) in the millimolar concentration range provokes nevertheless a large mobility increase of the W190 residue, while interaction with the membranes reduces it slightly. In the N-terminal region, the W5 residue, inserted in the central pore of the protein, is weakly accessible to the solvent and less mobile than W190. Its amplitude of rotation increases upon binding of Ca(2+) and returns to its original value when interacting with membranes. Ca(2+) concentration for half binding of the W5A mutant to negatively charged membranes is approximately 0.5 mM while it increases to approximately 1 mM for the ANX A3 wild type and to approximately 3 mM for the W190 ANX A3 mutant. In addition to the expected perturbation of the W190 environment at the contact surface between the protein and the membrane bilayer, binding of the protein to Ca(2+) and to membranes modulates the flexibility of the ANX A3 hinge region at the opposite of this interface and might affect its membrane permeabilizing properties.},
	journal = {Protein science : a publication of the Protein Society},
	author = {Sopkova, Jana and Raguenes-Nicol, Céline and Vincent, Michel and Chevalier, Anne and Lewit-Bentley, Anita and Russo-Marie, Françoise and Gallay, Jacques},
	year = {2002},
	keywords = {annexin, calcium inter-, conformational change, protein-membrane, tryptophan fluorescence},
	pages = {1613--1625},
}

@article{chirico_molecular_2001,
	title = {Molecular heterogeneity of {O}-acetylserine sulfhydrylase by two-photon excited fluorescence fluctuation spectroscopy.},
	volume = {80},
	doi = {10.1016/S0006-3495(01)76167-X},
	abstract = {O-acetylserine sulfhydrylase, a homo-dimeric enzyme from Salmonella typhimurium, covalently binds one pyridoxal 5'-phosphate molecule per subunit as a fluorescent coenzyme. Different tautomers of the Schiff base between the coenzyme and lysine 41 generate structured absorption and fluorescence spectra upon one-photon excitation. We investigated the protein population heterogeneity by fluorescence correlation spectroscopy and lifetime techniques upon two-photon excitation. We sampled the fluorescence intensity from a small number of molecules (approximately 10) and analyzed the distribution of photon counts to separately determine the number and the fluorescence brightness of excited protein molecules. The changes in the average number of molecules and in the fluorescence brightness with the excitation wavelength indicate the presence of at least two fluorescent species, with two-photon excitation maxima at 660 and 800 nm. These species have been identified as the enolimine and ketoenamine tautomers of the protein-coenzyme internal aldimine. Their relative abundance is estimated to be 4:1, whereas the ratio of their two-photon cross sections is reversed with respect to the single-photon excitation case. Consistent results are obtained from the measurement of the lifetime decays, which are sensitive to the excited-state heterogeneity. At least two components were detected, with lifetimes of approximately 2.5 and 0.5 ns. The lifetimes are very close to the values measured in bulk solutions upon one-photon excitation and attributed to the ketoenamine tautomer and to a dipolar species formed upon proton dissociation in the excited state.},
	number = {April},
	journal = {Biophysical journal},
	author = {Chirico, G and Bettati, S and Mozzarelli, a and Chen, Y and Müller, J D and Gratton, E},
	year = {2001},
	pages = {1973--1985},
}

@article{lamb_enhancing_2005,
	title = {Enhancing the sensitivity of fluorescence correlation spectroscopy by using time-correlated single photon counting.},
	volume = {6},
	abstract = {Fluorescence correlation spectroscopy (FCS) and fluorescence cross-correlation spectroscopy (FCCS) are methods that extract information about a sample from the influence of thermodynamic equilibrium fluctuations on the fluorescence intensity. This method allows dynamic information to be obtained from steady state equilibrium measurements and its popularity has dramatically increased in the last 10 years due to the development of high sensitivity detectors and its combination with confocal microscopy. Using time-correlated single-photon counting (TCSPC) detection and pulsed excitation, information over the duration of the excited state can be extracted and incorporated in the analysis. In this short review, we discuss new methodologies that have recently emerged which incorporated fluorescence lifetime information or TCSPC data in the FCS and FCCS analysis. Time-gated FCS discriminates between which photons are to be incorporated in the analysis dependent upon their arrival time after excitation. This allows for accurate FCS measurements in the presence of fluorescent background, determination of sample homogeneity, and the ability to distinguish between static and dynamic heterogeneities. A similar method, time-resolved FCS can be used to resolve the individual correlation functions from multiple fluorophores through the different fluorescence lifetimes. Pulsed interleaved excitation (PIE) encodes the excitation source into the TCSPC data. PIE can be used to perform dual-channel FCCS with a single detector and allows elimination of spectral cross-talk with dual-channel detection. For samples that undergo fluorescence resonance energy transfer (FRET), quantitative FCCS measurements can be performed in spite of the FRET and the static FRET efficiency can be determined.},
	journal = {Current pharmaceutical biotechnology},
	author = {Lamb, D C and Müller, B K and Bräuchle, C},
	year = {2005},
	keywords = {fluorescence correlation spectroscopy, alternating laser excitation, counting, fluorescence cross-correlation spectroscopy, pulsed interleaved excitation, time correlated single photon, time-resolved fluorescence correlation spectroscop},
	pages = {405--414},
}

@article{noauthor_single_2004,
	title = {single - molecule fluorescence resonance energy transfer {II}},
	number = {5},
	year = {2004},
}

@article{clayton_dynamic_2002-1,
	title = {Dynamic fluorescence anisotropy imaging microscopy in the frequency domain ({rFLIM}).},
	volume = {83},
	issn = {4955120113},
	doi = {10.1016/S0006-3495(02)73932-5},
	abstract = {We describe a novel variant of fluorescence lifetime imaging microscopy (FLIM), denoted anisotropy-FLIM or rFLIM, which enables the wide-field measurement of the anisotropy decay of fluorophores on a pixel-by-pixel basis. We adapted existing frequency-domain FLIM technology for rFLIM by introducing linear polarizers in the excitation and emission paths. The phase delay and intensity ratios (AC and DC) between the polarized components of the fluorescence signal are recorded, leading to estimations of rotational correlation times and limiting anisotropies. Theory is developed that allows all the parameters of the hindered rotator model to be extracted from measurements carried out at a single modulation frequency. Two-dimensional image detection with a sensitive CCD camera provides wide-field imaging of dynamic depolarization with parallel interrogation of different compartments of a complex biological structure such as a cell. The concepts and technique of rFLIM are illustrated with a fluorophore-solvent (fluorescein-glycerol) system as a model for isotropic rotational dynamics and with bacteria expressing enhanced green fluorescent protein (EGFP) exhibiting depolarization due to homotransfer of electronic excitation energy (emFRET). The frequency-domain formalism was extended to cover the phenomenon of emFRET and yielded data consistent with a concentration depolarization mechanism resulting from the high intracellular concentration of EGFP. These investigations establish rFLIM as a powerful tool for cellular imaging based on rotational dynamics and molecular proximity.},
	number = {September 2002},
	journal = {Biophysical journal},
	author = {Clayton, Andrew H a and Hanley, Quentin S and Arndt-Jovin, Donna J and Subramaniam, Vinod and Jovin, Thomas M},
	year = {2002},
	pages = {1631--1649},
}

@article{noauthor_spacer_nodate-1,
	title = {spacer},
}

@article{noauthor_1005757458_nodate,
	title = {1005757458},
}

@article{carpenter_call_2012,
	title = {A call for bioimaging software usability},
	volume = {9},
	issn = {1548-7105 (Electronic){\textbackslash}n1548-7091 (Linking)},
	url = {http://dx.doi.org/10.1038/nmeth.2073},
	doi = {10.1038/nmeth.2073},
	abstract = {Bioimaging software developed in a research setting often is not widely used by the scientific community. We suggest that, to maximize both the public's and researchers' investments, usability should be a more highly valued goal. We describe specific characteristics of usability toward which bioimaging software projects should aim.},
	number = {7},
	journal = {Nature Methods},
	author = {Carpenter, Anne E and Kamentsky, Lee and Eliceiri, Kevin W},
	year = {2012},
	note = {Publisher: Nature Publishing Group},
	pages = {666--670},
}

@article{cha_structural_1998,
	title = {Structural implications of fluorescence quenching in the {Shaker} {K}+ channel.},
	volume = {112},
	issn = {3107949612},
	abstract = {When attached to specific sites near the S4 segment of the nonconducting (W434F) Shaker potassium channel, the fluorescent probe tetramethylrhodamine maleimide undergoes voltage-dependent changes in intensity that correlate with the movement of the voltage sensor (Mannuzzu, L.M., M.M. Moronne, and E.Y. Isacoff. 1996. Science. 271:213-216; Cha, A., and F. Bezanilla. 1997. Neuron. 19:1127-1140). The characteristics of this voltage-dependent fluorescence quenching are different in a conducting version of the channel with a different pore substitution (T449Y). Blocking the pore of the T449Y construct with either tetraethylammonium or agitoxin removes a fluorescence component that correlates with the voltage dependence but not the kinetics of ionic activation. This pore-mediated modulation of the fluorescence quenching near the S4 segment suggests that the fluorophore is affected by the state of the external pore. In addition, this modulation may reflect conformational changes associated with channel opening that are prevented by tetraethylammonium or agitoxin. Studies of pH titration, collisional quenchers, and anisotropy indicate that fluorophores attached to residues near the S4 segment are constrained by a nearby region of protein. The mechanism of fluorescence quenching near the S4 segment does not involve either reorientation of the fluorophore or a voltage-dependent excitation shift and is different from the quenching mechanism observed at a site near the S2 segment. Taken together, these results suggest that the extracellular portion of the S4 segment resides in an aqueous protein vestibule and is influenced by the state of the external pore.},
	number = {October},
	journal = {The Journal of general physiology},
	author = {Cha, a and Bezanilla, F},
	year = {1998},
	pages = {391--408},
}

@article{noauthor_homebrew_wll_system_with_anisotropy_very_interestingpdf_nodate,
	title = {Homebrew\_WLL\_system\_with\_anisotropy\_VERY\_Interesting.pdf},
}

@article{kapanidis_fluorescence-aided_2004,
	title = {Fluorescence-aided molecule sorting: analysis of structure and interactions by alternating-laser excitation of single molecules.},
	volume = {101},
	issn = {0027-8424 (Print){\textbackslash}r0027-8424 (Linking)},
	doi = {10.1073/pnas.0401690101},
	abstract = {We use alternating-laser excitation to achieve fluorescence-aided molecule sorting (FAMS) and enable simultaneous analysis of biomolecular structure and interactions at the level of single molecules. This was performed by labeling biomolecules with fluorophores that serve as donor-acceptor pairs for Förster resonance energy transfer, and by using alternating-laser excitation to excite directly both donors and acceptors present in single diffusing molecules. Emissions were reduced to the distance-dependent ratio E, and a distance-independent, stoichiometry-based ratio S. Histograms of E and S sorted species based on the conformation and association status of each species. S was sensitive to the stoichiometry and relative brightness of fluorophores in single molecules, observables that can monitor oligomerization and local-environment changes, respectively. FAMS permits equilibrium and kinetic analysis of macromolecule-ligand interactions; this was validated by measuring equilibrium and kinetic dissociation constants for the interaction of Escherichia coli catabolite activator protein with DNA. FAMS is a general platform for ratiometric measurements that report on structure, dynamics, stoichiometries, environment, and interactions of diffusing or immobilized molecules, thus enabling detailed mechanistic studies and ultrasensitive diagnostics.},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Kapanidis, Achillefs N and Lee, Nam Ki and Laurence, Ted a and Doose, Sören and Margeat, Emmanuel and Weiss, Shimon},
	year = {2004},
	pages = {8936--8941},
}

@article{noauthor_taking_nodate,
	title = {Taking pictures with the {Loomar}},
}

@article{godin_revealing_2011,
	title = {Revealing protein oligomerization and densities in situ using spatial intensity distribution analysis.},
	volume = {108},
	issn = {1091-6490 (Electronic){\textbackslash}r0027-8424 (Linking)},
	doi = {10.1073/pnas.1018658108},
	abstract = {Measuring protein interactions is key to understanding cell signaling mechanisms, but quantitative analysis of these interactions in situ has remained a major challenge. Here, we present spatial intensity distribution analysis (SpIDA), an analysis technique for image data obtained using standard fluorescence microscopy. SpIDA directly measures fluorescent macromolecule densities and oligomerization states sampled within single images. The method is based on fitting intensity histograms calculated from images to obtain density maps of fluorescent molecules and their quantal brightness. Because spatial distributions are acquired by imaging, SpIDA can be applied to the analysis of images of chemically fixed tissue as well as live cells. However, the technique does not rely on spatial correlations, freeing it from biases caused by subcellular compartmentalization and heterogeneity within tissue samples. Analysis of computer-based simulations and immunocytochemically stained GABA(B) receptors in spinal cord samples shows that the approach yields accurate measurements over a broader range of densities than established procedures. SpIDA is applicable to sampling within small areas (6 μm(2)) and reveals the presence of monomers and dimers with single-dye labeling. Finally, using GFP-tagged receptor subunits, we show that SpIDA can resolve dynamic changes in receptor oligomerization in live cells. The advantages and greater versatility of SpIDA over current techniques open the door to quantificative studies of protein interactions in native tissue using standard fluorescence microscopy.},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Godin, Antoine G and Costantino, Santiago and Lorenzo, Louis-Etienne and Swift, Jody L and Sergeev, Mikhail and Ribeiro-da-Silva, Alfredo and De Koninck, Yves and Wiseman, Paul W},
	year = {2011},
	pages = {7010--7015},
}

@article{koltermann_rapid_1998,
	title = {Rapid assay processing by integration of dual-color fluorescence cross-correlation spectroscopy: high throughput screening for enzyme activity.},
	volume = {95},
	issn = {0027-8424 (Print) 0027-8424 (Linking)},
	doi = {10.1073/pnas.95.4.1421},
	abstract = {Dual-color fluorescence cross-correlation spectroscopy (dual-color FCS) has previously been shown to be a suitable tool not only for binding but also for catalytic rate studies. In this work, its application as a rapid method for high-throughput screening (HTS) and evolutionary biotechnology is described. This application is called RAPID FCS (rapid assay processing by integration of dual-color FCS) and does not depend on the characterization of diffusion parameters that is the prerequisite for conventional fluorescence correlation spectroscopy. Dual-color FCS parameters were optimized to achieve the shortest analysis times. A simulated HTS with homogeneous assays for different restriction endonucleases (EcoRI, BamHI, SspI, and HindIII) achieved precise yes-or-no decisions within analysis times of about 1 s per sample. RAPID FCS combines these short analysis times with the development of fast and flexible assays resulting in sensitive, homogeneous fluorescence-based assays, where a chemical linkage between different fluorophores is either cleaved or formed, or where differently labeled molecules interact by noncovalent binding. In principle, assay volumes can be reduced to submicroliters without decreasing the signal strength, making RAPID FCS an ideal tool for ultra-HTS when combined with nanotechnology. RAPID FCS can accurately probe 10(4) to 10(5) samples per day, and possibly more. In addition, this method has the potential to be an efficient tool for selection strategies in evolutionary biotechnology, where rare and specific binding or catalytic properties have to be screened in large numbers of samples.},
	number = {February},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Koltermann, a and Kettling, U and Bieschke, J and Winkler, T and Eigen, M},
	year = {1998},
	pages = {1421--1426},
}

@article{zhong_unsupervised_2012,
	title = {Unsupervised modeling of cell morphology dynamics for time-lapse microscopy},
	volume = {9},
	issn = {1548-7105 (Electronic){\textbackslash}r1548-7091 (Linking)},
	doi = {10.1038/nmeth.2046},
	abstract = {Analysis of cellular phenotypes in large imaging data sets conventionally involves supervised statistical methods, which require user-annotated training data. This paper introduces an unsupervised learning method, based on temporally constrained combinatorial clustering, for automatic prediction of cell morphology classes in time-resolved images. We applied the unsupervised method to diverse fluorescent markers and screening data and validated accurate classification of human cell phenotypes, demonstrating fully objective data labeling in image-based systems biology.},
	number = {7},
	journal = {Nature Methods},
	author = {Zhong, Qing and Busetto, Alberto Giovanni and Fededa, Juan P and Buhmann, Joachim M and Gerlich, Daniel W},
	year = {2012},
	pages = {711--713},
}

@article{gratton_fluorescence_2003-1,
	title = {Fluorescence lifetime imaging for the two-photon microscope: time-domain and frequency-domain methods.},
	volume = {8},
	issn = {1083-3668 (Print){\textbackslash}r1083-3668 (Linking)},
	doi = {10.1117/1.1586704},
	abstract = {Fluorescence lifetime images are obtained with the laser scanning microscope using two methods: the time-correlated single-photon counting method and the frequency-domain method. In the same microscope system, we implement both methods. We perform a comparison of the performance of the two approaches in terms of signal-to-noise ratio (SNR) and the speed of data acquisition. While in our practical implementation the time-correlated single-photon counting technique provides a better SNR for low-intensity images, the frequency-domain method is faster and provides less distortion for bright samples.},
	number = {3},
	journal = {Journal of biomedical optics},
	author = {Gratton, Enrico and Breusegem, Sophie and Sutin, Jason and Ruan, Qiaoqiao and Barry, Nicholas},
	year = {2003},
	keywords = {fluorescence lifetime imaging, frequency domain, time domain, two-},
	pages = {381--390},
}

@article{domingos_characterizing_2009,
	title = {Characterizing {Manufactured} {Nanoparticles} in the {Environment} : {Multimethod} {Determination} of {Particle} {Sizes} {Characterizing} {Manufactured} {Nanoparticles} in the {Environment} : {Multimethod} {Determination} of {Particle} {Sizes}},
	journal = {Environmental Science \& Technology},
	author = {Domingos, Rute F and Baalousha, Mohamed a and Ju-nam, Yon and Reid, M Marcia and Tufenkji, Nathalie and Lead, Jamie R and Leppard, Gary G and Wilkinson, Kevin J},
	year = {2009},
}

@article{goedhart_sensitive_2007-1,
	title = {Sensitive detection of p65 homodimers using red-shifted and fluorescent protein-based {FRET} couples},
	volume = {2},
	issn = {1932-6203 (Electronic){\textbackslash}n1932-6203 (Linking)},
	doi = {10.1371/journal.pone.0001011},
	abstract = {BACKGROUND: Fluorescence Resonance Energy Transfer (FRET) between the green fluorescent protein (GFP) variants CFP and YFP is widely used for the detection of protein-protein interactions. Nowadays, several monomeric red-shifted fluorescent proteins are available that potentially improve the efficiency of FRET. METHODOLOGY/PRINCIPAL FINDINGS: To allow side-by-side comparison of several fluorescent protein combinations for detection of FRET, yellow or orange fluorescent proteins were directly fused to red fluorescent proteins. FRET from yellow fluorescent proteins to red fluorescent proteins was detected by both FLIM and donor dequenching upon acceptor photobleaching, showing that mCherry and mStrawberry were more efficient acceptors than mRFP1. Circular permutated yellow fluorescent protein variants revealed that in the tandem constructs the orientation of the transition dipole moment influences the FRET efficiency. In addition, it was demonstrated that the orange fluorescent proteins mKO and mOrange are both suitable as donor for FRET studies. The most favorable orange-red FRET pair was mKO-mCherry, which was used to detect homodimerization of the NF-kappaB subunit p65 in single living cells, with a threefold higher lifetime contrast and a twofold higher FRET efficiency than for CFP-YFP. CONCLUSIONS/SIGNIFICANCE: The observed high FRET efficiency of red-shifted couples is in accordance with increased Förster radii of up to 64 A, being significantly higher than the Förster radius of the commonly used CFP-YFP pair. Thus, red-shifted FRET pairs are preferable for detecting protein-protein interactions by donor-based FRET methods in single living cells.},
	number = {10},
	journal = {PLoS ONE},
	author = {Goedhart, Joachim and Vermeer, J. E M and Adjobo-Hermans, M. J W and van Weeren, Laura and Gadella, T. W J},
	year = {2007},
	pages = {2--10},
}

@article{miranda-lorenzo_intracellular_2014,
	title = {Intracellular auto-fluorescence – {A} novel and universally inherited biomarker in epithelial cancer stem cells},
	url = {http://dx.doi.org/10.1038/nmeth.3112},
	doi = {10.1038/nmeth.3112},
	number = {July},
	journal = {Nature Methods},
	author = {Miranda-Lorenzo, Jorge; Lonardo, Enza; Alcala, Sonia; Serrano, Alicia G.; Clausell-Tormos, Jenifer; Cioffi, Michele; Megias Diego; Zagorac, Sladjana; Balic, Anamaria; Hidalgo, Manuel; Erkan, Mert; Kleeff, Joerg; Scarpa, Aldo; Sainz, Jr., Br, Christopher., Irene; Dorado},
	year = {2014},
	note = {Publisher: Nature Publishing Group},
	pages = {1--12},
}

@article{rizzo_improved_2004,
	title = {An improved cyan fluorescent protein variant useful for {FRET}.},
	volume = {22},
	issn = {1087-0156},
	doi = {10.1038/nbt945},
	abstract = {Many genetically encoded biosensors use Förster resonance energy transfer (FRET) between fluorescent proteins to report biochemical phenomena in living cells. Most commonly, the enhanced cyan fluorescent protein (ECFP) is used as the donor fluorophore, coupled with one of several yellow fluorescent protein (YFP) variants as the acceptor. ECFP is used despite several spectroscopic disadvantages, namely a low quantum yield, a low extinction coefficient and a fluorescence lifetime that is best fit by a double exponential. To improve the characteristics of ECFP for FRET measurements, we used a site-directed mutagenesis approach to overcome these disadvantages. The resulting variant, which we named Cerulean (ECFP/S72A/Y145A/H148D), has a greatly improved quantum yield, a higher extinction coefficient and a fluorescence lifetime that is best fit by a single exponential. Cerulean is 2.5-fold brighter than ECFP and replacement of ECFP with Cerulean substantially improves the signal-to-noise ratio of a FRET-based sensor for glucokinase activation.},
	number = {4},
	journal = {Nature biotechnology},
	author = {Rizzo, Mark a and Springer, Gerald H and Granada, Butch and Piston, David W},
	year = {2004},
	pages = {445--449},
}

@article{eggeling_analysis_2006,
	title = {Analysis of photobleaching in single-molecule multicolor excitation and {Förster} resonance energy transfer measurements},
	volume = {110},
	issn = {1089-5639{\textbackslash}n1520-5215},
	doi = {10.1021/jp054581w},
	abstract = {Dye photobleaching is a major constraint of fluorescence readout within a range of applications. In this study, we investigated the influence of photobleaching in fluorescence experiments applying multicolor laser as well as Förster resonance energy transfer (FRET) mediated excitation using several red-emitting dyes frequently used in multicolor experiments or as FRET acceptors. The chosen dyes (cyanine 5 (Cy5), MR121, Alexa660, Alexa680, Atto647N, Atto655) have chemically distinct chromophore systems and can be excited at 650 nm. Several fluorescence analysis techniques have been applied to detect photobleaching and to disclose the underlying photophysics, all of which are based on single-molecule detection: (1) fluorescence correlation spectroscopy (FCS) of bulk solutions, (2) fluorescence cross-correlation of single-molecule trajectories, and (3) multiparameter fluorescence detection (MFD) of single-molecule events. The maximum achievable fluorescence signals as well as the survival times of the red dyes were markedly reduced under additional laser irradiation in the range of 500 nm. Particularly at excitation levels at or close to saturation, the 500 nm irradiation effectively induced transitions to higher excited electronic states on already excited dye molecules, leading to a pronounced bleaching reactivity. A theoretical model for the observed laser irradiance dependence of the fluorescence brightness of a Cy5 FRET acceptor dye has been developed introducing the full description of the underlying photophysics. The model takes into account acceptor as well as donor photobleaching from higher excited electronic states, population of triplet states, and energy transfer to both the ground and excited states of the acceptor dye. Also, photoinduced reverse intersystem crossing via higher excited triplet states is included, which was found to be very efficient for Cy5 attached to DNA. Comparing continuous wave (cw) and pulsed donor excitation, a strong enhancement of acceptor photobleaching by a factor of 5 was observed for the latter. Thus, in the case of fluorescence experiments utilizing multicolor pulsed laser excitation, the application of the appropriate timing of synchronized green and red laser pulses in an alternating excitation mode can circumvent excessive photobleaching. Moreover, important new single-molecule analysis diagnosis tools are presented: (1) For the case of excessive acceptor photobleaching, cross-correlation analysis of single-molecule trajectories of the fluorescence signal detected in the donor and acceptor detection channels and vice versa shows an anticorrelated exponential decay and growth, respectively. (2) The time difference, Tg - Tr, of the mean observation times of all photons detected for the donor and acceptor detection channels within a single-molecule fluorescence burst allows one to identify and exclude molecules with an event of acceptor photobleaching. The presented single-molecule analysis methods can be constrained to, for example, FRET-active subpopulations, reducing bias from FRET-inactive molecules. The observations made are of strong relevance for and demand a careful choice of laser action in multicolor and FRET experiments, in particular when performed at or close to saturation.},
	journal = {Journal of Physical Chemistry A},
	author = {Eggeling, Christian and Widengren, Jerker and Brand, Leif and Schaffer, Jörg and Felekyan, Suren and Seidel, Claus a M},
	year = {2006},
	pages = {2979--2995},
}

@article{noauthor_nature_special_synbio_nodate,
	title = {Nature\_Special\_Synbio},
}

@article{baker_author_2012,
	title = {The author file: {Robert} {F}. {Murphy}},
	volume = {9},
	url = {http://dx.doi.org/10.1038/nmeth.2088},
	doi = {10.1038/nmeth.2088},
	number = {7},
	journal = {Nature Methods},
	author = {Baker, Monya},
	year = {2012},
	note = {Publisher: Nature Publishing Group},
	pages = {629--629},
}

@article{noauthor_ncomms3207-s7_nodate,
	title = {ncomms3207-s7},
}

@article{schmid_high-speed_2013,
	title = {High-speed panoramic light-sheet microscopy reveals global endodermal cell dynamics.},
	volume = {4},
	issn = {2041-1723 (Electronic){\textbackslash}n2041-1723 (Linking)},
	url = {http://www.nature.com/ncomms/2013/130725/ncomms3207/full/ncomms3207.html},
	doi = {10.1038/ncomms3207},
	abstract = {The ever-increasing speed and resolution of modern microscopes make the storage and post-processing of images challenging and prevent thorough statistical analyses in developmental biology. Here, instead of deploying massive storage and computing power, we exploit the spherical geometry of zebrafish embryos by computing a radial maximum intensity projection in real time with a 240-fold reduction in data rate. In our four-lens selective plane illumination microscope (SPIM) setup the development of multiple embryos is recorded in parallel and a map of all labelled cells is obtained for each embryo in {\textless}10 s. In these panoramic projections, cell segmentation and flow analysis reveal characteristic migration patterns and global tissue remodelling in the early endoderm. Merging data from many samples uncover stereotypic patterns that are fundamental to endoderm development in every embryo. We demonstrate that processing and compressing raw image data in real time is not only efficient but indispensable for image-based systems biology.},
	journal = {Nature communications},
	author = {Schmid, Benjamin and Shah, Gopi and Scherf, Nico and Weber, Michael and Thierbach, Konstantin and Campos, Citlali Pérez and Roeder, Ingo and Aanstad, Pia and Huisken, Jan},
	year = {2013},
	keywords = {Light, Animals, Body Patterning, Cell Differentiation, Cell Movement, Embryo, Nonmammalian, Endoderm, Endoderm: cytology, Endoderm: embryology, Image Processing, Computer-Assisted, Image Processing, Computer-Assisted: instrumentati, Image Processing, Computer-Assisted: methods, Microscopy, Fluorescence, Microscopy, Fluorescence: instrumentation, Microscopy, Fluorescence: methods, Zebrafish, Zebrafish: anatomy \& histology, Zebrafish: embryology},
	pages = {2207--2207},
}

@article{vercammen_dna-induced_2002,
	title = {{DNA}-induced polymerization of {HIV}-1 integrase analyzed with fluorescence fluctuation spectroscopy},
	volume = {277},
	issn = {0021-9258 (Print){\textbackslash}r0021-9258 (Linking)},
	doi = {10.1074/jbc.M205842200},
	abstract = {Human immunodeficiency virus type 1 (HIV-1) integrase is essential for viral replication. Integrase inserts the viral DNA into the host DNA. We studied the association of integrase to fluorescently labeled oligonucleotides using fluorescence correlation spectroscopy. The binding of integrase to the fluorescent oligonucleotides resulted in the appearance of bright spikes during fluorescence correlation spectroscopy measurements. These spikes arise from the formation of high molecular mass protein-DNA complexes. The fluorescence of the free DNA was separated from the spikes with a statistical method. From the decrease of the concentration of free oligonucleotides, a site association constant was determined. The DNA-protein complexes were formed rapidly in a salt-dependent manner with site association constants ranging between 5 and 40 microm(-1) under different conditions. We also analyzed the kinetics of the DNA-protein complex assembly and the effect of different buffer components. The formation of the fluorescent protein-DNA complex was inhibited by guanosine quartets, and the inhibition constant was determined at 1.8 +/- 0.6 x 10(8) m(-1). Displacement of bound DNA with G-quartets allowed the determination of the dissociation rate constant and proves the reversibility of the association process.},
	journal = {Journal of Biological Chemistry},
	author = {Vercammen, Jo and Maertens, Goedele and Gerard, Melanie and De Clercq, Erik and Debyser, Zeger and Engelborghs, Yves},
	year = {2002},
	pages = {38045--38052},
}

@article{wang_research_nodate,
	title = {Research {Article} {Methods} : {Use} of {Fluorescence} {Polarization} / {Anisotropy} to {Study} {Protein}-{DNA} {Interactions} {Outline} of {Lecture} {Regulation} of {Gene} {Transcription} {Modular} {Structure} of {Transcriptional} {Activators}},
	journal = {Physiological Reviews},
	author = {Wang, Stanley},
	pages = {1--13},
}

@article{edman_memory_2000,
	title = {Memory landscapes of single-enzyme molecules.},
	volume = {97},
	issn = {0027-8424 (Print){\textbackslash}n0027-8424 (Linking)},
	doi = {10.1073/pnas.130589397},
	abstract = {Immobilized single horseradish peroxidase enzymes were observed by confocal fluorescence spectroscopy during catalysis of the oxidation reaction of the nonfluorescent dihydrorhodamine 6G substrate into the highly fluorescent product rhodamine 6G. By extracting only the non-Markovian behavior of the spectroscopic two-state process of enzyme-product complex formation and release, memory landscapes were generated for single-enzyme molecules. The memory landscapes can be used to discriminate between different origins of stretched exponential kinetics that are found in the first-order correlation analysis. Memory landscapes of single-enzyme data shows oscillations that are expected in a single-enzyme system that possesses a set of transient states. Alternative origins of the oscillations may not, however, be ruled out. The data and analysis indicate that substrate interaction with the enzyme selects a set of conformational substates for which the enzyme is active.},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Edman, L and Rigler, R},
	year = {2000},
	pages = {8266--8271},
}

@article{kask_fluorescence-intensity_1999-1,
	title = {Fluorescence-intensity distribution analysis and its application in biomolecular detection technology.},
	volume = {96},
	issn = {0027-8424 (Print){\textbackslash}r0027-8424 (Linking)},
	doi = {10.1073/pnas.96.24.13756},
	abstract = {A methodology, fluorescence-intensity distribution analysis, has been developed for confocal microscopy studies in which the fluorescence intensity of a sample with a heterogeneous brightness profile is monitored. An adjustable formula, modeling the spatial brightness distribution, and the technique of generating functions for calculation of theoretical photon count number distributions serve as the two cornerstones of the methodology. The method permits the simultaneous determination of concentrations and specific brightness values of a number of individual fluorescent species in solution. Accordingly, we present an extremely sensitive tool to monitor the interaction of fluorescently labeled molecules or other microparticles with their respective biological counterparts that should find a wide application in life sciences, medicine, and drug discovery. Its potential is demonstrated by studying the hybridization of 5'-(6-carboxytetramethylrhodamine)-labeled and nonlabeled complementary oligonucleotides and the subsequent cleavage of the DNA hybrids by restriction enzymes.},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Kask, P and Palo, K and Ullmann, D and Gall, K},
	year = {1999},
	pages = {13756--13761},
}

@article{wouters_imaging_2001,
	title = {Imaging biochemistry inside cells},
	volume = {11},
	issn = {0962-8924 (Print){\textbackslash}r0962-8924},
	doi = {10.1016/S0962-8924(01)01982-1},
	abstract = {Proteins provide the building blocks for multicomponent molecular units, or pathways, from which higher cellular functions emerge. These units consist of either assemblies of physically interacting proteins or dispersed biochemical activities connected by rapidly diffusing second messengers, metabolic intermediates, ions or other proteins. It will probably remain within the realm of genetics to identify the ensemble of proteins that constitute these functional units and to establish the first-order connectivity. The dynamics of interactions within these protein machines can be assessed in living cells by the application of fluorescence spectroscopy on a microscopic level, using fluorescent proteins that are introduced within these functional units. Fluorescence is sensitive, specific and non-invasive, and the spectroscopic properties of a fluorescent probe can be analysed to obtain information on its molecular environment. The development and use of sensors based on the genetically encoded variants of green-fluorescent proteins has facilitated the observation of 'live' biochemistry on a microscopic level, with the advantage of preserving the cellular context of biochemical connectivity, compartmentalization and spatial organization. Protein activities and interactions can be imaged and localized within a single cell, allowing correlation with phenomena such as the cell cycle, migration and morphogenesis.},
	number = {5},
	journal = {Trends in Cell Biology},
	author = {Wouters, Fred S. and Verveer, Peter J. and Bastiaens, P. I H},
	year = {2001},
	pages = {203--211},
}

@article{noauthor_tocact_nodate,
	title = {{tocACT}},
}

@article{screening_emerging_2006,
	title = {Emerging {Importance} of {Novel}},
	author = {Screening, High Content},
	year = {2006},
}

@article{noauthor_paul_french_talk_leica_scientific_forum_2008pdf_nodate,
	title = {Paul\_French\_Talk\_Leica\_Scientific\_Forum\_2008.pdf},
}

@article{maus_new_2001-1,
	title = {New picosecond laser system for easy tunability over the whole ultraviolet/visible/near infrared wavelength range based on flexible harmonic generation and optical parametric oscillation},
	volume = {72},
	issn = {0034-6748},
	doi = {10.1063/1.1326930},
	abstract = {A new laser-based and time-correlated single photon counting (TCSPC) detection system which allows easy and fast tuning of excitation wavelengths over a broad range from 240 to 1300 nm, with small gaps from 335 to 360 nm and 660 to 720 nm, has been built. The unique combination of a mode-locked Ti:sapphire laser, an optical parametric oscillator, pulse selectors, and harmonic generators delivers ultrafast laser pulses (1-2 ps) with variable repetition rates and excitation wavelengths. Performance characteristics of the laser system at different excitation wavelengths are reported and the TCSPC setup, which is characterized by a total instrument response function of 25 ps full width at half maximum, is described. Typical TCSPC measurements demonstrate the capability of the system of deriving decay or species associated excitation spectra. (C) 2001 American Institute of Physics.},
	number = {1},
	journal = {Review of Scientific Instruments},
	author = {Maus, Michael and Rousseau, Els and Cotlet, Mircea and Schweitzer, Gerd and Hofkens, Johan and Van Der Auweraer, Mark and De Schryver, Frans C. and Krueger, Arnd},
	year = {2001},
	pages = {36--40},
}

@article{maxwell_chlorophyll_2000,
	title = {Chlorophyll fluorescence--a practical guide.},
	volume = {51},
	issn = {0022-0957},
	doi = {10.1093/jexbot/51.345.659},
	abstract = {Chlorophyll fluorescence analysis has become one of the most powerful and widely used techniques available to plant physiologists and ecophysiologists. This review aims to provide an introduction for the novice into the methodology and applications of chlorophyll fluorescence. After a brief introduction into the theoretical background of the technique, the methodology and some of the technical pitfalls that can be encountered are explained. A selection of examples is then used to illustrate the types of information that fluorescence can provide.},
	number = {345},
	journal = {Journal of experimental botany},
	author = {Maxwell, K and Johnson, G N},
	year = {2000},
	keywords = {chlorophyll fluorescence, electron transport},
	pages = {659--668},
}

@article{korn_cell-based_2007,
	title = {Cell-based high-content screening of small-molecule libraries},
	volume = {11},
	issn = {1367-5931 (Print){\textbackslash}r1367-5931 (Linking)},
	doi = {10.1016/j.cbpa.2007.08.030},
	abstract = {Advanced microscopy and the corresponding image analysis have been developed in recent years into a powerful tool for studying molecular and morphological events in cells and tissues. Cell-based high-content screening (HCS) is an upcoming methodology for the investigation of cellular processes and their alteration by multiple chemical or genetic perturbations. Multiparametric characterization of responses to such changes can be analyzed using intact live cells as reporter. These disturbances are screened for effects on a variety of molecular and cellular targets, including subcellular localization and redistribution of proteins. In contrast to biochemical screening, they detect the responses within the context of the intercellular structural and functional networks of normal and diseased cells, respectively. As cell-based HCS of small-molecule libraries is applied to identify and characterize new therapeutic lead compounds, large pharmaceutical companies are major drivers of the technology and have already shown image-based screens using more than 100 000 compounds. ?? 2007 Elsevier Ltd. All rights reserved.},
	journal = {Current Opinion in Chemical Biology},
	author = {Korn, Kerstin and Krausz, Eberhard},
	year = {2007},
	pages = {503--510},
}

@article{rizzo_improved_2004-1,
	title = {An improved cyan fluorescent protein variant useful for {FRET}.},
	volume = {22},
	issn = {1087-0156},
	doi = {10.1038/nbt945},
	abstract = {Many genetically encoded biosensors use Förster resonance energy transfer (FRET) between fluorescent proteins to report biochemical phenomena in living cells. Most commonly, the enhanced cyan fluorescent protein (ECFP) is used as the donor fluorophore, coupled with one of several yellow fluorescent protein (YFP) variants as the acceptor. ECFP is used despite several spectroscopic disadvantages, namely a low quantum yield, a low extinction coefficient and a fluorescence lifetime that is best fit by a double exponential. To improve the characteristics of ECFP for FRET measurements, we used a site-directed mutagenesis approach to overcome these disadvantages. The resulting variant, which we named Cerulean (ECFP/S72A/Y145A/H148D), has a greatly improved quantum yield, a higher extinction coefficient and a fluorescence lifetime that is best fit by a single exponential. Cerulean is 2.5-fold brighter than ECFP and replacement of ECFP with Cerulean substantially improves the signal-to-noise ratio of a FRET-based sensor for glucokinase activation.},
	number = {4},
	journal = {Nature biotechnology},
	author = {Rizzo, Mark a and Springer, Gerald H and Granada, Butch and Piston, David W},
	year = {2004},
	pages = {445--449},
}

@article{jose_photophysics_2007,
	title = {Photophysics of {Clomeleon} by {FLIM}: discriminating excited state reactions along neuronal development.},
	volume = {92},
	issn = {0006-3495 (Print){\textbackslash}r0006-3495 (Linking)},
	doi = {10.1529/biophysj.106.092841},
	abstract = {In this work, fluorescence lifetime imaging microscopy in the time domain was used to study the fluorescence dynamics of ECFP and of the ratiometric chloride sensor Clomeleon along neuronal development. The multiexponential analysis of fluorophores combined with the study of the contributions of the individual lifetimes (decay-associated spectra) was used to discriminate the presence of energy transfer from other excited state reactions. A characteristic change of sign of the pre-exponential factors of lifetimes from positive to negative near the acceptor emission maxima was observed in presence of energy transfer. By fluorescence lifetime imaging microscopy, we could show that the individual conformations of CFP display differential quenching properties depending on their microenvironment. Suitability of Clomeleon as an optical indicator to obtain a direct readout of the intracellular chloride concentrations in living cells was verified by steady-state and time-resolved spectroscopy. The simultaneous study of the photophysical properties of Clomeleon, the calcium indicator Cameleon, and ECFP with neuronal development provided a kinetic model for the mechanism when competitive quenching effects as well as energy transfer occur in the same molecule. Simultaneous analysis of donor and acceptor kinetics was necessary to discriminate Försters resonance energy transfer along neuronal development due to the different cellular effects involved.},
	number = {March},
	journal = {Biophysical journal},
	author = {Jose, Mini and Nair, Deepak K and Reissner, Carsten and Hartig, Roland and Zuschratter, Werner},
	year = {2007},
	pages = {2237--2254},
}

@article{hughes_detecting_2002,
	title = {Detecting protein-phospholipid interactions. {Epidermal} growth factor-induced activation of phospholipase {D1b} in situ},
	volume = {277},
	doi = {10.1074/jbc.M201391200},
	abstract = {Phospholipase D (PLD) proteins have been identified in secretory and endocytic vesicles, consistent with their proposed role in regulating membrane traffic. However, their sites of catalytic action remain obscure. We have developed here a novel, analytical approach to monitor PLD activation in intact cells employing lifetime imaging microscopy to measure fluorescence resonance energy transfer between protein and membrane phospholipid. Verification and application of this technique demonstrates a dispersed endosomal, epidermal growth factor-induced activation of the PLD1b isoform. Application of this approach will facilitate the spatial resolution of many protein-phospholipid interactions that are key events in the regulation of cellular processes.},
	number = {25},
	journal = {Journal of Biological Chemistry},
	author = {Hughes, William E. and Larijani, Banafshé and Parker, Peter J.},
	year = {2002},
	pages = {22974--22979},
}

@article{hess_quantitative_2003,
	title = {Quantitative analysis of the fluorescence properties of intrinsically fluorescent proteins in living cells.},
	volume = {85},
	issn = {8148630496},
	doi = {10.1016/S0006-3495(03)74679-7},
	abstract = {The main potential of intrinsically fluorescent proteins (IFPs), as noninvasive and site-specific markers, lies in biological applications such as intracellular visualization and molecular genetics. However, photophysical studies of IFPs have been carried out mainly in aqueous solution. Here, we provide a comprehensive analysis of the intracellular environmental effects on the steady-state spectroscopy and excited-state dynamics of green (EGFP) and red (DsRed) fluorescent proteins, using both one- and two-photon excitation. EGFP and DsRed are expressed either in the cytoplasm of rat basophilic leukemia (RBL-2H3) mucosal mast cells or anchored (via LynB protein) to the inner leaflet of the plasma membrane. The fluorescence lifetimes (within approximately 10\%) and spectra in live cells are basically the same as in aqueous solution, which indicate the absence of both IFP aggregation and cellular environmental effects on the protein folding under our experimental conditions. However, comparative time-resolved anisotropy measurements of EGFP reveal a cytoplasmic viscosity 2.5 +/- 0.3 times larger than that of aqueous solution at room temperature, and also provide some insights into the LynB-EGFP structure and the heterogeneity of the cytoplasmic viscosity. Further, the oligomer configuration and internal depolarization of DsRed, previously observed in solution, persists upon expression in these cells. DsRed also undergoes an instantaneous three-photon induced color change under 740-nm excitation, with efficiently nonradiative green species. These results confirm the implicit assumption that in vitro fluorescence properties of IFPs are essentially valid for in vivo applications, presumably due to the beta-barrel protection of the embodied chromophore. We also discuss the relevance of LynB-EGFP anisotropy for specialized domains studies in plasma membranes.},
	number = {October},
	journal = {Biophysical journal},
	author = {Hess, Samuel T and Sheets, Erin D and Wagenknecht-Wiesner, Alice and Heikal, Ahmed a},
	year = {2003},
	pages = {2566--2580},
}

@article{van_der_krogt_comparison_2008,
	title = {A comparison of donor-acceptor pairs for genetically encoded {FRET} sensors: {Application} to the {Epac} {cAMP} sensor as an example},
	volume = {3},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0001916},
	abstract = {We recently reported on CFP-Epac-YFP, an Epac-based single polypeptide FRET reporter to resolve cAMP levels in living cells. In this study, we compared and optimized the fluorescent protein donor/acceptor pairs for use in biosensors such as CFP-Epac-YFP. Our strategy was to prepare a wide range of constructs consisting of different donor and acceptor fluorescent proteins separated by a short linker. Constructs were expressed in HEK293 cells and tested for FRET and other relevant properties. The most promising pairs were subsequently used in an attempt to improve the FRET span of the Epac-based cAMP sensor. The results show significant albeit not perfect correlation between performance in the spacer construct and in the Epac sensor. Finally, this strategy enabled us to identify improved sensors both for detection by sensitized emission and by fluorescent lifetime imaging. The present overview should be helpful in guiding development of future FRET sensors.},
	number = {4},
	journal = {PLoS ONE},
	author = {Van der Krogt, G. N M and Ogink, Janneke and Ponsioen, Bas and Jalink, Kees},
	year = {2008},
}

@article{perroud_effect_2005-1,
	title = {Effect of bin time on the photon counting histogram for one-photon excitation},
	volume = {6},
	issn = {1439-4235 (Print)},
	doi = {10.1002/cphc.200400547},
	abstract = {We have demonstrated that our photon counting histogram (PCH) model with the correction for one-photon excitation is valid at multiple bin times. The fitted apparent brightness and concentration follow the three-dimensional diffusion model. More importantly, the semi-empirical parameter, F, introduced in the PCH model for one-photon excitation to correct for the non-Gaussian shape of the observation volume, shows small variations with different bin times. These variations are consistent with the physical interpretation of F, and they do not affect the resolving power of the PCH model for one-photon excitation. Based on these findings, we extend the time-independent PCH analysis to time-dependent photon counting multiple histograms (PCMH). This model considers the effect of bin time on the PCH parameters in a way that is similar to fluorescence intensity multiple distribution analysis (FIMDA). From the same set of data, PCMH extracts time-dependent parameters (diffusion time and triplet-state relaxation time) as well as time-independent parameters (true specific brightness and true average number of molecules). Given a three- to fourfold experimental difference in molecular brightness, we find that PCMH can resolve each species in a two-species sample and extract their respective diffusion times even when fluorescence correlation spectroscopy cannot.},
	journal = {ChemPhysChem},
	author = {Perroud, Thomas D. and Huang, Bo and Zare, Richard N.},
	year = {2005},
	keywords = {Single-molecule studies, Confocal microscopy, Photon counting histogram, Few-molecule specctroscopy, Fluorescence specctroscopy},
	pages = {905--912},
}

@article{rajaram_phenoripper_2012,
	title = {{PhenoRipper}: software for rapidly profiling microscopy images},
	volume = {9},
	issn = {1548-7105 (Electronic){\textbackslash}n1548-7091 (Linking)},
	doi = {10.1038/nmeth.2097},
	number = {7},
	journal = {Nature Methods},
	author = {Rajaram, Satwik and Pavie, Benjamin and Wu, Lani F and Altschuler, Steven J},
	year = {2012},
	pages = {635--637},
}

@article{eggeling_photobleaching_1998,
	title = {Photobleaching of {Fluorescent} {Dyes} under {Conditions} {Used} for {Single}-{Molecule} {Detection}:  {Evidence} of {Two}-{Step} {Photolysis}.},
	volume = {70},
	issn = {0003-2700},
	doi = {10.1021/ac980027p},
	abstract = {The photostability of fluorescent dyes is of crucial importance for the statistical accuracy of single-molecule detection (SMD) and for the image quality of scanning confocal microscopy. Concurrent results for the photostability were obtained by two different experimental techniques. First, the photostabilities of several coumarin and rhodamine derivatives in aqueous solution were obtained by monitoring the steady-state fluorescence decay in a quartz cell. Furthermore, an epi-illuminated microscope, continuous wave (CW) excitation at 514.5 nm, and fluorescence correlation spectroscopy (FCS) with a newly developed theory were used to study the photobleaching characteristics of rhodamines under conditions used for SMD. Depending on the rhodamine structure, the probability of photobleaching, p(b), is in the order of 10(-)(6)-10(-)(7) for irradiances below 10(3) W/cm(2). However, a considerable increase of p(b) for irradiances above this level was observed which can only be described by photobleaching reactions from higher excited states (two-step photolysis). In view of these observations, the probability of photobleaching, p(b), as well as a closed expression of its dependence on the CW excitation irradiance considering a five-level molecular electronic state model with the possibility of photobleaching from higher excited electronic states, is derived. From this model, optimal conditions for SMD with respect to the number of emitted fluorescence photons and to the signal-to-background ratio are discussed, taking into account both saturation and photobleaching. The additional photobleaching due to two-step photolysis limits the applicable irradiance.},
	number = {13},
	journal = {Analytical chemistry},
	author = {Eggeling, C and Widengren, J and Rigler, R and Seidel, C a},
	year = {1998},
	pages = {2651--2659},
}

@article{lumma_dynamics_2003,
	title = {Dynamics of large semiflexible chains probed by fluorescence correlation spectroscopy.},
	volume = {90},
	issn = {0031-9007{\textbackslash}n1079-7114},
	doi = {10.1103/PhysRevLett.90.218301},
	abstract = {Fluorescence correlation spectroscopy was used to probe the dynamics of lambda-phage DNA in aqueous solution labeled with the randomly intercalating dye TOTO. The linear macromolecules (i). carry more than one chromophore and (ii). are larger than the waist of the focal volume. The correlation function decays significantly faster than expected for a stiff globule of corresponding size but is in good agreement with the dynamic model of semiflexible chains including hydrodynamic interactions. As the chromophore density is lowered the correlation time decreases in accordance with this model.},
	number = {May},
	journal = {Physical review letters},
	author = {Lumma, D and Keller, S and Vilgis, T and Rädler, J O},
	year = {2003},
	pages = {218301--218301},
}

@article{schravendijk_bee-venom_2001,
	title = {{BEE}-{VENOM} : {BzzzBzzz}},
	author = {Schravendijk, Pim},
	year = {2001},
}

@article{liu_novel_2009,
	title = {A novel protocol of whole mount electro-immunofluorescence staining.},
	volume = {15},
	issn = {1090-0535 (Electronic){\textbackslash}r1090-0535 (Linking)},
	url = {http://dx.doi.org/10.1038/nature12107},
	doi = {10.1038/nature12107},
	abstract = {PURPOSE: To develop a new method of whole mount immunostaining that improves the penetration of staining reagents into the cornea and decreases non-specific binding and background. METHODS: Adult mouse corneas were fixed overnight in 4\% paraformaldehyde or a mixture of 4\% paraformaldehyde and 0.2\% glutaraldehyde in 0.1 M phosphate buffer, pH 7.4, at 4 degrees C. After washing with 0.1\% Triton X-100, corneas were embedded in 1\% solidified agarose in a plastic column and fluorescent staining reagents, e.g., FITC-IgG (Fluorescein isothiocyanate-immunoglobulinG) conjugates in 0.5\% solidified agarose was overlaid onto the specimens. The column was directionally immersed in a submarine gel electrophoresis apparatus filled with Tris-glycine buffer (TGB, pH=7.4) and electrophoresed at 4-10 mA for 10-24 h. For comparison, conventional protocols of immune fluorescent staining were also employed. The outcomes were evaluated by confocal microscopy. RESULTS: Antibody conjugates recognizing extracellular matrix (ECM) components, integral membrane protein, and intracellular structural proteins were used in whole mount corneas. The images of confocal laser scanning microscopy (CLSM) displayed a uniform distribution pattern of keratocan in corneal stroma, which is similar to that of section-staining. Anti-beta-tubulin antibodies bound to microtubes that are distributed within the whole cell body of superficial corneal epithelium cells and stromal keratocytes, but it was found perinuclear of corneal epithelial wing layers and endothelium; integral membrane protein, FAK (focal adhesion kinase), specifically labeled stromal cells of keratectomy corneas that healed for three weeks. In comparison, conventional protocols of immune fluorescent staining using the same antibody conjugates were also employed but did not yield satisfactory results. It was found that IgG conjugates examined did not readily penetrate into stroma and/or intact corneal epithelium. Phalloidin is a small molecule that can readily penetrate into deep tissue and preferentially binds to F-actin. After the whole mount electrofluorescent staining of phalloidin-rhodamine in the mouse cornea, the results were the same as conventional whole mount staining during the healing of epithelial debridement. The cytoplasmic protrusion formed by lamellipodia and filopodia can be clearly demonstrated. CONCLUSIONS: These results indicate that the whole mount electro-immunofluorescent staining allows the detection of antigens in all layers of cornea, i.e., epithelium, stroma, and endothelium.},
	journal = {Molecular vision},
	author = {Liu, Hongshan and Kao, Winston W Y},
	year = {2009},
	note = {Publisher: Nature Publishing Group},
	pages = {505--517},
}

@article{noauthor_fret_flim_abstracts_nodate,
	title = {{FRET}\_FLIM\_abstracts},
}

@article{peter_multiphoton-flim_2005,
	title = {Multiphoton-{FLIM} quantification of the {EGFP}-{mRFP1} {FRET} pair for localization of membrane receptor-kinase interactions.},
	volume = {88},
	issn = {0006-3495 (Print)},
	doi = {10.1529/biophysj.104.050153},
	abstract = {We present an improved monomeric form of the red fluorescent protein, mRFP1, as the acceptor in biological fluorescence resonance energy transfer (FRET) experiments using the enhanced green fluorescent protein as donor. We find particular advantage in using this fluorophore pair for quantitative measurements of FRET using multiphoton fluorescence lifetime imaging microscopy (FLIM). The technique was exploited to demonstrate a novel receptor-kinase interaction between the chemokine receptor (CXCR4) and protein kinase C (PKC) alpha in carcinoma cells for both live- and fixed-cell experiments. The CXCR4-EGFP: PKCalpha-mRFP1 complex was found to be localized precisely to intracellular vesicles and cell protrusions when imaged by multiphoton fluorescence-FLIM. A comparison of the FRET efficiencies obtained using mRFP1-tagged regulatory domain or full-length PKCalpha as the acceptor revealed that PKCalpha, in the closed (inactive) form, is restrained from associating with the cytoplasmic portion of CXCR4. Live-cell FLIM experiments show that the assembly of this receptor:kinase complex is concomitant with the endocytosis process. This is confirmed by experimental evidence suggesting that the recycling of the CXCR4 receptor is increased on stimulation with phorbol ester and blocked on inhibition of PKC by bisindolylmaleimide. The EGFP-mRFP1 couple should be widely applicable, particularly to live-cell quantitative FRET assays.},
	number = {February},
	journal = {Biophysical journal},
	author = {Peter, Marion and Ameer-Beg, Simon M and Hughes, Michael K Y and Keppler, Melanie D and Prag, Søren and Marsh, Mark and Vojnovic, Borivoj and Ng, Tony},
	year = {2005},
	pages = {1224--1237},
}

@article{foldes-papp_theory_2002,
	title = {Theory of {Measuring} the {Selfsame} {Single} {Fluorescent} {Molecule} in {Solution} {Suited} for {Studying} {Individual} {Molecular} {Interactions} by {SPSM}-{FCS}},
	volume = {13},
	number = {3},
	journal = {Pteridines},
	author = {Földes-papp, Zeno},
	year = {2002},
	keywords = {fluorescence spectroscopy, single cell, single molecule, spsm-fcs},
	pages = {73--82},
}

@article{maeder_spatial_2007,
	title = {Spatial regulation of {Fus3} {MAP} kinase activity through a reaction-diffusion mechanism in yeast pheromone signalling.},
	volume = {9},
	issn = {1465-7392 (Print){\textbackslash}r1465-7392 (Linking)},
	doi = {10.1038/ncb1652},
	abstract = {Signal transduction through mitogen-activated protein kinase (MAPK) cascades is thought to occur through the assembly of macromolecular complexes. We quantified the abundance of complexes in the cytoplasm among the MAPKs Ste11, Ste7, Fus3 and the scaffold protein Ste5 in yeast pheromone signalling using fluorescence cross-correlation spectroscopy (FCCS). Significant complex concentrations were observed that remained unchanged on pheromone stimulation, demonstrating that global changes in complex abundances do not contribute to the transmission of signal through the cytoplasm. On the other hand, investigation of the distribution of active Fus3 (Fus3(PP)) across the cytoplasm using fluorescence lifetime imaging microscopy (FLIM) revealed a gradient of Fus3(PP) activity emanating from the tip of the mating projection. Spatial partitioning of Fus3 activating kinases to this site and deactivating phosphatases in the cytoplasm maintain this Fus3(PP)-activity distribution. Propagation of signalling from the shmoo is, therefore, spatially constrained by a gradient-generating reaction-diffusion mechanism.},
	number = {August},
	journal = {Nature cell biology},
	author = {Maeder, Celine I and Hink, Mark a and Kinkhabwala, Ali and Mayr, Reinhard and Bastiaens, Philippe I H and Knop, Michael},
	year = {2007},
	pages = {1319--1326},
}

@article{bastiaens_fluorescence_1999,
	title = {Fluorescence lifetime imaging microscopy: spatial resolution of biochemical processes in the cell.},
	volume = {9},
	issn = {0962-8924},
	doi = {10.1016/S0962-8924(98)01410-X},
	abstract = {Fluorescence lifetime imaging microscopy (FLIM) is a technique in which the mean fluorescence lifetime of a chromophore is measured at each spatially resolvable element of a microscope image. The nanosecond excited-state lifetime is independent of probe concentration or light path length but dependent upon excited-state reactions such as fluorescence resonance energy transfer (FRET). These properties of fluorescence lifetimes allow exploration of the molecular environment of labelled macromolecules in the interior of cells. Imaging of fluorescence lifetimes enables biochemical reactions to be followed at each microscopically resolvable location within the cell.},
	number = {98},
	journal = {Trends in cell biology},
	author = {Bastiaens, P I and Squire, a},
	year = {1999},
	pages = {48--52},
}

@article{noauthor_alberto_nodate,
	title = {Alberto {Diaspro} 3.pdf},
}

@article{dertinger_optics_2008-1,
	title = {The optics and performance of dual-focus fluorescence correlation spectroscopy.},
	volume = {16},
	issn = {1094-4087 (Electronic){\textbackslash}r1094-4087 (Linking)},
	doi = {10.1364/OE.16.014353},
	abstract = {Fluorescence correlation spectroscopy (FCS) is an important spectroscopic technique which can be used for measuring the diffusion and thus size of fluorescing molecules at pico- to nanomolar concentrations. Recently, we introduced an extension of conventional FCS, which is called dual-focus FCS (2fFCS) and allows absolute diffusion measurements with high precision and repeatability. It was shown experimentally that the method is robust against most optical and sample artefacts which are troubling conventional FCS measurements, and is furthermore able to yield absolute values of diffusion coefficients without referencing against known standards. However, a thorough theoretical treatment of the performance of 2fFCS is still missing. The present paper aims at filling this gap. Here, we have systematically studied the performance of 2fFCS with respect to the most important optical and photophysical factors such as cover slide thick-ness, refractive index of the sample, laser beam geometry, and optical satu-ration. We show that 2fFCS has indeed a superior performance when com-pared with conventional FCS, being mostly insensitive to most potential ab-errations when working under optimized conditions.},
	number = {19},
	journal = {Optics express},
	author = {Dertinger, Thomas and Loman, Anastasia and Ewers, Benjamin and Müller, Claus B and Krämer, Benedikt and Enderlein, Jörg},
	year = {2008},
	pages = {14353--14368},
}

@article{noauthor_fluctuation_2008-1,
	title = {Fluctuation information extraction from digital images ( {N} \& {B} approach )},
	number = {April},
	year = {2008},
	pages = {2008--2008},
}

@article{cho_omerosearcher_2012,
	title = {{OMERO}.searcher: content-based image search for microscope images},
	volume = {9},
	issn = {1548-7091},
	url = {http://dx.doi.org/10.1038/nmeth.2086},
	doi = {10.1038/nmeth.2086},
	abstract = {Fluorescence microscopy is growing dramatically both in terms of technical capabilities and the volume of images generated. Online repositories have been created to provide public access to images and opportunities for joint research for many scientists1. This has reintroduced challenges faced when sequence and structure databases were being established: developing fast and effective means of searching for records (images) either by context (such as which protein is labeled) or content (such as which pattern it displays). Image databases normally contain context descriptors in the form of annotations that describe the source of the sample, the protocol used to prepare it, the instrument settings used and the laboratory where it was produced. Searches can readily be done on one or more of these annotations, but incomplete or inconsistent annotation remains a problem. Searching for images based on their contents is much less developed. Some content annotations may be provided in the form of labels (such as Gene Ontology terms) resulting from either visual or automated analysis, and therefore images can be retrieved using them in the same way as context terms. However, these are limited by the 'resolution' of the terms used and do not facilitate discovery of new patterns or of similarities between known patterns that were not previously recognized. Content-based image retrieval (also known as 'query by image content') was proposed many years ago to address this issue; this method takes one or more images as a query and retrieves the most similar images in terms of numerically computed features2. However, current fluorescence microscopy image databases do not provide these search methods. Here we present a content-based image searcher for microscope images, OMERO.searcher (http://murphylab.web.cmu.edu/software/searcher/), that can be used with any OMERO database (http://openmicroscopy.org/)3.},
	number = {7},
	journal = {Nature Methods},
	author = {Cho, Baek Hwan and Cao-Berg, Ivan and Bakal, Jennifer Ann and Murphy, Robert F},
	year = {2012},
	note = {Publisher: Nature Publishing Group},
	pages = {633--634},
}

@article{valentin_photoconversion_2005,
	title = {Photoconversion of {YFP} into a {CFP}-like species during acceptor photobleaching {FRET} experiments.},
	volume = {2},
	issn = {1548-7091},
	doi = {10.1038/nmeth1105-801},
	number = {11},
	journal = {Nature methods},
	author = {Valentin, Guillaume and Verheggen, Céline and Piolot, Tristan and Neel, Henry and Coppey-Moisan, Maïté and Bertrand, Edouard},
	year = {2005},
	pages = {801--801},
}

@article{gmbh_picosecond_nodate,
	title = {Picosecond {Laser} {Scanning} {Microscopy} {Upgrade} {Your} {Leica} {TCS} {SP}-2 for {Lifetime} {Imaging}},
	journal = {Scanning},
	author = {Gmbh, Hickl and Microsystems, Leica and Gmbh, Heidelberg},
	pages = {1--4},
}

@article{alonso-curbelo_rab7_2014,
	title = {{RAB7} {Controls} {Melanoma} {Progression} by {Exploiting} a {Lineage}-{Specific} {Wiring} of the {Endolysosomal} {Pathway}},
	volume = {26},
	issn = {1878-3686 (Electronic) 1535-6108 (Linking)},
	doi = {10.1016/j.ccr.2014.04.030},
	abstract = {Although common cancer hallmarks are well established, lineage-restricted oncogenes remain less understood. Here, we report an inherent dependency of melanoma cells on the small GTPase RAB7, identified within a lysosomal gene cluster that distinguishes this malignancy from over 35 tumor types. Analyses in human cells, clinical specimens, and mouse models demonstrated that RAB7 is an early-induced melanoma driver whose levels can be tuned to favor tumor invasion, ultimately defining metastatic risk. Importantly, RAB7 levels and function were independent of MITF, the best-characterized melanocyte lineage-specific transcription factor. Instead, we describe the neuroectodermal master modulator SOX10 and the oncogene MYC as RAB7 regulators. These results reveal a unique wiring of the lysosomal pathway that melanomas exploit to foster tumor progression. © 2014 Elsevier Inc.},
	journal = {Cancer Cell},
	author = {Alonso-Curbelo, Direna and Riveiro-Falkenbach, Erica and Pérez-Guijarro, Eva and Cifdaloz, Metehan and Karras, Panagiotis and Osterloh, Lisa and Megías, Diego and Cañón, Estela and Calvo, TonantzinG and Olmeda, David and Gómez-López, Gonzalo and Graña, Osvaldo and Sánchez-ArévaloLobo, VíctorJavier and Pisano, DavidG and Wang, Hao Wei and Ortiz-Romero, Pablo and Tormo, Damià and Hoek, Keith and Rodríguez-Peralto, JoséL and Joyce, JohannaA and Soengas, MaríaS},
	year = {2014},
	pages = {61--76},
}

@article{noauthor_1923073447topbottomright1right2bottom_nodate,
	title = {1923073447@{Top},{Bottom},{Right1},{Right2}!{Bottom}},
}

@article{digman_detecting_2009,
	title = {Detecting protein complexes in living cells from laser scanning confocal image sequences by the cross correlation raster image spectroscopy method},
	volume = {96},
	issn = {1542-0086 (Electronic){\textbackslash}n1542-0086 (Linking)},
	url = {http://dx.doi.org/10.1016/j.bpj.2008.09.051},
	doi = {10.1016/j.bpj.2008.09.051},
	abstract = {We describe a general method for detecting molecular complexes based on the analysis of single molecule fluorescence fluctuations from laser scanning confocal images. The method detects and quantifies complexes of two different fluorescent proteins noninvasively in living cells. Because in a raster scanned image successive pixels are measured at different times, the spatial correlation of the image contains information about dynamic processes occurring over a large time range, from the microseconds to seconds. The correlation of intensity fluctuations measured simultaneously in two channels detects protein complexes that carry two molecules of different colors. This information is obtained from the entire image. A map of the spatial distribution of protein complexes in the cell and their diffusion and/or binding properties can be constructed. Using this cross correlation raster image spectroscopy method, specific locations in the cell can be visualized where dynamics of binding and unbinding of fluorescent protein complexes occur. This fluctuation imaging method can be applied to commercial laser scanning microscopes thereby making it accessible to a large community of scientists. ?? 2009 by the Biophysical Society.},
	number = {2},
	journal = {Biophysical Journal},
	author = {Digman, Michelle a. and Wiseman, Paul W. and Horwitz, Alan R. and Gratton, Enrico},
	year = {2009},
	note = {Publisher: Biophysical Society},
	pages = {707--716},
}

@article{bowen_single-molecule_2003-1,
	title = {Single-molecule fluorescence spectroscopy of {TOTO} on poly-{AT} and poly-{GC} {DNA}.},
	volume = {78},
	doi = {10.1562/0031-8655(2003)078<0576:SFSOTO>2.0.CO;2},
	abstract = {Excited state lifetime and amplitude measurements were made on thiazole orange dimer (TOTO), a dimeric DNA-intercalating fluorophore, at single-molecule concentrations. As expected from previous study, the excited state lifetime of TOTO intercalated in DNA is dependent on the sequence of the double-stranded DNA, having values of 2.2 ns in poly-GC DNA and 1.8 ns in poly-AT DNA. The distribution of excited state lifetimes of single molecules of TOTO intercalated into oligonucleotides having varying proportions of poly-GC sequences relative to poly-AT sequences were analyzed as a function of the fraction of poly-GC. By using excited state lifetime distributions from the purely GC and purely AT oligonucleotides as a basis set, it was possible to estimate the GC content of oligonucleotides with intermediate GC composition to within a few percent error. This serves as a model for the analysis of equilibrium binding distributions in DNA using single-molecule methods.},
	journal = {Photochemistry and photobiology},
	author = {Bowen, Benjamin P and Enderlein, Jörg and Woodbury, Neal W},
	year = {2003},
	pages = {576--581},
}

@article{van_geest_flim_2003,
	title = {{FLIM} on a wide field fluorescence microscope},
	volume = {10},
	doi = {10.1007/BF02442582},
	abstract = {Summary\&nbsp;\&nbsp;FLIM (Fluorescence Lifetime Imaging Microscopy) is a new tool to detect interaction between proteins. The proteins under investigation  are fused with fluorescent donor and acceptor molecules. Interaction between the two proteins is accompanied by direct energy  transfer from donor to acceptor (FRET), resulting in a shorter lifetime of the fluorescence emitted by the donor molecule.  This change in lifetime is detected by FLIM.    Fluorescence lifetime imaging can now be done on a widefield fluorescence microscope by using an attachment that is easy to  install and simple to operate. The new LIFA attachment is equipped to use different excitation sources. High brightness modulated  LEDs as well as lasers modulated by an Accousto Optical Modulator can be used as excitation light source. A modulated image  intensifier with digital camera is used as a detector. Power supplies and signal generator are integrated in one control unit  that is connected to the light source, detector and computer. All parameters for image acquisition, processing and viewing  are easy accessible in the user interface of the software package that uses a modular structure. Lifetime images showing FRET  in MCF7 cells with ErbB1-GFP as donor and Py72/Cy3 as acceptor that were taken at EMBL, Heidelberg are shown.},
	journal = {Letters in Peptide Science},
	author = {Van Geest, L. K. and Stoop, K. W J},
	year = {2003},
	keywords = {Microscopy, FRET, Fluorescence, FLIM, LED, Lifetime},
	pages = {501--510},
}

@article{avena_disaggregation_2002,
	title = {Disaggregation kinetics of a peat humic acid: {Mechanism} and {pH} effects},
	volume = {36},
	doi = {10.1021/es025582u},
	abstract = {Fluorescence correlation spectroscopy was used to study the disaggregation kinetics of a peat humic acid (PPHA) at several pH. FCS measures diffusion coefficients of fluorescent molecules and aggregates, thus allowing for the determination of disaggregation rates with a temporal resolution of seconds to minutes. Disaggregation was initiated by dilution of a peat concentrate consisting of a mixture containing 80\% large aggregates (average hydrodynamic radius, rH, of about 300 nm) and free monomers (average rH of about 1 nm). Upon dilution at different pH values, aggregate size decreased, and the proportion of free monomers in solution increased until complete disaggregation occurred. The mechanism appeared to involve the release of monomers from the surface of the aggregates. The pH markedly affected the disaggregation rate. Complete disaggregation took 1 month at pH 3.6, took less than 1 h at pH 5.6, and was extremely rapid in alkaline solutions. The results suggested that at least two processes were operating in parallel with the overall rate being the sum of both processes. At pH higher than 4.5, the disaggregation rate increased more than 3 orders of magnitude per pH unit increase. For concentrations lower than 30 mg L(-1), the equilibrium condition for the PPHA was complete disaggregation even for a pH as low as 3.6.},
	number = {23},
	journal = {Environmental Science and Technology},
	author = {Avena, Marcelo J. and Wilkinson, Kevin J.},
	year = {2002},
	pages = {5100--5105},
}

@article{doeven_distribution_2005,
	title = {Distribution, lateral mobility and function of membrane proteins incorporated into giant unilamellar vesicles.},
	volume = {88},
	issn = {0006-3495 (Print) 0006-3495 (Linking)},
	doi = {10.1529/biophysj.104.053413},
	abstract = {GUVs have been widely used for studies on lipid mobility, membrane dynamics and lipid domain (raft) formation, using single molecule techniques like fluorescence correlation spectroscopy. Reports on membrane protein dynamics in these types of model membranes are by far less advanced due to the difficulty of incorporating proteins into GUVs in a functional state. We have used sucrose to prevent four distinct membrane protein(s) (complexes) from inactivating during the dehydration step of the GUV-formation process. The amount of sucrose was optimized such that the proteins retained 100\% biological activity, and many proteo-GUVs were obtained. Although GUVs could be formed by hydration of lipid mixtures composed of neutral and anionic lipids, an alternate current electric field was required for GUV formation from neutral lipids. Distribution, lateral mobility, and function of an ATP-binding cassette transport system, an ion-linked transporter, and a mechanosensitive channel in GUVs were determined by confocal imaging, fluorescence correlation spectroscopy, patch-clamp measurements, and biochemical techniques. In addition, we show that sucrose slows down the lateral mobility of fluorescent lipid analogs, possibly due to hydrogen-bonding with the lipid headgroups, leading to larger complexes with reduced mobility.},
	journal = {Biophysical journal},
	author = {Doeven, Mark K and Folgering, Joost H a and Krasnikov, Victor and Geertsma, Eric R and van den Bogaart, Geert and Poolman, Bert},
	year = {2005},
	keywords = {abbreviations, abc, ac, alternating current, atp-binding cassette, ddm, dodecyl- -d-maltoside, fluorescence correlation, giant unilamellar vesicles, membrane protein, n -, single-molecule spectroscopy and electrophysiology, spectroscopy},
	pages = {1134--1142},
}

@article{digman_fluctuation_2005,
	title = {Fluctuation correlation spectroscopy with a laser-scanning microscope: exploiting the hidden time structure.},
	volume = {88},
	issn = {0006-3495},
	doi = {10.1529/biophysj.105.061788},
	abstract = {Images obtained with a laser-scanning microscope contain a time structure that can be exploited to measure fast dynamics of molecules in solution and in cells. The spatial correlation approach provides a simple algorithm to extract this information. We describe the analysis used to process laser-scanning images of solutions and cells to obtain molecular diffusion constant in the microsecond to second timescale.},
	journal = {Biophysical journal},
	author = {Digman, Michelle a and Sengupta, Parijat and Wiseman, Paul W and Brown, Claire M and Horwitz, Alan R and Gratton, Enrico},
	year = {2005},
	pages = {L33--L36},
}

@article{skript_rotationsschwingungsspektren_nodate,
	title = {Rotationsschwingungsspektren / {Infrarotspektroskopie} {Ramanspektroskopie} {Karls}-{Universität} zu {Heidelberg}},
	journal = {Frequenz},
	author = {Skript, Ein and Roth, Vorgelegt Von Christian},
	pages = {0--23},
}

@article{schnitzer_nih_2010,
	title = {{NIH} {Public} {Access}},
	doi = {10.1146/annurev.neuro.051508.135540.Advances},
	author = {Schnitzer, J},
	year = {2010},
	keywords = {super-resolution, fiber optics, fluorescence labeling, laser-scanning, two-photon fluorescence},
}

@article{noauthor_cy5_nodate,
	title = {Cy5},
}

@article{evanko_imaging_2009,
	title = {Imaging through automation},
	volume = {6},
	doi = {10.1038/nmeth.f.238},
	abstract = {Methods on the cusp of profoundly impacting their field, areas in which methodological developments are needed and updates on some of last year's picks for Methods to Watch: here is our (subjective) selection for this year.Automated imaging has the power to transform microscopy into a more quantitative technique with new capabilities.},
	number = {1},
	journal = {Nature Methods},
	author = {Evanko, Daniel},
	year = {2009},
	pages = {34--34},
}

@article{murata_texture_2001-1,
	title = {Texture analysis of fluorescence lifetime images of nuclear {DNA} with effect of fluorescence resonance energy transfer.},
	volume = {43},
	doi = {10.1002/1097-0320(20010201)43:2<94::AID-CYTO1023>3.0.CO;2-4},
	abstract = {BACKGROUND: Fluorescence lifetime imaging microscopy (FLIM) is becoming an important tool in cellular imaging. In FLIM, the image contrast is concentration insensitive, whereas it is sensitive to the local environment and interactions of fluorophores such as fluorescence resonance energy transfer (RET). METHODS: Fluorescence microscopy, lifetime imaging, and texture analysis were used to study the spatial distribution of fluorophores bound to nuclear DNA. 3T3-Swiss albino mice fibroblast nuclei were labeled with Hoechst 33258 (Ho), an AT-specific dye, and 7-aminoactinomycin D (7-AAD), a GC-specific dye. Ho is a RET donor to the 7-AAD acceptor. RESULTS: Texture analysis of 50 alcohol-fixed nuclei quantitatively showed changes of spatial distribution of apparent donor lifetimes. RET increased the spatial heterogeneity in the phase and modulation lifetime images. In most of the doubly stained cells (about 80\%), the phase and modulation lifetime distributions were spatially homogeneous. In about 20\% of the cells, we noticed that lower phase and modulation lifetimes caused by RET were correlated with regions of high Ho intensity in the nuclei. CONCLUSIONS: The spatial lifetime heterogeneity of Ho in presence of 7-AAD seems to be caused by RET between closely spaced strands in the three dimensionally condensed regions of DNA.},
	journal = {Cytometry},
	author = {Murata, S and Herman, P and Lakowicz, J R},
	year = {2001},
	pages = {94--100},
}

@article{lamb_sensitivity_2000-1,
	title = {Sensitivity enhancement in fluorescence correlation spectroscopy of multiple species using time-gated detection.},
	volume = {79},
	issn = {0006-3495 (Print) 0006-3495 (Linking)},
	doi = {10.1016/S0006-3495(00)76366-1},
	abstract = {Fluorescence correlation spectroscopy (FCS) is a powerful technique to measure chemical reaction rates and diffusion coefficients of molecules in thermal equilibrium. The capabilities of FCS can be enhanced by measuring the energy, polarization, or delay time between absorption and emission of the collected fluorescence photons in addition to their arrival times. This information can be used to change the relative intensities of multiple fluorescent species in FCS measurements and, thus, the amplitude of the intensity autocorrelation function. Here we demonstrate this strategy using lifetime gating in FCS experiments. Using pulsed laser excitation and laser-synchronized gating in the detection channel, we suppress photons emitted within a certain time interval after excitation. Three applications of the gating technique are presented: suppression of background fluorescence, simplification of FCS reaction studies, and investigation of lifetime heterogeneity of fluorescently labeled biomolecules. The usefulness of this technique for measuring forward and backward rates of protein fluctuations in equilibrium and for distinguishing between static and dynamic heterogeneity makes it a promising tool in the investigation of chemical reactions and conformational fluctuations in biomolecules.},
	number = {August},
	journal = {Biophysical journal},
	author = {Lamb, D C and Schenk, a and Röcker, C and Scalfi-Happ, C and Nienhaus, G U},
	year = {2000},
	pages = {1129--1138},
}

@article{noauthor_finder_nodate,
	title = {{FINDER}},
}

@article{giepmans_fluorescent_2006,
	title = {The fluorescent toolbox for assessing protein location and function.},
	volume = {312},
	issn = {0036-8075},
	doi = {10.1126/science.1124618},
	abstract = {Advances in molecular biology, organic chemistry, and materials science have recently created several new classes of fluorescent probes for imaging in cell biology. Here we review the characteristic benefits and limitations of fluorescent probes to study proteins. The focus is on protein detection in live versus fixed cells: determination of protein expression, localization, activity state, and the possibility for combination of fluorescent light microscopy with electron microscopy. Small organic fluorescent dyes, nanocrystals ("quantum dots"), autofluorescent proteins, small genetic encoded tags that can be complexed with fluorochromes, and combinations of these probes are highlighted.},
	number = {2006},
	journal = {Science (New York, N.Y.)},
	author = {Giepmans, Ben N G and Adams, Stephen R and Ellisman, Mark H and Tsien, Roger Y},
	year = {2006},
	pages = {217--224},
}

@article{model_concentrated_2008,
	title = {Concentrated dyes as a source of two-dimensional fluorescent field for characterization of a confocal microscope},
	volume = {229},
	doi = {10.1111/j.1365-2818.2007.01880.x},
	abstract = {The axial spread function is a useful tool for evaluation of a confocal microscope. It can be obtained experimentally by scanning a uniform fluorescent layer whose thickness is significantly below the resolution limit. Previous researchers have created thin fluorescent films by chemical synthesis. We show here that concentrated fluorescent dyes with a strong absorption at the excitation wavelength can serve as a good approximation of thin fluorescent films. The vertical intensity profiles of such dyes are symmetrical and represent the true axial resolution of a microscope. Solutions of dyes sufficiently opaque to test confocal microscopes with high-NA objectives can be prepared from sodium fluorescein, acid fuchsin and acid blue 9 for excitation at 488 nm, 543 nm and 633 nm, respectively.},
	number = {September 2007},
	journal = {Journal of Microscopy},
	author = {Model, M. a. and Blank, J. L.},
	year = {2008},
	keywords = {Confocal microscopy, Absorption cofficients, Axial resolution, Concentrated dyes, Full width at half maximum, Sectioning imaging property},
	pages = {12--16},
}

@article{palpandi_extraction_2009,
	title = {Extraction of chitin and chitosan from shell and operculum of mangrove gastropod {Nerita} ( {Dostia} ) crepidularia {Lamarck}},
	volume = {1},
	number = {5},
	journal = {International Journal of Medicine and Medical Sciences},
	author = {Palpandi, Chendur and Shanmugam, Vairamani and Shanmugam, Annaian},
	year = {2009},
	keywords = {chitin, chitosan, ft, ir, operculum, shell},
	pages = {198--205},
}

@article{noauthor_openaccess-large_nodate,
	title = {openaccess-large},
}

@article{abbott_seeing_2009,
	title = {Seeing the system},
	volume = {459},
	doi = {10.1038/459630a},
	abstract = {"We don’t pretend to have invented new physics,” says Ernst Stelzer modestly, standing next to an equally modest layout of lasers, mirrors and lenses. “It has all just been plain common sense.” Stelzer has been applying his common sense to microscopes ever since he arrived at the European Molecular Biology Laboratory in Heidelberg, Germany, in 1983 as a fresh-faced physics PhD student, and stayed on to head a team developing three-dimensional light microscopy. A quarter of a century in the field has now led to single plane illumination microscopy (SPIM), the technique with a modest face but an extravagant view: beautiful and unprecedented moving images of whole organisms as they grow one cell division at a time. (.....)},
	number = {June},
	journal = {Nature},
	author = {Abbott, Alison},
	year = {2009},
	pages = {630--631},
}

@article{digman_mapping_2008-2,
	title = {Mapping the number of molecules and brightness in the laser scanning microscope.},
	volume = {94},
	issn = {9498242992},
	url = {http://dx.doi.org/10.1529/biophysj.107.114645},
	doi = {10.1529/biophysj.107.114645},
	abstract = {We describe a technique based on moment-analysis for the measurement of the average number of molecules and brightness in each pixel in fluorescence microscopy images. The average brightness of the particle is obtained from the ratio of the variance to the average intensity at each pixel. To obtain the average number of fluctuating particles, we divide the average intensity at one pixel by the brightness. This analysis can be used in a wide range of concentrations. In cells, the intensity at any given pixel may be due to bright immobile structures, dim fast diffusing particles, and to autofluorescence or scattering. The total variance is given by the variance of each of the above components in addition to the variance due to detector noise. Assuming that all sources of variance are independent, the total variance is the sum of the variances of the individual components. The variance due to the particles fluctuating in the observation volume is proportional to the square of the particle brightness while the variance of the immobile fraction, the autofluorescence, scattering, and that of the detector is proportional to the intensity of these components. Only the fluctuations that depend on the square of the brightness (the mobile particles) will have a ratio of the variance to the intensity {\textgreater}1. Furthermore, changing the fluorescence intensity by increasing the illumination power, distinguishes between these possible contributions. We show maps of molecular brightness and number of cell migration proteins obtained using a two-photon scanning microscope operating with a photon-counting detector. These brightness maps reveal binding dynamics at the focal adhesions with pixel resolution and provide a picture of the binding and unbinding process in which dim molecules attach to the adhesions or large molecular aggregates dissociate from adhesion.},
	number = {6},
	journal = {Biophysical journal},
	author = {Digman, Michelle a and Dalal, Rooshin and Horwitz, Alan F and Gratton, Enrico},
	year = {2008},
	note = {Publisher: Elsevier},
	pages = {2320--2332},
}

@article{kapusta_fluorescence_2007,
	title = {Fluorescence lifetime correlation spectroscopy},
	volume = {17},
	doi = {10.1007/s10895-006-0145-1},
	abstract = {This article explains the basic principles of FLCS, a genuine fusion of Time-Correlated Single Photon Counting (TCSPC) and Fluorescence Correlation Spectroscopy (FCS), using common terms and minimum mathematics. The usefulness of the method is demonstrated on simple FCS experiments. The method makes possible to separate the autocorrelation function of individual components of a mixture of fluorophores, as well as purging the result from parasitic contributions like scattered light or detector afterpulsing.},
	journal = {Journal of Fluorescence},
	author = {Kapusta, Peter and Wahl, Michael and Benda, Aleš and Hof, Martin and Enderlein, Jörg},
	year = {2007},
	keywords = {FCS, Lifetime, Afterpulsing, Correlation, Multichannel detection, Scattering, TCSPC},
	pages = {43--48},
}

@article{wachsmuth_analyzing_2003,
	title = {Analyzing intracellular binding and diffusion with continuous fluorescence photobleaching.},
	volume = {84},
	issn = {4962214233},
	doi = {10.1016/S0006-3495(03)70059-9},
	abstract = {Transport and binding of molecules to specific sites are necessary for the assembly and function of ordered supramolecular structures in cells. For analyzing these processes in vivo, we have developed a confocal fluorescence fluctuation microscope that allows both imaging of the spatial distribution of fluorescent molecules with confocal laser scanning microscopy and probing their mobility at specific positions in the cell with fluorescence correlation spectroscopy and continuous fluorescence photobleaching (CP). Because fluorescence correlation spectroscopy is restricted to rapidly diffusing particles and CP to slower processes, these two methods complement each other. For the analysis of binding-related contributions to mobility we have derived analytical expressions for the temporal behavior of CP curves from which the bound fraction and/or the dissociation rate or residence time at binding sites, respectively, can be obtained. In experiments, we investigated HeLa cells expressing different fluorescent proteins: Although enhanced green fluorescent protein (EGFP) shows high mobility, fusions of histone H2B with the yellow fluorescent protein are incorporated into chromatin, and these nuclei exhibit the presence of a stably bound and a freely diffusing species. Nonpermanent binding was found for mTTF-I, a transcription termination factor for RNA polymerase I, fused with EGFP. The cells show fluorescent nucleoli, and binding is transient. CP yields residence times for mTTF-I-EGFP of approximately 13 s.},
	number = {May},
	journal = {Biophysical journal},
	author = {Wachsmuth, Malte and Weidemann, Thomas and Müller, Gabriele and Hoffmann-Rohrer, Urs W and Knoch, Tobias a and Waldeck, Waldemar and Langowski, Jörg},
	year = {2003},
	pages = {3353--3363},
}

@article{noauthor_no_nodate-1,
	title = {No {Title}},
}

@article{muller_pulsed_2005,
	title = {Pulsed interleaved excitation.},
	volume = {89},
	issn = {4989218077560},
	doi = {10.1529/biophysj.105.064766},
	abstract = {In this article, we demonstrate the new method of pulsed interleaved excitation (PIE), which can be used to extend the capabilities of multiple-color fluorescence imaging, fluorescence cross-correlation spectroscopy (FCCS), and single-pair fluorescence resonance energy transfer (spFRET) measurements. In PIE, multiple excitation sources are interleaved such that the fluorescence emission generated from one pulse is complete before the next excitation pulse arrives. Hence, the excitation source for each detected photon is known. Typical repetition rates used for PIE are between approximately 1 and 50 MHz. PIE has many applications in various fluorescence methods. Using PIE, dual-color measurements can be performed with a single detector. In fluorescence imaging with multicolor detection, spectral cross talk can be removed, improving the contrast of the image. Using PIE with FCCS, we can eliminate spectral cross talk, making the method sensitive to weaker interactions. FCCS measurements with complexes that undergo FRET can be analyzed quantitatively. Under specific conditions, the FRET efficiency can be determined directly from the amplitude of the measured correlation functions without any calibration factors. We also show the application of PIE to spFRET measurements, where complexes that have low FRET efficiency can be distinguished from those that do not have an active acceptor.},
	number = {November},
	journal = {Biophysical journal},
	author = {Müller, Barbara K and Zaychikov, Evgeny and Bräuchle, Christoph and Lamb, Don C},
	year = {2005},
	pages = {3508--3522},
}

@article{nagy_observation_2005,
	title = {Observation volumes and \{gamma\}-factors in two-photon fluorescence fluctuation spectroscopy.},
	volume = {89},
	doi = {10.1529/biophysj.104.052779},
	abstract = {Fluorescence fluctuation spectroscopy has become an important measurement tool for investigating molecular dynamics, molecular interactions, and chemical kinetics in biological systems. Although the basic theory of fluctuation spectroscopy is well established, it is not widely recognized that saturation of the fluorescence excitation can dramatically alter the size and profile of the fluorescence observation volume from which fluorescence fluctuations are measured, even at relatively modest excitation levels. A precise model for these changes is needed for accurate analysis and interpretation of fluctuation spectroscopy data. We here introduce a combined analytical and computational approach to characterize the observation volume under saturating conditions and demonstrate how the variation in the volume is important in two-photon fluorescence correlation spectroscopy. We introduce a simple approach for analysis of fluorescence correlation spectroscopy data that can fully account for the effects of saturation, and demonstrate its success for characterizing the observed changes in both the amplitude and relaxation timescale of measured correlation curves. We also discuss how a quantitative model for the observed phenomena may be of broader importance in fluorescence fluctuation spectroscopy.},
	number = {September},
	journal = {Biophysical journal},
	author = {Nagy, Attila and Wu, Jianrong and Berland, Keith M},
	year = {2005},
	pages = {2077--2090},
}

@article{noauthor_cy5_nodate-1,
	title = {Cy5},
}

@article{van_munster_suppression_2004,
	title = {Suppression of photobleaching-induced artifacts in frequency-domain {FLIM} by permutation of the recording order.},
	volume = {58},
	issn = {1552-4922 (Print)},
	doi = {10.1002/cyto.a.20013},
	abstract = {BACKGROUND: Photobleaching can lead to significant errors in frequency-domain fluorescence lifetime imaging microscopy (FLIM). Existing correction methods for photobleaching require additional recordings and processing time and can result in additional noise. A method is introduced that suppresses the effects of photobleaching without the need for extra recordings or processing. METHODS: Existing bleach correction methods and the method introduced in this report whereby the recording order of the phases is permuted were compared using numerical simulations. RESULTS: Certain orders were found to make measurements virtually insensitive to photobleaching. At 12 recordings, errors in measured phase and modulation depth decreased by a factor 512 and 393, respectively, compared to recordings using sequential recording order. The optimal order is independent of modulation depth, phase, and extent of photobleaching. Thus, the same order can be used for practically all situations. Application of the method in FLIM measurements of EYFP-transfected HeLa cells was found effectively to suppress photobleaching induced artifacts. CONCLUSIONS: In view of the ease of implementation, its inherent robustness, and the possibility to still apply existing correction methods afterward, there is no good reason not to use the permuted recording order presented in this report instead of a sequential order.},
	number = {March},
	journal = {Cytometry. Part A : the journal of the International Society for Analytical Cytology},
	author = {van Munster, Erik B and Gadella, Theodorus W J},
	year = {2004},
	pages = {185--194},
}

@article{palo_fluorescence_2002-1,
	title = {Fluorescence intensity and lifetime distribution analysis: toward higher accuracy in fluorescence fluctuation spectroscopy.},
	volume = {83},
	issn = {0006-3495},
	doi = {10.1016/S0006-3495(02)75195-3},
	abstract = {Fluorescence fluctuation methods such as fluorescence correlation spectroscopy and fluorescence intensity distribution analysis (FIDA) have proven to be versatile tools for studying molecular interactions with single molecule sensitivity. Another well-known fluorescence technique is the measurement of the fluorescence lifetime. Here, we introduce a method that combines the benefits of both FIDA and fluorescence lifetime analysis. It is based on fitting the two-dimensional histogram of the number of photons detected in counting time intervals of given width and the sum of excitation to detection delay times of these photons. Referred to as fluorescence intensity and lifetime distribution analysis (FILDA), the technique distinguishes fluorescence species on the basis of both their specific molecular brightness and the lifetime of the excited state and is also able to determine absolute fluorophore concentrations. The combined information yielded by FILDA results in significantly increased accuracy compared to that of FIDA or fluorescence lifetime analysis alone. In this paper, the theory of FILDA is elaborated and applied to both simulated and experimental data. The outstanding power of this technique in resolving different species is shown by quantifying the binding of calmodulin to a peptide ligand, thus indicating the potential for application of FILDA to similar problems in the life sciences.},
	number = {August},
	journal = {Biophysical journal},
	author = {Palo, Kaupo and Brand, Leif and Eggeling, Christian and Jäger, Stefan and Kask, Peet and Gall, Karsten},
	year = {2002},
	pages = {605--618},
}

@article{noauthor_spectral_lifetime_nodate,
	title = {spectral\_lifetime detector},
}

@article{hink_imaging_2002,
	title = {Imaging protein-protein interactions in living cells},
	volume = {50},
	url = {://000179513300005},
	abstract = {The complex organization of plant cells makes it likely that the molecular behaviour of proteins in the test tube and the cell is different. For this reason, it is essential though a challenge to study proteins in their natural environment. Several innovative microspectroscopic approaches provide such possibilities, combining the high spatial resolution of microscopy with spectroscopic techniques to obtain information about the dynamical behaviour of molecules. Methods to visualize interaction can be based on FRET ( fluorescence detected resonance energy transfer), for example in fluorescence lifetime imaging microscopy (FLIM). Another method is based on fluorescence correlation spectroscopy (FCS) by which the diffusion rate of single molecules can be determined, giving insight into whether a protein is part of a larger complex or not. Here, both FRET- and FCS-based approaches to study protein-protein interactions in vivo are reviewed.},
	journal = {Plant Molecular Biology},
	author = {Hink, M a and Bisseling, T and Visser, Ajwg},
	year = {2002},
	keywords = {fcs, fret, fluorescence spectroscopy, cells, protein-protein interactions},
	pages = {871--883},
}

@article{van_sark_fast_2000,
	title = {Fast imaging of single molecules and nanoparticles by wide-field microscopy and spectrally resolved confocal microscopy},
	volume = {1},
	doi = {10.1002/1438-5171(200012)1:4<291::AID-SIMO291>3.0.CO;2-F},
	abstract = {The dynamic photophys. behavior of single Cy5-maleimide mols. and{\textbackslash}ncore-shell (\{C\}d\{S\}e)\{Z\}n\{S\} quantum dots is compared. To this end{\textbackslash}na home-built fast and sensitive wide-field microscopy system and{\textbackslash}na scanning confocal microscope coupled to a spectrograph for spectral{\textbackslash}nimaging are employed. The ability to measure time-resolved spectra{\textbackslash}nof quantum dots is particularly useful in characterizing single quantum{\textbackslash}ndots. The difference in blinking behavior of single Cy5-maleimide{\textbackslash}nmols. and (\{C\}d\{S\}e)\{Z\}n\{S\} quantum dots can be understood by their{\textbackslash}nphys. origin, i.e. the presence of a single dark state in case of{\textbackslash}nCy5 vs. a distribution of dark states in case of (\{C\}d\{S\}e)\{Z\}n\{S\}.{\textbackslash}nThis intrinsic difference results in higher probabilities to find{\textbackslash}na quantum dot in a short-lived ({\textless}0.02 s) or long-lived ({\textgreater}0.2 s) state{\textbackslash}nthan a Cy5-mal mol. Also, from a comparison to data in the literature,{\textbackslash}nthe lifetime of the off-state of the Cy5-mal mol. is longer than{\textbackslash}nfor the non derivatized Cy5 mol.},
	journal = {Single Molecules},
	author = {Van Sark, W G J H M and Frederix, P L T M and den Heuvel, D J and Asselbergs, M a H and Senf, I and Gerritsen, H C},
	year = {2000},
	keywords = {single mol nanoparticle imaging wide field microsc},
	pages = {291--298},
}

@article{becker_high-speed_2004,
	title = {High-speed {FLIM} data acquisition by time-correlated single photon counting},
	volume = {5323},
	doi = {10.1117/12.529113},
	abstract = {In this study, we describe a time-correlated single photon counting (TCSPC) technique for multi-wavelength lifetime imaging in laser-scanning microscopes. The technique is based on a four-dimensional histogramming process that records the photon density versus the time in the fluorescence decay, the x-y coordinates of the scanning area and the wavelength. It avoids any time gating or wavelength scanning and, therefore, yields a near-ideal counting efficiency. The decay functions are recorded in a large number of time channels, and the components of a multi-exponential decay can be resolved down to less than 30 ps. A single TCSPC imaging channel works with a high detection efficiency up to a photon count rate of about 5.106 s-1. A modified version of the TCSPC fluorescence lifetime imaging (FLIM) technique uses several fully parallel detector and TCSPC channels. It operates at a count rate of more than 107 photons per second and records double-exponential FLIM data within less than 10 seconds.},
	journal = {Proceedings of the Society of Photographic Instrumentation Engineers},
	author = {Becker, W and Bergmann, a and Biscotti, G and Koenig, K and Riemann, I and Kelbauskas, L and Biskup, C},
	year = {2004},
	keywords = {autofluorescence, flim, fluorescence lifetime imaging, fret, time-correlated single photon counting},
	pages = {27--35},
}

@article{swedlow_innovation_2012,
	title = {Innovation in biological microscopy: {Current} status and future directions},
	volume = {34},
	issn = {0265-9247},
	doi = {10.1002/bies.201100168},
	abstract = {The current revolution in biological microscopy stems from the realisation that advances in optics and computational tools and automation make the modern microscope an instrument that can access all scales relevant to modern biology - from individual molecules all the way to whole tissues and organisms and from single snapshots to time-lapse recordings sampling from milliseconds to days. As these and more new technologies appear, the challenges of delivering them to the community grows as well. I discuss some of these challenges, and the examples where openly shared technology have made an impact on the field.},
	journal = {BioEssays},
	author = {Swedlow, Jason R.},
	year = {2012},
	keywords = {Microscopy, Imaging},
	pages = {333--340},
}

@article{simmons_ed_2011,
	title = {Ed {Boyden}},
	volume = {18},
	doi = {10.1145/2000775.2000785},
	journal = {XRDS: Crossroads, The ACM Magazine for Students},
	author = {Simmons, Robert J.},
	year = {2011},
	pages = {23--23},
}

@article{noauthor_stem-cell_2009,
	title = {Stem-cell clarity.},
	volume = {459},
	doi = {10.1038/459615b},
	number = {7247},
	journal = {Nature},
	year = {2009},
	pages = {615--616},
}

@article{zamir_fluorescence_2010,
	title = {Fluorescence fluctuations of quantum-dot sensors capture intracellular protein interaction dynamics.},
	volume = {7},
	issn = {1548-7105 (Electronic){\textbackslash}n1548-7091 (Linking)},
	url = {http://dx.doi.org/10.1038/nmeth.1441},
	doi = {10.1038/nmeth.1441},
	abstract = {We extend the in vitro principle of co-immunoprecipitation to quantify dynamic protein interactions in living cells. Using a multiresolution implementation of fluorescence correlation spectroscopy to achieve maximal temporal resolution, we monitored the interactions of endogenous bait proteins, recruited by quantum dots, with fluorescently tagged prey. With this approach, we analyzed the rapid physiological regulation of protein kinase A.},
	number = {4},
	journal = {Nature methods},
	author = {Zamir, Eli and Lommerse, Piet H M and Kinkhabwala, Ali and Grecco, Hernán E and Bastiaens, Philippe I H},
	year = {2010},
	note = {Publisher: Nature Publishing Group},
	pages = {295--298},
}

@article{duncan_multi-dimensional_2004-1,
	title = {Multi-dimensional time-correlated single photon counting ({TCSPC}) fluorescence lifetime imaging microscopy ({FLIM}) to detect {FRET} in cells},
	volume = {215},
	issn = {0022-2720},
	doi = {10.1111/j.0022-2720.2004.01343.x},
	abstract = {We present a novel, multi-dimensional, time-correlated single photon counting (TCSPC) technique to perform fluorescence lifetime imaging with a laser-scanning microscope operated at a pixel dwell-time in the microsecond range. The unsurpassed temporal accuracy of this approach combined with a high detection efficiency was applied to measure the fluorescent lifetimes of enhanced cyan fluorescent protein (ECFP) in isolation and in tandem with EYFP (enhanced yellow fluorescent protein). This technique enables multi-exponential decay analysis in a scanning microscope with high intrinsic time resolution, accuracy and counting efficiency, particularly at the low excitation levels required to maintain cell viability and avoid photobleaching. Using a construct encoding the two fluorescent proteins separated by a fixed-distance amino acid spacer, we were able to measure the fluorescence resonance energy transfer (FRET) efficiency determined by the interchromophore distance. These data revealed that ECFP exhibits complex exponential fluorescence decays under both FRET and non-FRET conditions, as previously reported. Two approaches to calculate the distance between donor and acceptor from the lifetime delivered values within a 10\% error range. To confirm that this method can be used also to quantify intermolecular FRET, we labelled cultured neurones with the styryl dye FM1-43, quantified the fluorescence lifetime, then quenched its fluorescence using FM4-64, an efficient energy acceptor for FM1-43 emission. These experiments confirmed directly for the first time that FRET occurs between these two chromophores, characterized the lifetimes of these probes, determined the interchromophore distance in the plasma membrane and provided high-resolution two-dimensional images of lifetime distributions in living neurones.},
	number = {December 2003},
	journal = {Journal of Microscopy},
	author = {Duncan, R. R. and Bergmann, a. and Cousin, M. a. and Apps, D. K. and Shipston, M. J.},
	year = {2004},
	keywords = {FM1-43, GFP, Multi-photon, Two-photon},
	pages = {1--12},
}

@article{hwang_single_2005,
	title = {Single wavelength excitation fluorescence cross-correlation spectroscopy with spectrally similar fluorophores: {Resolution} for binding studies},
	volume = {122},
	doi = {10.1063/1.1862614},
	abstract = {It was shown recently that fluorescence cross-correlation spectroscopy (FCCS) can be performed using a single laser wavelength for excitation (SW-FCCS) [L. C. Hwang and T. Wohland, Chem. Phys. Chem 5, 549 (2004).]. This method simplifies the FCCS setup since it does not require the simultaneous alignment of two lasers to the same focal spot. But up to now the method was shown to work only with dyes possessing large Stokes' shifts, and thus was limited to the use of quantum dots and tandem dyes. In this work we show that standard organic dyes with overlapping emission spectra, for instance fluorescein and tetramethylrhodamine, can be used as fluorescent pairs in SW-FCCS. As a biological model system for ligand-receptor interactions we studied the binding of biotin to streptavidin. To investigate the applicability of SW-FCCS for binding studies we adapt the existing FCCS theory for SW-FCCS and calculate limits for the measurement of dissociation constants in dependence on sample concentration, sample purity, and spectral cross talk between the different detection channels.},
	journal = {Journal of Chemical Physics},
	author = {Hwang, Ling Chin and Wohland, Thorsten},
	year = {2005},
	pages = {1--11},
}

@article{eggeling_molecular_2005,
	title = {Molecular photobleaching kinetics of {Rhodamine} {6G} by one- and two-photon induced confocal fluorescence microscopy},
	volume = {6},
	issn = {1439-7641},
	doi = {10.1002/cphc.200400509},
	abstract = {Under high-excitation irradiance conditions in one- and two-photon induced fluorescence microscopy, the photostability of fluorescent dyes is of crucial importance for the detection sensitivity of single molecules and for the contrast in fluorescence imaging. Herein, we report on the dependence of photobleaching on the excitation conditions, using the dye Rhodamine 6G as a typical example. The different excitation modes investigated include 1) one-photon excitation into the first-excited singlet state in the range of 500 to 528 nm by continuous wave and picosecond-pulsed lasers and 2) two- and one-photon excitation to higher-excited singlet states at 800 and 350 nm, respectively, by femtosecond pulses. Experimental strategies are presented, which allow resolving the photophysics. From single-molecule trajectories and fluorescence correlation spectroscopy, as well as with a simple theoretical model based on steady-state solutions of molecular rate equation analysis, we determined the underlying photobleaching mechanisms and quantified the photokinetic parameters describing the dependence of the fluorescence signal on the excitation irradiance. The comparison with experimental data and an exact theoretical model show that only minor deviations between the different theoretical approaches can be observed for high-pulsed excitation irradiances. It is shown that fluorescence excitation is in all cases limited by photolysis from higher-excited electronic states. In contrast to picosecond-pulsed excitation, this is extremely severe for both one- and two-photon excitation with femtosecond pulses. Furthermore, the photostability of the higher-excited electronic states is strongly influenced by environmental conditions, such as polarity and temperature.},
	journal = {ChemPhysChem},
	author = {Eggeling, Christian and Volkmer, Andreas and Seidel, Claus a M},
	year = {2005},
	keywords = {Fluorescence spectroscopy, Single-molecule studies, Fluorescence correlation spectroscopy, Multiphoton excitation, Photochemistry},
	pages = {791--804},
}

@article{de_smedt_studying_2005,
	title = {Studying biophysical barriers to {DNA} delivery by advanced light microscopy},
	volume = {57},
	issn = {0169-409X},
	doi = {10.1016/j.addr.2004.06.003},
	abstract = {Advanced light microscopy (ALM) has been intensively employed by biophysicists to reveal cellular mechanisms. As described in this review, ALM clearly has potential to enhance our understanding of the mechanisms that affect macromolecular therapeutics or nanoscopic drug vectors in biological environments. However, while in recent years confocal microscopy and related techniques became rather routinely used in drug delivery it remains challenging to extract reliable information on the biophysical behaviour of drug delivery systems from ALM measurements. This review discusses studies in which confocal imaging, fluorescence recovery after photobleaching (FRAP), fluorescence correlation spectroscopy (FCS) and fluorescence energy transfer were employed to reveal biophysical properties of DNA and DNA containing nanoparticles in extra- and intracellular media. ?? 2004 Published by Elsevier B.V.},
	journal = {Advanced Drug Delivery Reviews},
	author = {De Smedt, S. C. and Remaut, K. and Lucas, B. and Braeckmans, K. and Sanders, N. N. and Demeester, J.},
	year = {2005},
	keywords = {Fluorescence correlation spectroscopy, Antisense oligonucleotides, DNA degradation, Fluorescence fluctuation spectroscopy, Fluorescence recovery after photobleaching, Fluorescence resonance energy transfer, Gene delivery, Light microscopy, Microphotolysis},
	pages = {191--210},
}

@article{ai_directed_2006,
	title = {Directed evolution of a monomeric, bright and photostable version of {Clavularia} cyan fluorescent protein: structural characterization and applications in fluorescence imaging.},
	volume = {400},
	issn = {1470-8728 (Electronic){\textbackslash}r0264-6021 (Linking)},
	doi = {10.1042/BJ20060874},
	abstract = {The arsenal of engineered variants of the GFP [green FP (fluorescent protein)] from Aequorea jellyfish provides researchers with a powerful set of tools for use in biochemical and cell biology research. The recent discovery of diverse FPs in Anthozoa coral species has provided protein engineers with an abundance of alternative progenitor FPs from which improved variants that complement or supersede existing Aequorea GFP variants could be derived. Here, we report the engineering of the first monomeric version of the tetrameric CFP (cyan FP) cFP484 from Clavularia coral. Starting from a designed synthetic gene library with mammalian codon preferences, we identified dimeric cFP484 variants with fluorescent brightness significantly greater than the wild-type protein. Following incorporation of dimer-breaking mutations and extensive directed evolution with selection for blue-shifted emission, high fluorescent brightness and photostability, we arrived at an optimized variant that we have named mTFP1 [monomeric TFP1 (teal FP 1)]. The new mTFP1 is one of the brightest and most photostable FPs reported to date. In addition, the fluorescence is insensitive to physiologically relevant pH changes and the fluorescence lifetime decay is best fitted as a single exponential. The 1.19 A crystal structure (1 A=0.1 nm) of mTFP1 confirms the monomeric structure and reveals an unusually distorted chromophore conformation. As we experimentally demonstrate, the high quantum yield of mTFP1 (0.85) makes it particularly suitable as a replacement for ECFP (enhanced CFP) or Cerulean as a FRET (fluorescence resonance energy transfer) donor to either a yellow or orange FP acceptor.},
	journal = {The Biochemical journal},
	author = {Ai, Hui-wang and Henderson, J Nathan and Remington, S James and Campbell, Robert E},
	year = {2006},
	keywords = {fluorescence, fret, clavularia, fluorescence imaging, genetic fusion, resonance energy transfer, teal fluorescent},
	pages = {531--540},
}

@article{noauthor_cy3_nodate-1,
	title = {Cy3},
}

@article{bacskai_fluorescence_2003-1,
	title = {Fluorescence resonance energy transfer determinations using multiphoton fluorescence lifetime imaging microscopy to characterize amyloid-beta plaques.},
	volume = {8},
	issn = {1083-3668 (Print){\textbackslash}r1083-3668 (Linking)},
	doi = {10.1117/1.1584442},
	abstract = {We describe the implementation of a commercial fluorescence lifetime imaging microscopy (FLIM) instrument used in conjunction with a commercial laser scanning multiphoton microscope. The femtosecond-pulsed near-infrared laser is an ideal excitation source for time-domain fluorescence lifetime measurements. With synchronization from the x-y scanners, fluorescence lifetimes can be acquired on a pixel-by-pixel basis, with high spatial resolution. Multiexponential curve fits for each pixel result in two-dimensional fluorescence resonance energy transfer (FRET) measurements that allow the determination of both proximity of fluorescent FRET pairs, as well as the fraction of FRET pairs close enough for FRET to occur. Experiments are described that characterize this system, as well as commonly used reagents valuable for FRET determinations in biological systems. Constructs of CFP and YFP were generated to demonstrate FRET between this pair of green fluorescent protein (GFP) color variants. The lifetime characteristics of the FRET pair fluorescein and rhodamine, commonly used for immunohistochemistry, were also examined. Finally, these fluorophores were used to demonstrate spatially resolved FRET with senile plaques obtained from transgenic mouse brain. Together these results demonstrate that FLIM allows sensitive measurements of protein-protein interactions on a spatial scale less than 10 nm using commercially available components.},
	number = {3},
	journal = {Journal of biomedical optics},
	author = {Bacskai, Brian J and Skoch, Jesse and Hickey, Gregory a and Allen, Racquel and Hyman, Bradley T},
	year = {2003},
	keywords = {microscopy, 13, 2002, 2003, 22, 7, accepted for publication jan, alzheimer, amyloid-, croscopy, fluorescence lifetime, fluorescence lifetime imaging mi-, multiphoton, paper mm-06 received oct, revised manuscript received jan, s disease},
	pages = {368--375},
}

@article{noauthor_ncomms3207-s13_nodate,
	title = {ncomms3207-s13},
}

@article{bieschke_ultrasensitive_2000,
	title = {Ultrasensitive detection of pathological prion protein aggregates by dual-color scanning for intensely fluorescent targets.},
	volume = {97},
	issn = {0027-8424 (Print){\textbackslash}r0027-8424 (Linking)},
	doi = {10.1073/pnas.97.10.5468},
	abstract = {A definite diagnosis of prion diseases such as Creutzfeldt-Jakob disease (CJD) relies on the detection of pathological prion protein (PrP(Sc)). However, no test for PrP(Sc) in cerebrospinal fluid (CSF) has been available thus far. Based on a setup for confocal dual-color fluorescence correlation spectroscopy, a technique suitable for single molecule detection, we developed a highly sensitive detection method for PrP(Sc). Pathological prion protein aggregates were labeled by specific antibody probes tagged with fluorescent dyes, resulting in intensely fluorescent targets, which were measured by dual-color fluorescence intensity distribution analysis in a confocal scanning setup. In a diagnostic model system, PrP(Sc) aggregates were detected down to a concentration of 2 pM PrP(Sc), corresponding to an aggregate concentration of approximately 2 fM, which was more than one order of magnitude more sensitive than Western blot analysis. A PrP(Sc)-specific signal could also be detected in a number of CSF samples from patients with CJD but not in control samples, providing the basis for a rapid and specific test for CJD and other prion diseases. Furthermore, this method could be adapted to the sensitive detection of other disease-associated amyloid aggregates such as in Alzheimer's disease.},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Bieschke, J and Giese, a and Schulz-Schaeffer, W and Zerr, I and Poser, S and Eigen, M and Kretzschmar, H},
	year = {2000},
	pages = {5468--5473},
}

@article{haupts_dynamics_1998,
	title = {Dynamics of fluorescence fluctuations in green fluorescent protein observed by fluorescence correlation spectroscopy.},
	volume = {95},
	issn = {0027-8424 (Print){\textbackslash}r0027-8424 (Linking)},
	doi = {10.1073/pnas.95.23.13573},
	abstract = {We have investigated the pH dependence of the dynamics of conformational fluctuations of green fluorescent protein mutants EGFP (F64L/S65T) and GFP-S65T in small ensembles of molecules in solution by using fluorescence correlation spectroscopy (FCS). FCS utilizes time-resolved measurements of fluctuations in the molecular fluorescence emission for determination of the intrinsic dynamics and thermodynamics of all processes that affect the fluorescence. Fluorescence excitation of a bulk solution of EGFP decreases to zero at low pH (pKa = 5.8) paralleled by a decrease of the absorption at 488 nm and an increase at 400 nm. Protonation of the hydroxyl group of Tyr-66, which is part of the chromophore, induces these changes. When FCS is used the fluctuations in the protonation state of the chromophore are time resolved. The autocorrelation function of fluorescence emission shows contributions from two chemical relaxation processes as well as diffusional concentration fluctuations. The time constant of the fast, pH-dependent chemical process decreases with pH from 300 microseconds at pH 7 to 45 microseconds at pH 5, while the time-average fraction of molecules in a nonfluorescent state increases to 80\% in the same range. A second, pH-independent, process with a time constant of 340 microseconds and an associated fraction of 13\% nonfluorescent molecules is observed between pH 8 and 11, possibly representing an internal proton transfer process and associated conformational rearrangements. The FCS data provide direct measures of the dynamics and the equilibrium properties of the protonation processes. Thus FCS is a convenient, intrinsically calibrated method for pH measurements in subfemtoliter volumes with nanomolar concentrations of EGFP.},
	number = {November},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Haupts, U and Maiti, S and Schwille, P and Webb, W W},
	year = {1998},
	pages = {13573--13578},
}

@article{pinaud_advances_2006,
	title = {Advances in fluorescence imaging with quantum dot bio-probes},
	volume = {27},
	issn = {0142-9612 (Print){\textbackslash}r0142-9612 (Linking)},
	doi = {10.1016/j.biomaterials.2005.11.018},
	abstract = {After much effort in surface chemistry development and optimization by several groups, fluorescent semiconductor nanocrystals probes, also known as quantum dots or qdots, are now entering the realm of biological applications with much to offer to biologists. The road to success has been paved with hurdles but from these efforts has stemmed a multitude of original surface chemistries that scientists in the biological fields can draw from for their specific biological applications. The ability to easily modulate the chemical nature of qdot surfaces by employing one or more of the recently developed qdot coatings, together with their exceptional photophysics have been key elements for qdots to acquire a status of revolutionary fluorescent bio-probes. Indeed, the unique properties of qdots not only give biologists the opportunity to explore advanced imaging techniques such as single molecule or lifetime imaging but also to revisit traditional fluorescence imaging methodologies and extract yet unobserved or inaccessible information in vitro or in vivo. ?? 2005 Elsevier Ltd. All rights reserved.},
	journal = {Biomaterials},
	author = {Pinaud, Fabien and Michalet, Xavier and Bentolila, Laurent a. and Tsay, James M. and Doose, Soren and Li, Jack J. and Iyer, Gopal and Weiss, Shimon},
	year = {2006},
	keywords = {Fluorescence, Confocal microscopy, Biomimetic material, Molecular imaging, Nanoparticle, Surface modification},
	pages = {1679--1687},
}

@article{mcconnell_time-correlated_2007,
	title = {Time-correlated single-photon counting fluorescence lifetime confocal imaging of decayed and sound dental structures with a white-light supercontinuum source},
	volume = {225},
	issn = {0022-2720 (Print){\textbackslash}r0022-2720 (Linking)},
	doi = {10.1111/j.1365-2818.2007.01724.x},
	abstract = {We report the demonstration of time-correlated single-photon counting (TCSPC) fluorescence lifetime imaging (FLIM) to ex vivo decayed and healthy dentinal tooth structures, using a white-light supercontinuum excitation source. By using a 100 fs-pulsed Ti:Sapphire laser with a low-frequency chirp to pump a 30-cm long section of photonic crystal fibre, a ps-pulsed white-light supercontinuum was created. Optical bandpass interference filters were then applied to this broad-bandwidth source to select the 488-nm excitation wavelength required to perform TCSPC FLIM of dental structures. Decayed dentine showed significantly shorter lifetimes, discriminating it from healthy tissue and hard, stained and thus affected but non-infected material. The white-light generation source provides a flexible method of producing variable-bandwidth visible and ps-pulsed light for TCSPC FLIM. The results from the dental tissue indicate a potential method of discriminating diseased tissue from sound, but stained tissue, which could be of crucial importance in limiting tissue resection during preparation for clinical restorations.},
	number = {August 2006},
	journal = {Journal of Microscopy},
	author = {McConnell, G. and Girkin, J. M. and Ameer-Beg, S. M. and Barber, P. R. and Vojnovic, B. and Ng, T. and Banerjee, a. and Watson, T. F. and Cook, R. J.},
	year = {2007},
	keywords = {Fluorescence spectroscopy, 42.65.-k nonlinear optics, 87.64.Ni optical absorption, Magnetic circular dichroism, PACS: 42.62.Be Biological and medical applications},
	pages = {126--136},
}

@article{lippincott-schwartz_putting_2009-1,
	title = {Putting super-resolution fluorescence microscopy to work.},
	volume = {6},
	issn = {1548-7105 (Electronic){\textbackslash}r1548-7091 (Linking)},
	doi = {10.1038/nmeth.f.233},
	abstract = {Super-resolution microscopy is poised to revolutionize our understanding of the workings of the cell. But the technology still has some limitations, and these must be taken into consideration if widespread application is to yield biological insight.},
	number = {1},
	journal = {Nature methods},
	author = {Lippincott-Schwartz, Jennifer and Manley, Suliana},
	year = {2009},
	pages = {21--23},
}

@article{doose_comparison_2005,
	title = {Comparison of photophysical and colloidal properties of biocompatible semiconductor nanocrystals using fluorescence correlation spectroscopy},
	volume = {77},
	issn = {0003-2700},
	doi = {10.1021/ac050035n},
	abstract = {A number of different surface chemistries have been developed in recent years to render semiconductor nanocrystals (NCs) stable in water and biocompatible. However, most of these surface modifications affect NCs' photophysical properties, calling for a method to simultaneously monitor colloidal and fluorescence properties. Fluorescence correlation spectroscopy (FCS) combined with ensemble spectroscopic methods and Monte Carlo simulations were used to interpret and derive photophysical as well as colloidal properties of four different NC surface treatments. Using a novel FCS scheme with alternating laser excitation at two different intensities, we first ruled out influences from optical gradient forces (optical trapping). We then compared concentration of emitting particles, brightness per particle, saturation intensity, blinking (intermittency), hydrodynamic radius, and propensity for aggregation of the different bioconjugated NCs. This approach was successfully applied during the development and optimization of peptide-coated NCs.},
	number = {7},
	journal = {Analytical Chemistry},
	author = {Doose, Sören and Tsay, James M. and Pinaud, Fabien and Weiss, Shimon},
	year = {2005},
	pages = {2235--2242},
}

@article{noauthor_ai_n9210025_nodate,
	title = {ai\_n9210025},
}

@article{digman_measuring_2005-3,
	title = {Measuring fast dynamics in solutions and cells with a laser scanning microscope.},
	volume = {89},
	issn = {0006-3495},
	doi = {10.1529/biophysj.105.062836},
	abstract = {Single-point fluorescence correlation spectroscopy (FCS) allows measurements of fast diffusion and dynamic processes in the microsecond-to-millisecond time range. For measurements on living cells, image correlation spectroscopy (ICS) and temporal ICS extend the FCS approach to diffusion times as long as seconds to minutes and simultaneously provide spatially resolved dynamic information. However, ICS is limited to very slow dynamics due to the frame acquisition rate. Here we develop novel extensions to ICS that probe spatial correlations in previously inaccessible temporal windows. We show that using standard laser confocal imaging techniques (raster-scan mode) not only can we reach the temporal scales of single-point FCS, but also have the advantages of ICS in providing spatial information. This novel method, called raster image correlation spectroscopy (RICS), rapidly measures during the scan many focal points within the cell providing the same concentration and dynamic information of FCS as well as information on the spatial correlation between points along the scanning path. Longer time dynamics are recovered from the information in successive lines and frames. We exploit the hidden time structure of the scan method in which adjacent pixels are a few microseconds apart thereby accurately measuring dynamic processes such as molecular diffusion in the microseconds-to-seconds timescale. In conjunction with simulated data, we show that a wide range of diffusion coefficients and concentrations can be measured by RICS. We used RICS to determine for the first time spatially resolved diffusions of paxillin-EGFP stably expressed in CHOK1 cells. This new type of data analysis has a broad application in biology and it provides a powerful tool for measuring fast as well as slower dynamic processes in cellular systems using any standard laser confocal microscope.},
	number = {August},
	journal = {Biophysical journal},
	author = {Digman, Michelle a and Brown, Claire M and Sengupta, Parijat and Wiseman, Paul W and Horwitz, Alan R and Gratton, Enrico},
	year = {2005},
	pages = {1317--1327},
}

@article{handl_lanthanide-based_2005,
	title = {Lanthanide-based luminescent assays for ligand-receptor interactions},
	volume = {77},
	issn = {0024-3205},
	doi = {10.1016/j.lfs.2005.01.009},
	abstract = {The evaluation of receptor ligand interactions is important in the field of drug discovery and development. Currently these interactions are typically measured with cumbersome (low throughput) radiolabels. Higher throughput screens are available such as fluorescent measurements of G-protein coupled receptor-induced Ca2+ increases or fluorescence anisotropy, yet these have limited applicability and/or low signal to noise. Hence, there is a need to develop more widely applicable and more sensitive labels that can be used to monitor ligand-receptor interactions. Lanthanides provide an attractive alternative to the traditional labels used for monitoring ligand-receptor interactions. The incorporation of lanthanide labels into traditional assays used to assess receptor-ligand interactions can make these assays more affordable, less time consuming and amenable to automation. Lanthanides can be coupled to ligands and provide strong luminescent signals that can be detected using time-resolved fluorescence (TRF) methods. This approach takes advantage of the long fluorescence lifetime of the lanthanide and can detect less than one attomole of europium in a multiwell plate sample. This short review provides a basic introduction into lanthanides and TRF and describes some of the recent assays which have utilized lanthanides as labels to assess ligand-receptor interactions. ?? 2005 Elsevier Inc. All rights reserved.},
	journal = {Life Sciences},
	author = {Handl, Heather L. and Gillies, Robert J.},
	year = {2005},
	keywords = {Binding assay, Lanthanide, Receptor-ligand interaction, Time-resolved flourescence},
	pages = {361--371},
}

@article{biskup_interaction_2004-1,
	title = {Interaction of {PSD}-95 with potassium channels visualized by fluorescence lifetime-based resonance energy transfer imaging.},
	volume = {9},
	issn = {1083-3668 (Print)},
	doi = {10.1117/1.1755721},
	abstract = {Resonance energy transfer (RET) has been extensively used to estimate the distance between two different fluorophores. This study demonstrates how protein-protein interactions can be visualized and quantified in living cells by time-correlated single-photon counting (TCSPC) imaging techniques that exploit the RET between appropriate fluorescent labels. We used this method to investigate the association of the potassium inward rectifier channel Kir2.1 and the neuronal PDZ protein PSD-95, which has been implicated in subcellular targeting and clustering of ion channels. Our data show that the two proteins not only colocalize within clusters but also interact with each other. Moreover, the data allow a spatially resolved quantification of this protein-protein interaction with respect to the relative number and the proximity between interacting molecules. Depending on the subcellular localization, a fraction of 20 to 60\% of PSD-95 molecules interacted with Kir2.1 channels, approximating their fluorescent labels by less than 5 nm.},
	number = {4},
	journal = {Journal of biomedical optics},
	author = {Biskup, Christoph and Kelbauskas, Laimonas and Zimmer, Thomas and Benndorf, Klaus and Bergmann, Axel and Becker, Wolfgang and Ruppersberg, J Peter and Stockklausner, Clemens and Klöcker, Nikolaj},
	year = {2004},
	keywords = {flim, fluorescence lifetime imaging, 2003, resonance energy transfer, 23, 29, 8, accepted for publication oct, inwardly rectifying potassium channels, paper 03091 received jul, postsynaptic density protein, psd-95, revised manuscript received sep},
	pages = {753--759},
}

@article{noauthor_1471-2121-2-8-1_nodate,
	title = {1471-2121-2-8-1},
}

@article{noauthor_dapi_nodate,
	title = {{DAPI}},
}

@article{chen_fluorescence_1999,
	title = {Fluorescence fluctuation spectroscopy.},
	volume = {19},
	issn = {3331220002312},
	doi = {10.1006/meth.1999.0854},
	abstract = {The analysis of the intensity fluctuation of a fluorescence signal from a relatively small volume and from a few molecules contains information about the distribution of different species present in the solution and about kinetic parameters of the system. The same information is generally averaged out when the fluorescence experiment is performed in a much larger volume, typically a cuvette experiment. The fundamental reason for this difference is that the fluctuations of the fluorescence signal from a few molecules directly reflect the molecular nature of the matter. Only recently, with the advent of confocal microscopy and two-photon excitation, it has become practical to achieve small excitation volumes in which only a few fluorescent molecules are present. We introduce the concept of fluctuation spectroscopy and highlight some of the technical aspects. We discuss different analysis methods used in fluctuation spectroscopy and evaluate their use for studying protein-protein interactions.},
	journal = {Methods (San Diego, Calif.)},
	author = {Chen, Y and Müller, J D and Berland, K M and Gratton, E},
	year = {1999},
	pages = {234--252},
}

@article{noauthor_headsquare_nodate,
	title = {headsquare},
}

@article{kim_fully_2007-4,
	title = {Fully automated segmentation and morphometrical analysis of muscle fiber images.},
	volume = {71},
	doi = {10.1002/cyto.a},
	abstract = {BACKGROUND: Measurement of muscle fiber size and determination of size distribution is important in the assessment of neuromuscular disease. Fiber size estimation by simple inspection is inaccurate and subjective. Manual segmentation and measurement are time-consuming and tedious. We therefore propose an automated image analysis method for objective, reproducible, and time-saving measurement of muscle fibers in routinely hematoxylin-eosin stained cryostat sections. METHODS: The proposed segmentation technique makes use of recent advances in level set based segmentation, where classical edge based active contours are extended by region based cues, such as color and texture. Segmentation and measurement are performed fully automatically. Multiple morphometric parameters, i.e., cross sectional area, lesser diameter, and perimeter are assessed in a single pass. The performance of the computed method was compared to results obtained by manual measurement by experts. RESULTS: The correct classification rate of the computed method was high (98\%). Segmentation and measurement results obtained manually or automatically did not reveal any significant differences. CONCLUSIONS: The presented region based active contour approach has been proven to accurately segment and measure muscle fibers. Complete automation minimizes user interaction, thus, batch processing, as well as objective and reproducible muscle fiber morphometry are provided.},
	number = {January},
	journal = {Cytometry. Part A : the journal of the International Society for Analytical Cytology},
	author = {Kim, Yoo-Jin and Brox, Thomas and Feiden, Wolfgang and Weickert, Joachim},
	year = {2007},
	pages = {8--15},
}

@article{noauthor_alberto_nodate-1,
	title = {Alberto {Diaspro} 1.pdf},
}

@article{guan_adaptive_2008-4,
	title = {Adaptive correction technique for {3D} reconstruction of fluorescence microscopy images.},
	volume = {71},
	issn = {1059-910X (Print)},
	doi = {10.1002/jemt},
	abstract = {Recent advances in high-resolution imaging have provided valuable novel insights into structural relationships within cells and tissues both in vitro and in vivo. An analysis of this kind is regularly done by optical sectioning using either confocal or deconvolution microscopy. However, the reconstruction of 3D images suffers from light scattering and absorption with increasing depth by finite transparency of the used media. Photobleaching of fluorochromes has been especially troublesome and often the only remedy for loss of signal during optical sectioning is to reduce the number of sections. This causes disparities in the x-y and z dimensions of voxels, which lead to vertical distortion of the original stack of images and necessitates interpolation. Interpolation is necessary to fill up the gaps between consecutive sections in the original image stack to obtain cubic voxels. The present manuscript describes a novel method for adaptive compensation of attenuation of light intensity in stacks of fluorescence microscopy images that is based on a physical model of light attenuation. First, we use a fast interpolation technique to generate a cubic voxel-based volume stack with the aid of a contribution look up table. With the contribution look up table, multiple calculations are avoided, which substantially reduces the computational time without compromising the accuracy of the restoration procedure. Second, each section within the resulting volume is processed to rectify its intensity values that have been altered due to photobleaching and scattering and absorption. The method allows to define the last good section in the stack and the correction is then done automatically.},
	number = {September 2006},
	journal = {Microscopy research and technique},
	author = {Guan, Y Q and Cai, Y Y and Zhang, X and Lee, Y T and Opas, M},
	year = {2008},
	keywords = {autofluorescence, flim, tcspc, fret, multispectral flim},
	pages = {146--157},
}

@article{noauthor_setup_nodate,
	title = {Setup testprotocol\_Confocal\_for outside {NKI}},
}

@article{larson_multiphoton_2011,
	title = {Multiphoton {Microscopy}},
	volume = {5},
	number = {July},
	journal = {Nature Photonics},
	author = {Larson, Adam},
	year = {2011},
	pages = {1--4},
}

@article{goedhart_bright_2010,
	title = {Bright cyan fluorescent protein variants identified by fluorescence lifetime screening.},
	volume = {7},
	issn = {1548-7091},
	doi = {10.1038/nmeth.1415},
	abstract = {Optimization of autofluorescent proteins by intensity-based screening of bacteria does not necessarily identify the brightest variant for eukaryotes. We report a strategy to screen excited state lifetimes, which identified cyan fluorescent proteins with long fluorescence lifetimes ({\textgreater}3.7 ns) and high quantum yields ({\textgreater}0.8). One variant, mTurquoise, was 1.5-fold brighter than mCerulean in mammalian cells and decayed mono-exponentially, making it an excellent fluorescence resonance energy transfer (FRET) donor.},
	journal = {Nature methods},
	author = {Goedhart, Joachim and van Weeren, Laura and Hink, Mark a and Vischer, Norbert O E and Jalink, Kees and Gadella, Theodorus W J},
	year = {2010},
	pages = {137--139},
}

@article{noauthor_090514_fb_biospektrum_konfokal_depdf_nodate,
	title = {090514\_FB\_BIOspektrum\_Konfokal\_de.pdf},
}

@article{gadella_oligomerization_1995,
	title = {Oligomerization of epidermal growth factor receptors on {A431} cells studied by time-resolved fluorescence imaging microscopy. {A} stereochemical model for tyrosine kinase receptor activation.},
	volume = {129},
	doi = {10.1083/jcb.129.6.1543},
	abstract = {The aggregation states of the epidermal growth factor receptor (EGFR) on single A431 human epidermoid carcinoma cells were assessed with two new techniques for determining fluorescence resonance energy transfer: donor photobleaching fluorescence resonance energy transfer (pbFRET) microscopy and fluorescence lifetime imaging microscopy (FLIM). Fluorescein-(donor) and rhodamine-(acceptor) labeled EGF were bound to the cells and the extent of oligomerization was monitored by the spatially resolved FRET efficiency as a function of the donor/acceptor ratio and treatment conditions. An average FRET efficiency of 5\% was determined after a low temperature (4 degrees C) incubation with the fluorescent EGF analogs for 40 min. A subsequent elevation of the temperature for 5 min caused a substantial increase of the average FRET efficiency to 14\% at 20 degrees C and 31\% at 37 degrees C. In the context of a two-state (monomer/dimer) model for the EGFR, these FRET efficiencies were consistent with minimal average receptor dimerizations of 13, 36, and 69\% at 4, 20, and 37 degrees C, respectively. A431 cells were pretreated with the monoclonal antibody mAb 2E9 that specifically blocks EGF binding to the predominant population of low affinity EGFR (15). The average FRET efficiency increased dramatically to 28\% at 4 degrees C, indicative of a minimal receptor dimerization of 65\% for the subpopulation of high affinity receptors. These results are in accordance with prior studies indicating that binding of EGF leads to a fast and temperature-dependent microclustering of EGFR, but suggest in addition that the high affinity functional subclass of receptors on quiescent A431 cells are present in a predimerized or oligomerized state. We propose that the transmission of the external ligand-binding signal to the cytoplasmic domain is effected by a concerted relative rotational rearrangement of the monomeric units comprising the dimeric receptor, thereby potentiating a mutual activation of the tyrosine kinase domains.},
	number = {6},
	journal = {The Journal of cell biology},
	author = {Gadella, T W and Jovin, T M},
	year = {1995},
	pages = {1543--1558},
}

@article{noauthor_searchact_nodate,
	title = {{searchACT}},
}

@article{noauthor_subscriberact_nodate,
	title = {{subscriberACT}},
}

@article{miyawaki_fluorescent_2004,
	title = {Fluorescent proteins in a new light.},
	volume = {22},
	doi = {10.1038/nbt1104-1374},
	number = {11},
	journal = {Nature biotechnology},
	author = {Miyawaki, Atsushi},
	year = {2004},
	pages = {1374--1376},
}

@article{ries_studying_2006,
	title = {Studying slow membrane dynamics with continuous wave scanning fluorescence correlation spectroscopy.},
	volume = {91},
	issn = {0006-3495 (Print){\textbackslash}r0006-3495 (Linking)},
	doi = {10.1529/biophysj.106.082297},
	abstract = {Here we discuss the application of scanning fluorescence correlation spectroscopy (SFCS) using continuous wave excitation to analyze membrane dynamics. The high count rate per molecule enables the study of very slow diffusion in model and cell membranes, as well as the application of two-foci fluorescence cross-correlation spectroscopy for parameter-free determination of diffusion constants. The combination with dual-color fluorescence cross-correlation spectroscopy with continuous or pulsed interleaved excitation allows binding studies on membranes. Reduction of photobleaching, higher reproducibility, and stability compared to traditional FCS on membranes, and the simple implementation in a commercial microscopy setup make SFCS a valuable addition to the pool of fluorescence fluctuation techniques.},
	number = {September},
	journal = {Biophysical journal},
	author = {Ries, Jonas and Schwille, Petra},
	year = {2006},
	pages = {1915--1924},
}

@article{noauthor_application-note-leica1-france_nodate,
	title = {application-note-{Leica1}-{France}},
}

@article{weber_genotyping_2002,
	title = {Genotyping of human platelet antigen-1 by gene amplification and labelling in one system and automated fluorescence correlation spectroscopy},
	volume = {116},
	doi = {10.1046/j.0007-1048.2002.03355.x},
	abstract = {Genotyping of human platelet antigen-1 (HPA-1) is required for the diagnosis and appropriate therapy of alloimmunization. Recently, the HPA-1 polymorphism has been identified as an inherited risk factor for thrombosis. Most currently used methods for HPA-1 genotyping have the disadvantage of time-consuming post-polymerase chain reaction (PCR) processes such as ligation (oligonucleotide ligation assay), restriction enzyme digestion (allele-specific restriction enzyme analysis) and electrophoresis (single-strand conformation polymorphism). We present a novel method for HPA-1 genotyping based on a homogeneous PCR strategy (GALIOS, gene amplification and labelling in one system) combined with automated fluorescence correlation spectroscopy (FCS). The PCR uses one pair of gene-specific amplification primers and two allele-specific, semi-nested labelling primers. The allele-specific labelling primers differ in a single nucleotide (T for HPA-1a/1a, C for HPA-1b/1b) and are coupled to different fluorescent dyes. The quantities of generated fluorescent PCR products are analysed by FCS at 543 nm and 633 nm excitation wavelength respectively. The genotypes determined using this method were in 100\% concordance with the results obtained by allele-specific restriction analysis (n = 380 samples). The assay was validated for specificity, reliability and the dynamic range. This innovative method of rapid HPA-1 genotyping offers a specific and robust system, which is applicable for routine HPA-1 genotyping.},
	journal = {British Journal of Haematology},
	author = {Weber, Stephanie and Hummel, Stefan a. and Weber, Artur Aron and Zirwes, Rudolf F. and Weiner, Olaf H. and Reuber, Bernadette E.},
	year = {2002},
	keywords = {Fluorescence correlation spectroscopy, GALIOS®, Genotyping, Human platelet antigen-1, Polymerase chain reaction},
	pages = {839--843},
}

@article{tertilt_vorlesung_nodate,
	title = {Vorlesung {Immunologie} {Teil} : {Transgene} {Mausmodelle} {Transgene} {Mausmodelle}},
	author = {Tertilt, Christine},
}

@article{cole_measuring_2011,
	title = {Measuring and interpreting point spread functions to determine confocal microscope resolution and ensure quality control.},
	volume = {6},
	issn = {1754-2189},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/22082987},
	doi = {10.1038/nprot.2011.407},
	abstract = {This protocol outlines a procedure for collecting and analyzing point spread functions (PSFs). It describes how to prepare fluorescent microsphere samples, set up a confocal microscope to properly collect 3D confocal image data of the microspheres and perform PSF measurements. The analysis of the PSF is used to determine the resolution of the microscope and to identify any problems with the quality of the microscope's images. The PSF geometry is used as an indicator to identify problems with the objective lens, confocal laser scanning components and other relay optics. Identification of possible causes of PSF abnormalities and solutions to improve microscope performance are provided. The microsphere sample preparation requires 2-3 h plus an overnight drying period. The microscope setup requires 2 h (1 h for laser warm up), whereas collecting and analyzing the PSF images require an additional 2-3 h.},
	number = {12},
	journal = {Nature protocols},
	author = {Cole, Richard W and Jinadasa, Tushare and Brown, Claire M},
	year = {2011},
	note = {Publisher: Nature Publishing Group},
	keywords = {Imaging, Three-Dimensional, Imaging, Three-Dimensional: methods, Microscopy, Confocal, Microscopy, Confocal: methods, Microscopy, Confocal: standards, Microspheres, Quality Control, Weights and Measures},
	pages = {1929--41},
}

@article{maiti_fluorescence_1997-1,
	title = {Fluorescence correlation spectroscopy: diagnostics for sparse molecules.},
	volume = {94},
	issn = {0027-8424 (Print){\textbackslash}r0027-8424 (Linking)},
	doi = {10.1073/pnas.94.22.11753},
	abstract = {The robust glow of molecular fluorescence renders even sparse molecules detectable and susceptible to analysis for concentration, mobility, chemistry, and photophysics. Correlation spectroscopy, a statistical-physics-based tool, gleans quantitative information from the spontaneously fluctuating fluorescence signals obtained from small molecular ensembles. This analytical power is available for studying molecules present at minuscule concentrations in liquid solutions (less than one nanomolar), or even on the surfaces of living cells at less than one macromolecule per square micrometer. Indeed, routines are becoming common to detect, locate, and examine individual molecules under favorable conditions.},
	number = {October},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Maiti, S and Haupts, U and Webb, W W},
	year = {1997},
	pages = {11753--11757},
}

@article{davidson_engineered_2009,
	title = {Engineered fluorescent proteins:  innovations and applications.},
	volume = {6},
	issn = {1548-7091},
	url = {http://dx.doi.org/10.1038/nmeth1009-713},
	doi = {10.1038/nmeth1009-713},
	abstract = {Nature Methods 6, 713 (2009). doi:10.1038/nmeth1009-713},
	number = {10},
	journal = {Nature methods},
	author = {Davidson, Michael W and Campbell, Robert E},
	year = {2009},
	note = {Publisher: Nature Publishing Group},
	pages = {713--717},
}

@article{manuscript_nih_2012,
	title = {{NIH} {Public} {Access}},
	volume = {29},
	issn = {2122633255},
	doi = {10.1016/j.biotechadv.2011.08.021.Secreted},
	number = {5},
	journal = {Changes},
	author = {Manuscript, Author},
	year = {2012},
	pages = {997--1003},
}

@article{noauthor_feedback_nodate,
	title = {feedback},
}

@article{tsien_constructing_2009,
	title = {Constructing and exploiting the fluorescent protein paintbox ({Nobel} {Lecture}).},
	volume = {48},
	issn = {1757-9708 (Electronic){\textbackslash}n1757-9694 (Linking)},
	doi = {10.1002/anie.200901916},
	abstract = {Trip the light fantastic: The green fluorescent protein (GFP) is an invaluable tool for biochemical and medicinal research. It can make tumors, amyloid plaques from Alzheimerprimes disease, or pathogenic bacteria equally visible. Ground-breaking contributions in this field have resulted in the 2008 Nobel Prize for Chemistry being awarded to Osamu Shimomura, Martin Chalife, and Roger Tsien. The Nobel Laureates describe first-hand their research.},
	journal = {Angewandte Chemie (International ed. in English)},
	author = {Tsien, Roger Y.},
	year = {2009},
	pages = {5612--5626},
}

@article{medina_fluorescence_2002,
	title = {Fluorescence correlation spectroscopy for the detection and study of single molecules in biology.},
	volume = {24},
	doi = {10.1002/bies.10118},
	abstract = {The recent development of single molecule detection techniques has opened new horizons for the study of individual macromolecules under physiological conditions. Conformational subpopulations, internal dynamics and activity of single biomolecules, parameters that have so far been hidden in large ensemble averages, are now being unveiled. Herein, we review a particular attractive solution-based single molecule technique, fluorescence correlation spectroscopy (FCS). This time-averaging fluctuation analysis which is usually performed in Confocal setups combines maximum sensitivity with high statistical confidence. FCS has proven to be a very versatile and powerful tool for detection and temporal investigation of biomolecules at ultralow concentrations on surfaces, in solution, and in living cells. The introduction of dual-color cross-correlation and two-photon excitation in FCS experiments is currently increasing the number of promising applications of FCS to biological research.},
	journal = {BioEssays : news and reviews in molecular, cellular and developmental biology},
	author = {Medina, Miguel Angel and Schwille, Petra},
	year = {2002},
	pages = {758--764},
}

@article{bacia_probing_2002-1,
	title = {Probing the endocytic pathway in live cells using dual-color fluorescence cross-correlation analysis.},
	volume = {83},
	issn = {0006-3495 (Print){\textbackslash}n0006-3495 (Linking)},
	doi = {10.1016/S0006-3495(02)75242-9},
	abstract = {Fluorescence (auto)correlation spectroscopy (FCS) has developed into a widely used method for investigating molecular dynamics and mobility of molecules in vitro and in vivo. Dual-color cross-correlation, an extension of this technique, also assesses the concomitant movement of two spectrally distinguishable fluorescent molecules and has therefore proven superior to autocorrelation analysis to study interactions between different molecular species in solution. Here we explore the benefits of cross-correlation analysis when applied to live cells, by demonstrating its potential in analyzing endocytic processes. Bacterial cholera toxin (CTX) was labeled with Cy2 and Cy5 dyes on different subunits of the same holotoxin. Along the endocytic pathway, positive cross-correlation between the A and B subunits was first preserved, later followed by a loss in cross-correlation upon their separation in the Golgi. Furthermore, endocytosis of a mixture of only Cy2- and only Cy5-labeled holotoxins also gave rise to cross-correlation. Our results suggest that cross-correlation may be used to recognize whether different cargoes use the same endocytic pathway. Additionally, we show that cross-correlation is applicable to two-dimensional membrane diffusion. CTX bound to GM1-containing artificial giant unilamellar vesicles was diffusible, whereas CTX bound to the plasma membrane was immobile on the FCS time-scale, possibly because of raft-association of GM1.},
	number = {August},
	journal = {Biophysical journal},
	author = {Bacia, Kirsten and Majoul, Irina V and Schwille, Petra},
	year = {2002},
	pages = {1184--1193},
}

@article{noauthor_polarization_2002,
	title = {Polarization {Microscopy}},
	issn = {0471143030},
	journal = {Current},
	year = {2002},
	pages = {1--27},
}

@article{angus_retinoblastoma_nodate,
	title = {Retinoblastoma tumor suppressor : complex dynamics in vivo},
	author = {Angus, Steven P and Solomon, David A and Kuschel, Lioba and Hennigan, Robert F and Knudsen, Erik S},
}

@article{bacia_probing_2002-2,
	title = {Probing the endocytic pathway in live cells using dual-color fluorescence cross-correlation analysis.},
	volume = {83},
	issn = {0006-3495 (Print){\textbackslash}n0006-3495 (Linking)},
	doi = {10.1016/S0006-3495(02)75242-9},
	abstract = {Fluorescence (auto)correlation spectroscopy (FCS) has developed into a widely used method for investigating molecular dynamics and mobility of molecules in vitro and in vivo. Dual-color cross-correlation, an extension of this technique, also assesses the concomitant movement of two spectrally distinguishable fluorescent molecules and has therefore proven superior to autocorrelation analysis to study interactions between different molecular species in solution. Here we explore the benefits of cross-correlation analysis when applied to live cells, by demonstrating its potential in analyzing endocytic processes. Bacterial cholera toxin (CTX) was labeled with Cy2 and Cy5 dyes on different subunits of the same holotoxin. Along the endocytic pathway, positive cross-correlation between the A and B subunits was first preserved, later followed by a loss in cross-correlation upon their separation in the Golgi. Furthermore, endocytosis of a mixture of only Cy2- and only Cy5-labeled holotoxins also gave rise to cross-correlation. Our results suggest that cross-correlation may be used to recognize whether different cargoes use the same endocytic pathway. Additionally, we show that cross-correlation is applicable to two-dimensional membrane diffusion. CTX bound to GM1-containing artificial giant unilamellar vesicles was diffusible, whereas CTX bound to the plasma membrane was immobile on the FCS time-scale, possibly because of raft-association of GM1.},
	journal = {Biophysical journal},
	author = {Bacia, Kirsten and Majoul, Irina V and Schwille, Petra},
	year = {2002},
	keywords = {fcs, confocal microscopy, fluorescence correlation spectroscopy, endocytosis, experimental biophysics group, intracellular trafficking, single molecule analysis},
	pages = {1184--1193},
}

@article{material_no_nodate,
	title = {No {Title}},
	author = {Material, Supplementary},
	pages = {1--7},
}

@article{van_kuppeveld_homomultimerization_2002-1,
	title = {Homomultimerization of the coxsackievirus {2B} protein in living cells visualized by fluorescence resonance energy transfer microscopy.},
	volume = {76},
	issn = {0022-538X (Print). 0022-538X (Linking)},
	doi = {10.1128/JVI.76.18.9446-9456.2002},
	abstract = {The 2B protein of enteroviruses is the viral membrane-active protein that is responsible for the modifications in host cell membrane permeability that take place in enterovirus-infected cells. The 2B protein shows structural similarities to the group of lytic polypeptides, polypeptides that permeate membranes either by forming multimeric membrane-integral pores or, alternatively, by lying parallel to the lipid bilayer and disturbing the curvature and symmetry of the membrane. Our aim is to gain more insight into the molecular architecture of the 2B protein in vivo. In this study, the possible existence of multimers of the coxsackie B3 virus 2B protein in single living cells was explored by fluorescence resonance energy transfer (FRET) microscopy. FRET between fusion proteins 2B-ECFP and 2B-EYFP (enhanced cyan and yellow fluorescent variants of green fluorescent protein) was monitored by using spectral imaging microscopy (SPIM) and fluorescence lifetime imaging microscopy (FLIM). Both techniques revealed the occurrence of intermolecular FRET between 2B-ECFP and 2B-EYFP, providing evidence for the formation of protein 2B homomultimers. Putative models for the mode of action of the membrane-active 2B protein and the formation of membrane-integral pores by 2B multimers are discussed.},
	number = {18},
	journal = {Journal of virology},
	author = {van Kuppeveld, Frank J M and Melchers, Willem J G and Willems, Peter H G M and Gadella, Theodorus W J},
	year = {2002},
	pages = {9446--9456},
}

@article{guan_adaptive_2008-5,
	title = {Adaptive correction technique for {3D} reconstruction of fluorescence microscopy images.},
	volume = {71},
	issn = {1059-910X (Print)},
	doi = {10.1002/jemt},
	abstract = {Recent advances in high-resolution imaging have provided valuable novel insights into structural relationships within cells and tissues both in vitro and in vivo. An analysis of this kind is regularly done by optical sectioning using either confocal or deconvolution microscopy. However, the reconstruction of 3D images suffers from light scattering and absorption with increasing depth by finite transparency of the used media. Photobleaching of fluorochromes has been especially troublesome and often the only remedy for loss of signal during optical sectioning is to reduce the number of sections. This causes disparities in the x-y and z dimensions of voxels, which lead to vertical distortion of the original stack of images and necessitates interpolation. Interpolation is necessary to fill up the gaps between consecutive sections in the original image stack to obtain cubic voxels. The present manuscript describes a novel method for adaptive compensation of attenuation of light intensity in stacks of fluorescence microscopy images that is based on a physical model of light attenuation. First, we use a fast interpolation technique to generate a cubic voxel-based volume stack with the aid of a contribution look up table. With the contribution look up table, multiple calculations are avoided, which substantially reduces the computational time without compromising the accuracy of the restoration procedure. Second, each section within the resulting volume is processed to rectify its intensity values that have been altered due to photobleaching and scattering and absorption. The method allows to define the last good section in the stack and the correction is then done automatically.},
	number = {August},
	journal = {Microscopy research and technique},
	author = {Guan, Y Q and Cai, Y Y and Zhang, X and Lee, Y T and Opas, M},
	year = {2008},
	keywords = {fluorescence lifetime imaging, fluorescence resonance energy transfer, fluorescent proteins, imaging, live cell, protein-protein interaction},
	pages = {146--157},
}

@article{materials_ece_nodate,
	title = {{ECE} 5616 {Curtis}},
	author = {Materials, Crystalline},
}

@article{noauthor_methods_nodate,
	title = {Methods {Supplemental}},
	volume = {1},
	journal = {Imaging},
}

@article{bacia_practical_2007,
	title = {Practical guidelines for dual-color fluorescence cross-correlation spectroscopy.},
	volume = {2},
	issn = {1750-2799 (Electronic)},
	doi = {10.1038/nprot.2007.410},
	abstract = {Dual-color fluorescence cross-correlation spectroscopy (FCCS) allows for the determination of molecular mobility and concentrations and for the quantitative analysis of molecular interactions such as binding or cleavage at very low concentrations. This protocol discusses considerations for preparing a biological system for FCCS experiments and offers practical advice for performing FCCS on a commercially available setup. Although FCCS is closely related to two-color confocal microscopy, critical adjustments and test measurements are necessary to establish successful FCCS measurements, which are described in a step-by-step manner. Moreover, we discuss control experiments for a negative cross-correlation artifact, arising from a lack of detection volume overlap, and a positive artifact, arising from cross-talk. FCCS has been applied to follow molecular interactions in solutions, on membranes and in cells and to analyze dynamic colocalization during intracellular transport. It is a technique that is expected to see new applications in various fields of biochemical and cell biological research.},
	journal = {Nature protocols},
	author = {Bacia, Kirsten and Schwille, Petra},
	year = {2007},
	pages = {2842--2856},
}

@article{noauthor_1471-2121-2-8-5_nodate,
	title = {1471-2121-2-8-5},
}

@article{noauthor_support_nodate,
	title = {support},
}

@article{gregor_optical_2005,
	title = {Optical saturation in fluorescence correlation spectroscopy under continuous-wave and pulsed excitation},
	volume = {6},
	issn = {1439-4235 (Print){\textbackslash}r1439-4235 (Linking)},
	doi = {10.1002/cphc.200400319},
	abstract = {A detailed theoretical and experimental study of the dependence of fluorescence correlation measurements on optical excitation power due to optical saturation effects is presented. It is shown that the sensitivity of a fluorescence correlation measurement on excitation power becomes increasingly stronger for decreasing excitation power. This makes exact measurements or diffusion coefficients with fluorescence correlation spectroscopy rather difficult. A strong difference of this behavior for continuous-wave and pulsed excitation is found.},
	journal = {ChemPhysChem},
	author = {Gregor, Ingo and Patra, Digambara and Enderlein, Jörg},
	year = {2005},
	keywords = {Diffusion coefficients, Fluorescence spectroscopy, Fluorescence, Optical saturation},
	pages = {164--170},
}

@article{goedhart_bright_2010-1,
	title = {Bright cyan fluorescent protein variants identified by fluorescence lifetime screening.},
	volume = {7},
	issn = {1548-7091},
	url = {http://dx.doi.org/10.1038/nmeth.1415},
	doi = {10.1038/nmeth.1415},
	abstract = {Optimization of autofluorescent proteins by intensity-based screening of bacteria does not necessarily identify the brightest variant for eukaryotes. We report a strategy to screen excited state lifetimes, which identified cyan fluorescent proteins with long fluorescence lifetimes ({\textgreater}3.7 ns) and high quantum yields ({\textgreater}0.8). One variant, mTurquoise, was 1.5-fold brighter than mCerulean in mammalian cells and decayed mono-exponentially, making it an excellent fluorescence resonance energy transfer (FRET) donor.},
	number = {2},
	journal = {Nature methods},
	author = {Goedhart, Joachim and van Weeren, Laura and Hink, Mark a and Vischer, Norbert O E and Jalink, Kees and Gadella, Theodorus W J},
	year = {2010},
	note = {Publisher: Nature Publishing Group},
	pages = {137--139},
}

@article{points_protein_nodate,
	title = {Protein : {Protein} {Interactions}},
	author = {Points, Key},
}

@article{lumma_dynamics_2003-1,
	title = {Dynamics of large semiflexible chains probed by fluorescence correlation spectroscopy.},
	volume = {90},
	issn = {0031-9007{\textbackslash}n1079-7114},
	doi = {10.1103/PhysRevLett.90.218301},
	abstract = {Fluorescence correlation spectroscopy was used to probe the dynamics of lambda-phage DNA in aqueous solution labeled with the randomly intercalating dye TOTO. The linear macromolecules (i). carry more than one chromophore and (ii). are larger than the waist of the focal volume. The correlation function decays significantly faster than expected for a stiff globule of corresponding size but is in good agreement with the dynamic model of semiflexible chains including hydrodynamic interactions. As the chromophore density is lowered the correlation time decreases in accordance with this model.},
	number = {May},
	journal = {Physical review letters},
	author = {Lumma, D and Keller, S and Vilgis, T and Rädler, J O},
	year = {2003},
	pages = {218301--218301},
}

@article{subach_photoactivatable_2009,
	title = {Photoactivatable {mCherry} for high-resolution two-color fluorescence microscopy.},
	volume = {6},
	issn = {1548-7091},
	doi = {10.1038/nmeth0409-311},
	abstract = {The reliance of modern microscopy techniques on photoactivatable fluorescent proteins prompted development of mCherry variants that are initially dark but become red fluorescent after violet-light irradiation. Using ensemble and single-molecule characteristics as selection criteria, we developed PAmCherry1 with excitation/emission maxima at 564/595 nm. Compared to other monomeric red photoactivatable proteins, it has faster maturation, better pH stability, faster photoactivation, higher photoactivation contrast and better photostability. Lack of green fluorescence and single-molecule behavior make monomeric PAmCherry1 a preferred tag for two-color diffraction-limited photoactivation imaging and for super-resolution techniques such as one- and two-color photoactivated localization microscopy (PALM). We performed PALM imaging using PAmCherry1-tagged transferrin receptor expressed alone or with photoactivatable GFP-tagged clathrin light chain. Pair correlation and cluster analyses of the resulting PALM images identified {\textless} or =200 nm clusters of transferrin receptor and clathrin light chain at {\textless} or =25 nm resolution and confirmed the utility of PAmCherry1 as an intracellular probe.},
	number = {2},
	journal = {Nature methods},
	author = {Subach, Fedor V and Patterson, George H and Manley, Suliana and Gillette, Jennifer M and Lippincott-Schwartz, Jennifer and Verkhusha, Vladislav V},
	year = {2009},
	pages = {153--159},
}

@article{deniz_single-pair_1999,
	title = {Single-pair fluorescence resonance energy transfer on freely diffusing molecules: observation of {Förster} distance dependence and subpopulations.},
	volume = {96},
	issn = {0027-8424},
	doi = {10.1073/pnas.96.7.3670},
	abstract = {Photon bursts from single diffusing donor-acceptor labeled macromolecules were used to measure intramolecular distances and identify subpopulations of freely diffusing macromolecules in a heterogeneous ensemble. By using DNA as a rigid spacer, a series of constructs with varying intramolecular donor-acceptor spacings were used to measure the mean and distribution width of fluorescence resonance energy transfer (FRET) efficiencies as a function of distance. The mean single-pair FRET efficiencies qualitatively follow the distance dependence predicted by Förster theory. Possible contributions to the widths of the FRET efficiency distributions are discussed, and potential applications in the study of biopolymer conformational dynamics are suggested. The ability to measure intramolecular (and intermolecular) distances for single molecules implies the ability to distinguish and monitor subpopulations of molecules in a mixture with different distances or conformational states. This is demonstrated by monitoring substrate and product subpopulations before and after a restriction endonuclease cleavage reaction. Distance measurements at single-molecule resolution also should facilitate the study of complex reactions such as biopolymer folding. To this end, the denaturation of a DNA hairpin was examined by using single-pair FRET.},
	number = {March},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Deniz, a a and Dahan, M and Grunwell, J R and Ha, T and Faulhaber, a E and Chemla, D S and Weiss, S and Schultz, P G},
	year = {1999},
	pages = {3670--3675},
}

@article{schmid_high-speed_2013-1,
	title = {High-speed panoramic light-sheet microscopy reveals global endodermal cell dynamics.},
	volume = {4},
	issn = {2041-1723 (Electronic){\textbackslash}n2041-1723 (Linking)},
	url = {http://www.nature.com/ncomms/2013/130725/ncomms3207/full/ncomms3207.html},
	doi = {10.1038/ncomms3207},
	abstract = {The ever-increasing speed and resolution of modern microscopes make the storage and post-processing of images challenging and prevent thorough statistical analyses in developmental biology. Here, instead of deploying massive storage and computing power, we exploit the spherical geometry of zebrafish embryos by computing a radial maximum intensity projection in real time with a 240-fold reduction in data rate. In our four-lens selective plane illumination microscope (SPIM) setup the development of multiple embryos is recorded in parallel and a map of all labelled cells is obtained for each embryo in {\textless}10 s. In these panoramic projections, cell segmentation and flow analysis reveal characteristic migration patterns and global tissue remodelling in the early endoderm. Merging data from many samples uncover stereotypic patterns that are fundamental to endoderm development in every embryo. We demonstrate that processing and compressing raw image data in real time is not only efficient but indispensable for image-based systems biology.},
	journal = {Nature communications},
	author = {Schmid, Benjamin and Shah, Gopi and Scherf, Nico and Weber, Michael and Thierbach, Konstantin and Campos, Citlali Pérez and Roeder, Ingo and Aanstad, Pia and Huisken, Jan},
	year = {2013},
	keywords = {Light, Animals, Body Patterning, Cell Differentiation, Cell Movement, Embryo, Nonmammalian, Endoderm, Endoderm: cytology, Endoderm: embryology, Image Processing, Computer-Assisted, Image Processing, Computer-Assisted: instrumentati, Image Processing, Computer-Assisted: methods, Microscopy, Fluorescence, Microscopy, Fluorescence: instrumentation, Microscopy, Fluorescence: methods, Zebrafish, Zebrafish: anatomy \& histology, Zebrafish: embryology},
	pages = {2207--2207},
}

@article{schutz_properties_2000,
	title = {Properties of lipid microdomains in a muscle cell membrane visualized by single molecule microscopy.},
	volume = {19},
	issn = {0261-4189},
	doi = {10.1093/emboj/19.5.892},
	abstract = {The lateral motion of single fluorescence labeled lipid molecules was imaged in native cell membranes on a millisecond time scale and with positional accuracy of approximately 50 nm, using 'single dye tracing'. This first application of single molecule microscopy to living cells rendered possible the direct observation of lipid-specific membrane domains. These domains were sensed by a lipid probe with saturated acyl chains as small areas in a liquid-ordered phase: the probe showed confined but fast diffusion, with high partitioning (approximately 100-fold) and long residence time (approximately 13 s). The analogous probe with mono-unsaturated chains diffused predominantly unconfined within the membrane. With approximately 15 saturated probes per domain, the locations, sizes, shapes and motions of individual domains became clearly visible. Domains had a size of 0.7 micrometer (0.2-2 micrometer), covering approximately 13\% of total membrane area. Both the liquid-ordered phase characteristics and the sizes of domains match properties of membrane fractions described as detergent-resistant membranes (DRMs), strongly suggesting that the domains seen are the in vivo correlate of DRMs and thus may be identified as lipid rafts.},
	number = {5},
	journal = {The EMBO journal},
	author = {Schütz, G J and Kada, G and Pastushenko, V P and Schindler, H},
	year = {2000},
	keywords = {fluorescence imaging, human coronary artery, lipid rafts, liquid-ordered lipid phase, single molecule diffusion, smooth muscle cell},
	pages = {892--901},
}

@article{noauthor_alberto_nodate-2,
	title = {Alberto {Diaspro} 2.pdf},
}

@article{photon_photon_2002,
	title = {The {Photon} {Counting} {Histogram} ( {PCH} )},
	journal = {Imaging},
	author = {Photon, The and Histogram, Counting},
	year = {2002},
}

@article{schenk_photodynamics_2004,
	title = {Photodynamics of red fluorescent proteins studied by fluorescence correlation spectroscopy.},
	volume = {86},
	issn = {0006-3495 (Print) 0006-3495 (Linking)},
	doi = {10.1016/S0006-3495(04)74114-4},
	abstract = {Red fluorescent proteins are important tools in fluorescence-based life science research. Recently, we have introduced eqFP611, a red fluorescent protein with advantageous properties from the sea anemone Entacmaea quadricolor. Here, we have studied the submillisecond light-driven intramolecular dynamics between bright and dark states of eqFP611 and, for comparison, drFP583 (DsRed) by using fluorescence correlation spectroscopy on protein solutions. A three-state model with one dark and two fluorescent states describes the power-dependence of the flickering dynamics of both proteins at different excitation wavelengths. It involves two light-driven conformational transitions. We have also studied the photodynamics of individual (monomeric) eqFP611 molecules immobilized on surfaces. The flickering rates and dark state fractions of eqFP611 bound to polyethylene glycol-covered glass surfaces were identical to those measured in solution, showing that the bound FPs behaved identically. A second, much slower flickering process was observed on the 10-ms timescale. Deposition of eqFP611 molecules on bare glass surfaces yielded bright fluorescence without any detectable flickering and a {\textgreater}10-fold decreased photobleaching yield. These observations underscore the intimate connection between protein motions and photophysical processes in fluorescent proteins.},
	number = {January},
	journal = {Biophysical journal},
	author = {Schenk, Andreas and Ivanchenko, Sergey and Röcker, Carlheinz and Wiedenmann, Jörg and Nienhaus, G Ulrich},
	year = {2004},
	pages = {384--394},
}

@article{schwille_molecular_1999-1,
	title = {Molecular {Dynamics} in {Living} {Cells} {Observed} by {Fluorescence} {Correlation} {Spectroscopy} with {One}- and {Two}-{Photon} {Excitation}},
	volume = {77},
	url = {http://www.biophysj.org/cgi/content/abstract/77/4/2251},
	abstract = {Multiphoton excitation (MPE) of fluorescent probes has become an attractive alternative in biological applications of laser scanning microscopy because many problems encountered in spectroscopic measurements of living tissue such as light scattering, autofluorescence, and photodamage can be reduced. The present study investigates the characteristics of two-photon excitation (2PE) in comparison with confocal one-photon excitation (1PE) for intracellular applications of fluorescence correlation spectroscopy (FCS). FCS is an attractive method of measuring molecular concentrations, mobility parameters, chemical kinetics, and fluorescence photophysics. Several FCS applications in mammalian and plant cells are outlined, to illustrate the capabilities of both 1PE and 2PE. Photophysical properties of fluorophores required for quantitative FCS in tissues are analyzed. Measurements in live cells and on cell membranes are feasible with reasonable signal-to-noise ratios, even with fluorophore concentrations as low as the single-molecule level in the sampling volume. Molecular mobilities can be measured over a wide range of characteristic time constants from {\textasciitilde}10[-]3 to 103 ms. While both excitation alternatives work well for intracellular FCS in thin preparations, 2PE can substantially improve signal quality in turbid preparations like plant cells and deep cell layers in tissue. At comparable signal levels, 2PE minimizes photobleaching in spatially restrictive cellular compartments, thereby preserving long-term signal acquisition.},
	number = {October},
	journal = {Biophys. J.},
	author = {Schwille, Petra and Haupts, Ulrich and Maiti, Sudipta and Webb, Watt W},
	year = {1999},
	pages = {2251--2265},
}

@article{schwille_fluorescence_2001-1,
	title = {Fluorescence correlation spectroscopy and its potential for intracellular applications.},
	volume = {34},
	issn = {1085-9195},
	doi = {10.1385/CBB:34:3:383},
	abstract = {Fluorescence correlation spectroscopy (FCS) is a time-averaging fluctuation analysis of small molecular ensembles, combining maximum sensitivity with high statistical confidence. Among a multitude of physical parameters that are, in principle, accessible by FCS, it most conveniently allows to determine local concentrations, mobility coefficients, and characteristic rate constants of fast-reversible and slow-irreversible reactions of fluorescently labeled biomolecules at very low (nanomolar) concentrations, under equilibrium conditions and without physical separation. Its presently most popular instrumentation by confocal-microscope setups allows for a spatial resolution of fractions of femtoliters for the measurement volumes, containing sparse or even single molecules at any time, and encourages the adaptation of the solution-based technique for cellular applications. The scope of this review is thus, to introduce the FCS technique in particular to the reader with biological background, searching for new methods for a precise quantification of physical parameters governing cellular mechanisms and dynamics, especially if high sensitivity and fast dynamic resolution are required. After a short theoretical introduction, examples are given for the so far most important experimental applications, with respect to their implementation in cellular systems. As an interesting alternative to the confocal instrumentation, two-photon excitation will be introduced, offering a number of important advantages especially in cellular systems with high-noise and low-signal levels.},
	journal = {Cell biochemistry and biophysics},
	author = {Schwille, P},
	year = {2001},
	pages = {383--408},
}

@article{chen_measurement_2006,
	title = {Measurement of {FRET} efficiency and ratio of donor to acceptor concentration in living cells.},
	volume = {91},
	issn = {0006-3495 (Print)},
	doi = {10.1529/biophysj.106.088773},
	abstract = {Measurement of fluorescence resonance energy transfer (FRET) efficiency and the relative concentration of donor and acceptor fluorophores in living cells using the three-filter cube approach requires the determination of two constants: 1), the ratio of sensitized acceptor emission to donor fluorescence quenching (G factor) and 2), the ratio of donor/acceptor fluorescence intensity for equimolar concentrations in the absence of FRET (k factor). We have developed a method to determine G and k that utilizes two donor-acceptor fusion proteins with differing FRET efficiencies-the value of which need not be known. We validated the method by measuring the FRET efficiency and concentration ratio of the fluorescent proteins Cerulean and Venus in mammalian cells expressing a series of fusion proteins with varying stoichiometries. The method greatly simplifies quantitative FRET measurement in living cells as it does not require cell fixation, acceptor photobleaching, protein purification, or specialized equipment for determining fluorescence spectra or lifetime.},
	number = {7},
	journal = {Biophysical journal},
	author = {Chen, Huanmian and Puhl, Henry L and Koushik, Srinagesh V and Vogel, Steven S and Ikeda, Stephen R},
	year = {2006},
	pages = {L39--L41},
}

@article{noauthor_mcnamara_boswell_000_2006_index_dyes_fps_filters_lamps_other_spectra_nodate,
	title = {{McNamara}\_Boswell\_000\_2006\_Index\_Dyes\_FPs\_Filters\_Lamps\_Other\_Spectra},
}

@article{jakobs_egfp_2000-1,
	title = {{EGFP} and {DsRed} expressing cultures of {Escherichia} coli imaged by confocal, two-photon and fluorescence lifetime microscopy},
	volume = {479},
	issn = {0014-5793 (Print)},
	doi = {10.1016/S0014-5793(00)01896-2},
	abstract = {The green fluorescent protein (GFP) has become an invaluable marker for monitoring protein localization and gene expression in vivo. Recently a new red fluorescent protein (drFP583 or DsRed), isolated from tropical corals, has been described [Matz, M.V. et al. (1999) Nature Biotech. 17, 969-973]. With emission maxima at 509 and 583 nm respectively, EGFP and DsRed are suited for almost crossover free dual color labeling upon simultaneous excitation. We imaged mixed populations of Escherichia coli expressing either EGFP or DsRed by one-photon confocal and by two-photon microscopy. Both excitation modes proved to be suitable for imaging cells expressing either of the fluorescent proteins. DsRed had an extended maturation time and E. coli expressing this fluorescent protein were significantly smaller than those expressing EGFP. In aging bacterial cultures DsRed appeared to aggregate within the cells, accompanied by a strong reduction in its fluorescence lifetime as determined by fluorescence lifetime imaging microscopy. Copyright (C) 2000 Federation of European Biochemical Societies.},
	journal = {FEBS Letters},
	author = {Jakobs, Stefan and Subramaniam, Vinod and Schönle, Andreas and Jovin, Thomas M. and Hell, Stefan W.},
	year = {2000},
	keywords = {Two-photon, Confocal, Escherichia coli, Fluorescence lifetime microscopy, Green fluorescent protein, Red fluorescent protein},
	pages = {131--135},
}

@article{noauthor_cy2_nodate,
	title = {Cy2},
}

@article{heikal_molecular_2000-1,
	title = {Molecular spectroscopy and dynamics of intrinsically fluorescent proteins: coral red ({dsRed}) and yellow ({Citrine}).},
	volume = {97},
	issn = {0027-8424 (Print){\textbackslash}r0027-8424 (Linking)},
	doi = {10.1073/pnas.97.22.11996},
	abstract = {Gene expression of intrinsically fluorescent proteins in biological systems offers new noninvasive windows into cellular function, but optimization of these probes relies on understanding their molecular spectroscopy, dynamics, and structure. Here, the photophysics of red fluorescent protein (dsRed) from discosoma (coral), providing desired longer emission/absorption wavelengths, and an improved yellow fluorescent protein mutant (Citrine) (S65G/V68L/Q69 M/S72A/T203Y) for significant comparison, are characterized by using fluorescence correlation spectroscopy and time-correlated single-photon counting. dsRed fluorescence decays as a single exponential with a 3.65 +/- 0.07-ns time constant, indicating a single emitting state/species independent of pH 4.4-9.0, in contrast with Citrine. However, laser excitation drives reversible fluorescence flicker at 10(3)-10(4) Hz between dark and bright states with a constant partition fraction f(1) = 0.42 +/- 0.06 and quantum yield of approximately 3 x 10(-3). Unlike Citrine (pKa approximately 5.7), pH-dependent proton binding is negligible (pH 3. 9-11) in dsRed. Time-resolved anisotropy of dsRed reveals rapid depolarization (211 +/- 6 ps) plus slow rotational motion (53 +/- 8 ns), in contrast with a single rotational time (16 +/- 2 ns) for Citrine. The molecular dimensions, calculated from rotational and translational diffusion, indicate that dsRed is hydrodynamically 3.8 +/- 0.4 times larger than predicted for a monomer, which suggests an oligomer (possibly a tetramer) configuration even at approximately 10(-9) M. The fast depolarization is attributed to intraoligomer energy transfer between mobile nonparallel chromophores with the initial anisotropy implying a 24 +/- 3 degrees depolarization angle. Large two-photon excitation cross sections ( approximately 100 GM at 990 nm for dsRed and approximately 50 GM at 970 nm for Citrine), advantageous for two-photon-fluorescence imaging in cells, are measured.},
	number = {22},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Heikal, a a and Hess, S T and Baird, G S and Tsien, R Y and Webb, W W},
	year = {2000},
	pages = {11996--12001},
}

@article{schwille_fluorescence_2009-1,
	title = {Fluorescence {Correlation} {Spectroscopy}: {An} {Introduction} to its {Concepts} and {Applications}},
	volume = {94},
	doi = {10.1002/lpor.200910041},
	abstract = {An alternative version of fluorescence correlation spectroscopy is presented, where the signal from a medium surrounding the particles of interest is analyzed, as opposed to a signal from the particles themselves. This allows for analysis of unlabeled particles and potentially of biomolecules. Here, the concept together with principal experiments on polystyrene beads of 100, 200, 400, and 800 nm diameter in an aqueous solution of alexa 488-fluorophores are presented. The use of photo detectors allowing higher photon fluxes, or of reduced detection volumes, should enable analysis of significantly smaller particles or even biomolecules.},
	journal = {Spectroscopy},
	author = {Schwille, Petra and Haustein, Elke},
	year = {2009},
	pages = {1--33},
}

@article{gosch_fluorescence_2005-1,
	title = {Fluorescence correlation spectroscopy of molecular motions and kinetics},
	volume = {57},
	doi = {10.1016/j.addr.2004.07.016},
	abstract = {The foundations for fluorescence correlation spectroscopy (FCS) were already laid in the early 1970s, but this technique did not become widely used until single-molecule detection was established almost 20 years later with the use of diffraction-limited confocal volume element. The analysis of molecular noise from the GHz- to the Hz-region facilitates measurements over a large dynamic range covering photophysics, conformational transitions and interactions as well as transport properties of fluorescent biomolecules. From the Poissonian nature of the noise spectrum the absolute number of molecules is obtainable. Originally used for the analysis of molecular interactions in solutions, the strength of FCS lies also in its applicability to molecular processes at either the surface or interior of single cells. Examples for the analysis of surface kinetics including on and off rates of ligand-receptor interactions will be given. The possibility of obtaining this type of information by FCS will be of particular interest for cell-based drug screening. © 2004 Elsevier B.V. All rights reserved.},
	journal = {Advanced Drug Delivery Reviews},
	author = {Gösch, Michael and Rigler, Rudolf},
	year = {2005},
	keywords = {Fluorescence spectroscopy, Autocorrelation, FCS, FRAP, FRET, Parallel detection, PCH, Photophysics, TIR},
	pages = {169--190},
}

@article{feature_big_2009,
	title = {The big and the bold},
	volume = {459},
	url = {http://www.nature.com/news/2009/090603/full/459634a.html},
	abstract = {1: Nature. 2009 Jun 4;459(7247):634-5. Microscopic marvels: . Cyranoski D. Publication Types: News. Mesh Terms},
	number = {June},
	journal = {Nature},
	author = {Feature, News and Marvels, Microscopic},
	year = {2009},
	pages = {634--5},
}

@article{prummer_multiparameter_2004,
	title = {Multiparameter {Microscopy} and {Spectroscopy} for {Single}-{Molecule} {Analytics}},
	volume = {76},
	issn = {0003-2700 (Print){\textbackslash}r0003-2700 (Linking)},
	doi = {10.1021/ac034976g},
	abstract = {The ability to monitor several parameters simultaneously from distinct individual fluorescent reporter molecules facilitates the disentanglement of complex and interacting systems and opens new perspectives in areas from basic science to biopharmaceutical technology. By combining annular illumination microscopy, time-correlated single-photon counting, and multichannel detection, we were able to determine 14 independent parameters from one individual fluorophore. The whole set of parameters was deduced from the few properties of the fluorescence photons, i.e., arrival time, wavelength, and polarization. With this approach, the intensity, the polarization, and the spectral dynamics can be analyzed on a nanosecond time scale and the mean values can be monitored with submillisecond time resolution. Nanosecond spectral dynamics of single molecules has been observed, to the best of our knowledge, for the first time. From our experience, we can determine all parameters for more than 30\% of the illuminated fluorophores in biological samples and for more than 80\% in doped polymeric films.},
	number = {6},
	journal = {Analytical Chemistry},
	author = {Prummer, Michael and Sick, Beate and Renn, Alois and Wild, Urs P.},
	year = {2004},
	pages = {1633--1640},
}

@article{digman_mapping_2008-3,
	title = {Mapping the number of molecules and brightness in the laser scanning microscope.},
	volume = {94},
	issn = {9498242992},
	doi = {10.1529/biophysj.107.114645},
	abstract = {We describe a technique based on moment-analysis for the measurement of the average number of molecules and brightness in each pixel in fluorescence microscopy images. The average brightness of the particle is obtained from the ratio of the variance to the average intensity at each pixel. To obtain the average number of fluctuating particles, we divide the average intensity at one pixel by the brightness. This analysis can be used in a wide range of concentrations. In cells, the intensity at any given pixel may be due to bright immobile structures, dim fast diffusing particles, and to autofluorescence or scattering. The total variance is given by the variance of each of the above components in addition to the variance due to detector noise. Assuming that all sources of variance are independent, the total variance is the sum of the variances of the individual components. The variance due to the particles fluctuating in the observation volume is proportional to the square of the particle brightness while the variance of the immobile fraction, the autofluorescence, scattering, and that of the detector is proportional to the intensity of these components. Only the fluctuations that depend on the square of the brightness (the mobile particles) will have a ratio of the variance to the intensity {\textgreater}1. Furthermore, changing the fluorescence intensity by increasing the illumination power, distinguishes between these possible contributions. We show maps of molecular brightness and number of cell migration proteins obtained using a two-photon scanning microscope operating with a photon-counting detector. These brightness maps reveal binding dynamics at the focal adhesions with pixel resolution and provide a picture of the binding and unbinding process in which dim molecules attach to the adhesions or large molecular aggregates dissociate from adhesion.},
	number = {March},
	journal = {Biophysical journal},
	author = {Digman, Michelle a and Dalal, Rooshin and Horwitz, Alan F and Gratton, Enrico},
	year = {2008},
	pages = {2320--2332},
}

@article{noauthor_biophotonics_intern_may_2002pdf_nodate,
	title = {Biophotonics\_Intern\_May\_2002.pdf},
}

@article{stephens_cell_nodate,
	title = {Cell {Imaging}},
	author = {Stephens, D},
}

@article{zumbusch_single_2000,
	title = {Single {Molecule} {Spectroscopy} of the {Green} {Fluorescent} {Protein} : {A} {Critical} {Assessment}},
	volume = {1},
	author = {Zumbusch, Andreas and Jung, Gregor},
	year = {2000},
	pages = {261--270},
}

@article{palczewska_noninvasive_2014,
	title = {Noninvasive two-photon microscopy imaging of mouse retina and retinal pigment epithelium through the pupil of the eye.},
	volume = {20},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/24952647},
	doi = {10.1038/nm.3590},
	abstract = {Two-photon excitation microscopy can image retinal molecular processes in vivo. Intrinsically fluorescent retinyl esters in subcellular structures called retinosomes are an integral part of the visual chromophore regeneration pathway. Fluorescent condensation products of all-trans-retinal accumulate in the eye with age and are also associated with age-related macular degeneration (AMD). Here, we report repetitive, dynamic imaging of these compounds in live mice through the pupil of the eye. By leveraging advanced adaptive optics, we developed a data acquisition algorithm that permitted the identification of retinosomes and condensation products in the retinal pigment epithelium by their characteristic localization, spectral properties and absence in genetically modified or drug-treated mice. This imaging approach has the potential to detect early molecular changes in retinoid metabolism that trigger light- and AMD-induced retinal defects and to assess the effectiveness of treatments for these conditions.},
	number = {November 2013},
	journal = {Nature medicine},
	author = {Palczewska, Grazyna and Dong, Zhiqian and Golczak, Marcin and Hunter, Jennifer J and Williams, David R and Alexander, Nathan S and Palczewski, Krzysztof},
	year = {2014},
	note = {Publisher: Nature Publishing Group},
	pages = {785--9},
}

@article{kim_spontaneous_2007,
	title = {Spontaneous {Ultraweak} {Photon} {Emission} during the {Growth} of the {Cell} {Population} of {Cultured} {HeLa} {Cell} {Line}},
	volume = {53},
	doi = {10.1248/jhs.53.481},
	number = {4},
	journal = {Journal of Health Science},
	author = {Kim, Jungdae and Kim, Yong-ung and Lee, Young Joo and Kobayashi, Masaki and Tsutsumi, Yuji and Kondo, Ryuichiro and Lee, Seung Ki and Soh, Kwang-Sup},
	year = {2007},
	pages = {481--485},
}

@article{noauthor_bacskai_03pdf_nodate,
	title = {Bacskai\_03.pdf},
}

@article{noauthor_abstract_nodate,
	title = {Abstract for {SPIE}\_2002\_FCS},
}

@article{fluorescence_use_2007,
	title = {The use of fluorescence anisotropy of {DPH} to monitor the fluidity of biological membranes},
	author = {Fluorescence, Introduction},
	year = {2007},
	pages = {1--7},
}

@article{isothiocyanate_environmental_1995-1,
	title = {Environmental {Factors} {Recorded} with a {Confocal} {Laser} {Scanning} {Microscope} ’ {I} {\textbackslash} {TexanRed}},
	volume = {43},
	number = {7},
	author = {Isothiocyanate, Tetramethylrhodamine and Brismar, Hjalmar and Trepte, Oliver and Ulfhake, Brun},
	year = {1995},
	keywords = {antibody conjuga-, cence lifetime, confocal laser microscopy, emission, excitation, fluores-, fluorophore, ph, spectrum, tion, tissue embedding},
	pages = {699--707},
}

@article{borsch_single-molecule_2011,
	title = {Single-molecule fluorescence resonance energy transfer techniques},
	volume = {392},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21073359},
	doi = {10.1515/BC.2011.001},
	number = {1},
	journal = {Biological chemistry},
	author = {Börsch, Michael},
	year = {2011},
	pages = {135--42},
}

@article{bombarda_time-resolved_1999,
	title = {Time-resolved fluorescence investigation of the human immunodeficiency virus type 1 nucleocapsid protein: influence of the binding of nucleic acids.},
	volume = {76},
	doi = {10.1016/S0006-3495(99)77315-7},
	abstract = {Depending on the HIV-1 isolate, MN or BH10, the nucleocapsid protein, NCp7, corresponds to a 55- or 71-amino acid length product, respectively. The MN NCp7 contains a single Trp residue at position 37 in the distal zinc finger motif, and the BH10 NCp7 contains an additional Trp, at position 61 in the C-terminal chain. The time-resolved intensity decay parameters of the zinc-saturated BH10 NCp7 were determined and compared to those of single-Trp-containing derivatives. The fluorescence decay of BH10 NCp7 could be clearly represented as a linear combination (with respect to both lifetimes and fractional intensities) of the individual emitting Trp residues. This suggested the absence of interactions between the two Trp residues, a feature that was confirmed by molecular modeling and fluorescence energy transfer studies. In the presence of tRNAPhe, taken as a RNA model, the same conclusions hold true despite the large fluorescence decrease induced by the binding of tRNAPhe. Indeed, the fluorescence of Trp37 appears almost fully quenched, in keeping with a stacking of this residue with the bases of tRNAPhe. Despite the multiple binding sites in tRNAPhe, the large prevalence of ultrashort lifetimes, associated with the stacking of Trp37, suggests that this stacking constitutes a major feature in the binding process of NCp7 to nucleic acids. In contrast, Trp61 only stacked to a small extent with tRNAPhe. The behavior of this residue in the tRNAPhe-NCp7 complexes appeared to be rather heterogeneous, suggesting that it does not constitute a major determinant in the binding process. Finally, our data suggested that the binding of NCp7 proteins from the two HIV-1 strains to nonspecific nucleic acid sequences was largely similar.},
	number = {March},
	journal = {Biophysical journal},
	author = {Bombarda, E and Ababou, a and Vuilleumier, C and Gérard, D and Roques, B P and Piémont, E and Mély, Y},
	year = {1999},
	pages = {1561--1570},
}

@article{koushik_cerulean_2006,
	title = {Cerulean, {Venus}, and {VenusY67C} {FRET} reference standards.},
	volume = {91},
	issn = {0006-3495 (Print){\textbackslash}n0006-3495 (Linking)},
	doi = {10.1529/biophysj.106.096206},
	abstract = {Förster's resonance energy transfer (FRET) can be used to study protein-protein interactions in living cells. Numerous methods to measure FRET have been devised and implemented; however, the accuracy of these methods is unknown, which makes interpretation of FRET efficiency values difficult if not impossible. This problem exists due to the lack of standards with known FRET efficiencies that can be used to validate FRET measurements. The advent of spectral variants of green fluorescent protein and easy access to cell transfection technology suggests a simple solution to this problem: the development of genetic constructs with known FRET efficiencies that can be replicated with high fidelity and freely distributed. In this study, fluorescent protein constructs with progressively larger separation distances between donors and acceptors were generated and FRET efficiencies were measured using fluorescence lifetime spectroscopy, sensitized acceptor emission, and spectral imaging. Since the results from each method were in good agreement, the FRET efficiency value of each construct could be determined with high accuracy and precision, thereby justifying their use as standards.},
	journal = {Biophysical journal},
	author = {Koushik, Srinagesh V and Chen, Huanmian and Thaler, Christopher and Puhl, Henry L and Vogel, Steven S},
	year = {2006},
	pages = {L99--L101},
}

@article{lin_lifetime-based_1999,
	title = {Lifetime-based {pH} sensors: indicators for acidic environments.},
	volume = {269},
	doi = {10.1006/abio.1999.4011},
	abstract = {We characterized the pH-dependent intensity decays of three fluorophores, Oregon green 514 carboxylic acid, Cl-NERF, and DM-NERF, using frequency-domain fluorometry, with the objective of identifying lifetime-based sensors for low pH values. These three probes were originally designed as dual excitation wavelength-ratiometric probes, with high photostability and high quantum yields in aqueous solutions. We found that their fluorescence intensity decays were strongly dependent on pH. Moreover, global intensity decays analysis reveals that these probes have double exponential intensity decays at intermediate pH values and that the decay time amplitudes are greatly dependent on pH. The longer lifetime components originated from the unprotonated forms and the shorter components from the protonated forms. Both forms can emit fluorescence at intermediate pH values. The apparent pKa values were also determined from the titration curves of phase angles and modulations versus pH for the purpose of pH sensing. The apparent pKa values range from pH 3 to 5, a range where lifetime-based sensors are not presently reported. Since these probes show low pKa values and display substantial phase and modulation changes with pH, they are suitable as lifetime-based pH sensors to monitor the pH changes in acidic environments. One potential application of these probes is to trace the pH in different cellular compartments.},
	journal = {Analytical biochemistry},
	author = {Lin, H J and Szmacinski, H and Lakowicz, J R},
	year = {1999},
	pages = {162--167},
}

@article{chattopadhyay_measurement_2002,
	title = {Measurement of microsecond dynamic motion in the intestinal fatty acid binding protein by using fluorescence correlation spectroscopy.},
	volume = {99},
	doi = {10.1073/pnas.172524899},
	abstract = {Fluorescence correlation spectroscopy (FCS) measurements have been carried out on the intestinal fatty acid binding protein (IFABP) to study microsecond dynamics of the protein in its native state as well as in pH-induced intermediates. IFABP is a small (15 kDa) protein that consists mostly of antiparallel beta-strands enclosing a large central cavity into which the ligand binds. Because this protein does not contain cysteine, two cysteine mutants (Val60Cys and Phe62Cys) have been prepared and covalently modified with fluorescein. Based on fluorescence measurements, one of the mutants (Val60Flu) has the fluorescein moiety inside the cavity of the protein, whereas the fluorescein is exposed to solvent in the other (Phe62Flu). The protein modified at position 60 demonstrates the presence of a conformational event on the order of 35 microsec, which is not seen in the other mutant (Phe62Flu). The amplitude of this fast conformational event decreases sharply at low pH as the protein unfolds. Experiments measuring the diffusion as a function of pH indicate the formation of a compact state distinct from the native state at about pH 3.5. Steady state fluorescence and far-UV CD indicates that unfolding occurs at pH values below pH 3.},
	number = {22},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Chattopadhyay, Krishnananda and Saffarian, Saveez and Elson, Elliot L and Frieden, Carl},
	year = {2002},
	pages = {14171--14176},
}

@article{michalet_quantum_2013,
	title = {Quantum {Dots} for {Live} {Cells} ,},
	volume = {538},
	doi = {10.1126/science.1104274},
	number = {January},
	author = {Michalet, X},
	year = {2013},
	pages = {538--544},
}

@article{wahlby_image_2012,
	title = {An image analysis toolbox for high-throughput {C}. elegans assays},
	volume = {9},
	issn = {1548-7105},
	doi = {10.1038/nmeth.1984},
	abstract = {We present a toolbox for high-throughput screening of image-based Caenorhabditis elegans phenotypes. The image analysis algorithms measure morphological phenotypes in individual worms and are effective for a variety of assays and imaging systems. This WormToolbox is available through the open-source CellProfiler project and enables objective scoring of whole-worm high-throughput image-based assays of C. elegans for the study of diverse biological pathways that are relevant to human disease.},
	number = {7},
	journal = {Nature Methods},
	author = {Wählby, Carolina and Kamentsky, Lee and Liu, Zihan H and Riklin-Raviv, Tammy and Conery, Annie L and O'Rourke, Eyleen J and Sokolnicki, Katherine L and Visvikis, Orane and Ljosa, Vebjorn and Irazoqui, Javier E and Golland, Polina and Ruvkun, Gary and Ausubel, Frederick M and Carpenter, Anne E},
	year = {2012},
	pages = {714--716},
}

@article{manuscript_nih_2012-1,
	title = {{NIH} {Public} {Access}},
	volume = {29},
	issn = {2122633255},
	doi = {10.1016/j.biotechadv.2011.08.021.Secreted},
	number = {1},
	journal = {Changes},
	author = {Manuscript, Author},
	year = {2012},
	keywords = {confocal microscopy, acetowhitening, basal cell carcinomas, mohs surgery, mosaicing},
	pages = {997--1003},
}

@article{lerner_calibration_2004,
	title = {Calibration and validation of confocal spectral imaging systems},
	volume = {62},
	issn = {1552-4922 (Print)},
	doi = {10.1002/cyto.a.20087},
	abstract = {Confocal spectral imaging (CSI) microscopic systems currently on the market delineate multiple fluorescent proteins, labels, or dyes within biological specimens by performing spectral characterizations. However, some CSI systems have been found to present inconsistent spectral profiles of reference spectra within a particular system and between related and unrelated instruments. This variability confirms that there is a need for a standardized, objective calibration and validation protocol.},
	number = {October},
	journal = {Cytometry Part A},
	author = {Lerner, Jeremy M. and Zucker, Robert M.},
	year = {2004},
	keywords = {Confocal microscope, Leica, PARISS, Photomultiplier tube, Quality assurance, SKY, Spectral calibration, Spectral imaging, Validation, Wavelength calibration, Zeiss},
	pages = {8--34},
}

@article{noauthor_lakowicz_1271pdf_nodate,
	title = {Lakowicz\_1271.pdf},
}

@article{noauthor_dapi_nodate-1,
	title = {{DAPI}},
}

@article{colyer_novel_2008,
	title = {A novel fluorescence lifetime imaging system that optimizes photon efficiency},
	volume = {71},
	issn = {1059-910X (Print){\textbackslash}r1059-910X (Linking)},
	doi = {10.1002/jemt.20540},
	abstract = {Fluorescence lifetime imaging (FLIM) is a powerful microscopy technique for providing contrast of biological and other systems by differences in molecular species or their environments. However, the cost of equipment and the complexity of data analysis have limited the application of FLIM. We present a mathematical model and physical implementation for a low cost digital frequency domain FLIM (DFD-FLIM) system, which can provide lifetime resolution with quality comparable to time-correlated single photon counting methods. Our implementation provides data natively in the form of phasors. On the basis of the mathematical model, we present an error analysis that shows the precise parameters for maximizing the quality of lifetime acquisition, as well as data to support this conclusion. The hardware and software of the proposed DFD-FLIM method simplifies the process of data acquisition for FLIM, presents a new interface for data display and interpretation, and optimizes the accuracy of lifetime determination.},
	journal = {Microscopy Research and Technique},
	author = {Colyer, Ryan a. and Lee, Claudia and Gratton, Enrico},
	year = {2008},
	keywords = {Confocal microscopy, FLIM, Fluorescence lifetime, Phasor plot},
	pages = {201--213},
}

@article{siegel_whole-field_2001-1,
	title = {Whole-field five-dimensional fluorescence microscopy combining lifetime and spectral resolution with optical sectioning.},
	volume = {26},
	doi = {10.1364/OL.26.001338},
	abstract = {We report a novel whole-field three-dimensional fluorescence lifetime imaging microscope that incoporates multispectral imaging to provide five-dimensional (5-D) fluorescence microscopy. This instrument, which can acquire a 5-D data set in less than a minute, is based on potentially compact and inexpensive diode-pumped solid-state laser technology. We demonstrate that spectral discrimination as well as optical sectioning minimize artifacts in lifetime determination and illustrate how spectral discrimination improves the lifetime contrast of biological tissue.},
	number = {17},
	journal = {Optics letters},
	author = {Siegel, J and Elson, D S and Webb, S E and Parsons-Karavassilis, D and Lévêque-Fort, S and Cole, M J and Lever, M J and French, P M and Neil, M a and Juskaitis, R and Sucharov, L O and Wilson, T},
	year = {2001},
	pages = {1338--1340},
}

@book{bass_solid-state_2002,
	title = {Solid-{State} {Lasers} : {A} {Graduate} {Text}},
	isbn = {0-387-95590-9},
	url = {http://www.springer.de/phys/},
	abstract = {Solid-state lasers have seen a fast and steady development and are the ubiquitous tool both for research and industrial applications. The author's monograph Solid-State Lasersnbsp;has become the most-used reference book in this area. The present graduate text on solid-state lasers takes advantage of this rich source by focusing on the needs at the graduate level and those who need an introduction. Numerous exercises with hints for solution, new text and updated material where needed make this text very accessible.},
	author = {Bass, Michael and Koechner, Walter},
	year = {2002},
	doi = {10.1007/b97423},
	note = {Pages: 409},
}

@article{becker_fluorescence_2004,
	title = {Fluorescence {Lifetime} {Imaging} by {Time}-{Correlated} {Single}-{Photon} {Counting}},
	volume = {63},
	issn = {1059-910X (Print){\textbackslash}r1059-910X (Linking)},
	doi = {10.1002/jemt.10421},
	abstract = {We present a time-correlated single photon counting (TCPSC) technique that allows time-resolved multi-wavelength imaging in conjunction with a laser scanning microscope and a pulsed excitation source. The technique is based on a four-dimensional histogramming process that records the photon density over the time of the fluorescence decay, the x-y coordinates of the scanning area, and the wavelength. The histogramming process avoids any time gating or wavelength scanning and, therefore, yields a near-perfect counting efficiency. The time resolution is limited only by the transit time spread of the detector. The technique can be used with almost any confocal or two-photon laser scanning microscope and works at any scanning rate. We demonstrate the application to samples stained with several dyes and to CFP-YFP FRET.},
	number = {August 2003},
	journal = {Microscopy Research and Technique},
	author = {Becker, Wolfgang and Bergmann, a. and Hink, M. a. and König, K. and Benndorf, K. and Biskup, C.},
	year = {2004},
	keywords = {Fluorescence lifetime imaging, FRET, Confocal microscopy, Multiphoton microscopy, FLIM, TCSPC, Fluorescence resonance energy transfer, Multi-wavelength, Time-correlated single photon counting},
	pages = {58--66},
}

@article{krawczyk-ba_identification_2007,
	title = {Identification of {Fluorescent} {U} ( {V} ) and {U} ( {VI} ) {Microparticles} in a {Multispecies} {Biofilm} by {Confocal} {Laser} {Scanning} {Microscopy} and {Fluorescence} {Spectroscopy}},
	volume = {41},
	doi = {10.1021/es0710609},
	number = {18},
	author = {Krawczyk-ba, Evelyn and Diessner, Susann and Wobus, Axel and Bernhard, Gert},
	year = {2007},
	pages = {6498--6504},
}

@article{zucker_evaluation_2006,
	title = {Evaluation of confocal microscopy system performance.},
	volume = {319},
	issn = {0196-4763 (Print){\textbackslash}r0196-4763 (Linking)},
	doi = {10.1007/978-1-59259-993-6_5},
	abstract = {The confocal laser scanning microscope (CLSM) has enormous potential in many biological fields. When tests are made to evaluate the performance of a CLSM, the usual subjective assessment is accomplished by using a histological test slide to create a "pretty picture." Without the use of functional tests, many of the machines could be working at suboptimal performance levels, delivering suboptimum performance and possibly misleading data. To replace the subjectivity in evaluating a confocal microscope, tests were derived or perfected that measure field illumination, lens clarity, laser power, laser stability, dichroic functionality, spectral registration, axial resolution, scanning stability, photomultiplier tube quality, overall machine stability, and system noise. These tests will help serve as a guide for other investigators to ensure that their machines are working correctly to provide data that are accurate with the necessary resolution, sensitivity, and precision. Utilization of this proposed testing approach will help eliminate the subjective nature of assessing the CLSM and allow different machines to be compared. These tests are essential if one is to make intensity measurements.},
	journal = {Methods in molecular biology (Clifton, N.J.)},
	author = {Zucker, Robert M},
	year = {2006},
	pages = {77--135},
}

@article{peter_w_atkins_physikalische_2006,
	title = {Physikalische {Chemie}},
	issn = {9783835100404},
	author = {{Peter. W Atkins} and Paula, Julio De},
	year = {2006},
	pages = {489--489},
}

@article{kask_fluorescence-intensity_1999-2,
	title = {Fluorescence-intensity distribution analysis and its application in biomolecular detection technology.},
	volume = {96},
	issn = {0027-8424 (Print){\textbackslash}r0027-8424 (Linking)},
	doi = {10.1073/pnas.96.24.13756},
	abstract = {A methodology, fluorescence-intensity distribution analysis, has been developed for confocal microscopy studies in which the fluorescence intensity of a sample with a heterogeneous brightness profile is monitored. An adjustable formula, modeling the spatial brightness distribution, and the technique of generating functions for calculation of theoretical photon count number distributions serve as the two cornerstones of the methodology. The method permits the simultaneous determination of concentrations and specific brightness values of a number of individual fluorescent species in solution. Accordingly, we present an extremely sensitive tool to monitor the interaction of fluorescently labeled molecules or other microparticles with their respective biological counterparts that should find a wide application in life sciences, medicine, and drug discovery. Its potential is demonstrated by studying the hybridization of 5'-(6-carboxytetramethylrhodamine)-labeled and nonlabeled complementary oligonucleotides and the subsequent cleavage of the DNA hybrids by restriction enzymes.},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Kask, P and Palo, K and Ullmann, D and Gall, K},
	year = {1999},
	pages = {13756--13761},
}

@article{cardoso_untersuchungen_2002,
	title = {Untersuchungen zum {Diffusionsverhalten} in {Lösung}},
	author = {Cardoso, A G},
	year = {2002},
}

@article{wohland_standard_2001,
	title = {The standard deviation in fluorescence correlation spectroscopy.},
	volume = {80},
	issn = {0006-3495},
	doi = {10.1016/S0006-3495(01)76264-9},
	abstract = {The standard deviation (SD) in fluorescence correlation spectroscopy (FCS) has been mostly neglected in applications. However, the knowledge of the correct SD is necessary for an accurate data evaluation, especially when fitting theoretical models to experimental data. In this work, an algorithm is presented that considers the essential features of FCS. It allows prediction of the performance of FCS measurements in various cases, which is important for finding optimal experimental conditions. The program calculates the SD of the experimental autocorrelation function online. This procedure leads to improved parameter estimation, compared to currently used theoretical approximations for the SD. Three methods for the calculation of the SD are presented and compared to earlier analytical solutions (D. E. Koppel. 1974. Phys. Rev. A. 10:1938-1945.), calculation directly from fluorescence intensity values, by averaging several FCS measurements, or by dividing one measurement into a set of shorter data packages. Although the averaging over several measurements yields accurate estimates for the SD, the other two methods are considerably less time consuming, can be run online, and yield comparable results.},
	number = {March},
	journal = {Biophysical journal},
	author = {Wohland, T and Rigler, R and Vogel, H},
	year = {2001},
	pages = {2987--2999},
}

@article{szent-gyorgyi_fluorogen-activating_2008,
	title = {Fluorogen-activating single-chain antibodies for imaging cell surface proteins.},
	volume = {26},
	issn = {1546-1696 (Electronic)},
	doi = {10.1038/nbt1368},
	abstract = {Imaging of live cells has been revolutionized by genetically encoded fluorescent probes, most famously green and other fluorescent proteins, but also peptide tags that bind exogenous fluorophores. We report here the development of protein reporters that generate fluorescence from otherwise dark molecules (fluorogens). Eight unique fluorogen activating proteins (FAPs) have been isolated by screening a library of human single-chain antibodies (scFvs) using derivatives of thiazole orange and malachite green. When displayed on yeast or mammalian cell surfaces, these FAPs bind fluorogens with nanomolar affinity, increasing green or red fluorescence thousands-fold to brightness levels typical of fluorescent proteins. Spectral variation can be generated by combining different FAPs and fluorogen derivatives. Visualization of FAPs on the cell surface or within the secretory apparatus of mammalian cells can be achieved by choosing membrane permeant or impermeant fluorogens. The FAP technique is extensible to a wide variety of nonfluorescent dyes.},
	number = {2},
	journal = {Nature biotechnology},
	author = {Szent-Gyorgyi, Christopher and Schmidt, Brigitte F and Creeger, Yehuda and Fisher, Gregory W and Zakel, Kelly L and Adler, Sally and Fitzpatrick, James a J and Woolford, Carol a and Yan, Qi and Vasilev, Kalin V and Berget, Peter B and Bruchez, Marcel P and Jarvik, Jonathan W and Waggoner, Alan},
	year = {2008},
	pages = {235--240},
}

@article{discovery_high-content_2013,
	title = {High-{Content} {Screening} {Broadens} {Its} {Scope}},
	author = {Discovery, Drug and Medicine, Translational},
	year = {2013},
	pages = {12--15},
}

@article{ganesan_dark_2006-1,
	title = {A dark yellow fluorescent protein ({YFP})-based {Resonance} {Energy}-{Accepting} {Chromoprotein} ({REACh}) for {Förster} resonance energy transfer with {GFP}.},
	volume = {103},
	issn = {0027-8424},
	doi = {10.1073/pnas.0509922103},
	abstract = {Förster resonance energy transfer (FRET) microscopy is a powerful technique that enables the visualization of signaling intermediates, protein interactions, and protein conformational and biochemical status. With the availability of an ever-increasing collection of fluorescent proteins, pairs of spectrally different variants have been used for the study of FRET in living cells. However, suitable spectral overlap, necessary for efficient FRET, is limited by the requirement for proper emission separation. Currently used FRET pairs represent compromises between these opposing spectral demands that reduce the maximally attainable FRET sensitivity. We present a previously undescribed FRET acceptor, a nonfluorescent yellow fluorescent protein (YFP) mutant called REACh (for Resonance Energy-Accepting Chromoprotein). REACh allows the use of the photophysically superior FRET donor EGFP, with which it exhibits optimal spectral overlap, which obviates the need for narrow spectral filtering and allows additional fluorescent labels to be used within the same cell. The latter allows the generation of sophisticated bioassays for complex biological questions. We show that this dark acceptor is ideally suited for donor fluorescence lifetime imaging microscopy (FLIM) and confirm these measurements with an independent intensity-based donor fluorescence quenching resonance energy transfer (FqRET) assay. REACh also can be used in donor photobleaching kinetics-based FRET studies. By detecting FRET between a GFP-tagged ubiquitination substrate and REACh-labeled ubiquitin, we imaged the active ubiquitination machinery inside cells. This assay therefore can be used to study proteins whose function is regulated by ubiquitination.},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Ganesan, Sundar and Ameer-Beg, Simon M and Ng, Tony T C and Vojnovic, Borivoj and Wouters, Fred S},
	year = {2006},
	pages = {4089--4094},
}

@article{tramier_picosecond-hetero-fret_2002,
	title = {Picosecond-hetero-{FRET} microscopy to probe protein-protein interactions in live cells.},
	volume = {83},
	issn = {0006-3495 (Print){\textbackslash}n0006-3495 (Linking)},
	doi = {10.1016/S0006-3495(02)75357-5},
	abstract = {By using a novel time- and space-correlated single-photon counting detector, we show that fluorescence resonance energy transfer (FRET) between cyan fluorescent protein (CFP) and yellow fluorescent protein (YFP) fused to herpes simplex virus thymidine kinase (TK) monomers can be used to reveal homodimerization of TK in the nucleus and cytoplasm of live cells. However, the quantification of energy transfer was limited by the intrinsic biexponential fluorescence decay of the donor CFP (lifetimes of 1.3 +/- 0.2 ns and 3.8 +/- 0.4 ns) and by the possibility of homodimer formation between two TK-CFP. In contrast, the heterodimerization of the transcriptional factor NF-E2 in the nucleus of live cells was quantified from the analysis of the fluorescence decays of GFP in terms of 1) FRET efficiency between GFP and DsRed chromophores fused to p45 and MafG, respectively, the two subunits of NF-E2 (which corresponds to an interchromophoric distance of 39 +/- 1 A); and 2) fractions of GFP-p45 bound to DsRed-MafG (constant in the nucleus, varying in the range of 20\% to 70\% from cell to cell). The picosecond resolution of the fluorescence kinetics allowed us to discriminate between very short lifetimes of immature green species of DsRed-MafG and that of GFP-p45 involved in FRET with DsRed-MafG.},
	number = {December},
	journal = {Biophysical journal},
	author = {Tramier, Marc and Gautier, Isabelle and Piolot, Tristan and Ravalet, Sylvie and Kemnitz, Klaus and Coppey, Jacques and Durieux, Christiane and Mignotte, Vincent and Coppey-Moisan, Maïté},
	year = {2002},
	pages = {3570--3577},
}

@article{helix_germination_2003,
	title = {Germination genetics},
	number = {January},
	journal = {North Star},
	author = {Helix, Double and Cells, Natural Killer},
	year = {2003},
}

@article{becker_multi-wavelength_2002,
	title = {Multi-wavelength {TCSPC} lifetime imaging},
	volume = {4620},
	doi = {10.1117/12.470679},
	journal = {Photonics West - BIOS},
	author = {Becker, W and Bergmann, a and Biskup, C and Zimmer, T and Klocker, N and Benndorf, K},
	year = {2002},
}

@article{korlach_detection_2005,
	title = {Detection of motional heterogeneities in lipid bilayer membranes by dual probe fluorescence correlation spectroscopy},
	volume = {1668},
	issn = {0005-2736},
	doi = {10.1016/j.bbamem.2004.11.016},
	abstract = {We report the detection of heterogeneities in the diffusion of lipid molecules for the three-component mixture dipalmitoyl-PC/dilauroyl-PC/ cholesterol, a chemically simple lipid model for the mammalian plasma membrane outer leaflet. Two-color fluorescence correlation spectroscopy (FCS) was performed on giant unilamellar vesicles (GUVs) using fluorescent probes that have differential lipid phase partition behavior-DiO-C18:2 favors disordered fluid lipid phases, whereas DiI-C20:0 prefers spatially ordered lipid phases. Simultaneously-obtained fluorescence autocorrelation functions from the same excitation volume for each dye showed that, depending on the lipid composition of this ternary mixture, the two dyes exhibited different lateral mobilities in regions of the phase diagram with previously proposed submicroscopic two-phase coexistence. In one-phase regions, both dyes reported identical diffusion coefficients. Two-color FCS thus may be detecting local membrane heterogeneities at size scales below the optical resolution limit, either due to short-range order in a single phase or due to submicroscopic phase separation. ?? 2005 Elsevier B.V. All rights reserved.},
	journal = {Biochimica et Biophysica Acta - Biomembranes},
	author = {Korlach, Jonas and Baumgart, Tobias and Webb, Watt W. and Feigenson, Gerald W.},
	year = {2005},
	keywords = {Fluorescence correlation spectroscopy, Lipid bilayer phase diagram, Membrane phase behavior, Ternary lipid mixture},
	pages = {158--163},
}

@article{noauthor_no_1944-1,
	title = {No {Title}},
	year = {1944},
}

@article{thompson_maf_2003-1,
	title = {{MAF} 2003, {Prague}, {August} 24 – 27, 2003},
	author = {Thompson, Nancy L and Lieto, Alena M and Starr, Tammy E and Cush, Randall C},
	year = {2003},
	pages = {2003--2003},
}

@article{ji_adaptive_2010-1,
	title = {Adaptive optics via pupil segmentation for high-resolution imaging in biological tissues.},
	volume = {7},
	issn = {1548-7105 (Electronic){\textbackslash}n1548-7091 (Linking)},
	doi = {10.1038/nmeth.1411},
	abstract = {Biological specimens are rife with optical inhomogeneities that seriously degrade imaging performance under all but the most ideal conditions. Measuring and then correcting for these inhomogeneities is the province of adaptive optics. Here we introduce an approach to adaptive optics in microscopy wherein the rear pupil of an objective lens is segmented into subregions, and light is directed individually to each subregion to measure, by image shift, the deflection faced by each group of rays as they emerge from the objective and travel through the specimen toward the focus. Applying our method to two-photon microscopy, we could recover near-diffraction-limited performance from a variety of biological and nonbiological samples exhibiting aberrations large or small and smoothly varying or abruptly changing. In particular, results from fixed mouse cortical slices illustrate our ability to improve signal and resolution to depths of 400 microm.},
	number = {2},
	journal = {Nature methods},
	author = {Ji, Na and Milkie, Daniel E and Betzig, Eric},
	year = {2010},
	pages = {141--147},
}

@article{noauthor_aobs_nodate,
	title = {{AOBS} publications},
}

@article{schwille_molecular_1999-2,
	title = {Molecular dynamics in living cells observed by fluorescence correlation spectroscopy with one- and two-photon excitation.},
	volume = {77},
	issn = {0006-3495 (Print){\textbackslash}r0006-3495 (Linking)},
	doi = {10.1016/S0006-3495(99)77065-7},
	abstract = {Multiphoton excitation (MPE) of fluorescent probes has become an attractive alternative in biological applications of laser scanning microscopy because many problems encountered in spectroscopic measurements of living tissue such as light scattering, autofluorescence, and photodamage can be reduced. The present study investigates the characteristics of two-photon excitation (2PE) in comparison with confocal one-photon excitation (1PE) for intracellular applications of fluorescence correlation spectroscopy (FCS). FCS is an attractive method of measuring molecular concentrations, mobility parameters, chemical kinetics, and fluorescence photophysics. Several FCS applications in mammalian and plant cells are outlined, to illustrate the capabilities of both 1PE and 2PE. Photophysical properties of fluorophores required for quantitative FCS in tissues are analyzed. Measurements in live cells and on cell membranes are feasible with reasonable signal-to-noise ratios, even with fluorophore concentrations as low as the single-molecule level in the sampling volume. Molecular mobilities can be measured over a wide range of characteristic time constants from approximately 10(-3) to 10(3) ms. While both excitation alternatives work well for intracellular FCS in thin preparations, 2PE can substantially improve signal quality in turbid preparations like plant cells and deep cell layers in tissue. At comparable signal levels, 2PE minimizes photobleaching in spatially restrictive cellular compartments, thereby preserving long-term signal acquisition.},
	number = {October},
	journal = {Biophysical journal},
	author = {Schwille, P and Haupts, U and Maiti, S and Webb, W W},
	year = {1999},
	pages = {2251--2265},
}

@article{hassler_high_2005,
	title = {High {Count} {Rates} with {Total} {Internal} {Reflection} {Fluorescence} {Correlation} {Spectroscopy}},
	volume = {88},
	issn = {00063495},
	doi = {10.1529/biophysj.104.053884},
	abstract = {We achieved photon count rates per molecule as high as with commonly used confocal fluorescence correlation spectroscopy instruments using a new total internal reflection fluorescence correlation spectroscopy system based on an epi-illumination configuration.},
	journal = {Biophysical journal},
	author = {Hassler, Kai and Anhut, Tiemo and Rigler, Rudolf and Gösch, Michael and Lasser, Theo},
	year = {2005},
	pages = {L01--L03},
}

@article{mukhopadhyay_contrasting_2002,
	title = {Contrasting friction and diffusion in molecularly thin confined films.},
	volume = {89},
	doi = {10.1103/PhysRevLett.89.136103},
	abstract = {We study, using fluorescence correlation spectroscopy, translational diffusion in molecularly thin liquids confined within a surface forces apparatus. The diffusion coefficient decreases exponentially from the edges towards the center of the Hertzian contact and further suggests the presence of a small number of distinct diffusion processes. This holds alike a crystallizable fluid (OMCTS) and a glass-former (1,2-propane diol), both of which displayed static friction. We conclude that friction, the average of an ensemble of molecules, masked massively heterogeneous molecular mobility.},
	journal = {Physical review letters},
	author = {Mukhopadhyay, Ashis and Zhao, Jiang and Bae, Sung Chul and Granick, Steve},
	year = {2002},
	pages = {136103--136103},
}

@article{guan_adaptive_2008-6,
	title = {Adaptive correction technique for {3D} reconstruction of fluorescence microscopy images.},
	volume = {71},
	issn = {1059-910X (Print)},
	doi = {10.1002/jemt},
	abstract = {Recent advances in high-resolution imaging have provided valuable novel insights into structural relationships within cells and tissues both in vitro and in vivo. An analysis of this kind is regularly done by optical sectioning using either confocal or deconvolution microscopy. However, the reconstruction of 3D images suffers from light scattering and absorption with increasing depth by finite transparency of the used media. Photobleaching of fluorochromes has been especially troublesome and often the only remedy for loss of signal during optical sectioning is to reduce the number of sections. This causes disparities in the x-y and z dimensions of voxels, which lead to vertical distortion of the original stack of images and necessitates interpolation. Interpolation is necessary to fill up the gaps between consecutive sections in the original image stack to obtain cubic voxels. The present manuscript describes a novel method for adaptive compensation of attenuation of light intensity in stacks of fluorescence microscopy images that is based on a physical model of light attenuation. First, we use a fast interpolation technique to generate a cubic voxel-based volume stack with the aid of a contribution look up table. With the contribution look up table, multiple calculations are avoided, which substantially reduces the computational time without compromising the accuracy of the restoration procedure. Second, each section within the resulting volume is processed to rectify its intensity values that have been altered due to photobleaching and scattering and absorption. The method allows to define the last good section in the stack and the correction is then done automatically.},
	number = {September 2006},
	journal = {Microscopy research and technique},
	author = {Guan, Y Q and Cai, Y Y and Zhang, X and Lee, Y T and Opas, M},
	year = {2008},
	keywords = {confocal microscopy, fret, time-resolved, rster resonance energy transfer},
	pages = {146--157},
}

@article{kinetics_bio-logic_2002,
	title = {Bio-{Logic} - {Spectrometers} {Spectrometers} {Bio}-{Logic} - {Spectrometers}},
	author = {Kinetics, Rapid},
	year = {2002},
	pages = {1--27},
}

@article{noauthor_20110525_feature_value_rnd_approvalpdf_nodate,
	title = {20110525\_Feature\_Value\_RnD\_approval.pdf},
}

@article{qian_distribution_1990,
	title = {Distribution of molecular aggregation by analysis of fluctuation moments.},
	volume = {87},
	issn = {0027-8424 (Print)},
	doi = {10.1073/pnas.87.14.5479},
	abstract = {The fluorescence from an open volume of a solution of fluorescent molecules fluctuates as the molecules randomly diffuse into and out of the volume. The distribution of degrees of aggregation or polymerization of the fluorescent molecules can be characterized without perturbing the system by measuring either the moments or the amplitude distribution of these fluctuations. We present an experimental verification of this approach applied to simple model systems consisting of solutions of fluorescent particles of well-defined size. We have also characterized the response of the photon-detection device (typically a photomultiplier), which is essential to the analysis of the fluorescence fluctuations, and have compared two methods for determining shot-noise contributions.},
	number = {July 1990},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Qian, H and Elson, E L},
	year = {1990},
	pages = {5479--5483},
}

@article{sterke_supercontinuum_nodate,
	title = {Supercontinuum generation},
	author = {Sterke, Martijn De},
}

@article{clayton_dynamic_2002-2,
	title = {Dynamic fluorescence anisotropy imaging microscopy in the frequency domain ({rFLIM}).},
	volume = {83},
	issn = {4955120113},
	doi = {10.1016/S0006-3495(02)73932-5},
	abstract = {We describe a novel variant of fluorescence lifetime imaging microscopy (FLIM), denoted anisotropy-FLIM or rFLIM, which enables the wide-field measurement of the anisotropy decay of fluorophores on a pixel-by-pixel basis. We adapted existing frequency-domain FLIM technology for rFLIM by introducing linear polarizers in the excitation and emission paths. The phase delay and intensity ratios (AC and DC) between the polarized components of the fluorescence signal are recorded, leading to estimations of rotational correlation times and limiting anisotropies. Theory is developed that allows all the parameters of the hindered rotator model to be extracted from measurements carried out at a single modulation frequency. Two-dimensional image detection with a sensitive CCD camera provides wide-field imaging of dynamic depolarization with parallel interrogation of different compartments of a complex biological structure such as a cell. The concepts and technique of rFLIM are illustrated with a fluorophore-solvent (fluorescein-glycerol) system as a model for isotropic rotational dynamics and with bacteria expressing enhanced green fluorescent protein (EGFP) exhibiting depolarization due to homotransfer of electronic excitation energy (emFRET). The frequency-domain formalism was extended to cover the phenomenon of emFRET and yielded data consistent with a concentration depolarization mechanism resulting from the high intracellular concentration of EGFP. These investigations establish rFLIM as a powerful tool for cellular imaging based on rotational dynamics and molecular proximity.},
	number = {September 2002},
	journal = {Biophysical journal},
	author = {Clayton, Andrew H a and Hanley, Quentin S and Arndt-Jovin, Donna J and Subramaniam, Vinod and Jovin, Thomas M},
	year = {2002},
	pages = {1631--1649},
}

@article{treanor_microclusters_2006-1,
	title = {Microclusters of inhibitory killer immunoglobulin-like receptor signaling at natural killer cell immunological synapses},
	volume = {174},
	issn = {0021-9525 (Print){\textbackslash}r0021-9525 (Linking)},
	doi = {10.1083/jcb.200601108},
	abstract = {We report the supramolecular organization of killer Ig-like receptor (KIR) phosphorylation using a technique applicable to imaging phosphorylation of any green fluorescent protein-tagged receptor at an intercellular contact or immune synapse. Specifically, we use fluorescence lifetime imaging (FLIM) to report Förster resonance energy transfer (FRET) between GFP-tagged KIR2DL1 and a Cy3-tagged generic anti-phosphotyrosine monoclonal antibody. Visualization of KIR phosphorylation in natural killer (NK) cells contacting target cells expressing cognate major histocompatibility complex class I proteins revealed that inhibitory signaling is spatially restricted to the immune synapse. This explains how NK cells respond appropriately when simultaneously surveying susceptible and resistant target cells. More surprising, phosphorylated KIR was confined to microclusters within the aggregate of KIR, contrary to an expected homogeneous distribution of KIR signaling across the immune synapse. Also, yellow fluorescent protein-tagged Lck, a kinase important for KIR phosphorylation, accumulated in a multifocal distribution at inhibitory synapses. Spatial confinement of receptor phosphorylation within the immune synapse may be critical to how activating and inhibitory signals are integrated in NK cells.},
	number = {1},
	journal = {Journal of Cell Biology},
	author = {Treanor, Bebhinn and Lanigan, Peter M P and Kumar, Sunil and Dunsby, Chris and Munro, Ian and Auksorius, Egidijus and Culley, Fiona J. and Purbhoo, Marco a. and Phillips, David and Neil, Mark a a and Burshtyn, Deborah N. and French, Paul M W and Davis, Daniel M.},
	year = {2006},
	pages = {153--161},
}

@article{laurence_interactions_2004,
	title = {Interactions},
	volume = {10},
	author = {Laurence, Ted A and Kapanidis, Achillefs N and Kong, Xiangxu and Chemla, Daniel S and Weiss, Shimon},
	year = {2004},
	pages = {3051--3067},
}

@article{haj_imaging_2002,
	title = {Imaging sites of receptor dephosphorylation by {PTP1B} on the surface of the endoplasmic reticulum.},
	volume = {295},
	issn = {1095-9203 (Electronic)},
	doi = {10.1126/science.1067566},
	abstract = {When bound by extracellular ligands, receptor tyrosine kinases (RTKs) on the cell surface transmit critical signals to the cell interior. Although signal termination is less well understood, protein tyrosine phosphatase-1B (PTP1B) is implicated in the dephosphorylation and inactivation of several RTKs. However, PTP1B resides on the cytoplasmic surface of the endoplasmic reticulum (ER), so how and when it accesses RTKs has been unclear. Using fluorescence resonance energy transfer (FRET) methods, we monitored interactions between the epidermal- and platelet-derived growth factor receptors and PTP1B. PTP1B-catalyzed dephosphorylation required endocytosis of the receptors and occurred at specific sites on the surface of the ER. Most of the RTKs activated at the cell surface showed interaction with PTP1B after internalization, establishing that RTK activation and inactivation are spatially and temporally partitioned within cells.},
	number = {March},
	journal = {Science (New York, N.Y.)},
	author = {Haj, Fawaz G and Verveer, Peter J and Squire, Anthony and Neel, Benjamin G and Bastiaens, Philippe I H},
	year = {2002},
	pages = {1708--1711},
}

@article{becker_fret_2001-1,
	title = {{FRET} imaging by picosecond {TCSPC} laser scanning microscopy},
	issn = {1-55752-662-1},
	doi = {10.1109/CLEO.2001.948243},
	abstract = {Two-photon laser scanning microscopy and a new TCSPC imaging technique was used to obtain FRET intensity-lifetime images of living cells. Double exponential decay analysis separated the FRET fluorescence from the fluorescence of the unquenched acceptor molecules.},
	journal = {Technical Digest. Summaries of papers presented at the Conference on Lasers and Electro-Optics. Postconference Technical Digest (IEEE Cat. No.01CH37170)},
	author = {Becker, W. and Bergmann, a. and Benndorf, K. and Biskup, C. and Zimmer, T.},
	year = {2001},
	pages = {1--8},
}

@article{lee_application_2001-1,
	title = {Application of the stretched exponential function to fluorescence lifetime imaging.},
	volume = {81},
	doi = {10.1016/S0006-3495(01)75784-0},
	abstract = {Conventional analyses of fluorescence lifetime measurements resolve the fluorescence decay profile in terms of discrete exponential components with distinct lifetimes. In complex, heterogeneous biological samples such as tissue, multi-exponential decay functions can appear to provide a better fit to fluorescence decay data than the assumption of a mono-exponential decay, but the assumption of multiple discrete components is essentially arbitrary and is often erroneous. Moreover, interactions, both between fluorophores and with their environment, can result in complex fluorescence decay profiles that represent a continuous distribution of lifetimes. Such continuous distributions have been reported for tryptophan, which is one of the main fluorophores in tissue. This situation is better represented by the stretched-exponential function (StrEF). In this work, we have applied, for the first time to our knowledge, the StrEF to time-domain whole-field fluorescence lifetime imaging (FLIM), yielding both excellent tissue contrast and goodness of fit using data from rat tissue. We note that for many biological samples for which there is no a priori knowledge of multiple discrete exponential fluorescence decay profiles, the StrEF is likely to provide a truer representation of the underlying fluorescence dynamics. Furthermore, fitting to a StrEF significantly decreases the required processing time, compared with a multi-exponential component fit and typically provides improved contrast and signal/noise in the resulting FLIM images. In addition, the stretched-exponential decay model can provide a direct measure of the heterogeneity of the sample, and the resulting heterogeneity map can reveal subtle tissue differences that other models fail to show.},
	number = {September},
	journal = {Biophysical journal},
	author = {Lee, K C and Siegel, J and Webb, S E and Lévêque-Fort, S and Cole, M J and Jones, R and Dowling, K and Lever, M J and French, P M},
	year = {2001},
	pages = {1265--1274},
}

@article{fara_microscopic_2009,
	title = {A microscopic reality tale.},
	volume = {459},
	issn = {0028-0836},
	doi = {10.1038/459642a},
	number = {June},
	journal = {Nature},
	author = {Fara, Patricia},
	year = {2009},
	pages = {642--644},
}

@article{noauthor_moticpdf_nodate,
	title = {Motic.pdf},
}

@article{burnett_fluorescence_2004,
	title = {Fluorescence correlation spectroscopy of water-in-oil microemulsions: {An} application in specific characterisation of droplets containing biomolecules},
	volume = {250},
	issn = {0927-7757},
	doi = {10.1016/j.colsurfa.2004.05.025},
	abstract = {Fluorescence correlation spectroscopy (FCS) has been successfully used to characterise water-in-oil (w/o) microemulsions. The investigated systems were stabilised by sodium bis-2-ethylhexyl sulphosuccinate (AOT) and the measured diffusion times have been related to the radii of the aggregated species, which for some systems, were separately determined by small-angle neutron scattering (SANS). We demonstrate that FCS is capable of measuring hydrodynamic radii of microemulsions rapidly and at surfactant concentrations lower than previously reported for other techniques. FCS was also used to specifically interrogate microemulsion droplets containing a fluorescently-labelled biomolecule, specifically phalloidin, a peptide fungal toxin from Amanita phalloides, and the enzyme ??-chymotrypsin (??-CT). The microemulsion droplets are only marginally increased in size if a small peptide (phalloidin) is included in the water phase, whereas the droplet size is significantly increased when a larger protein (??-CT) is included. ?? 2004 Published by Elsevier B.V.},
	journal = {Colloids and Surfaces A: Physicochemical and Engineering Aspects},
	author = {Burnett, Gary R. and Rees, Gareth D. and Steytler, David C. and Robinson, Brian H.},
	year = {2004},
	keywords = {Fluorescence correlation spectroscopy (FCS), Biomolecules, Microemulsion},
	pages = {171--178},
}

@article{noauthor_cy2_nodate-1,
	title = {Cy2},
}

@article{chambenoit_specific_2001,
	title = {Specific {Docking} of {Apolipoprotein} {A}-{I} at the {Cell} {Surface} {Requires} a {Functional} {ABCA1} {Transporter}},
	volume = {276},
	issn = {0021-9258},
	doi = {10.1074/jbc.M010265200},
	abstract = {The identification of defects in ABCA1 as the molecular basis of Tangier disease has highlighted its crucial role in the loading with phospholipids and cholesterol of nascent apolipoprotein particles. Indeed the expression of ABCA1 affects apolipoprotein A-I (apoA-I)-mediated removal of lipids from cell membranes, and the possible role of ABCA1 as an apoA-I surface receptor has been recently suggested. In the present study, we have investigated the role of the ABCA1 transporter as an apoA-I receptor with the analysis of a panel of transfectants expressing functional or mutant forms of ABCA1. We provide experimental evidence that the forced expression of a functional ABCA1 transporter confers surface competence for apoA-I binding. This, however, appears to be dependent on ABCA1 function. Structurally intact but ATPase-deficient forms of the transporter fail to elicit a specific cell association of the ligand. In addition the diffusion parameters of membrane-associated apoA-I indicate an interaction with membrane lipids rather than proteins. These results do not support a direct molecular interaction between ABCA1 and apoA-I, but rather suggest that the ABCA1-induced modification of the lipid distribution in the membrane, evidenced by the phosphatidylserine exofacial flopping, generates a biophysical microenvironment required for the docking of apoA-I at the cell surface.},
	number = {13},
	journal = {Journal of Biological Chemistry},
	author = {Chambenoit, Olivier and Hamon, Yannick and Marguet, Didier and Rigneault, Hervé and Rosseneu, Maryvonne and Chimini, Giovanna},
	year = {2001},
	pages = {9955--9960},
}

@article{schwille_fluorescence_1999-1,
	title = {Fluorescence correlation spectroscopy with single-molecule sensitivity on cell and model membranes.},
	volume = {36},
	issn = {0196-4763},
	doi = {10.1002/(SICI)1097-0320(19990701)36:3<176::AID-CYTO5>3.0.CO;2-F},
	abstract = {We report on the successful application of fluorescence correlation spectroscopy (FCS) to the analysis of single fluorescently labeled lipid analogue molecules diffusing laterally in lipid bilayers, as exemplified by time traces of fluorescence bursts of individual molecules entering and leaving the excitation area. FCS measurements performed on lipid probes in rat basophilic leukemia cell membranes showed deviations from two-dimensional Brownian motion with a single uniform diffusion constant. Giant unilamellar vesicles were employed as model systems to characterize diffusion of fluorescent lipid analogues in both homogeneous and mixed lipid phases with diffusion heterogeneity. Comparing the results of cell membrane diffusion with the findings on the model systems suggests possible explanations for the observations: (a) anomalous subdiffusion in which evanescent attractive interactions with disparate mobile molecules modifies the diffusion statistics; (b) alternatively, probe molecules are localized in microdomains of submicroscopic size, possibly in heterogeneous membrane phases.},
	journal = {Cytometry},
	author = {Schwille, P and Korlach, J and Webb, W W},
	year = {1999},
	pages = {176--182},
}

@article{doi_novel_2002,
	title = {Novel {Fluorescence} {Labeling} and {High}-{Throughput} {Assay} {Technologies} for {In} {Vitro} {Analysis} of {Protein} {Interactions} {Novel} {Fluorescence} {Labeling} and {High}-{Throughput} {Assay} {Technologies} for {In} {Vitro} {Analysis} of {Protein} {Interactions}},
	doi = {10.1101/gr.218802},
	author = {Doi, Nobuhide and Takashima, Hideaki and Kinjo, Masataka and Sakata, Kyoko and Kawahashi, Yuko and Oishi, Yuko and Miyamoto-sato, Etsuko and Sawasaki, Tatsuya and Endo, Yaeta and Yanagawa, Hiroshi and Oyama, Rieko},
	year = {2002},
	pages = {487--492},
}

@article{kobayashi_detection_2004,
	title = {Detection of protein-{DNA} interactions in crude cellular extracts by fluorescence correlation spectroscopy},
	volume = {332},
	issn = {0003-2697 (Print){\textbackslash}n0003-2697 (Linking)},
	doi = {10.1016/j.ab.2004.05.053},
	abstract = {Fluorescence correlation spectroscopy (FCS) is a methodology to examine directly the translational diffusion of individual fluorescence-labeled molecules in solutions. Recent studies using FCS have quantified various bimolecular reactions without any need for amplification. To evaluate further the applicability of FCS, we studied the specific binding between proteins and DNA in crude biological samples. Using an automated FCS system that was recently developed in our laboratories and is capable of distinguishing two or more molecular species in a multicomponent analysis, we detected the binding of two representative transcription factors, activator protein-1 (AP-1) and nuclear factor kappa B (NF-κB), in nuclear extracts of HeLa cells quantitatively with each sequence-specific DNA. The binding rates of these specific interactions were markedly augmented when cells were treated with tumor necrosis factor α which is known to activate both AP-1 and NF-κB. We also observed the pyrrolidine-dithiocarbamate-induced reciprocal regulation of these transcription factors. These results indicated that FCS is a useful tool for the analysis of complex interactions of transcription factors with DNA even in crude cellular extracts, suggesting that it is a powerful methodology for the study of a wide variety of molecular events under various experimental conditions. © 2004 Elsevier Inc. All rights reserved.},
	journal = {Analytical Biochemistry},
	author = {Kobayashi, Tamiyo and Okamoto, Naoaki and Sawasaki, Tatsuya and Endo, Yaeta},
	year = {2004},
	keywords = {Fluorescence correlation spectroscopy, Protein-DNA interaction, Transcription factor},
	pages = {58--66},
}

@article{becker_advanced_2004,
	title = {Advanced time-correlated single photon counting technique for spectroscopy and imaging of biological systems},
	volume = {5340},
	url = {http://cat.inist.fr/?aModele=afficheN&cpsidt=18774552},
	doi = {10.1117/12.529143},
	abstract = {Time-correlated single photon counting (TCSPC) is based on the detection of single photons of a periodic light signal, measurement of the detection time of the photons, and the build-up of the photon distribution versus the time in the signal period. TCSPC achieves a near ideal counting efficiency and transit-time-spread-limited time resolution for a given detector. The drawback of traditional TCSPC is the low count rate, long acquisition time, and the fact that the technique is one-dimensional, i.e. limited to the recording of the pulse shape of light signals. We present an advanced TCSPC technique featuring multi-dimensional photon acquisition and a count rate close to the capability of currently available detectors. The technique is able to acquire photon distributions versus wavelength, spatial coordinates, and the time on the ps scale, and to record fast changes in the fluorescence lifetime and fluorescence intensity of a sample. Biomedical applications of advanced TCSPC techniques are time-domain optical tomography, recording of transient phenomena in biological systems, spectrally resolved fluorescence lifetime imaging, FRET experiments in living cells, and the investigation of dye-protein complexes by fluorescence correlation spectroscopy. We demonstrate the potential of the technique for selected applications.},
	journal = {Proc. SPIE 5340, Commercial and Biomedical Applications of Ultrafast Lasers IV, 104},
	author = {Becker, Wolfgang and Bergmann, Axel and Biscotti, Giovanni and Rueck, Angelika},
	year = {2004},
	keywords = {autofluorescence, flim, fret, time-correlated single photon counting, time-resolved fluorescence},
	pages = {1--9},
}

@article{brakenhoff_characterization_2005,
	title = {Characterization of sectioning fluorescence microscopy with thin uniform fluorescent layers: {Sectioned} {Imaging} {Property} or {SIPcharts}},
	volume = {219},
	doi = {10.1111/j.1365-2818.2005.01504.x},
	abstract = {Thin, uniformly fluorescing reference layers can be used to characterize the imaging conditions in confocal, or more general, sectioning microscopy. Through-focus datasets of such layers obtained by standard microscope routines provide the basis for the approach. A set of parameters derived from these datasets is developed for defining a number of relevant sectioned imaging properties. The main characteristics of a particular imaging situation can then be summarized in a Sectioned Imaging Property-chart or SIPchart. We propose the use of such charts for the characterization of imaging properties in confocal and multiphoton microscopy. As such, they can be the basis for comparison of sectioned imaging condition characteristics, quality control, maintenance or reproduction of sectioned imaging conditions and other applications. Such charts could prove useful in documenting the more relevant properties of the instrumentation used in microscopy studies. The method carries the potential to provide the basis for a general characterization of sectioned imaging conditions as the layers employed can be characterized and fabricated to standard specifications. A limited number of such thin, uniformly fluorescing layers is available from our group for this purpose. Extension of the method to multiphoton microscopy is discussed.},
	number = {September},
	journal = {Journal of Microscopy},
	author = {Brakenhoff, Godefridus J. and Wurpel, G. W H and Jalink, K. and Oomen, L. and Brocks, L. and Zwier, J. M.},
	year = {2005},
	keywords = {Fluorescence microscopy, 3D microscopy, Confocal microscopy, Fluorescence photobleaching, Image correction, Multiphoton microscopy, Sectioned imaging, Sectioning microscopy, SIPcharts, Two-photon microscopy},
	pages = {122--132},
}

@article{schindelin_fiji_2012,
	title = {Fiji: an open-source platform for biological-image analysis},
	volume = {9},
	issn = {1548-7091},
	doi = {10.1038/nmeth.2019},
	abstract = {Fiji is a distribution of the popular open-source software ImageJ focused on biological-image analysis. Fiji uses modern software engineering practices to combine powerful software libraries with a broad range of scripting languages to enable rapid prototyping of image-processing algorithms. Fiji facilitates the transformation of new algorithms into ImageJ plugins that can be shared with end users through an integrated update system. We propose Fiji as a platform for productive collaboration between computer science and biology research communities.},
	number = {7},
	journal = {Nature Methods},
	author = {Schindelin, Johannes and Arganda-Carreras, Ignacio and Frise, Erwin and Kaynig, Verena and Longair, Mark and Pietzsch, Tobias and Preibisch, Stephan and Rueden, Curtis and Saalfeld, Stephan and Schmid, Benjamin and Tinevez, Jean-Yves and White, Daniel James and Hartenstein, Volker and Eliceiri, Kevin and Tomancak, Pavel and Cardona, Albert},
	year = {2012},
	pages = {676--682},
}

@article{duncan_multi-dimensional_2004-2,
	title = {Multi-dimensional time-correlated single photon counting ({TCSPC}) fluorescence lifetime imaging microscopy ({FLIM}) to detect {FRET} in cells},
	volume = {215},
	issn = {0022-2720},
	doi = {10.1111/j.0022-2720.2004.01343.x},
	abstract = {We present a novel, multi-dimensional, time-correlated single photon counting (TCSPC) technique to perform fluorescence lifetime imaging with a laser-scanning microscope operated at a pixel dwell-time in the microsecond range. The unsurpassed temporal accuracy of this approach combined with a high detection efficiency was applied to measure the fluorescent lifetimes of enhanced cyan fluorescent protein (ECFP) in isolation and in tandem with EYFP (enhanced yellow fluorescent protein). This technique enables multi-exponential decay analysis in a scanning microscope with high intrinsic time resolution, accuracy and counting efficiency, particularly at the low excitation levels required to maintain cell viability and avoid photobleaching. Using a construct encoding the two fluorescent proteins separated by a fixed-distance amino acid spacer, we were able to measure the fluorescence resonance energy transfer (FRET) efficiency determined by the interchromophore distance. These data revealed that ECFP exhibits complex exponential fluorescence decays under both FRET and non-FRET conditions, as previously reported. Two approaches to calculate the distance between donor and acceptor from the lifetime delivered values within a 10\% error range. To confirm that this method can be used also to quantify intermolecular FRET, we labelled cultured neurones with the styryl dye FM1-43, quantified the fluorescence lifetime, then quenched its fluorescence using FM4-64, an efficient energy acceptor for FM1-43 emission. These experiments confirmed directly for the first time that FRET occurs between these two chromophores, characterized the lifetimes of these probes, determined the interchromophore distance in the plasma membrane and provided high-resolution two-dimensional images of lifetime distributions in living neurones.},
	number = {Pt 1},
	journal = {Journal of Microscopy},
	author = {Duncan, R. R. and Bergmann, a. and Cousin, M. a. and Apps, D. K. and Shipston, M. J.},
	year = {2004},
	keywords = {FM1-43, GFP, Multi-photon, Two-photon},
	pages = {1--12},
}

@article{malo_x-ray_2007,
	title = {X-ray structure of {Cerulean} {GFP}: {A} tryptophan-based chromophore useful for fluorescence lifetime imaging},
	volume = {46},
	issn = {0006-2960},
	doi = {10.1021/bi602664c},
	abstract = {The crystal structure of the cyan-fluorescent Cerulean green fluorescent protein (GFP), a variant of enhanced cyan fluorescent protein (ECFP), has been determined to 2.0 A. Cerulean bears an internal fluorophore composed of an indole moiety derived from Y66W, conjugated to the GFP-like imidazolinone ring via a methylene bridge. Cerulean undergoes highly efficient fluorescence resonance energy transfer (FRET) to yellow acceptor molecules and exhibits significantly reduced excited-state heterogeneity. This feature was rationally engineered in ECFP by substituting His148 with an aspartic acid [Rizzo et al. (2004) Nat. Biotechnol. 22, 445], rendering Cerulean useful for fluorescence lifetime imaging microscopy (FLIM). The X-ray structure is consistent with a single conformation of the chromophore and surrounding residues and may therefore provide a structural rationale for the previously described monoexponential fluorescence decay. Unexpectedly, the carboxyl group of H148D is found in a buried position, directly contacting the indole nitrogen of the chromophore via a bifurcated hydrogen bond. Compared to the similarly constructed ECFP chromophore, the indole group of Cerulean is rotated around the methylene bridge to adopt a cis-coplanar conformation with respect to the imidazolinone ring, resulting in a close edge-to-edge contact of the two ring systems. The double-humped absorbance spectrum persists in single-crystal absorbance measurements, casting doubt on the idea that ground state conformational heterogeneity forms the basis of the two overlapping transitions. At low pH, a blue shift in absorbance of 10-15 nm suggests a pH-induced structural transition that proceeds with a time constant of 47 (+/-2) min and is reversible. Possible interpretations in terms of chromophore isomerization are presented.},
	number = {35},
	journal = {Biochemistry},
	author = {Malo, Gabrielle D. and Pouwels, Lauren J. and Wang, Meitian and Weichsel, Andrzej and Montfort, William R. and Rizzo, Mark a. and Piston, David W. and Wachter, Rebekka M.},
	year = {2007},
	pages = {9865--9873},
}

@article{koushik_cerulean_2006-1,
	title = {Cerulean, {Venus}, and {VenusY67C} {FRET} reference standards.},
	volume = {91},
	issn = {0006-3495 (Print){\textbackslash}n0006-3495 (Linking)},
	doi = {10.1529/biophysj.106.096206},
	abstract = {Förster's resonance energy transfer (FRET) can be used to study protein-protein interactions in living cells. Numerous methods to measure FRET have been devised and implemented; however, the accuracy of these methods is unknown, which makes interpretation of FRET efficiency values difficult if not impossible. This problem exists due to the lack of standards with known FRET efficiencies that can be used to validate FRET measurements. The advent of spectral variants of green fluorescent protein and easy access to cell transfection technology suggests a simple solution to this problem: the development of genetic constructs with known FRET efficiencies that can be replicated with high fidelity and freely distributed. In this study, fluorescent protein constructs with progressively larger separation distances between donors and acceptors were generated and FRET efficiencies were measured using fluorescence lifetime spectroscopy, sensitized acceptor emission, and spectral imaging. Since the results from each method were in good agreement, the FRET efficiency value of each construct could be determined with high accuracy and precision, thereby justifying their use as standards.},
	journal = {Biophysical journal},
	author = {Koushik, Srinagesh V and Chen, Huanmian and Thaler, Christopher and Puhl, Henry L and Vogel, Steven S},
	year = {2006},
	pages = {L99--L101},
}

@article{hess_biological_2002-1,
	title = {Biological and chemical applications of fluorescence correlation spectroscopy: {A} review},
	volume = {41},
	issn = {0006-2960},
	doi = {10.1021/bi0118512},
	abstract = {... The excitation intensity profile and detection optics (in particular, the confocal aperture) define the ... rapidly with the axial distance (roughly as z - 4 ) from the focal plane. ... excitation confinement provides intrinsic 3D spatial resolution and a nearly 3D Gaussian observation volume . ...},
	number = {3},
	journal = {Biochemistry},
	author = {Hess, Samuel T. and Huang, Shaohui and Heikal, Ahmed a. and Webb, Watt W.},
	year = {2002},
	pages = {697--705},
}

@article{gennerich_fluorescence_2000,
	title = {Fluorescence correlation spectroscopy in small cytosolic compartments depends critically on the diffusion model used.},
	volume = {79},
	issn = {0006-3495},
	doi = {10.1016/S0006-3495(00)76561-1},
	abstract = {Fluorescence correlation spectroscopy (FCS) is a powerful technique for measuring low concentrations of fluorescent molecules and their diffusion constants. In the standard case, fluorescence fluctuations are measured in an open detection volume defined by the confocal optics. However, if FCS measurements are carried out in cellular processes that confine the detection volume, the standard FCS model leads to erroneous results. In this paper, we derive a modified FCS model that takes into account the confinement of the detection volume. Using this model, we have carried out the first FCS measurements in dendrites of cultured neurons. We further derive, for the case of confined diffusion, the limits within which the standard two- and three-dimensional diffusion models give reliable results.},
	number = {December},
	journal = {Biophysical journal},
	author = {Gennerich, a and Schild, D},
	year = {2000},
	pages = {3294--3306},
}

@article{noauthor_1471-2121-2-8-4_nodate,
	title = {1471-2121-2-8-4},
}

@article{kremers_cyan_2006,
	title = {Cyan and {Yellow} {Super} {Fluorescent} {Proteins} with {Improved} {Brightness} , {Protein} {Folding} , and {FRET} {F} ö rster {Radius} {Cyan} and {Yellow} {Super} {Fluorescent} {Proteins} with {Improved} {Brightness} , {Protein} {Folding} , and {FRET} {Fo}},
	doi = {10.1021/bi0516273},
	number = {2},
	author = {Kremers, Gert-jan and Goedhart, Joachim and Munster, Erik B Van and Gadella, Theodorus W J},
	year = {2006},
	pages = {6570--6580},
}

@article{wu_learning_2016,
	title = {Learning a {Probabilistic} {Latent} {Space} of {Object} {Shapes} via {3D} {Generative}-{Adversarial} {Modeling}},
	abstract = {We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.},
	author = {Wu, Jiajun and Zhang, Chengkai and Xue, Tianfan and Freeman, William T. and Tenenbaum, Joshua B.},
	year = {2016},
}

@article{xie_all_2017,
	title = {All {You} {Need} is {Beyond} a {Good} {Init}: {Exploring} {Better} {Solution} for {Training} {Extremely} {Deep} {Convolutional} {Neural} {Networks} with {Orthonormality} and {Modulation}},
	url = {http://arxiv.org/abs/1703.01827},
	abstract = {Deep neural network is difficult to train and this predicament becomes worse as the depth increases. The essence of this problem exists in the magnitude of backpropagated errors that will result in gradient vanishing or exploding phenomenon. We show that a variant of regularizer which utilizes orthonormality among different filter banks can alleviate this problem. Moreover, we design a backward error modulation mechanism based on the quasi-isometry assumption between two consecutive parametric layers. Equipped with these two ingredients, we propose several novel optimization solutions that can be utilized for training a specific-structured (repetitively triple modules of Conv-BNReLU) extremely deep convolutional neural network (CNN) WITHOUT any shortcuts/ identity mappings from scratch. Experiments show that our proposed solutions can achieve distinct improvements for a 44-layer and a 110-layer plain networks on both the CIFAR-10 and ImageNet datasets. Moreover, we can successfully train plain CNNs to match the performance of the residual counterparts. Besides, we propose new principles for designing network structure from the insights evoked by orthonormality. Combined with residual structure, we achieve comparative performance on the ImageNet dataset.},
	author = {Xie, Di and Xiong, Jiang and Pu, Shiliang},
	month = mar,
	year = {2017},
}

@article{chong_yeast_nodate,
	title = {Yeast {Proteome} {Dynamics} from {Single} {Cell} {Imaging} and {Automated} {Analysis}},
	url = {http://www.cell.com/cms/attachment/2053316055/2060145718/mmc1.pdf},
	author = {Chong, Yolanda T and Koh, Judice L Y and Friesen, Helena and Duffy, Kaluarachchi and Cox, Michael J and Moses, Alan and Moffat, Jason and Boone, Charles and Andrews, Brenda J},
}

@article{dosovitskiy_supplementary_nodate,
	title = {Supplementary material for " {Generating} {Images} with {Perceptual} {Similarity} {Metrics} based on {Deep} {Networks} "},
	url = {https://lmb.informatik.uni-freiburg.de/Publications/2016/DB16c/inverting_GAN_nips2016_supp_final.pdf},
	abstract = {Here we show additional technical details and results which could not be included into the paper because of space constraints. Discriminator architecture is shown in Figure 1 . In our setup the job of the discriminator is to analyze the local statistics of images. Therefore, after five convolutional layers with occasional stride we perform global average pooling. The result is processed by two fully connected layers, followed by a 2-way softmax. We perform 50\% dropout after the global average pooling layer and the first fully connected layer. More inversion results are shown in Figure 1 . The images are randomly selected from the ImageNet validation set. Nearest neighbors of some validation images are shown in Figure 2 . We retrieved the nearest neighbors based on Euclidean distance in the pixel space and in the representations extracted from different layers of AlexNet. For comparison we also show the reconstructions produced by the network. Interestingly, for one of the images there is a near-duplicate in the training set. For other images the nearest neighbors are mainly very low quality when computed in the pixel space of low-level feature spaces, but very semantically meaningful when computed in high-level feature spaces. In most cases the reconstructions produced by the network better reflect characteristic properties of the images than the retrieved nearest neighbors. This is especially so for classes with large variability, such as dogs. Extended ablation study of our loss function is shown in Figure 3 . Note how results without the image space loss are very similar to the results with the full loss, except for artifacts at the border and in some cases (second row) in the image.},
	author = {Dosovitskiy, Alexey and Brox, Thomas},
}

@article{mishkin_all_2015,
	title = {All you need is a good init},
	url = {http://arxiv.org/abs/1511.06422},
	abstract = {Layer-sequential unit-variance (LSUV) initialization - a simple method for weight initialization for deep net learning - is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.},
	author = {Mishkin, Dmytro and Matas, Jiri},
	month = nov,
	year = {2015},
}

@article{ruder_overview_2016,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	author = {Ruder, Sebastian},
	month = sep,
	year = {2016},
}

@article{glorot_understanding_nodate,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	url = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
	abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experi-mental results showing the superiority of deeper vs less deep architectures. All these experimen-tal results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations func-tions. We find that the logistic sigmoid activation is unsuited for deep networks with random ini-tialization because of its mean value, which can drive especially the top hidden layer into satu-ration. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during train-ing, with the idea that training may be more dif-ficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new ini-tialization scheme that brings substantially faster convergence.},
	author = {Glorot, Xavier and Bengio, Yoshua},
}

@article{dosovitskiy_generating_nodate,
	title = {Generating {Images} with {Perceptual} {Similarity} {Metrics} based on {Deep} {Networks}},
	url = {https://lmb.informatik.uni-freiburg.de/Publications/2016/DB16c/inverting_GAN_nips2016_final.pdf},
	abstract = {We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), allowing to generate sharp high resolution images from com-pressed abstract representations. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric reflects perceptual similarity of images much better and, thus, leads to better results. We demonstrate two examples of use cases of the proposed loss: (1) networks that invert the AlexNet convolutional network; (2) a modified version of a variational autoencoder that generates realistic high-resolution random images.},
	author = {Dosovitskiy, Alexey and Brox, Thomas},
}

@article{tatarchenko_multi-view_nodate,
	title = {Multi-view {3D} {Models} from {Single} {Images} with a {Convolutional} {Network}: {Supplementary} {Material}},
	url = {https://lmb.informatik.uni-freiburg.de/Publications/2016/TDB16a/tdb16_supp.pdf},
	abstract = {We present experimental results showing the effect of realistic rendering and the effect of adversarial training. We also analyze how the internal representation changes when the network is presented with different input views of the same object. 1 Realistic rendering As mentioned in the paper, we found that in order to achieve better generaliza-tion to real images special care has to be taken when rendering the training data. We trained networks with two kinds of training data: " realistic " and " basic " . The " realistic " rendering is described in the main paper: we randomly sam-pled the number of light sources, their intensities and the locations; performed alpha compositioning to avoid sharp transition between the model and the back-ground; and additionally smoothed the car image with a Gaussian filter. The " basic " rendering is with two light sources of fixed intensity, without alpha com-positioning and smoothing. Figure 1 compares the results of networks trained on these two kinds of data. The network trained on " basic " data (bottom row for each model) fails to correctly estimate the car shape in all cases but one. The network trained with " realistic " data performs much better, demonstrating how the quality of the training data is crucial for generalization to real images. 2 Adversarial training Tasks involving image generation are still mostly solved by optimizing L 2 ob-jective, which is robust but often leads to blurred results. This happens because of the fundamental uncertainty associated with novel view estimation, which in case of Euclidean loss leads to predicting the average of all possibilities. Alter-natively, one can use the idea of adversarial training introduced by Goodfellow et al. [1]. The aim is to train a generator G ψ (parametrized by a neural net-work with weights ψ) which takes random noise as input and generates realistic images. This is achieved by training the generator concurrently with another neural network – a discriminator D ϕ . The discriminator aims to distinguish the generated images from real ones, while the generator aims to trick the dis-criminator. Mathematically, the parameters ϕ of the discriminator are trained},
	author = {Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},
}

@article{tatarchenko_multi-view_nodate-1,
	title = {Multi-view {3D} {Models} from {Single} {Images} with a {Convolutional} {Network}},
	url = {https://lmb.informatik.uni-freiburg.de/Publications/2016/TDB16a/tdb16.pdf},
	abstract = {We present a convolutional network capable of inferring a 3D representation of a previously unseen object given a single image of this object. Concretely, the network can predict an RGB image and a depth map of the object as seen from an arbitrary view. Several of these depth maps fused together give a full point cloud of the object. The point cloud can in turn be transformed into a surface mesh. The network is trained on renderings of synthetic 3D models of cars and chairs. It successfully deals with objects on cluttered background and generates reasonable predictions for real images of cars.},
	author = {Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},
	keywords = {deep learning, 3D from single image, convolutional net-works},
}

@article{uhlen_human_2005,
	title = {A human protein atlas for normal and cancer tissues based on antibody proteomics.},
	volume = {4},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/16127175},
	doi = {10.1074/mcp.M500279-MCP200},
	abstract = {Antibody-based proteomics provides a powerful approach for the functional study of the human proteome involving the systematic generation of protein-specific affinity reagents. We used this strategy to construct a comprehensive, antibody-based protein atlas for expression and localization profiles in 48 normal human tissues and 20 different cancers. Here we report a new publicly available database containing, in the first version, approximately 400,000 high resolution images corresponding to more than 700 antibodies toward human proteins. Each image has been annotated by a certified pathologist to provide a knowledge base for functional studies and to allow queries about protein profiles in normal and disease tissues. Our results suggest it should be possible to extend this analysis to the majority of all human proteins thus providing a valuable tool for medical and biological research.},
	number = {12},
	journal = {Molecular \& cellular proteomics : MCP},
	author = {Uhlén, Mathias and Björling, Erik and Agaton, Charlotta and Szigyarto, Cristina Al-Khalili and Amini, Bahram and Andersen, Elisabet and Andersson, Ann-Catrin and Angelidou, Pia and Asplund, Anna and Asplund, Caroline and Berglund, Lisa and Bergström, Kristina and Brumer, Harry and Cerjan, Dijana and Ekström, Marica and Elobeid, Adila and Eriksson, Cecilia and Fagerberg, Linn and Falk, Ronny and Fall, Jenny and Forsberg, Mattias and Björklund, Marcus Gry and Gumbel, Kristoffer and Halimi, Asif and Hallin, Inga and Hamsten, Carl and Hansson, Marianne and Hedhammar, My and Hercules, Görel and Kampf, Caroline and Larsson, Karin and Lindskog, Mats and Lodewyckx, Wald and Lund, Jan and Lundeberg, Joakim and Magnusson, Kristina and Malm, Erik and Nilsson, Peter and Odling, Jenny and Oksvold, Per and Olsson, Ingmarie and Oster, Emma and Ottosson, Jenny and Paavilainen, Linda and Persson, Anja and Rimini, Rebecca and Rockberg, Johan and Runeson, Marcus and Sivertsson, Asa and Sköllermo, Anna and Steen, Johanna and Stenvall, Maria and Sterky, Fredrik and Strömberg, Sara and Sundberg, Mårten and Tegel, Hanna and Tourle, Samuel and Wahlund, Eva and Waldén, Annelie and Wan, Jinghong and Wernérus, Henrik and Westberg, Joakim and Wester, Kenneth and Wrethagen, Ulla and Xu, Lan Lan and Hober, Sophia and Pontén, Fredrik},
	month = dec,
	year = {2005},
	pages = {1920--32},
}

@article{fagerberg_analysis_2014,
	title = {Analysis of the human tissue-specific expression by genome-wide integration of transcriptomics and antibody-based proteomics.},
	volume = {13},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/24309898},
	doi = {10.1074/mcp.M113.035600},
	abstract = {Global classification of the human proteins with regards to spatial expression patterns across organs and tissues is important for studies of human biology and disease. Here, we used a quantitative transcriptomics analysis (RNA-Seq) to classify the tissue-specific expression of genes across a representative set of all major human organs and tissues and combined this analysis with antibody-based profiling of the same tissues. To present the data, we launch a new version of the Human Protein Atlas that integrates RNA and protein expression data corresponding to ∼80\% of the human protein-coding genes with access to the primary data for both the RNA and the protein analysis on an individual gene level. We present a classification of all human protein-coding genes with regards to tissue-specificity and spatial expression pattern. The integrative human expression map can be used as a starting point to explore the molecular constituents of the human body.},
	number = {2},
	journal = {Molecular \& cellular proteomics : MCP},
	author = {Fagerberg, Linn and Hallström, Björn M and Oksvold, Per and Kampf, Caroline and Djureinovic, Dijana and Odeberg, Jacob and Habuka, Masato and Tahmasebpoor, Simin and Danielsson, Angelika and Edlund, Karolina and Asplund, Anna and Sjöstedt, Evelina and Lundberg, Emma and Szigyarto, Cristina Al-Khalili and Skogs, Marie and Takanen, Jenny Ottosson and Berling, Holger and Tegel, Hanna and Mulder, Jan and Nilsson, Peter and Schwenk, Jochen M and Lindskog, Cecilia and Danielsson, Frida and Mardinoglu, Adil and Sivertsson, Asa and von Feilitzen, Kalle and Forsberg, Mattias and Zwahlen, Martin and Olsson, IngMarie and Navani, Sanjay and Huss, Mikael and Nielsen, Jens and Ponten, Fredrik and Uhlén, Mathias},
	month = feb,
	year = {2014},
	pages = {397--406},
}

@article{noauthor_supplementary_nodate,
	title = {Supplementary {Material} for "{FlowNet} 2.0: {Evolution} of {Optical} {Flow} {Estimation} with {Deep} {Networks}"},
	url = {https://lmb.informatik.uni-freiburg.de/Publications/2017/IMKDB17/FlowNet_2_0__CVPR_supplemental.pdf},
	abstract = {Figure 1. Flow field color coding used in this paper. The displace-ment of every pixel in this illustration is the vector from the center of the square to this pixel. The central pixel does not move. The value is scaled differently for different images to best visualize the most interesting range. 1. Video Please see the supplementary video for FlowNet2 results on a number of diverse video sequences, a comparison be-tween FlowNet2 and state-of-the-art methods, and an illus-tration of the speed/accuracy trade-off of the FlowNet 2.0 family of models. Optical flow color coding. For optical flow visualization we use the color coding of Butler et al. [3]. The color cod-ing scheme is illustrated in Figure 1. Hue represents the direction of the displacement vector, while the intensity of the color represents its magnitude. White color corresponds to no motion. Because the range of motions is very different in different image sequences, we scale the flow fields before visualization: independently for each image pair shown in figures, and independently for each video fragment in the supplementary video. Scaling is always the same for all methods being compared. 2. Dataset Schedules: KITTI2015 Results In Table 1 we show more results of training networks with the original FlowNet schedule S short [4] and the new FlowNet2 schedules S long and S fine . We provide the end-point error when testing on the KITTI2015 train dataset. Ta-ble 1 in the main paper shows the performance of the same networks on Sintel. One can observe that on KITTI2015, as well as on Sintel, training with S long + S fine on the com-Architecture Datasets S short S long S fine},
}

@article{noauthor_atlas_nodate,
	title = {{AN} {ATLAS} {OF} {EXPRESSION}},
	url = {https://www.nature.com/nature/journal/v509/n7502/pdf/509645a.pdf},
}

@article{berglund_genecentric_2008,
	title = {A genecentric {Human} {Protein} {Atlas} for expression profiles based on antibodies.},
	volume = {7},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/18669619},
	doi = {10.1074/mcp.R800013-MCP200},
	abstract = {An attractive path forward in proteomics is to experimentally annotate the human protein complement of the genome in a genecentric manner. Using antibodies, it might be possible to design protein-specific probes for a representative protein from every protein-coding gene and to subsequently use the antibodies for systematical analysis of cellular distribution and subcellular localization of proteins in normal and disease tissues. A new version (4.0) of the Human Protein Atlas has been developed in a genecentric manner with the inclusion of all human genes and splice variants predicted from genome efforts together with a visualization of each protein with characteristics such as predicted membrane regions, signal peptide, and protein domains and new plots showing the uniqueness (sequence similarity) of every fraction of each protein toward all other human proteins. The new version is based on tissue profiles generated from 6120 antibodies with more than five million immunohistochemistry-based images covering 5067 human genes, corresponding to approximately 25\% of the human genome. Version 4.0 includes a putative list of members in various protein classes, both functional classes, such as kinases, transcription factors, G-protein-coupled receptors, etc., and project-related classes, such as candidate genes for cancer or cardiovascular diseases. The exact antigen sequence for the internally generated antibodies has also been released together with a visualization of the application-specific validation performed for each antibody, including a protein array assay, Western blot analysis, immunohistochemistry, and, for a large fraction, immunofluorescence-based confocal microscopy. New search functionalities have been added to allow complex queries regarding protein expression profiles, protein classes, and chromosome location. The new version of the protein atlas thus is a resource for many areas of biomedical research, including protein science and biomarker discovery.},
	number = {10},
	journal = {Molecular \& cellular proteomics : MCP},
	author = {Berglund, Lisa and Björling, Erik and Oksvold, Per and Fagerberg, Linn and Asplund, Anna and Szigyarto, Cristina Al-Khalili and Persson, Anja and Ottosson, Jenny and Wernérus, Henrik and Nilsson, Peter and Lundberg, Emma and Sivertsson, Asa and Navani, Sanjay and Wester, Kenneth and Kampf, Caroline and Hober, Sophia and Pontén, Fredrik and Uhlén, Mathias},
	month = oct,
	year = {2008},
	pages = {2019--27},
}

@article{wilhelm_mass-spectrometry-based_2014,
	title = {Mass-spectrometry-based draft of the human proteome},
	volume = {509},
	doi = {10.1038/nature13319},
	journal = {Nature},
	author = {Wilhelm, M.},
	year = {2014},
}

@article{ilg_flownet_nodate,
	title = {{FlowNet} 2.0: {Evolution} of {Optical} {Flow} {Estimation} with {Deep} {Networks}},
	url = {https://lmb.informatik.uni-freiburg.de/Publications/2017/IMKDB17/FlowNet_2_0__CVPR.pdf},
	abstract = {The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small dis-placements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a sub-network specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50\%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that al-low optical flow computation at up to 140fps with accuracy matching the original FlowNet.},
	author = {Ilg, Eddy and Mayer, Nikolaus and Saikia, Tonmoy and Keuper, Margret and Dosovitskiy, Alexey and Brox, Thomas},
}

@article{makhzani_adversarial_2015,
	title = {Adversarial {Autoencoders}},
	url = {http://arxiv.org/abs/1511.05644},
	abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
	author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
	month = nov,
	year = {2015},
}

@article{norouzi_zero-shot_2013,
	title = {Zero-{Shot} {Learning} by {Convex} {Combination} of {Semantic} {Embeddings}},
	url = {https://arxiv.org/pdf/1312.5650.pdf},
	abstract = {Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces. In some cases the embedding space is trained jointly with the image transformation. In other cases the semantic embedding space is established by an independent natural language processing task, and then the image transformation into that space is learned in a second stage. Proponents of these image embedding systems have stressed their advantages over the traditional {\textbackslash}nway\{\} classification framing of image understanding, particularly in terms of the promise for zero-shot learning -- the ability to correctly annotate images of previously unseen object categories. In this paper, we propose a simple method for constructing an image embedding system from any existing {\textbackslash}nway\{\} image classifier and a semantic word embedding model, which contains the \${\textbackslash}n\$ class labels in its vocabulary. Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no additional training. We show that this simple and direct method confers many of the advantages associated with more complex image embedding schemes, and indeed outperforms state of the art methods on the ImageNet zero-shot learning task.},
	author = {Norouzi, Mohammad and Mikolov, Tomas and Bengio, Samy and Singer, Yoram and Shlens, Jonathon and Frome, Andrea and Corrado, Greg S and Dean, Jeffrey},
	year = {2013},
}

@article{kraus_automated_2017,
	title = {Automated analysis of high-content microscopy data with deep learning},
	volume = {13},
	url = {http://msb.embopress.org/content/msb/13/4/924.full.pdf},
	doi = {10.15252/msb},
	abstract = {Existing computational pipelines for quantitative analysis of high-content microscopy data rely on traditional machine learn-ing approaches that fail to accurately classify more than a single dataset without substantial tuning and training, requiring extensive analysis. Here, we demonstrate that the application of deep learning to biological image data can overcome the pitfalls associated with conventional machine learning classi-fiers. Using a deep convolutional neural network (DeepLoc) to analyze yeast cell images, we show improved performance over traditional approaches in the automated classification of protein subcellular localization. We also demonstrate the ability of DeepLoc to classify highly divergent image sets, including images of pheromone-arrested cells with abnormal cellular morphology, as well as images generated in different genetic backgrounds and in different laboratories. We offer an open-source implementation that enables updating DeepLoc on new microscopy datasets. This study highlights deep learning as an important tool for the expedited analysis of high-content microscopy data.},
	journal = {Mol Syst Biol},
	author = {Kraus, Oren Z and Grys, Ben T and Ba, Jimmy and Chong, Yolanda and Frey, Brendan J and Boone, Charles and Andrews, Brenda J},
	year = {2017},
	keywords = {machine learning, deep learning, image analysis, high-content screening, Methods \& Resources},
}

@article{dai_deformable_2017,
	title = {Deformable {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1703.06211},
	abstract = {Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in its building modules. In this work, we introduce two new modules to enhance the transformation modeling capacity of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the effectiveness of our approach on sophisticated vision tasks of object detection and semantic segmentation. The code would be released.},
	author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
	month = mar,
	year = {2017},
}

@article{wang_residual_2017,
	title = {Residual {Attention} {Network} for {Image} {Classification}},
	url = {http://arxiv.org/abs/1704.06904},
	abstract = {In this work, we propose "Residual Attention Network", a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion. Our Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers. Extensive analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the effectiveness of every module mentioned above. Our Residual Attention Network achieves state-of-the-art object recognition performance on three benchmark datasets including CIFAR-10 (3.90\% error), CIFAR-100 (20.45\% error) and ImageNet (4.8\% single model and single crop, top-5 error). Note that, our method achieves 0.6\% top-1 accuracy improvement with 46\% trunk depth and 69\% forward FLOPs comparing to ResNet-200. The experiment also demonstrates that our network is robust against noisy labels.},
	author = {Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou},
	month = apr,
	year = {2017},
}

@article{chong_yeast_2015,
	title = {Yeast proteome dynamics from single cell imaging and automated analysis},
	volume = {161},
	issn = {0092-8674},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/26046442},
	doi = {10.1016/j.cell.2015.04.051},
	abstract = {Proteomics has proved invaluable in generating large-scale quantitative data; however, the development of systems approaches for examining the proteome in vivo has lagged behind. To evaluate protein abundance and localization on a proteome scale, we exploited the yeast GFP-fusion collection in a pipeline combining automated genetics, high-throughput microscopy, and computational feature analysis. We developed an ensemble of binary classifiers to generate localization data from single-cell measurements and constructed maps of 3,000 proteins connected to 16 localization classes. To survey proteome dynamics in response to different chemical and genetic stimuli, we measure proteome-wide abundance and localization and identified changes over time. We analyzed {\textgreater}20 million cells to identify dynamic proteins that redistribute among multiple localizations in hydroxyurea, rapamycin, and in an rpd3?? background. Because our localization and abundance data are quantitative, they provide the opportunity for many types of comparative studies, single cell analyses, modeling, and prediction.},
	number = {6},
	journal = {Cell},
	author = {Chong, Yolanda T. and Koh, Judice L Y and Friesen, Helena and Duffy, Kaluarachchi and Cox, Michael J. and Moses, Alan and Moffat, Jason and Boone, Charles and Andrews, Brenda J.},
	month = jun,
	year = {2015},
	pages = {1413--1424},
	file = {Full Text:/home/zwerg/Zotero/storage/75G9JN6G/Chong et al. - 2015 - Yeast proteome dynamics from single cell imaging a.pdf:application/pdf},
}

@article{gustafsdottir_multiplex_2013,
	title = {Multiplex {Cytological} {Profiling} {Assay} to {Measure} {Diverse} {Cellular} {States}},
	volume = {8},
	url = {http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0080999&type=printable},
	doi = {10.1371/journal.pone.0080999},
	abstract = {Computational methods for image-based profiling are under active development, but their success hinges on assays that can capture a wide range of phenotypes. We have developed a multiplex cytological profiling assay that " paints the cell " with as many fluorescent markers as possible without compromising our ability to extract rich, quantitative profiles in high throughput. The assay detects seven major cellular components. In a pilot screen of bioactive compounds, the assay detected a range of cellular phenotypes and it clustered compounds with similar annotated protein targets or chemical structure based on cytological profiles. The results demonstrate that the assay captures subtle patterns in the combination of morphological labels, thereby detecting the effects of chemical compounds even though their targets are not stained directly. This image-based assay provides an unbiased approach to characterize compound-and disease-associated cell states to support future probe discovery.},
	number = {12},
	journal = {PLoS ONE},
	author = {Gustafsdottir, Sigrun M and Ljosa, Vebjorn and Sokolnicki, Katherine L and Anthony Wilson, J and Walpita, Deepika and Kemp, Melissa M and Petri Seiler, Kathleen and Carrel, Hyman A and Golub, Todd R and Schreiber, Stuart L and Clemons, Paul A and Carpenter, Anne E and Shamji, Alykhan F},
	year = {2013},
}

@article{hou_single-cell_2016,
	title = {Single-cell triple omics sequencing reveals genetic, epigenetic, and transcriptomic heterogeneity in hepatocellular carcinomas.},
	volume = {26},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/26902283},
	doi = {10.1038/cr.2016.23},
	abstract = {Single-cell genome, DNA methylome, and transcriptome sequencing methods have been separately developed. However, to accurately analyze the mechanism by which transcriptome, genome and DNA methylome regulate each other, these omic methods need to be performed in the same single cell. Here we demonstrate a single-cell triple omics sequencing technique, scTrio-seq, that can be used to simultaneously analyze the genomic copy-number variations (CNVs), DNA methylome, and transcriptome of an individual mammalian cell. We show that large-scale CNVs cause proportional changes in RNA expression of genes within the gained or lost genomic regions, whereas these CNVs generally do not affect DNA methylation in these regions. Furthermore, we applied scTrio-seq to 25 single cancer cells derived from a human hepatocellular carcinoma tissue sample. We identified two subpopulations within these cells based on CNVs, DNA methylome, or transcriptome of individual cells. Our work offers a new avenue of dissecting the complex contribution of genomic and epigenomic heterogeneities to the transcriptomic heterogeneity within a population of cells.},
	number = {3},
	journal = {Cell research},
	author = {Hou, Yu and Guo, Huahu and Cao, Chen and Li, Xianlong and Hu, Boqiang and Zhu, Ping and Wu, Xinglong and Wen, Lu and Tang, Fuchou and Huang, Yanyi and Peng, Jirun},
	month = mar,
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	pages = {304--19},
}

@article{angermueller_deep_2016,
	title = {Deep learning for computational biology},
	volume = {12},
	url = {http://msb.embopress.org/content/msb/12/7/878.full.pdf},
	doi = {10.15252/msb},
	abstract = {Technological advances in genomics and imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples. This rapid increase in biological data dimen-sion and acquisition rate is challenging conventional analysis strategies. Modern machine learning methods, such as deep learn-ing, promise to leverage very large data sets for finding hidden structure within them, and for making accurate predictions. In this review, we discuss applications of this new breed of analysis approaches in regulatory genomics and cellular imaging. We provide background of what deep learning is, and the settings in which it can be successfully applied to derive biological insights. In addition to presenting specific applications and providing tips for practical use, we also highlight possible pitfalls and limitations to guide computational biologists when and how to make the most use of this new technology.},
	journal = {Mol Syst Biol},
	author = {Angermueller, Christof and Pärnamaa, Tanel and Parts, Leopold and Stegle, Oliver},
	year = {2016},
	keywords = {machine learning, deep learning, computational biology, cellular imaging, regulatory genomics},
}

@article{he_deep_2016,
	title = {Deep residual learning for image recognition},
	volume = {2016-Decem},
	issn = {9781467388504},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2016},
	pages = {770--778},
	file = {Full Text:/home/zwerg/Zotero/storage/Q9RCT53S/He et al. - 2016 - Deep residual learning for image recognition.pdf:application/pdf},
}

@article{che_distilling_2015,
	title = {Distilling {Knowledge} from {Deep} {Networks} with {Applications} to {Healthcare} {Domain}},
	url = {http://arxiv.org/abs/1512.03542},
	abstract = {Exponential growth in Electronic Healthcare Records (EHR) has resulted in new opportunities and urgent needs for discovery of meaningful data-driven representations and patterns of diseases in Computational Phenotyping research. Deep Learning models have shown superior performance for robust prediction in computational phenotyping tasks, but suffer from the issue of model interpretability which is crucial for clinicians involved in decision-making. In this paper, we introduce a novel knowledge-distillation approach called Interpretable Mimic Learning, to learn interpretable phenotype features for making robust prediction while mimicking the performance of deep learning models. Our framework uses Gradient Boosting Trees to learn interpretable features from deep learning models such as Stacked Denoising Autoencoder and Long Short-Term Memory. Exhaustive experiments on a real-world clinical time-series dataset show that our method obtains similar or better performance than the deep learning models, and it provides interpretable phenotypes for clinical decision making.},
	author = {Che, Zhengping and Purushotham, Sanjay and Khemani, Robinder and Liu, Yan},
	month = dec,
	year = {2015},
}

@article{frome_devise_2013,
	title = {Devise: {A} deep visual-semantic embedding model},
	url = {https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41869.pdf},
	abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their pre- dictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as seman- tic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recogni- tion challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18\% across thou- sands of novel labels never seen by the visual model.},
	journal = {Advances in Neural …},
	author = {Frome, Andrea and Corrado, Gs and Shlens, Jonathon},
	year = {2013},
	pages = {1--11},
}

@article{cleveland_robust_1979,
	title = {Robust {Locally} {Weighted} {Regression} and {Smoothing} {Scatterplots}},
	volume = {74},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1979.10481038},
	doi = {10.1080/01621459.1979.10481038},
	abstract = {Recent technological advances have enabled DNA methylation to be assayed at single-cell resolution. However, current protocols are limited by incomplete CpG coverage and hence methods to predict missing methylation states are critical to enable genome-wide analyses. We report DeepCpG, a computational approach based on deep neural networks to predict methylation states in single cells. We evaluate DeepCpG on single-cell methylation data from five cell types generated using alternative sequencing protocols. DeepCpG yields substantially more accurate predictions than previous methods. Additionally, we show that the model parameters can be interpreted, thereby providing insights into how sequence composition affects methylation variability.},
	number = {368},
	journal = {Journal of the American Statistical Association},
	author = {Cleveland, William S. and Lee, Heather J. and Reik, Wolf and Stegle, Oliver and Grant, CE and Clementi, L},
	month = dec,
	year = {1979},
	note = {Publisher: BioMed Central},
	keywords = {Bioinformatics, Animal Genetics and Genomics, Evolutionary Biology, Human Genetics, Microbial Genetics and Genomics, Plant Genetics \& Genomics},
	pages = {829--836},
}

@article{xu_deep_2016,
	title = {Deep {Interactive} {Object} {Selection}},
	url = {http://arxiv.org/abs/1603.04042},
	abstract = {Interactive object selection is a very important research problem and has many applications. Previous algorithms require substantial user interactions to estimate the foreground and background distributions. In this paper, we present a novel deep learning based algorithm which has a much better understanding of objectness and thus can reduce user interactions to just a few clicks. Our algorithm transforms user provided positive and negative clicks into two Euclidean distance maps which are then concatenated with the RGB channels of images to compose (image, user interactions) pairs. We generate many of such pairs by combining several random sampling strategies to model user click patterns and use them to fine tune deep Fully Convolutional Networks (FCNs). Finally the output probability maps of our FCN 8s model is integrated with graph cut optimization to refine the boundary segments. Our model is trained on the PASCAL segmentation dataset and evaluated on other datasets with different object classes. Experimental results on both seen and unseen objects clearly demonstrate that our algorithm has a good generalization ability and is superior to all existing interactive object selection approaches.},
	author = {Xu, Ning and Price, Brian and Cohen, Scott and Yang, Jimei and Huang, Thomas},
	month = mar,
	year = {2016},
}

@article{pont-tuset_2017_2017,
	title = {The 2017 {DAVIS} {Challenge} on {Video} {Object} {Segmentation}},
	url = {http://arxiv.org/abs/1704.00675},
	abstract = {We present the 2017 DAVIS Challenge, a public competition specifically designed for the task of video object segmentation. Following the footsteps of other successful initiatives, such as ILSVRC and PASCAL VOC, which established the avenue of research in the fields of scene classification and semantic segmentation, the DAVIS Challenge comprises a dataset, an evaluation methodology, and a public competition with a dedicated workshop co-located with CVPR 2017. The DAVIS Challenge follows up on the recent publication of DAVIS (Densely-Annotated VIdeo Segmentation), which has fostered the development of several novel state-of-the-art video object segmentation techniques. In this paper we describe the scope of the benchmark, highlight the main characteristics of the dataset and define the evaluation metrics of the competition.},
	author = {Pont-Tuset, Jordi and Perazzi, Federico and Caelles, Sergi and Arbeláez, Pablo and Sorkine-Hornung, Alex and Van Gool, Luc},
	month = apr,
	year = {2017},
}

@article{ement_farabet_learning_nodate,
	title = {Learning {Hierarchical} {Features} for {Scene} {Labeling}},
	url = {http://yann.lecun.com/exdb/publis/pdf/farabet-pami-13.pdf},
	abstract = {—Scene labeling consists in labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape and contextual information. We report results using multiple post-processing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, e.g. they can be taken from a segmentation tree, or from any family of over-segmentations. The system yields record accuracies on the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) and near-record accuracy on Stanford Background Dataset (8 classes), while being an order of magnitude faster than competing approaches, producing a 320 × 240 image labeling in less than a second, including feature extraction.},
	author = {Ement Farabet, Cí and Couprie, Camille and Najman, Laurent and Lecun, Yann},
	keywords = {()},
}

@article{mottaghi_role_nodate,
	title = {The {Role} of {Context} for {Object} {Detection} and {Semantic} {Segmentation} in the {Wild}},
	url = {http://www.stat.ucla.edu/~ccvl/datasets/pascal-context/mottaghi_et_al_cvpr14.pdf},
	abstract = {In this paper we study the role of context in existing state-of-the-art detection and segmentation approaches. Towards this goal, we label every pixel of PASCAL VOC 2010 de-tection challenge with a semantic category. We believe this data will provide plenty of challenges to the community, as it contains 520 additional classes for semantic segmenta-tion and object detection. Our analysis shows that near-est neighbor based approaches perform poorly on semantic segmentation of contextual classes, showing the variability of PASCAL imagery. Furthermore, improvements of exist-ing contextual models for detection is rather modest. In order to push forward the performance in this difficult sce-nario, we propose a novel deformable part-based model, which exploits both local context around each candidate de-tection as well as global context at the level of the scene. We show that this contextual reasoning significantly helps in detecting objects at all scales.},
	author = {Mottaghi, Roozbeh and Chen, Xianjie and Liu, Xiaobai and Cho, Nam-Gyu and Lee, Seong-Whan and Fidler, Sanja and Urtasun, Raquel and Yuille, Alan},
}

@article{arnab_pixelwise_2017,
	title = {Pixelwise {Instance} {Segmentation} with a {Dynamically} {Instantiated} {Network}},
	url = {http://arxiv.org/abs/1704.02386},
	abstract = {Semantic segmentation and object detection research have recently achieved rapid progress. However, the former task has no notion of different instances of the same object, and the latter operates at a coarse, bounding-box level. We propose an Instance Segmentation system that produces a segmentation map where each pixel is assigned an object class and instance identity label. Most approaches adapt object detectors to produce segments instead of boxes. In contrast, our method is based on an initial semantic segmentation module, which feeds into an instance subnetwork. This subnetwork uses the initial category-level segmentation, along with cues from the output of an object detector, within an end-to-end CRF to predict instances. This part of our model is dynamically instantiated to produce a variable number of instances per image. Our end-to-end approach requires no post-processing and considers the image holistically, instead of processing independent proposals. Therefore, unlike some related work, a pixel cannot belong to multiple instances. Furthermore, far more precise segmentations are achieved, as shown by our state-of-the-art results (particularly at high IoU thresholds) on the Pascal VOC and Cityscapes datasets.},
	author = {Arnab, Anurag and Torr, Philip H. S},
	month = apr,
	year = {2017},
}

@article{shi_is_nodate-1,
	title = {Is the deconvolution layer the same as a convolutional layer?},
	url = {https://arxiv.org/pdf/1609.07009.pdf},
	abstract = {1 In our CVPR 2016 paper [1], we proposed a novel network architecture to perform single image super­resolution (SR). Most existing convolutional neural network (CNN) based super­resolution methods [10,11] first upsample the image using a bicubic interpolation, then apply a convolutional network. We will refer to these types of networks as high­resolution (HR) networks because the images are upsampled first. Instead, we feed the low­resolution (LR) input directly to a sub­pixel CNN as shown in Fig.1 : Figure 1: An illustration of the ESCPN framework where r denotes the upscaling ratio. Let denote the upscaling ratio ­ e.g if the input LR image is then the output HR image will be r 1 × 1 . We then output number of channels instead of one high­resolution (HR) image and use periodic r × r r 2 shuffling to recreate the HR image. The exact details about how our efficient sub­pixel convolutional layer works can be found in the paper. We will refer to our network as a LR network. In this note, we want to focus on two aspects related to two questions most people asked us at CVPR when they saw this network. Firstly, how can channels magically become a HR image? And secondly, r 2 why are convolution in LR space a better choice? These are actually the key questions we tried to answer in the paper, but we were not able to go into as much depth and clarity as we would've liked given the page limit. To better answer these questions, we first discuss the relationships between the deconvolution layer in the form of the transposed convolution layer, the sub­pixel convolutional layer and our efficient sub­pixel convolutional layer, which we'll go through in Sec. 1 and Sec. 2. We will refer to our efficient sub­pixel convolutional layer as a convolutional layer in LR space to distinguish it from the common sub­pixel convolutional layer [5]. We will then show that for a fixed computational budget and complexity, a network with convolutions exclusively in LR space has more representation power at the same speed than a network that first upsamples the input in HR space.},
	author = {Shi, Wenzhe and Caballero, Jose and Theis, Lucas and Huszar, Ferenc and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Ledig, Christian and Wang, Zehan},
}

@article{dumoulin_guide_2016-2,
	title = {A guide to convolution arithmetic for deep learning},
	url = {https://arxiv.org/pdf/1603.07285.pdf},
	author = {Dumoulin, Vincent and Visin, Francesco and Box, George E P},
	year = {2016},
}

@article{shelhamer_fully_nodate,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {https://arxiv.org/pdf/1605.06211.pdf},
	abstract = {—Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build " fully convolutional " networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves improved segmentation of PASCAL VOC (30\% relative improvement to 67.2\% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
	author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
	keywords = {Deep Learning, Convolutional Networks, Index Terms—Semantic Segmentation, Transfer Learning !},
}

@article{broad_geometry-based_2017,
	title = {Geometry-{Based} {Region} {Proposals} for {Real}-{Time} {Robot} {Detection} of {Tabletop} {Objects}},
	url = {http://arxiv.org/abs/1703.04665},
	abstract = {We present a novel object detection pipeline for localization and recognition in three dimensional environments. Our approach makes use of an RGB-D sensor and combines state-of-the-art techniques from the robotics and computer vision communities to create a robust, real-time detection system. We focus specifically on solving the object detection problem for tabletop scenes, a common environment for assistive manipulators. Our detection pipeline locates objects in a point cloud representation of the scene. These clusters are subsequently used to compute a bounding box around each object in the RGB space. Each defined patch is then fed into a Convolutional Neural Network (CNN) for object recognition. We also demonstrate that our region proposal method can be used to develop novel datasets that are both large and diverse enough to train deep learning models, and easy enough to collect that end-users can develop their own datasets. Lastly, we validate the resulting system through an extensive analysis of the accuracy and run-time of the full pipeline.},
	author = {Broad, Alexander and Argall, Brenna},
	month = mar,
	year = {2017},
}

@article{gao_improving_2017,
	title = {Improving {Object} {Detection} with {Region} {Similarity} {Learning}},
	url = {http://arxiv.org/abs/1703.00234},
	abstract = {Object detection aims to identify instances of semantic objects of a certain class in images or videos. The success of state-of-the-art approaches is attributed to the significant progress of object proposal and convolutional neural networks (CNNs). Most promising detectors involve multi-task learning with an optimization objective of softmax loss and regression loss. The first is for multi-class categorization, while the latter is for improving localization accuracy. However, few of them attempt to further investigate the hardness of distinguishing different sorts of distracting background regions (i.e., negatives) from true object regions (i.e., positives). To improve the performance of classifying positive object regions vs. a variety of negative background regions, we propose to incorporate triplet embedding into learning objective. The triplet units are formed by assigning each negative region to a meaningful object class and establishing class- specific negatives, followed by triplets construction. Over the benchmark PASCAL VOC 2007, the proposed triplet em- bedding has improved the performance of well-known FastRCNN model with a mAP gain of 2.1\%. In particular, the state-of-the-art approach OHEM can benefit from the triplet embedding and has achieved a mAP improvement of 1.2\%.},
	author = {Gao, Feng and Lou, Yihang and Bai, Yan and Wang, Shiqi and Huang, Tiejun and Duan, Ling-Yu},
	month = mar,
	year = {2017},
}

@article{pinheiro_recurrent_nodate,
	title = {Recurrent {Convolutional} {Neural} {Networks} for {Scene} {Labeling}},
	url = {http://machinelearning.wustl.edu/mlpapers/paper_files/icml2014c1_pinheiro14.pdf},
	abstract = {The goal of the scene labeling task is to assign a class label to each pixel in an image. To ensure a good visual coherence and a high class accu-racy, it is essential for a model to capture long range (pixel) label dependencies in images. In a feed-forward architecture, this can be achieved simply by considering a sufficiently large input context patch, around each pixel to be labeled. We propose an approach that consists of a re-current convolutional neural network which al-lows us to consider a large input context while limiting the capacity of the model. Contrary to most standard approaches, our method does not rely on any segmentation technique nor any task-specific features. The system is trained in an end-to-end manner over raw pixels, and mod-els complex spatial dependencies with low infer-ence cost. As the context size increases with the built-in recurrence, the system identifies and cor-rects its own errors. Our approach yields state-of-the-art performance on both the Stanford Back-ground Dataset and the SIFT Flow Dataset, while remaining very fast at test time.},
	author = {Pinheiro, Pedro O and Ch, Pedro Pinheiro@idiap and Collobert, Ronan and Com, Ronan@collobert},
}

@article{koenderink_surface_1992,
	title = {Surface shape and curvature scales},
	volume = {10},
	issn = {0262-8856},
	doi = {10.1016/0262-8856(92)90076-F},
	abstract = {The classical surface curvature measures, such as the Gaussian and the mean curvature at a point of a surface, are not very indicative of local shape. The two principal curvatures (taken as a pair) are more informative, but one would prefer a single shape indicator rather than a pair of numbers. Moreover, the shape indicator should preferably be independent of the size i.e. the amount of curvature, as distinct from the type of curvature. We propose two novel measures of local shape, the 'curvedness' and the 'shape index'. The curvedness is a positive number that specifies the amount of curvature, whereas the shape index is a number in the range [-1, +1] and is scale invariant. The shape index captures the intuitive notion of 'local shape' particularly well. The shape index can be mapped upon an intuitively natural colour scale. Two complementary shapes (like stamp and mould) map to complementary hues. The symmetrical saddle (which is very special because it is self-complementary) maps to white. When a surface is tinted according to this colour scheme, this induces an immediate perceptual segmentation of convex, concave, and hyperbolic areas. We propose it as a useful tool in graphics representation of 3D shape. ?? 1992.},
	number = {8},
	journal = {Image and Vision Computing},
	author = {Koenderink, Jan J. and van Doorn, Andrea J.},
	year = {1992},
	keywords = {curvature scales, surface shape},
	pages = {557--564},
}

@article{yoshida_three-dimensional_2001,
	title = {Three-dimensional computer-aided diagnosis scheme for detection of colonic polyps},
	volume = {20},
	issn = {0278-0062 (Print){\textbackslash}r0278-0062},
	doi = {10.1109/42.974921},
	abstract = {We have developed a three-dimensional (3-D) computer-aided diagnosis scheme for automated detection of colonic polyps in computed tomography (CT) colonographic data sets, and assessed its performance based on colonoscopy as the gold standard. In this scheme, a thick region encompassing the entire colonic wall is extracted from an isotropic volume reconstructed from the CT images in CT colonography. Polyp candidates are detected by first computing of 3-D geometric features that characterize polyps, folds, and colonic walls at each voxel in the extracted colon, and then segmenting of connected components corresponding to suspicious regions by hysteresis thresholding based on these geometric features. We apply fuzzy clustering to these connected components to obtain the polyp candidates. False-positive (FP) detections are then reduced by computation of several 3-D volumetric features characterizing the internal structures of the polyp candidates, followed by the application of discriminant analysis to the feature space generated by these volumetric features. The locations of the polyps detected by our computerized method were compared to the gold standard of conventional colonoscopy. The performance was evaluated based on 43 clinical cases, including 12 polyps determined by colonoscopy. Our computerized scheme was shown to have the potential to detect polyps in CT colonography with a clinically acceptable high sensitivity and a low FP rate.},
	number = {12},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Yoshida, Hiroyuki and Näppi, Janne},
	year = {2001},
	keywords = {Colon cancer, Computer-aided diagnosis, CT colonography, Polyp detection, Virtual colonoscopy},
	pages = {1261--1274},
}

@article{balestriero_neural_2017,
	title = {Neural {Decision} {Trees}},
	url = {http://arxiv.org/abs/1702.07360},
	abstract = {In this paper we propose a synergistic melting of neural networks and decision trees (DT) we call neural decision trees (NDT). NDT is an architecture a la decision tree where each splitting node is an independent multilayer perceptron allowing oblique decision functions or arbritrary nonlinear decision function if more than one layer is used. This way, each MLP can be seen as a node of the tree. We then show that with the weight sharing asumption among those units, we end up with a Hashing Neural Network (HNN) which is a multilayer perceptron with sigmoid activation function for the last layer as opposed to the standard softmax. The output units then jointly represent the probability to be in a particular region. The proposed framework allows for global optimization as opposed to greedy in DT and differentiability w.r.t. all parameters and the input, allowing easy integration in any learnable pipeline, for example after CNNs for computer vision tasks. We also demonstrate the modeling power of HNN allowing to learn union of disjoint regions for final clustering or classification making it more general and powerful than standard softmax MLP requiring linear separability thus reducing the need on the inner layer to perform complex data transformations. We finally show experiments for supervised, semi-suppervised and unsupervised tasks and compare results with standard DTs and MLPs.},
	author = {Balestriero, Randall},
	month = feb,
	year = {2017},
}

@article{berthelot_began_2017,
	title = {{BEGAN}: {Boundary} {Equilibrium} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1703.10717},
	abstract = {We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.},
	author = {Berthelot, David and Schumm, Thomas and Metz, Luke},
	month = mar,
	year = {2017},
}

@article{breiman_statistical_2001,
	title = {Statistical {Modeling}: {The} {Two} {Cultures}},
	volume = {16},
	url = {http://projecteuclid.org/download/pdf_1/euclid.ss/1009213726},
	abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commit-ment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current prob-lems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
	number = {3},
	journal = {Statistical Science},
	author = {Breiman, Leo},
	year = {2001},
	pages = {199--231},
}

@article{zhou_learning_nodate,
	title = {Learning {Deep} {Features} for {Discriminative} {Localization}},
	url = {http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf},
	abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remark-able localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actu-ally builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014 without training on any bounding box anno-tation.We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task 1 .},
	author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
}

@article{guo_entity_2016,
	title = {Entity {Embeddings} of {Categorical} {Variables}},
	url = {https://arxiv.org/pdf/1604.06737.pdf},
	abstract = {We map categorical variables in a function approximation problem into Euclidean spaces, which
are the entity embeddings of the categorical variables. The mapping is learned by a neural network
during the standard supervised training process. Entity embedding not only reduces memory usage
and speeds up neural networks compared with one-hot encoding, but more importantly by mapping
similar values close to each other in the embedding space it reveals the intrinsic properties of the
categorical variables. We applied it successfully in a recent Kaggle competitiona
and were able to
reach the third position with relative simple features. We further demonstrate in this paper that
entity embedding helps the neural network to generalize better when the data is sparse and statistics
is unknown. Thus it is especially useful for datasets with lots of high cardinality features, where
other methods tend to overfit. We also demonstrate that the embeddings obtained from the trained
neural network boost the performance of all tested machine learning methods considerably when
used as the input features instead. As entity embedding defines a distance measure for categorical
variables it can be used for visualizing categorical data and for data clustering},
	author = {Guo, Cheng and Berkhahn, Felix},
	year = {2016},
}

@article{wexler_space-time_2007,
	title = {Space-{Time} {Completion} of {Video}},
	volume = {29},
	number = {3},
	journal = {Pami},
	author = {Wexler, Y and Shechtman, E and Irani, M},
	year = {2007},
	pages = {463--476},
}

@article{isola_image--image_nodate,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {https://arxiv.org/pdf/1611.07004v1.pdf},
	abstract = {Labels to Facade BW to Color Aerial to Map Labels to Street Scene Edges to Photo input output input input input input output output output output input output Day to Night Figure 1: Many problems in image processing, graphics, and vision involve translating an input image into a corresponding output image. These problems are often treated with application-specific algorithms, even though the setting is always the same: map pixels to pixels. Conditional adversarial nets are a general-purpose solution that appears to work well on a wide variety of these problems. Here we show results of the method on several. In each case we use the same architecture and objective, and simply train on different data. Abstract We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss func-tion to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demon-strate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a commu-nity, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
}

@article{cicek_3d_2016,
	title = {{3D} {U}-{Net}: {Learning} {Dense} {Volumetric} {Segmentation} from {Sparse} {Annotation}},
	url = {http://arxiv.org/abs/1606.06650},
	abstract = {This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases.},
	author = {Çiçek, Özgün and Abdulkadir, Ahmed and Lienkamp, Soeren S. and Brox, Thomas and Ronneberger, Olaf},
	month = jun,
	year = {2016},
}

@article{reed_learning_2016,
	title = {Learning {What} and {Where} to {Draw}},
	url = {http://arxiv.org/abs/1610.02454},
	abstract = {Generative Adversarial Networks (GANs) have recently demonstrated the capability to synthesize compelling real-world images, such as room interiors, album covers, manga, faces, birds, and flowers. While existing models can synthesize images based on global constraints such as a class label or caption, they do not provide control over pose or object location. We propose a new model, the Generative Adversarial What-Where Network (GAWWN), that synthesizes images given instructions describing what content to draw in which location. We show high-quality 128 x 128 image synthesis on the Caltech-UCSD Birds dataset, conditioned on both informal text descriptions and also object location. Our system exposes control over both the bounding box around the bird and its constituent parts. By modeling the conditional distributions over part locations, our system also enables conditioning on arbitrary subsets of parts (e.g. only the beak and tail), yielding an efficient interface for picking part locations. We also show preliminary results on the more challenging domain of text- and location-controllable synthesis of images of human actions on the MPII Human Pose dataset.},
	author = {Reed, Scott and Akata, Zeynep and Mohan, Santosh and Tenka, Samuel and Schiele, Bernt and Lee, Honglak},
	month = oct,
	year = {2016},
}

@article{chao_empirical_2016,
	title = {An {Empirical} {Study} and {Analysis} of {Generalized} {Zero}-{Shot} {Learning} for {Object} {Recognition} in the {Wild}},
	url = {http://arxiv.org/abs/1605.04253},
	abstract = {Zero-shot learning (ZSL) methods have been studied in the unrealistic setting where test data are assumed to come from unseen classes only. In this paper, we advocate studying the problem of generalized zero-shot learning (GZSL) where the test data's class memberships are unconstrained. We show empirically that naively using the classifiers constructed by ZSL approaches does not perform well in the generalized setting. Motivated by this, we propose a simple but effective calibration method that can be used to balance two conflicting forces: recognizing data from seen classes versus those from unseen ones. We develop a performance metric to characterize such a trade-off and examine the utility of this metric in evaluating various ZSL approaches. Our analysis further shows that there is a large gap between the performance of existing approaches and an upper bound established via idealized semantic embeddings, suggesting that improving class semantic embeddings is vital to GZSL.},
	author = {Chao, Wei-Lun and Changpinyo, Soravit and Gong, Boqing and Sha, Fei},
	month = may,
	year = {2016},
}

@article{matula_cell_2015,
	title = {Cell {Tracking} {Accuracy} {Measurement} {Based} on {Comparison} of {Acyclic} {Oriented} {Graphs}},
	volume = {10},
	url = {http://dx.plos.org/10.1371/journal.pone.0144959},
	doi = {10.1371/journal.pone.0144959},
	number = {12},
	journal = {PLOS ONE},
	author = {Matula, Pavel and Maška, Martin and Sorokin, Dmitry V. and Matula, Petr and Ortiz-de-Solórzano, Carlos and Kozubek, Michal},
	editor = {Abraham, Thomas},
	month = dec,
	year = {2015},
	note = {Publisher: Public Library of Science},
	pages = {e0144959--e0144959},
}

@article{svoboda_mitogen_2017,
	title = {{MitoGen}: {A} {Framework} for {Generating} {3D} {Synthetic} {Time}-{Lapse} {Sequences} of {Cell} {Populations} in {Fluorescence} {Microscopy}},
	volume = {36},
	url = {http://ieeexplore.ieee.org/document/7562482/},
	doi = {10.1109/TMI.2016.2606545},
	number = {1},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Svoboda, David and Ulman, Vladimir},
	month = jan,
	year = {2017},
	pages = {310--321},
}

@article{zhu_unpaired_nodate,
	title = {Unpaired {Image}-to-{Image} {Translation} using {Cycle}-{Consistent} {Adversarial} {Networks} {Monet} {Photos}},
	url = {https://arxiv.org/pdf/1703.10593.pdf},
	abstract = {Monet photo photo Monet Figure 1: Given any two unordered image collections X and Y , our algorithm learns to automatically " translate " an image from one into the other and vice versa: (left) 1074 Monet paintings and 6753 landscape photos from Flickr; (center) 1177 ze-bras and 939 horses from ImageNet; (right) 1273 summer and 854 winter Yosemite photos from Flickr. Example application (bottom): using a collection of paintings of a famous artist, learn to render a user's photograph into their style. Abstract Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a train-ing set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistin-guishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we cou-ple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to push F (G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including col-lection style transfer, object transfiguration, season trans-fer, and photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	author = {Zhu, Jun-Yan and Park, Taesung},
}

@article{romano_raisr_2016,
	title = {{RAISR}: {Rapid} and {Accurate} {Image} {Super} {Resolution}},
	url = {http://arxiv.org/abs/1606.01299},
	abstract = {Given an image, we wish to produce an image of larger size with significantly more pixels and higher image quality. This is generally known as the Single Image Super-Resolution (SISR) problem. The idea is that with sufficient training data (corresponding pairs of low and high resolution images) we can learn set of filters (i.e. a mapping) that when applied to given image that is not in the training set, will produce a higher resolution version of it, where the learning is preferably low complexity. In our proposed approach, the run-time is more than one to two orders of magnitude faster than the best competing methods currently available, while producing results comparable or better than state-of-the-art. A closely related topic is image sharpening and contrast enhancement, i.e., improving the visual quality of a blurry image by amplifying the underlying details (a wide range of frequencies). Our approach additionally includes an extremely efficient way to produce an image that is significantly sharper than the input blurry one, without introducing artifacts such as halos and noise amplification. We illustrate how this effective sharpening algorithm, in addition to being of independent interest, can be used as a pre-processing step to induce the learning of more effective upscaling filters with built-in sharpening and contrast enhancement effect.},
	author = {Romano, Yaniv and Isidoro, John and Milanfar, Peyman},
	month = jun,
	year = {2016},
}

@techreport{noauthor_-datacenter_nodate,
	title = {In-{Datacenter} {Performance} {Analysis} of a {Tensor} {Processing} {Unit}​},
	url = {https://drive.google.com/file/d/0Bx4hafXDDq2EMzRNcy1vSUxtcEk/view},
	abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific
hardware. This paper evaluates a custom ASIC—called a ​Tensor Pro​cessing Unit (TPU)— deployed in datacenters
since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC
matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB)
software-managed on-chip memory. The TPU’s deterministic execution model is a better match to the 99th-percentile
response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs
(caches, out-of-order execution, multithreading, multiprocessing, prefetching, ...) that help average throughput more
than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big
memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an
Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level
TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our
datacenters’ NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -
30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X - 80X higher. Moreover, using the GPU’s
GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the
CPU.
Index terms–DNN, MLP, CNN, RNN, LSTM, neural network, domain-specific architecture, accelerator},
}

@article{zhu_unpaired_2017,
	title = {Unpaired {Image}-to-{Image} {Translation} using {Cycle}-{Consistent} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = mar,
	year = {2017},
}

@article{lee_pseudo-label_nodate-1,
	title = {Pseudo-{Label} : {The} {Simple} and {Efficient} {Semi}-{Supervised} {Learning} {Method} for {Deep} {Neural} {Networks}},
	url = {http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf},
	abstract = {We propose the simple and efficient method of semi-supervised learning for deep neural networks. Basically, the proposed network is trained in a supervised fashion with labeled and unlabeled data simultaneously. For un-labeled data, Pseudo-Label s, just picking up the class which has the maximum predicted probability, are used as if they were true la-bels. This is in effect equivalent to Entropy Regularization. It favors a low-density sepa-ration between classes, a commonly assumed prior for semi-supervised learning. With De-noising Auto-Encoder and Dropout, this sim-ple method outperforms conventional meth-ods for semi-supervised learning with very small labeled data on the MNIST handwrit-ten digit dataset.},
	author = {Lee, Dong-Hyun},
}

@article{mas_ka_benchmark_2014,
	title = {A benchmark for comparison of cell tracking algorithms},
	volume = {30},
	url = {https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/30/11/10.1093/bioinformatics/btu080/2/btu080.pdf?Expires=1491776021&Signature=g-SPo6OCpRbKmPhIrLAqnSKKvNemH6tL2Uc80l4OJK9cbyNVwJ6NSCUaFjJMrHxP3EhdAS9FAhZHyoVGWhzmw~-MTEvjyGvE72k43V~EuvOCLFF6g8g9HlBq-FLOMZZ1XHOZrQT0uDHJOedF5Kj7Ph5PV4n8fiNELdk0nysYS-DUe6i0Q-37-HA8oO-E6MakDC3EWt6vVnZEr0te6tgZR7qdoygZpIw3LS~1kWEX3dUMZNk-0h6T~j77NskionBBjHQOl6Yqk6B~dYl8nnJFKBj~aDtwc~63urqqCcb0q~XNE~p~cPMgQbeesHAzQ9on6D5fnWCpgaA2N9n5iod-VA__&Key-Pair-Id=APKAIUCZBIA4LVPAVW3Q},
	doi = {10.1093/bioinformatics/btu080},
	abstract = {Motivation: Automatic tracking of cells in multidimensional time-lapse fluorescence microscopy is an important task in many biomedical ap-plications. A novel framework for objective evaluation of cell tracking algorithms has been established under the auspices of the IEEE International Symposium on Biomedical Imaging 2013 Cell Tracking Challenge. In this article, we present the logistics, datasets, methods and results of the challenge and lay down the principles for future uses of this benchmark. Results: The main contributions of the challenge include the creation of a comprehensive video dataset repository and the definition of ob-jective measures for comparison and ranking of the algorithms. With this benchmark, six algorithms covering a variety of segmentation and tracking paradigms have been compared and ranked based on their performance on both synthetic and real datasets. Given the diversity of the datasets, we do not declare a single winner of the challenge. Instead, we present and discuss the results for each individual dataset separately. Availability and implementation: The challenge Web site (http:// www.codesolorzano.com/celltrackingchallenge) provides access to the training and competition datasets, along with the ground truth of the training videos. It also provides access to Windows and Linux executable files of the evaluation software and most of the algorithms that competed in the challenge.},
	number = {11},
	author = {Maš Ka, Martin and Ulman, Vladimír and Svoboda, David and Matula, Pavel and Matula, Petr and Ederra, Cristina and Urbiola, Ainhoa and Españ, Tomá S and Venkatesan, Subramanian and Balak, Deepak M W and Karas, Pavel and Bolcková, Tereza and Ta, Marké and Treitová, Š and Carthel, Craig and Coraluppi, Stefano and Harder, Nathalie and Rohr, Karl and Magnusson, Klas E G and Jaldé N, Joakim and Blau, Helen M and Dzyubachyk, Oleh and Kříž Ek, Pavel and Hagen, Guy M and Pastor-Escuredo, David and Jimenez-Carretero, Daniel and Ledesma-Carbayo, Maria J and Muñ Oz-Barrutia, Arrate and Meijering, Erik and Kozubek, Michal and Ortiz-De-Solorzano, Carlos},
	year = {2014},
	pages = {1609--1617},
}

@article{berthelot_began_2017-1,
	title = {{BEGAN}: {Boundary} {Equilibrium} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1703.10717},
	abstract = {We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.},
	author = {Berthelot, David and Schumm, Tom and Metz, Luke},
	month = mar,
	year = {2017},
}

@article{huang_arbitrary_2017,
	title = {Arbitrary {Style} {Transfer} in {Real}-time with {Adaptive} {Instance} {Normalization}},
	url = {http://arxiv.org/abs/1703.06868},
	abstract = {Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color \& spatial controls, all using a single feed-forward neural network.},
	author = {Huang, Xun and Belongie, Serge},
	month = mar,
	year = {2017},
}

@article{gulrajani_improved_2017,
	title = {Improved {Training} of {Wasserstein} {GANs}},
	url = {http://arxiv.org/abs/1704.00028},
	abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes significant progress toward stable training of GANs, but can still generate low-quality samples or fail to converge in some settings. We find that these training failures are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to pathological behavior. We propose an alternative method for enforcing the Lipschitz constraint: instead of clipping weights, penalize the norm of the gradient of the critic with respect to its input. Our proposed method converges faster and generates higher-quality samples than WGAN with weight clipping. Finally, our method enables very stable GAN training: for the first time, we can train a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data.},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
	month = mar,
	year = {2017},
}

@article{chen_deep_2013,
	title = {Deep {Learning} {Shape} {Priors} for {Object} {Segmentation}},
	issn = {978-0-7695-4989-7},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619088},
	doi = {10.1109/CVPR.2013.244},
	abstract = {In this paper we introduce a new shape-driven approach for object segmentation. Given a training set of shapes, we first use deep Boltzmann machine to learn the hierarchical architecture of shape priors. This learned hierarchical architecture is then used to model shape variations of global and local structures in an energetic form. Finally, it is applied to data-driven variational methods to perform object extraction of corrupted data based on shape probabilistic representation. Experiments demonstrate that our model can be applied to dataset of arbitrary prior shapes, and can cope with image noise and clutter, as well as partial occlusions.},
	journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition CVPR2013},
	author = {Chen, Fei and Yu, Huimin and Hu, Roland and Xunxun, Zeng},
	year = {2013},
	pages = {1870--1877},
}

@article{xu_deep_2017,
	title = {Deep {Image} {Matting}},
	url = {http://arxiv.org/abs/1703.03872},
	abstract = {Image matting is a fundamental computer vision problem and has many applications. Previous algorithms have poor performance when an image has similar foreground and background colors or complicated textures. The main reasons are prior methods 1) only use low-level features and 2) lack high-level context. In this paper, we propose a novel deep learning based algorithm that can tackle both these problems. Our deep model has two parts. The first part is a deep convolutional encoder-decoder network that takes an image and the corresponding trimap as inputs and predict the alpha matte of the image. The second part is a small convolutional network that refines the alpha matte predictions of the first network to have more accurate alpha values and sharper edges. In addition, we also create a large-scale image matting dataset including 49300 training images and 1000 testing images. We evaluate our algorithm on the image matting benchmark, our testing set, and a wide variety of real images. Experimental results clearly demonstrate the superiority of our algorithm over previous methods.},
	author = {Xu, Ning and Price, Brian and Cohen, Scott and Huang, Thomas},
	month = mar,
	year = {2017},
}

@article{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
}

@article{huang_speedaccuracy_2016-1,
	title = {Speed/accuracy trade-offs for modern convolutional object detectors},
	url = {http://arxiv.org/abs/1611.10012},
	abstract = {In this paper, we study the trade-off between accuracy and speed when building an object detection system based on convolutional neural networks. We consider three main families of detectors --- Faster R-CNN, R-FCN and SSD --- which we view as "meta-architectures". Each of these can be combined with different kinds of feature extractors, such as VGG, Inception or ResNet. In addition, we can vary other parameters, such as the image resolution, and the number of box proposals. We develop a unified framework (in Tensorflow) that enables us to perform a fair comparison between all of these variants. We analyze the performance of many different previously published model combinations, as well as some novel ones, and thus identify a set of models which achieve different points on the speed-accuracy tradeoff curve, ranging from fast models, suitable for use on a mobile phone, to a much slower model that achieves a new state of the art on the COCO detection challenge.},
	author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin},
	month = nov,
	year = {2016},
}

@article{salimans_evolution_2017,
	title = {Evolution {Strategies} as a {Scalable} {Alternative} to {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1703.03864},
	abstract = {We explore the use of Evolution Strategies, a class of black box optimization algorithms, as an alternative to popular RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using hundreds to thousands of parallel workers, ES can solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training time. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
	author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sutskever, Ilya},
	month = mar,
	year = {2017},
}

@article{chen_deep_2017,
	title = {Deep {Learning} {Features} at {Scale} for {Visual} {Place} {Recognition}},
	url = {http://arxiv.org/abs/1701.05105},
	abstract = {The success of deep learning techniques in the computer vision domain has triggered a range of initial investigations into their utility for visual place recognition, all using generic features from networks that were trained for other types of recognition tasks. In this paper, we train, at large scale, two CNN architectures for the specific place recognition task and employ a multi-scale feature encoding method to generate condition- and viewpoint-invariant features. To enable this training to occur, we have developed a massive Specific PlacEs Dataset (SPED) with hundreds of examples of place appearance change at thousands of different places, as opposed to the semantic place type datasets currently available. This new dataset enables us to set up a training regime that interprets place recognition as a classification problem. We comprehensively evaluate our trained networks on several challenging benchmark place recognition datasets and demonstrate that they achieve an average 10\% increase in performance over other place recognition algorithms and pre-trained CNNs. By analyzing the network responses and their differences from pre-trained networks, we provide insights into what a network learns when training for place recognition, and what these results signify for future research in this area.},
	author = {Chen, Zetao and Jacobson, Adam and Sunderhauf, Niko and Upcroft, Ben and Liu, Lingqiao and Shen, Chunhua and Reid, Ian and Milford, Michael},
	month = jan,
	year = {2017},
}

@article{ruder_artistic_2016,
	title = {Artistic style transfer for videos},
	url = {http://arxiv.org/abs/1604.08610},
	doi = {10.1007/978-3-319-45886-1_3},
	abstract = {In the past, manually re-drawing an image in a certain artistic style required a professional artist and a long time. Doing this for a video sequence single-handed was beyond imagination. Nowadays computers provide new possibilities. We present an approach that transfers the style from one image (for example, a painting) to a whole video sequence. We make use of recent advances in style transfer in still images and propose new initializations and loss functions applicable to videos. This allows us to generate consistent and stable stylized video sequences, even in cases with large motion and strong occlusion. We show that the proposed method clearly outperforms simpler baselines both qualitatively and quantitatively.},
	author = {Ruder, Manuel and Dosovitskiy, Alexey and Brox, Thomas},
	month = apr,
	year = {2016},
}

@article{vondrick_generating_2016,
	title = {Generating {Videos} with {Scene} {Dynamics}},
	url = {http://arxiv.org/abs/1609.02612},
	abstract = {We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.},
	author = {Vondrick, Carl and Pirsiavash, Hamed and Torralba, Antonio},
	month = sep,
	year = {2016},
}

@article{kang_object_2017,
	title = {Object {Detection} in {Videos} with {Tubelet} {Proposal} {Networks}},
	url = {http://arxiv.org/abs/1702.06355},
	abstract = {Object detection in videos has drawn increasing attention recently with the introduction of the large-scale ImageNet VID dataset. Different from object detection in static images, temporal information in videos provides vital information for object detection. To fully utilize temporal information, state-of-the-art methods are therefore based on spatiotemporal tubelets, which are essentially sequences of associated bounding boxes across time. However, the existing methods have major limitations in generating tubelets in terms of quality and efficiency. Motion-based methods are able to obtain dense tubelets, but the lengths are generally only several frames, which is not optimal to incorporate long-term temporal information. Appearance-based methods, usually involving generic object tracking, could generate long tubelets, but are usually computational expensive. In this work, we propose a framework for object detection in videos, which consists of a novel tubelet proposal network to efficiently generate spatiotemporal proposals, and a Long Short-term Memory (LSTM) network that incorporates temporal information from tubelet proposals for achieving high object detection accuracy in videos. The experiments on the large-scale ImageNet VID dataset demonstrate the effectiveness of the proposed framework for object detection in videos.},
	author = {Kang, Kai and Li, Hongsheng and Xiao, Tong and Ouyang, Wanli and Yan, Junjie and Liu, Xihui and Wang, Xiaogang},
	month = feb,
	year = {2017},
}

@article{he_mask_2017,
	title = {Mask {R}-{CNN}},
	url = {https://arxiv.org/pdf/1703.06870.pdf},
	abstract = {We present a conceptually simple, flexible, and general
framework for object instance segmentation. Our approach
efficiently detects objects in an image while simultaneously
generating a high-quality segmentation mask for each instance.
The method, called Mask R-CNN, extends Faster
R-CNN by adding a branch for predicting an object mask in
parallel with the existing branch for bounding box recognition.
Mask R-CNN is simple to train and adds only a small
overhead to Faster R-CNN, running at 5 fps. Moreover,
Mask R-CNN is easy to generalize to other tasks, e.g., allowing
us to estimate human poses in the same framework.
We show top results in all three tracks of the COCO suite of
challenges, including instance segmentation, bounding-box
object detection, and person keypoint detection. Without
tricks, Mask R-CNN outperforms all existing, single-model
entries on every task, including the COCO 2016 challenge
winners. We hope our simple and effective approach will
serve as a solid baseline and help ease future research in
instance-level recognition. Code will be made available.},
	author = {He, Kaiming and Gkioxari, Georgia and {Dollar Piotr} and Girshick, Ross},
	year = {2017},
}

@article{ahsan_complex_2017,
	title = {Complex {Event} {Recognition} from {Images} with {Few} {Training} {Examples}},
	url = {http://arxiv.org/abs/1701.04769},
	abstract = {We propose to leverage concept-level representations for complex event recognition in photographs given limited training examples. We introduce a novel framework to discover event concept attributes from the web and use that to extract semantic features from images and classify them into social event categories with few training examples. Discovered concepts include a variety of objects, scenes, actions and event sub-types, leading to a discriminative and compact representation for event images. Web images are obtained for each discovered event concept and we use (pretrained) CNN features to train concept classifiers. Extensive experiments on challenging event datasets demonstrate that our proposed method outperforms several baselines using deep CNN features directly in classifying images into events with limited training examples. We also demonstrate that our method achieves the best overall accuracy on a dataset with unseen event categories using a single training example.},
	author = {Ahsan, Unaiza and Sun, Chen and Hays, James and Essa, Irfan},
	month = jan,
	year = {2017},
}

@article{ma_person_2016,
	title = {Person {Re}-{Identification} by {Unsupervised} {Video} {Matching}},
	url = {http://arxiv.org/abs/1611.08512},
	abstract = {Most existing person re-identification (ReID) methods rely only on the spatial appearance information from either one or multiple person images, whilst ignore the space-time cues readily available in video or image-sequence data. Moreover, they often assume the availability of exhaustively labelled cross-view pairwise data for every camera pair, making them non-scalable to ReID applications in real-world large scale camera networks. In this work, we introduce a novel video based person ReID method capable of accurately matching people across views from arbitrary unaligned image-sequences without any labelled pairwise data. Specifically, we introduce a new space-time person representation by encoding multiple granularities of spatio-temporal dynamics in form of time series. Moreover, a Time Shift Dynamic Time Warping (TS-DTW) model is derived for performing automatically alignment whilst achieving data selection and matching between inherently inaccurate and incomplete sequences in a unified way. We further extend the TS-DTW model for accommodating multiple feature-sequences of an image-sequence in order to fuse information from different descriptions. Crucially, this model does not require pairwise labelled training data (i.e. unsupervised) therefore readily scalable to large scale camera networks of arbitrary camera pairs without the need for exhaustive data annotation for every camera pair. We show the effectiveness and advantages of the proposed method by extensive comparisons with related state-of-the-art approaches using two benchmarking ReID datasets, PRID2011 and iLIDS-VID.},
	author = {Ma, Xiaolong and Zhu, Xiatian and Gong, Shaogang and Xie, Xudong and Hu, Jianming and Lam, Kin-Man and Zhong, Yisheng},
	month = nov,
	year = {2016},
}

@article{ardeshir_egoreid_2016,
	title = {{EgoReID}: {Cross}-view {Self}-{Identification} and {Human} {Re}-identification in {Egocentric} and {Surveillance} {Videos}},
	url = {http://arxiv.org/abs/1612.08153},
	abstract = {Human identification remains to be one of the challenging tasks in computer vision community due to drastic changes in visual features across different viewpoints, lighting conditions, occlusion, etc. Most of the literature has been focused on exploring human re-identification across viewpoints that are not too drastically different in nature. Cameras usually capture oblique or side views of humans, leaving room for a lot of geometric and visual reasoning. Given the recent popularity of egocentric and top-view vision, re-identification across these two drastically different views can now be explored. Having an egocentric and a top view video, our goal is to identify the cameraman in the content of the top-view video, and also re-identify the people visible in the egocentric video, by matching them to the identities present in the top-view video. We propose a CRF-based method to address the two problems. Our experimental results demonstrates the efficiency of the proposed approach over a variety of video recorded from two views.},
	author = {Ardeshir, Shervin and Sharma, Sandesh and Broji, Ali},
	month = dec,
	year = {2016},
}

@article{xu_csvideonet_2016,
	title = {{CSVideoNet}: {A} {Real}-time {End}-to-end {Learning} {Framework} for {High}-frame-rate {Video} {Compressive} {Sensing}},
	url = {http://arxiv.org/abs/1612.05203},
	abstract = {This paper addresses the real-time encoding-decoding problem for high-frame-rate video compressive sensing (CS). Unlike prior works that perform reconstruction using iterative optimization-based approaches, we propose a non-iterative model, named "CSVideoNet". CSVideoNet directly learns the inverse mapping of CS and reconstructs the original input in a single forward propagation. To overcome the limitations of the existing CS cameras, we propose a multi-rate CNN and a synthesizing RNN to improve compression ratio and spatial-temporal resolution of the reconstructed videos. The experimental results demonstrate that CSVideoNet significantly outperforms the state-of-the-art approaches. With no pre/post-processing, we achieve 25dB PSNR recovery qulity at 100x compression ratio(CR), with a frame rate of 125 fps. Due to the feedforward and high-data-concurrency natures of CSVideoNet, it can take advantage of GPU acceleration to achieve three orders of magnitude speed-up over conventional iterative-based approaches. We share source code at https://github.com/PSCLab-ASU/CSVideoNet.},
	author = {Xu, Kai and Ren, Fengbo},
	month = dec,
	year = {2016},
}

@article{canziani_analysis_2016,
	title = {An {Analysis} of {Deep} {Neural} {Network} {Models} for {Practical} {Applications}},
	url = {http://arxiv.org/abs/1605.07678},
	author = {Canziani, Alfredo and Paszke, Adam and Culurciello, Eugenio},
	year = {2016},
}

@article{reed_training_2014,
	title = {Training {Deep} {Neural} {Networks} on {Noisy} {Labels} with {Bootstrapping}},
	url = {http://arxiv.org/abs/1412.6596},
	author = {Reed, Scott and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
	year = {2014},
}

@article{mishkin_systematic_2016,
	title = {Systematic evaluation of {CNN} advances on the {ImageNet}},
	url = {http://arxiv.org/abs/1606.02228},
	author = {Mishkin, Dmytro and Sergievskiy, Nikolay and Matas, Jiri},
	year = {2016},
}

@article{britz_massive_2017,
	title = {Massive {Exploration} of {Neural} {Machine} {Translation} {Architectures}},
	url = {http://arxiv.org/abs/1703.03906},
	abstract = {Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically requiring days to weeks of GPU time to converge. This makes exhaustive hyperparameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analysis of NMT architecture hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architectures. As part of this contribution, we release an open-source NMT framework that enables researchers to easily experiment with novel techniques and reproduce state of the art results.},
	author = {Britz, Denny and Goldie, Anna and Luong, Minh-Thang and Le, Quoc},
	month = mar,
	year = {2017},
}

@article{dumoulin_learned_2016,
	title = {A {Learned} {Representation} {For} {Artistic} {Style}},
	url = {http://arxiv.org/abs/1610.07629},
	abstract = {The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style.},
	author = {Dumoulin, Vincent and Shlens, Jonathon and Kudlur, Manjunath},
	month = oct,
	year = {2016},
}

@article{luan_deep_2017,
	title = {Deep {Photo} {Style} {Transfer}},
	url = {http://arxiv.org/abs/1703.07511},
	abstract = {This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom CNN layer through which we can backpropagate. We show that this approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.},
	author = {Luan, Fujun and Paris, Sylvain and Shechtman, Eli and Bala, Kavita},
	month = mar,
	year = {2017},
}

@article{smith_deep_2016,
	title = {Deep {Convolutional} {Neural} {Network} {Design} {Patterns}},
	url = {http://arxiv.org/abs/1611.00847},
	author = {Smith, Leslie N. and Topin, Nicholay},
	year = {2016},
}

@article{milletari_v-net_2016-1,
	title = {V-{Net}: {Fully} {Convolutional} {Neural} {Networks} for {Volumetric} {Medical} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1606.04797},
	author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	year = {2016},
}

@article{milletari_v-net_2016-2,
	title = {V-{Net}: {Fully} {Convolutional} {Neural} {Networks} for {Volumetric} {Medical} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1606.04797},
	author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	year = {2016},
}

@article{reed_training_2014-1,
	title = {Training {Deep} {Neural} {Networks} on {Noisy} {Labels} with {Bootstrapping}},
	url = {http://arxiv.org/abs/1412.6596},
	abstract = {Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overfitting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the- art results, and can also benefit from unlabeled face images with no modification to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection.},
	author = {Reed, Scott and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
	month = dec,
	year = {2014},
}

@article{miao_real-time_nodate,
	title = {{REAL}-{TIME} {2D}/{3D} {REGISTRATION} {VIA} {CNN} {REGRESSION}},
	url = {https://arxiv.org/pdf/1507.07505.pdf},
	abstract = {In this paper, we present a Convolutional Neural Network (CNN) regression approach for real-time 2-D/3-D registra-tion. Different from optimization-based methods, which iter-atively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registra-tion, the proposed method exploits the information embedded in the appearances of the Digitally Reconstructed Radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. The CNN regressors are trained for local zones and applied in a hierarchical man-ner to break down the complex regression task into simpler sub-tasks that can be learned separately. Our experiment re-sults demonstrate the advantage of the proposed method in computational efficiency with negligible degradation of reg-istration accuracy compared to intensity-based methods.},
	author = {Miao, Shun and Wang, Jane and Zheng, Yefeng and Liao, Rui},
	keywords = {Deep Learning, Convolutional Neural Network, Image Guided In-tervention, Index Terms— 2-D/3-D Registration},
}

@article{wu_scalable_2016,
	title = {Scalable {High}-{Performance} {Image} {Registration} {Framework} by {Unsupervised} {Deep} {Feature} {Representations} {Learning}},
	volume = {63},
	url = {http://ieeexplore.ieee.org/document/7314894/},
	doi = {10.1109/TBME.2015.2496253},
	number = {7},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Wu, Guorong and Kim, Minjeong and Wang, Qian and Munsell, Brent C. and Shen, Dinggang},
	month = jul,
	year = {2016},
	pages = {1505--1516},
}

@article{lin_microsoft_2014,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	url = {http://arxiv.org/abs/1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = may,
	year = {2014},
}

@article{richter_playing_2016,
	title = {Playing for {Data}: {Ground} {Truth} from {Computer} {Games}},
	url = {http://arxiv.org/abs/1608.02192},
	abstract = {Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game. Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data: models trained with game data and just 1/3 of the CamVid training set outperform models trained on the complete CamVid training set.},
	author = {Richter, Stephan R. and Vineet, Vibhav and Roth, Stefan and Koltun, Vladlen},
	month = aug,
	year = {2016},
}

@article{nguyen_plug_2016,
	title = {Plug \& {Play} {Generative} {Networks}: {Conditional} {Iterative} {Generation} of {Images} in {Latent} {Space}},
	url = {http://arxiv.org/abs/1612.00005},
	abstract = {Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. (2016) showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227x227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models "Plug and Play Generative Networks". PPGNs are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replaceable "condition" network C that tells the generator what to draw. We demonstrate the generation of images conditioned on a class (when C is an ImageNet or MIT Places classification network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visualization, which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.},
	author = {Nguyen, Anh and Yosinski, Jason and Bengio, Yoshua and Dosovitskiy, Alexey and Clune, Jeff},
	month = nov,
	year = {2016},
}

@article{yosinski_understanding_2015,
	title = {Understanding {Neural} {Networks} {Through} {Deep} {Visualization}},
	url = {https://arxiv.org/pdf/1506.06579.pdf},
	abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), in-cluding notable successes in training convolu-tional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what compu-tations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of bet-ter tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations pro-duced on each layer of a trained convnet as it processes an image or video (e.g. a live web-cam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how con-vnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regular-ization methods that combine to produce qualita-tively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
	author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
	year = {2015},
}

@article{litjens_survey_nodate,
	title = {A {Survey} on {Deep} {Learning} in {Medical} {Image} {Analysis}},
	url = {https://arxiv.org/pdf/1702.05747.pdf},
	abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks and provide concise overviews of studies per application area. Open challenges and directions for future research are discussed.},
	author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Arindra, Arnaud and Setio, Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and Van Der Laak, Jeroen A W M and Van Ginneken, Bram and Sánchez, Clara I},
	keywords = {medical imaging, deep learning, convolutional neural networks, survey},
}

@article{wu_unsupervised_2013,
	title = {Unsupervised deep feature learning for deformable registration of {MR} brain images.},
	volume = {16},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/24579196},
	abstract = {Establishing accurate anatomical correspondences is critical for medical image registration. Although many hand-engineered features have been proposed for correspondence detection in various registration applications, no features are general enough to work well for all image data. Although many learning-based methods have been developed to help selection of best features for guiding correspondence detection across subjects with large anatomical variations, they are often limited by requiring the known correspondences (often presumably estimated by certain registration methods) as the ground truth for training. To address this limitation, we propose using an unsupervised deep learning approach to directly learn the basis filters that can effectively represent all observed image patches. Then, the coefficients by these learnt basis filters in representing the particular image patch can be regarded as the morphological signature for correspondence detection during image registration. Specifically, a stacked two-layer convolutional network is constructed to seek for the hierarchical representations for each image patch, where the high-level features are inferred from the responses of the low-level network. By replacing the hand-engineered features with our learnt data-adaptive features for image registration, we achieve promising registration results, which demonstrates that a general approach can be built to improve image registration by using data-adaptive features through unsupervised deep learning.},
	number = {Pt 2},
	journal = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
	author = {Wu, Guorong and Kim, Minjeong and Wang, Qian and Gao, Yaozong and Liao, Shu and Shen, Dinggang},
	year = {2013},
	note = {Publisher: NIH Public Access},
	pages = {649--56},
}

@article{mustikovela_can_2016,
	title = {Can {Ground} {Truth} {Label} {Propagation} from {Video} help {Semantic} {Segmentation}?},
	url = {http://arxiv.org/abs/1610.00731},
	abstract = {For state-of-the-art semantic segmentation task, training convolutional neural networks (CNNs) requires dense pixelwise ground truth (GT) labeling, which is expensive and involves extensive human effort. In this work, we study the possibility of using auxiliary ground truth, so-called {\textbackslash}textit\{pseudo ground truth\} (PGT) to improve the performance. The PGT is obtained by propagating the labels of a GT frame to its subsequent frames in the video using a simple CRF-based, cue integration framework. Our main contribution is to demonstrate the use of noisy PGT along with GT to improve the performance of a CNN. We perform a systematic analysis to find the right kind of PGT that needs to be added along with the GT for training a CNN. In this regard, we explore three aspects of PGT which influence the learning of a CNN: i) the PGT labeling has to be of good quality; ii) the PGT images have to be different compared to the GT images; iii) the PGT has to be trusted differently than GT. We conclude that PGT which is diverse from GT images and has good quality of labeling can indeed help improve the performance of a CNN. Also, when PGT is multiple folds larger than GT, weighing down the trust on PGT helps in improving the accuracy. Finally, We show that using PGT along with GT, the performance of Fully Convolutional Network (FCN) on Camvid data is increased by \$2.7{\textbackslash}\%\$ on IoU accuracy. We believe such an approach can be used to train CNNs for semantic video segmentation where sequentially labeled image frames are needed. To this end, we provide recommendations for using PGT strategically for semantic segmentation and hence bypass the need for extensive human efforts in labeling.},
	author = {Mustikovela, Siva Karthik and Yang, Michael Ying and Rother, Carsten},
	month = oct,
	year = {2016},
}

@article{jegou_one_2016,
	title = {The {One} {Hundred} {Layers} {Tiramisu}: {Fully} {Convolutional} {DenseNets} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1611.09326},
	abstract = {State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets. Code to reproduce the experiments is available here : https://github.com/SimJeg/FC-DenseNet/blob/master/train.py},
	author = {Jégou, Simon and Drozdzal, Michal and Vazquez, David and Romero, Adriana and Bengio, Yoshua},
	month = nov,
	year = {2016},
}

@inproceedings{ye_shape-based_2009,
	title = {Shape-based ct lung nodule segmentation using five-dimensional mean shift clustering and {MEM} with shape information},
	isbn = {978-1-4244-3931-7},
	url = {http://ieeexplore.ieee.org/document/5193089/},
	doi = {10.1109/ISBI.2009.5193089},
	publisher = {IEEE},
	author = {Ye, Xujiong and Siddique, Musib and Douiri, Abdel and Beddoe, Gareth and Slabaugh, Greg},
	month = jun,
	year = {2009},
	pages = {482--485},
}

@article{james_visualizing_2015,
	title = {Visualizing {Interstellar}'s {Wormhole}},
	url = {http://arxiv.org/abs/1502.03809},
	doi = {10.1119/1.4916949},
	abstract = {Christopher Nolan's science fiction movie Interstellar offers a variety of opportunities for students in elementary courses on general relativity theory. This paper describes such opportunities, including: (i) At the motivational level, the manner in which elementary relativity concepts underlie the wormhole visualizations seen in the movie. (ii) At the briefest computational level, instructive calculations with simple but intriguing wormhole metrics, including, e.g., constructing embedding diagrams for the three-parameter wormhole that was used by our visual effects team and Christopher Nolan in scoping out possible wormhole geometries for the movie. (iii) Combining the proper reference frame of a camera with solutions of the geodesic equation, to construct a light-ray-tracing map backward in time from a camera's local sky to a wormhole's two celestial spheres. (iv) Implementing this map, for example in Mathematica, Maple or Matlab, and using that implementation to construct images of what a camera sees when near or inside a wormhole. (v) With the student's implementation, exploring how the wormhole's three parameters influence what the camera sees---which is precisely how Christopher Nolan, using our implementation, chose the parameters for {\textbackslash}emph\{Interstellar\}'s wormhole. (vi) Using the student's implementation, exploring the wormhole's Einstein ring, and particularly the peculiar motions of star images near the ring; and exploring what it looks like to travel through a wormhole.},
	author = {James, Oliver and von Tunzelmann, Eugenie and Franklin, Paul and Thorne, Kip S.},
	month = feb,
	year = {2015},
}

@article{james_gravitational_2015,
	title = {Gravitational {Lensing} by {Spinning} {Black} {Holes} in {Astrophysics}, and in the {Movie} {Interstellar}},
	url = {http://arxiv.org/abs/1502.03808},
	doi = {10.1088/0264-9381/32/6/065001},
	abstract = {Interstellar is the first Hollywood movie to attempt depicting a black hole as it would actually be seen by somebody nearby. For this we developed a code called DNGR (Double Negative Gravitational Renderer) to solve the equations for ray-bundle (light-beam) propagation through the curved spacetime of a spinning (Kerr) black hole, and to render IMAX-quality, rapidly changing images. Our ray-bundle techniques were crucial for achieving IMAX-quality smoothness without flickering. This paper has four purposes: (i) To describe DNGR for physicists and CGI practitioners . (ii) To present the equations we use, when the camera is in arbitrary motion at an arbitrary location near a Kerr black hole, for mapping light sources to camera images via elliptical ray bundles. (iii) To describe new insights, from DNGR, into gravitational lensing when the camera is near the spinning black hole, rather than far away as in almost all prior studies. (iv) To describe how the images of the black hole Gargantua and its accretion disk, in the movie {\textbackslash}emph\{Interstellar\}, were generated with DNGR. There are no new astrophysical insights in this accretion-disk section of the paper, but disk novices may find it pedagogically interesting, and movie buffs may find its discussions of Interstellar interesting.},
	author = {James, Oliver and von Tunzelmann, Eugenie and Franklin, Paul and Thorne, Kip S.},
	month = feb,
	year = {2015},
}

@article{sabour_dynamic_nodate,
	title = {Dynamic {Routing} {Between} {Capsules}},
	abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation paramters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E and Toronto, Google Brain},
}

@article{noauthor_matrix_nodate,
	title = {{MATRIX} {CAPSULES} {WITH} {EM} {ROUTING}},
	abstract = {A capsule is a group of neurons whose outputs represent different properties of the same entity. We describe a version of capsules in which each capsule has a logis-tic unit to represent the presence of an entity and a 4x4 pose matrix which could learn to represent the relationship between that entity and the viewer. A capsule in one layer votes for the pose matrix of many different capsules in the layer above by multiplying its own pose matrix by viewpoint-invariant transformation matri-ces that could learn to represent part-whole relationships. Each of these votes is weighted by an assignment coefficient. These coefficients are iteratively updated using the EM algorithm such that the output of each capsule is routed to a cap-sule in the layer above that receives a cluster of similar votes. The whole system is trained discriminatively by unrolling 3 iterations of EM between each pair of adjacent layers. On the smallNORB benchmark, capsules reduce the number of test errors by 45\% compared to the state-of-the-art. Capsules also show far more resistance to white box adversarial attack than our baseline convolutional neural network.},
}

@article{niklaus_video_2017,
	title = {Video {Frame} {Interpolation} via {Adaptive} {Separable} {Convolution}},
	url = {http://arxiv.org/abs/1708.01692},
	abstract = {Standard video frame interpolation methods first estimate optical flow between input frames and then synthesize an intermediate frame guided by motion. Recent approaches merge these two steps into a single convolution process by convolving input frames with spatially adaptive kernels that account for motion and re-sampling simultaneously. These methods require large kernels to handle large motion, which limits the number of pixels whose kernels can be estimated at once due to the large memory demand. To address this problem, this paper formulates frame interpolation as local separable convolution over input frames using pairs of 1D kernels. Compared to regular 2D kernels, the 1D kernels require significantly fewer parameters to be estimated. Our method develops a deep fully convolutional neural network that takes two input frames and estimates pairs of 1D kernels for all pixels simultaneously. Since our method is able to estimate kernels and synthesizes the whole video frame at once, it allows for the incorporation of perceptual loss to train the neural network to produce visually pleasing frames. This deep neural network is trained end-to-end using widely available video data without any human annotation. Both qualitative and quantitative experiments show that our method provides a practical solution to high-quality video frame interpolation.},
	author = {Niklaus, Simon and Mai, Long and Liu, Feng},
	year = {2017},
}

@article{riegler_octnetfusion_2017,
	title = {{OctNetFusion}: {Learning} {Depth} {Fusion} from {Data}},
	abstract = {In this paper, we present a learning based approach to depth fusion, i.e., dense 3D reconstruction from multiple depth images. The most common approach to depth fusion is based on averaging truncated signed distance functions, which was originally proposed by Curless and Levoy in 1996. While this method is simple and provides great results, it is not able to reconstruct (partially) occluded surfaces and requires a large number frames to filter out sensor noise and outliers. Motivated by the availability of large 3D model repositories and recent advances in deep learning, we present a novel 3D CNN architecture that learns to predict an implicit surface representation from the input depth maps. Our learning based method significantly outperforms the traditional volumetric fusion approach in terms of noise reduction and outlier suppression. By learning the structure of real world 3D objects and scenes, our approach is further able to reconstruct occluded regions and to fill in gaps in the reconstruction. We demonstrate that our learning based approach outperforms both vanilla TSDF fusion as well as TV-L1 fusion on the task of volumetric fusion. Further, we demonstrate state-of-the-art 3D shape completion results.},
	author = {Riegler, Gernot and Ulusoy, Ali Osman and Bischof, Horst and Geiger, Andreas},
	year = {2017},
}

@article{bojanowski_optimizing_2017,
	title = {Optimizing the {Latent} {Space} of {Generative} {Networks}},
	abstract = {Generative Adversarial Networks (GANs) have been shown to be able to sample impressively realistic images. GAN training consists of a saddle point optimization problem that can be thought of as an adversarial game between a generator which produces the images, and a discriminator, which judges if the images are real. Both the generator and the discriminator are commonly parametrized as deep convolutional neural networks. The goal of this paper is to disentangle the contribution of the optimization procedure and the network parametrization to the success of GANs. To this end we introduce and study Generative Latent Optimization (GLO), a framework to train a generator without the need to learn a discriminator, thus avoiding challenging adversarial optimization problems. We show experimentally that GLO enjoys many of the desirable properties of GANs: learning from large data, synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors.},
	author = {Bojanowski, Piotr and Joulin, Armand and Lopez-Paz, David and Szlam, Arthur},
	year = {2017},
}

@article{bojanowski_optimizing_2017-1,
	title = {Optimizing the {Latent} {Space} of {Generative} {Networks}},
	url = {http://arxiv.org/abs/1707.05776},
	abstract = {Generative Adversarial Networks (GANs) have been shown to be able to sample impressively realistic images. GAN training consists of a saddle point optimization problem that can be thought of as an adversarial game between a generator which produces the images, and a discriminator, which judges if the images are real. Both the generator and the discriminator are commonly parametrized as deep convolutional neural networks. The goal of this paper is to disentangle the contribution of the optimization procedure and the network parametrization to the success of GANs. To this end we introduce and study Generative Latent Optimization (GLO), a framework to train a generator without the need to learn a discriminator, thus avoiding challenging adversarial optimization problems. We show experimentally that GLO enjoys many of the desirable properties of GANs: learning from large data, synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors.},
	author = {Bojanowski, Piotr and Joulin, Armand and Lopez-Paz, David and Szlam, Arthur},
	year = {2017},
}

@article{foerster_learning_2017,
	title = {Learning with {Opponent}-{Learning} {Awareness}},
	url = {http://arxiv.org/abs/1709.04326},
	abstract = {Multi-agent settings are quickly gathering importance in machine learning. Beyond a plethora of recent work on deep multi-agent reinforcement learning, hierarchical reinforcement learning, generative adversarial networks and decentralized optimization can all be seen as instances of this setting. However, the presence of multiple learning agents in these settings renders the training problem non-stationary and often leads to unstable training or undesired final results. We present Learning with Opponent-Learning Awareness (LOLA), a method that reasons about the anticipated learning of the other agents. The LOLA learning rule includes an additional term that accounts for the impact of the agent's policy on the anticipated parameter update of the other agents. We show that the LOLA update rule can be efficiently calculated using an extension of the likelihood ratio policy gradient update, making the method suitable for model-free reinforcement learning. This method thus scales to large parameter and input spaces and nonlinear function approximators. Preliminary results show that the encounter of two LOLA agents leads to the emergence of tit-for-tat and therefore cooperation in the infinitely iterated prisoners' dilemma, while independent learning does not. In this domain, LOLA also receives higher payouts compared to a naive learner, and is robust against exploitation by higher order gradient-based methods. Applied to infinitely repeated matching pennies, only LOLA agents converge to the Nash equilibrium. We also apply LOLA to a grid world task with an embedded social dilemma using deep recurrent policies. Again, by considering the learning of the other agent, LOLA agents learn to cooperate out of selfish interests.},
	author = {Foerster, Jakob N. and Chen, Richard Y. and Al-Shedivat, Maruan and Whiteson, Shimon and Abbeel, Pieter and Mordatch, Igor},
	month = sep,
	year = {2017},
}

@article{olivecrona_molecular_2017,
	title = {Molecular {De} {Novo} {Design} through {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1704.07555},
	abstract = {This work introduces a method to tune a sequence-based generative model for molecular de novo design that through augmented episodic likelihood can learn to generate structures with certain specified desirable properties. We demonstrate how this model can execute a range of tasks such as generating analogues to a query structure and generating compounds predicted to be active against a biological target. As a proof of principle, the model is first trained to generate molecules that do not contain sulphur. As a second example, the model is trained to generate analogues to the drug Celecoxib, a technique that could be used for scaffold hopping or library expansion starting from a single molecule. Finally, when tuning the model towards generating compounds predicted to be active against the dopamine receptor type 2, the model generates structures of which more than 95\% are predicted to be active, including experimentally confirmed actives that have not been included in either the generative model nor the activity prediction model.},
	author = {Olivecrona, Marcus and Blaschke, Thomas and Engkvist, Ola and Chen, Hongming},
	month = apr,
	year = {2017},
	file = {Full Text:/home/zwerg/Zotero/storage/9KX8VQ2D/Olivecrona et al. - 2017 - Molecular De Novo Design through Deep Reinforcemen.pdf:application/pdf},
}

@article{kusner_grammar_2017,
	title = {Grammar {Variational} {Autoencoder}},
	url = {http://arxiv.org/abs/1703.01925},
	abstract = {Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as video and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which encodes and decodes directly to and from these parse trees, ensuring the generated outputs are always valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecular synthesis.},
	author = {Kusner, Matt J. and Paige, Brooks and Hernández-Lobato, José Miguel},
	month = mar,
	year = {2017},
}

@article{noauthor_no_nodate-2,
	title = {No {Title}},
	url = {http://research.nvidia.com/sites/default/files/publications/karras2017siggraph-paper_0.pdf},
}

@article{teh_distral_2017,
	title = {Distral: {Robust} {Multitask} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1707.04175},
	abstract = {Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (Distill \& transfer learning). Instead of sharing parameters between the different workers, we propose to share a "distilled" policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning.},
	author = {Teh, Yee Whye and Bapst, Victor and Czarnecki, Wojciech Marian and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan},
	month = jul,
	year = {2017},
}

@article{van_seijen_hybrid_2017,
	title = {Hybrid {Reward} {Architecture} for {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1706.04208},
	abstract = {One of the main challenges in reinforcement learning (RL) is generalisation. In typical deep RL methods this is achieved by approximating the optimal value function with a low-dimensional representation using a deep network. While this approach works well in many domains, in domains where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable. This paper contributes towards tackling such challenging domains, by proposing a new method, called Hybrid Reward Architecture (HRA). HRA takes as input a decomposed reward function and learns a separate value function for each component reward function. Because each component typically only depends on a subset of all features, the overall value function is much smoother and can be easier approximated by a low-dimensional representation, enabling more effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human performance.},
	author = {van Seijen, Harm and Fatemi, Mehdi and Romoff, Joshua and Laroche, Romain and Barnes, Tavian and Tsang, Jeffrey},
	month = jun,
	year = {2017},
}

@article{dosovitskiy_learning_nodate,
	title = {{LEARNING} {TO} {ACT} {BY} {PREDICTING} {THE} {FUTURE}},
	url = {https://arxiv.org/pdf/1611.01779.pdf},
	abstract = {We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by in-teracting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sen-sory input from a complex three-dimensional environment. The presented formu-lation enables learning without a fixed goal at training time, and pursuing dynam-ically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.},
	author = {Dosovitskiy, Alexey and Koltun, Vladlen},
}

@article{gomez-bombarelli_automatic_2016,
	title = {Automatic chemical design using a data-driven continuous representation of molecules},
	url = {http://arxiv.org/abs/1610.02415},
	abstract = {We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This generative model allows efficient search and optimization through open-ended spaces of chemical compounds. We train deep neural networks on hundreds of thousands of existing chemical structures to construct two coupled functions: an encoder and a decoder. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to the discrete representation from this latent space. Continuous representations allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the design of drug-like molecules as well as organic light-emitting diodes.},
	author = {Gómez-Bombarelli, Rafael and Duvenaud, David and Hernández-Lobato, José Miguel and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D. and Adams, Ryan P. and Aspuru-Guzik, Alán},
	month = oct,
	year = {2016},
}

@article{altae-tran_low_2016,
	title = {Low {Data} {Drug} {Discovery} with {One}-shot {Learning}},
	url = {http://arxiv.org/abs/1611.03199},
	abstract = {Recent advances in machine learning have made significant contributions to drug discovery. Deep neural networks in particular have been demonstrated to provide significant boosts in predictive power when inferring the properties and activities of small-molecule compounds. However, the applicability of these techniques has been limited by the requirement for large amounts of training data. In this work, we demonstrate how one-shot learning can be used to significantly lower the amounts of data required to make meaningful predictions in drug discovery applications. We introduce a new architecture, the residual LSTM embedding, that, when combined with graph convolutional neural networks, significantly improves the ability to learn meaningful distance metrics over small-molecules. We open source all models introduced in this work as part of DeepChem, an open-source framework for deep-learning in drug discovery.},
	author = {Altae-Tran, Han and Ramsundar, Bharath and Pappu, Aneesh S. and Pande, Vijay},
	month = nov,
	year = {2016},
}

@article{gomez-bombarelli_automatic_2016-1,
	title = {Automatic chemical design using a data-driven continuous representation of molecules},
	url = {http://arxiv.org/abs/1610.02415},
	abstract = {We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This generative model allows efficient search and optimization through open-ended spaces of chemical compounds. We train deep neural networks on hundreds of thousands of existing chemical structures to construct two coupled functions: an encoder and a decoder. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to the discrete representation from this latent space. Continuous representations allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the design of drug-like molecules as well as organic light-emitting diodes.},
	author = {Gómez-Bombarelli, Rafael and Duvenaud, David and Hernández-Lobato, José Miguel and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D. and Adams, Ryan P. and Aspuru-Guzik, Alán},
	month = oct,
	year = {2016},
}

@article{altae-tran_low_2016-1,
	title = {Low {Data} {Drug} {Discovery} with {One}-shot {Learning}},
	url = {http://arxiv.org/abs/1611.03199},
	abstract = {Recent advances in machine learning have made significant contributions to drug discovery. Deep neural networks in particular have been demonstrated to provide significant boosts in predictive power when inferring the properties and activities of small-molecule compounds. However, the applicability of these techniques has been limited by the requirement for large amounts of training data. In this work, we demonstrate how one-shot learning can be used to significantly lower the amounts of data required to make meaningful predictions in drug discovery applications. We introduce a new architecture, the residual LSTM embedding, that, when combined with graph convolutional neural networks, significantly improves the ability to learn meaningful distance metrics over small-molecules. We open source all models introduced in this work as part of DeepChem, an open-source framework for deep-learning in drug discovery.},
	author = {Altae-Tran, Han and Ramsundar, Bharath and Pappu, Aneesh S. and Pande, Vijay},
	month = nov,
	year = {2016},
}

@article{saffari_machine_nodate,
	title = {Machine {Learning} and {Drug} {Discovery}},
	url = {http://www.ymer.org/papers/files/2017-London-ML-Meetup.pdf},
	author = {Saffari, Amir},
}

@article{merity_regularizing_2017,
	title = {Regularizing and {Optimizing} {LSTM} {Language} {Models}},
	url = {http://arxiv.org/abs/1708.02182},
	abstract = {Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.},
	author = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
	month = aug,
	year = {2017},
}

@article{arandjelovic_look_2017,
	title = {Look, {Listen} and {Learn}},
	url = {http://arxiv.org/abs/1705.08168},
	abstract = {We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself -- the correspondence between the visual and the audio streams, and we introduce a novel "Audio-Visual Correspondence" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art self-supervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.},
	author = {Arandjelović, Relja and Zisserman, Andrew},
	month = may,
	year = {2017},
}

@article{wu_moleculenet_2017,
	title = {{MoleculeNet}: {A} {Benchmark} for {Molecular} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1703.00564},
	abstract = {Molecular machine learning has been maturing rapidly over the last few years. Improved methods and the presence of larger datasets have enabled machine learning algorithms to make increasingly accurate predictions about molecular properties. However, algorithmic progress has been limited due to the lack of a standard benchmark to compare the efficacy of proposed methods; most new algorithms are benchmarked on different datasets making it challenging to gauge the quality of proposed methods. This work introduces MoleculeNet, a large scale benchmark for molecular machine learning. MoleculeNet curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the DeepChem open source library). MoleculeNet benchmarks demonstrate that learnable representations, and in particular graph convolutional networks, are powerful tools for molecular machine learning and broadly offer the best performance. However, for quantum mechanical and biophysical datasets, the use of physics-aware featurizations can be significantly more important than choice of particular learning algorithm.},
	author = {Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N. and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S. and Leswing, Karl and Pande, Vijay},
	month = mar,
	year = {2017},
}

@article{melis_state_nodate,
	title = {On the {State} of the {Art} of {Evaluation} in {Neural} {Language} {Models}},
	url = {https://arxiv.org/pdf/1707.05589.pdf},
	abstract = {Ongoing innovations in recurrent neu-ral network architectures have provided a steady influx of apparently state-of-the-art results on language modelling bench-marks. However, these have been evalu-ated using differing code bases and lim-ited computational resources, which rep-resent uncontrolled sources of experimen-tal variation. We reevaluate several pop-ular architectures and regularisation meth-ods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that stan-dard LSTM architectures, when properly regularised, outperform more recent mod-els. We establish a new state of the art on the Penn Treebank and Wikitext-2 cor-pora, as well as strong baselines on the Hutter Prize dataset.},
	author = {Melis, Gábor and Dyer, Chris and Deepmind, Phil Blunsom},
}

@article{johnson_backpropagation_2017,
	title = {Backpropagation for a {Linear} {Layer}},
	abstract = {In these notes we will explicitly derive the equations to use when backprop-agating through a linear layer, using minibatches. During the forward pass, the linear layer takes an input X of shape N × D and a weight matrix W of shape D × M , and computes an output Y = XW of shape N × M by computing the matrix product of the two inputs. To make things even more concrete, we will consider the case N = 2, D = 2, M = 3. We can then write out the forward pass in terms of the elements of the inputs:},
	author = {Johnson, Justin and Xw, Y =},
	year = {2017},
}

@article{johnson_derivatives_2017,
	title = {Derivatives, {Backpropagation}, and {Vectorization}},
	author = {Johnson, Justin},
	year = {2017},
}

@article{lecun_efficient_2012,
	title = {Efficient backprop},
	issn = {9783642352881},
	doi = {10.1007/978-3-642-35289-8-3},
	abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {LeCun, Yann A. and Bottou, L??on and Orr, Genevieve B. and M??ller, Klaus Robert},
	year = {2012},
}

@article{wei_network_nodate,
	title = {Network {Morphism}},
	abstract = {We present in this paper a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as net-work morphism in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this net-work morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morph-ing types for both classic and convolutional neu-ral networks. The second requirement for this network morphism is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activa-tion neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network mor-phism scheme.},
	author = {Wei, Tao and Wang, Changhu and Rui, Yong and Wen, Chang and Edu, Chen Chencw@buffalo},
}

@article{chen_net2net_2015,
	title = {{Net2Net}: {Accelerating} {Learning} via {Knowledge} {Transfer}},
	url = {http://arxiv.org/abs/1511.05641},
	abstract = {We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.},
	author = {Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
	month = nov,
	year = {2015},
}

@article{chaurasia_linknet_2017,
	title = {{LinkNet}: {Exploiting} {Encoder} {Representations} for {Efficient} {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1707.03718},
	abstract = {Pixel-wise semantic segmentation for visual scene understanding not only needs to be accurate, but also efficient in order to find any use in real-time application. Existing algorithms even though are accurate but they do not focus on utilizing the parameters of neural network efficiently. As a result they are huge in terms of parameters and number of operations; hence slow too. In this paper, we propose a novel deep neural network architecture which allows it to learn without any significant increase in number of parameters. Our network uses only 11.5 million parameters and 21.2 GFLOPs for processing an image of resolution 3x640x360. It gives state-of-the-art performance on CamVid and comparable results on Cityscapes dataset. We also compare our networks processing time on NVIDIA GPU and embedded system device with existing state-of-the-art architectures for different image resolutions.},
	author = {Chaurasia, Abhishek and Culurciello, Eugenio},
	month = jun,
	year = {2017},
}

@article{monti_geometric_2016,
	title = {Geometric deep learning on graphs and manifolds using mixture model {CNNs}},
	url = {http://arxiv.org/abs/1611.08402},
	abstract = {Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph- and 3D shape analysis and show that it consistently outperforms previous approaches.},
	author = {Monti, Federico and Boscaini, Davide and Masci, Jonathan and Rodolà, Emanuele and Svoboda, Jan and Bronstein, Michael M.},
	month = nov,
	year = {2016},
}

@article{bronstein_geometric_2016,
	title = {Geometric deep learning: going beyond {Euclidean} data},
	url = {http://arxiv.org/abs/1611.08097},
	doi = {10.1109/MSP.2017.2693418},
	abstract = {Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
	author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
	month = nov,
	year = {2016},
}

@article{liao_visual_2017,
	title = {Visual {Attribute} {Transfer} through {Deep} {Image} {Analogy}},
	url = {http://arxiv.org/abs/1705.01088},
	abstract = {We propose a new technique for visual attribute transfer across images that may have very different appearance but have perceptually similar semantic structure. By visual attribute transfer, we mean transfer of visual information (such as color, tone, texture, and style) from one image to another. For example, one image could be that of a painting or a sketch while the other is a photo of a real scene, and both depict the same type of scene. Our technique finds semantically-meaningful dense correspondences between two input images. To accomplish this, it adapts the notion of "image analogy" with features extracted from a Deep Convolutional Neutral Network for matching; we call our technique Deep Image Analogy. A coarse-to-fine strategy is used to compute the nearest-neighbor field for generating the results. We validate the effectiveness of our proposed method in a variety of cases, including style/texture transfer, color/style swap, sketch/painting to photo, and time lapse.},
	author = {Liao, Jing and Yao, Yuan and Yuan, Lu and Hua, Gang and Kang, Sing Bing},
	month = may,
	year = {2017},
}

@article{li_deep_2017,
	title = {Deep {Learning} {Segmentation} of {Optical} {Microscopy} {Images} {Improves} 3-{D} {Neuron} {Reconstruction}},
	volume = {36},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/28287966%5Cnhttp://ieeexplore.ieee.org/document/7874113/},
	doi = {10.1109/TMI.2017.2679713},
	abstract = {Digital reconstruction, or tracing, of 3-dimensional (3D) neuron structure from microscopy images is a critical step toward reversing engineering the wiring and anatomy of a brain. Despite a number of prior attempts, this task remains very challenging, especially when images are contaminated by noises or have discontinued segments of neurite patterns. An approach for addressing such problems is to identify the locations of neuronal voxels using image segmentation methods prior to applying tracing or reconstruction techniques. This preprocessing step is expected to remove noises in the data, thereby leading to improved reconstruction results. In this work, we proposed to use 3D Convolutional neural networks (CNNs) for segmenting the neuronal microscopy images. Specifically, we designed a novel CNN architecture that takes volumetric images as the inputs and their voxel-wise segmentation maps as the outputs. The developed architecture allows us to train and predict using large microscopy images in an end-to-end manner. We evaluated the performance of our model on a variety of challenging 3D microscopy images from different organisms. Results showed that the proposed methods improved the tracing performance significantly when combined with different reconstruction algorithms.},
	number = {7},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Li, Rongjian and Zeng, Tao and Peng, Hanchuan and Ji, Shuiwang},
	year = {2017},
	pages = {1533--1541},
}

@article{luc_semantic_nodate,
	title = {Semantic {Segmentation} using {Adversarial} {Networks}},
	url = {https://arxiv.org/pdf/1611.08408.pdf},
	abstract = {Adversarial training has been shown to produce state of the art results for generative image modeling. In this paper we propose an adversarial training approach to train semantic segmentation models. We train a convolutional semantic segmentation network along with an adversarial network that discriminates segmentation maps coming either from the ground truth or from the segmentation network. The moti-vation for our approach is that it can detect and correct higher-order inconsistencies between ground truth segmentation maps and the ones produced by the segmen-tation net. Our experiments show that our adversarial training approach leads to improved accuracy on the Stanford Background and PASCAL VOC 2012 datasets.},
	author = {Luc, Pauline and Couprie, Camille and Chintala, Soumith and Verbeek, Jakob},
}

@article{kohl_adversarial_2017,
	title = {Adversarial {Networks} for the {Detection} of {Aggressive} {Prostate} {Cancer}},
	url = {http://arxiv.org/abs/1702.08014},
	abstract = {Semantic segmentation constitutes an integral part of medical image analyses for which breakthroughs in the field of deep learning were of high relevance. The large number of trainable parameters of deep neural networks however renders them inherently data hungry, a characteristic that heavily challenges the medical imaging community. Though interestingly, with the de facto standard training of fully convolutional networks (FCNs) for semantic segmentation being agnostic towards the `structure' of the predicted label maps, valuable complementary information about the global quality of the segmentation lies idle. In order to tap into this potential, we propose utilizing an adversarial network which discriminates between expert and generated annotations in order to train FCNs for semantic segmentation. Because the adversary constitutes a learned parametrization of what makes a good segmentation at a global level, we hypothesize that the method holds particular advantages for segmentation tasks on complex structured, small datasets. This holds true in our experiments: We learn to segment aggressive prostate cancer utilizing MRI images of 152 patients and show that the proposed scheme is superior over the de facto standard in terms of the detection sensitivity and the dice-score for aggressive prostate cancer. The achieved relative gains are shown to be particularly pronounced in the small dataset limit.},
	author = {Kohl, Simon and Bonekamp, David and Schlemmer, Heinz-Peter and Yaqubi, Kaneschka and Hohenfellner, Markus and Hadaschik, Boris and Radtke, Jan-Philipp and Maier-Hein, Klaus},
	month = feb,
	year = {2017},
}

@article{dinh_sharp_nodate,
	title = {Sharp {Minima} {Can} {Generalize} {For} {Deep} {Nets}},
	url = {https://arxiv.org/pdf/1703.04933.pdf},
	abstract = {Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize rel-atively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter \& Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This pa-per argues that most notions of flatness are prob-lematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of pa-rameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper min-ima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its general-ization properties.},
	author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
}

@article{jiang_submitted_nodate,
	title = {{SUBMITTED} {TO} {IEEE} {TRANSACTIONS} {ON} {CIRCUITS} {AND} {SYSTEMS} {FOR} {VIDEO} {TECHNOLOGY} {An} {End}-to-{End} {Compression} {Framework} {Based} on {Convolutional} {Neural} {Networks}},
	url = {https://arxiv.org/pdf/1708.00838v1.pdf},
	abstract = {—Deep learning, e.g., convolutional neural networks (CNNs), has achieved great success in image processing and computer vision especially in high level vision applications such as recognition and understanding. However, it is rarely used to solve low-level vision problems such as image compression studied in this paper. Here, we move forward a step and propose a novel compression framework based on CNNs. To achieve high-quality image compression at low bit rates, two CNNs are seamlessly integrated into an end-to-end compression framework. The first CNN, named compact convolutional neural network (ComCNN), learns an optimal compact representation from an input image, which preserves the structural information and is then encoded using an image codec (e.g., JPEG, JPEG2000 or BPG). The second CNN, named reconstruction convolutional neural network (RecCNN), is used to reconstruct the decoded image with high-quality in the decoding end. To make two CNNs effectively collaborate, we develop a unified end-to-end learning algorithm to simultaneously learn ComCNN and RecCNN, which facilitates the accurate reconstruction of the decoded image using RecCNN. Such a design also makes the proposed compression framework compatible with existing image coding standards. Experimental results validate that the proposed compression framework greatly outperforms several compression frameworks that use existing image coding standards with state-of-the-art deblocking or denoising post-processing methods.},
	author = {Jiang, Feng and Tao, Wen and Liu, Shaohui and Ren, Jie and Guo, Xun and Zhao, Debin},
	keywords = {com-pact representation, compression framework, convolutional neural networks (CNNs), Index Terms—Deep learning},
}

@article{zhu_adversarial_2016,
	title = {Adversarial {Deep} {Structural} {Networks} for {Mammographic} {Mass} {Segmentation}},
	url = {http://arxiv.org/abs/1612.05970},
	abstract = {Mass segmentation is an important task in mammogram analysis, providing effective morphological features and regions of interest (ROI) for mass detection and classification. Inspired by the success of using deep convolutional features for natural image analysis and conditional random fields (CRF) for structural learning, we propose an end-to-end network for mammographic mass segmentation. The network employs a fully convolutional network (FCN) to model potential function, followed by a CRF to perform structural learning. Because the mass distribution varies greatly with pixel position, the FCN is combined with position priori for the task. Due to the small size of mammogram datasets, we use adversarial training to control over-fitting. Four models with different convolutional kernels are further fused to improve the segmentation results. Experimental results on two public datasets, INbreast and DDSM-BCRP, show that our end-to-end network combined with adversarial training achieves the-state-of-the-art results.},
	author = {Zhu, Wentao and Xiang, Xiang and Tran, Trac D. and Xie, Xiaohui},
	month = dec,
	year = {2016},
}

@article{shi_benchmarking_nodate,
	title = {Benchmarking {State}-of-the-{Art} {Deep} {Learning} {Software} {Tools}},
	url = {https://arxiv.org/pdf/1608.07249.pdf},
	abstract = {—Deep learning has been shown as a successful ma-chine learning method for a variety of tasks, and its popularity results in numerous open-source deep learning software tools. Training a deep network is usually a very time-consuming process. To address the computational challenge in deep learn-ing, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training time. However, different tools exhibit different features and running performance when training different types of deep networks on different hardware platforms, which makes it difficult for end users to select an appropriate pair of software and hardware. In this paper, we aim to make a comparative study of the state-of-the-art GPU-accelerated deep learning software tools, including Caffe, CNTK, MXNet, TensorFlow, and Torch. We first benchmark the running performance of these tools with three popular types of neural networks on two CPU platforms and three GPU platforms. We then benchmark some distributed versions on multiple GPUs. Our contribution is two-fold. First, for end users of deep learning tools, our bench-marking results can serve as a guide to selecting appropriate hardware platforms and software tools. Second, for software developers of deep learning tools, our in-depth analysis points out possible future directions to further optimize the running performance.},
	author = {Shi, Shaohuai and Wang, Qiang and Xu, Pengfei and Chu, Xiaowen},
	keywords = {Convolutional Neural Networks, Index Terms—Deep Learning, Caffe, CNTK, Feed-forward Neural Networks, GPU, MXNet, Recurrent Neural Networks, TensorFlow, Torch},
}

@article{wang_light_2017,
	title = {Light {Field} {Video} {Capture} {Using} a {Learning}-{Based} {Hybrid} {Imaging} {System}},
	url = {http://arxiv.org/abs/1705.02997},
	doi = {10.1145/3072959.3073614},
	abstract = {Light field cameras have many advantages over traditional cameras, as they allow the user to change various camera settings after capture. However, capturing light fields requires a huge bandwidth to record the data: a modern light field camera can only take three images per second. This prevents current consumer light field cameras from capturing light field videos. Temporal interpolation at such extreme scale (10x, from 3 fps to 30 fps) is infeasible as too much information will be entirely missing between adjacent frames. Instead, we develop a hybrid imaging system, adding another standard video camera to capture the temporal information. Given a 3 fps light field sequence and a standard 30 fps 2D video, our system can then generate a full light field video at 30 fps. We adopt a learning-based approach, which can be decomposed into two steps: spatio-temporal flow estimation and appearance estimation. The flow estimation propagates the angular information from the light field sequence to the 2D video, so we can warp input images to the target view. The appearance estimation then combines these warped images to output the final pixels. The whole process is trained end-to-end using convolutional neural networks. Experimental results demonstrate that our algorithm outperforms current video interpolation methods, enabling consumer light field videography, and making applications such as refocusing and parallax view generation achievable on videos for the first time.},
	author = {Wang, Ting-Chun and Zhu, Jun-Yan and Kalantari, Nima Khademi and Efros, Alexei A. and Ramamoorthi, Ravi},
	month = may,
	year = {2017},
}

@article{noauthor_full-text_nodate-2,
	title = {full-text},
}

@article{suganuma_genetic_2017,
	title = {A {Genetic} {Programming} {Approach} to {Designing} {Convolutional} {Neural} {Network} {Architectures}},
	url = {http://arxiv.org/abs/1704.00764},
	abstract = {The convolutional neural network (CNN), which is one of the deep learning models, has seen much success in a variety of computer vision tasks. However, designing CNN architectures still requires expert knowledge and a lot of trial and error. In this paper, we attempt to automatically construct CNN architectures for an image classification task based on Cartesian genetic programming (CGP). In our method, we adopt highly functional modules, such as convolutional blocks and tensor concatenation, as the node functions in CGP. The CNN structure and connectivity represented by the CGP encoding method are optimized to maximize the validation accuracy. To evaluate the proposed method, we constructed a CNN architecture for the image classification task with the CIFAR-10 dataset. The experimental result shows that the proposed method can be used to automatically find the competitive CNN architecture compared with state-of-the-art models.},
	author = {Suganuma, Masanori and Shirakawa, Shinichi and Nagao, Tomoharu},
	month = apr,
	year = {2017},
}

@article{poursaeed_vision-based_2017,
	title = {Vision-based {Real} {Estate} {Price} {Estimation}},
	url = {http://arxiv.org/abs/1707.05489},
	abstract = {Since the advent of online real estate database companies like Zillow, Trulia and Redfin, the problem of automatic estimation of market values for houses has received considerable attention. Several real estate websites provide such estimates using a proprietary formula. Although these estimates are often close to the actual sale prices, in some cases they are highly inaccurate. One of the key factors that affects the value of a house is its interior and exterior appearance, which is not considered in calculating automatic value estimates. In this paper, we evaluate the impact of visual characteristics of a house on its market value. Using deep convolutional neural networks on a large dataset of photos of home interiors and exteriors, we develop a method for estimating the luxury level of real estate photos. We also develop a novel framework for automated value assessment using the above photos in addition to home characteristics including size, offered price and number of bedrooms. Finally, by applying our proposed method for price estimation to a new dataset of real estate photos and metadata, we show that it outperforms Zillow's estimates.},
	author = {Poursaeed, Omid and Matera, Tomas and Belongie, Serge},
	month = jul,
	year = {2017},
}

@article{cisse_houdini_nodate,
	title = {Houdini: {Fooling} {Deep} {Structured} {Prediction} {Models}},
	url = {https://arxiv.org/pdf/1707.05373.pdf},
	abstract = {Generating adversarial examples is a critical step for evaluating and improving the robustness of learning machines. So far, most existing methods only work for classification and are not designed to alter the true performance measure of the problem at hand. We introduce a novel flexible approach named Houdini for generating adversarial examples specifically tailored for the final performance measure of the task considered, be it combinatorial and non-decomposable. We successfully apply Houdini to a range of applications such as speech recognition, pose estimation and semantic segmentation. In all cases, the attacks based on Houdini achieve higher success rate than those based on the traditional surrogates used to train the models while using a less perceptible adversarial perturbation.},
	author = {Cisse, Moustapha and Adi, Yossi and Neverova, Natalia and Keshet, Joseph},
}

@article{chen_enriching_nodate,
	title = {Enriching {Visual} {Knowledge} {Bases} via {Object} {Discovery} and {Segmentation}},
	url = {https://www.cs.cmu.edu/~xinleic/papers/cvpr14.pdf},
	abstract = {There have been some recent efforts to build visual knowledge bases from Internet images. But most of these approaches have focused on bounding box representation of objects. In this paper, we propose to enrich these knowl-edge bases by automatically discovering objects and their segmentations from noisy Internet images. Specifically, our approach combines the power of generative modeling for segmentation with the effectiveness of discriminative mod-els for detection. The key idea behind our approach is to learn and exploit top-down segmentation priors based on visual subcategories. The strong priors learned from these visual subcategories are then combined with discrim-inatively trained detectors and bottom up cues to produce clean object segmentations. Our experimental results indi-cate state-of-the-art performance on the difficult dataset in-troduced by [29]. We have integrated our algorithm in NEIL for enriching its knowledge base [5]. As of 14 th April 2014, NEIL has automatically generated approximately 500K seg-mentations using web data.},
	author = {Chen, Xinlei and Shrivastava, Abhinav and Gupta, Abhinav},
}

@article{moeskops_adversarial_2017,
	title = {Adversarial training and dilated convolutions for brain {MRI} segmentation},
	url = {http://arxiv.org/abs/1707.03195},
	abstract = {Convolutional neural networks (CNNs) have been applied to various automatic image segmentation tasks in medical image analysis, including brain MRI segmentation. Generative adversarial networks have recently gained popularity because of their power in generating images that are difficult to distinguish from real images. In this study we use an adversarial training approach to improve CNN-based brain MRI segmentation. To this end, we include an additional loss function that motivates the network to generate segmentations that are difficult to distinguish from manual segmentations. During training, this loss function is optimised together with the conventional average per-voxel cross entropy loss. The results show improved segmentation performance using this adversarial training procedure for segmentation of two different sets of images and using two different network architectures, both visually and in terms of Dice coefficients.},
	author = {Moeskops, Pim and Veta, Mitko and Lafarge, Maxime W. and Eppenhof, Koen A. J. and Pluim, Josien P. W.},
	month = jul,
	year = {2017},
}

@article{frogner_learning_2015,
	title = {Learning with a {Wasserstein} {Loss}},
	url = {http://arxiv.org/abs/1506.05439},
	abstract = {Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe an efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, outperforming a baseline that doesn't use the metric.},
	author = {Frogner, Charlie and Zhang, Chiyuan and Mobahi, Hossein and Araya-Polo, Mauricio and Poggio, Tomaso},
	month = jun,
	year = {2015},
}

@article{konyushkova_geometry_2016,
	title = {Geometry in {Active} {Learning} for {Binary} and {Multi}-class {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1606.09029},
	abstract = {We propose an Active Learning approach to image segmentation that exploits geometric priors to streamline the annotation process. We demonstrate this for both background-foreground and multi-class segmentation tasks in 2D images and 3D image volumes. Our approach combines geometric smoothness priors in the image space with more traditional uncertainty measures to estimate which pixels or voxels are most in need of annotation. For multi-class settings, we additionally introduce two novel criteria for uncertainty. In the 3D case, we use the resulting uncertainty measure to show the annotator voxels lying on the same planar patch, which makes batch annotation much easier than if they were randomly distributed in the volume. The planar patch is found using a branch-and-bound algorithm that finds a patch with the most informative instances. We evaluate our approach on Electron Microscopy and Magnetic Resonance image volumes, as well as on regular images of horses and faces. We demonstrate a substantial performance increase over state-of-the-art approaches.},
	author = {Konyushkova, Ksenia and Sznitman, Raphael and Fua, Pascal},
	month = jun,
	year = {2016},
}

@article{xue_segan_2017,
	title = {{SegAN}: {Adversarial} {Network} with {Multi}-scale \${L}\_1\$ {Loss} for {Medical} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1706.01805},
	abstract = {Inspired by classic generative adversarial networks (GAN), we propose a novel end-to-end adversarial neural network, called SegAN, for the task of medical image segmentation. Since image segmentation requires dense, pixel-level labeling, the single scalar real/fake output of a classic GAN's discriminator may be ineffective in producing stable and sufficient gradient feedback to the networks. Instead, we use a fully convolutional neural network as the segmentor to generate segmentation label maps, and propose a novel adversarial critic network with a multi-scale \$L\_1\$ loss function to force the critic and segmentor to learn both global and local features that capture long- and short-range spatial relationships between pixels. In our SegAN framework, the segmentor and critic networks are trained in an alternating fashion in a min-max game: The critic takes as input a pair of images, (original\_image \$*\$ predicted\_label\_map, original\_image \$*\$ ground\_truth\_label\_map), and then is trained by maximizing a multi-scale loss function; The segmentor is trained with only gradients passed along by the critic, with the aim to minimize the multi-scale loss function. We show that such a SegAN framework is more effective and stable for the segmentation task, and it leads to better performance than the state-of-the-art U-net segmentation method. We tested our SegAN method using datasets from the MICCAI BRATS brain tumor segmentation challenge. Extensive experimental results demonstrate the effectiveness of the proposed SegAN with multi-scale loss: on BRATS 2013 SegAN gives performance comparable to the state-of-the-art for whole tumor and tumor core segmentation while achieves better precision and sensitivity for Gd-enhance tumor core segmentation; on BRATS 2015 SegAN achieves better performance than the state-of-the-art in both dice score and precision.},
	author = {Xue, Yuan and Xu, Tao and Zhang, Han and Long, Rodney and Huang, Xiaolei},
	month = jun,
	year = {2017},
}

@article{pathak_constrained_2015,
	title = {Constrained {Convolutional} {Neural} {Networks} for {Weakly} {Supervised} {Segmentation}},
	url = {http://arxiv.org/abs/1506.03648},
	abstract = {We present an approach to learn a dense pixel-wise labeling from image-level tags. Each image-level tag imposes constraints on the output labeling of a Convolutional Neural Network (CNN) classifier. We propose Constrained CNN (CCNN), a method which uses a novel loss function to optimize for any set of linear constraints on the output space (i.e. predicted label distribution) of a CNN. Our loss formulation is easy to optimize and can be incorporated directly into standard stochastic gradient descent optimization. The key idea is to phrase the training objective as a biconvex optimization for linear models, which we then relax to nonlinear deep networks. Extensive experiments demonstrate the generality of our new learning framework. The constrained loss yields state-of-the-art results on weakly supervised semantic image segmentation. We further demonstrate that adding slightly more supervision can greatly improve the performance of the learning algorithm.},
	author = {Pathak, Deepak and Krähenbühl, Philipp and Darrell, Trevor},
	month = jun,
	year = {2015},
}

@article{souly_semi_2017,
	title = {Semi and {Weakly} {Supervised} {Semantic} {Segmentation} {Using} {Generative} {Adversarial} {Network}},
	url = {http://arxiv.org/abs/1703.09695},
	abstract = {Semantic segmentation has been a long standing challenging task in computer vision. It aims at assigning a label to each image pixel and needs significant number of pixellevel annotated data, which is often unavailable. To address this lack, in this paper, we leverage, on one hand, massive amount of available unlabeled or weakly labeled data, and on the other hand, non-real images created through Generative Adversarial Networks. In particular, we propose a semi-supervised framework ,based on Generative Adversarial Networks (GANs), which consists of a generator network to provide extra training examples to a multi-class classifier, acting as discriminator in the GAN framework, that assigns sample a label y from the K possible classes or marks it as a fake sample (extra class). The underlying idea is that adding large fake visual data forces real samples to be close in the feature space, enabling a bottom-up clustering process, which, in turn, improves multiclass pixel classification. To ensure higher quality of generated images for GANs with consequent improved pixel classification, we extend the above framework by adding weakly annotated data, i.e., we provide class level information to the generator. We tested our approaches on several challenging benchmarking visual datasets, i.e. PASCAL, SiftFLow, Stanford and CamVid, achieving competitive performance also compared to state-of-the-art semantic segmentation method},
	author = {Souly, Nasim and Spampinato, Concetto and Shah, Mubarak},
	month = mar,
	year = {2017},
}

@article{khoreva_simple_2016,
	title = {Simple {Does} {It}: {Weakly} {Supervised} {Instance} and {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1603.07485},
	abstract = {Semantic labelling and instance segmentation are two tasks that require particularly costly annotations. Starting from weak supervision in the form of bounding box detection annotations, we propose a new approach that does not require modification of the segmentation training procedure. We show that when carefully designing the input labels from given bounding boxes, even a single round of training is enough to improve over previously reported weakly supervised results. Overall, our weak supervision approach reaches {\textasciitilde}95\% of the quality of the fully supervised model, both for semantic labelling and instance segmentation.},
	author = {Khoreva, Anna and Benenson, Rodrigo and Hosang, Jan and Hein, Matthias and Schiele, Bernt},
	month = mar,
	year = {2016},
}

@article{hong_weakly_2017,
	title = {Weakly {Supervised} {Semantic} {Segmentation} using {Web}-{Crawled} {Videos}},
	url = {http://arxiv.org/abs/1701.00352},
	abstract = {We propose a novel algorithm for weakly supervised semantic segmentation based on image-level class labels only. In weakly supervised setting, it is commonly observed that trained model overly focuses on discriminative parts rather than the entire object area. Our goal is to overcome this limitation with no additional human intervention by retrieving videos relevant to target class labels from web repository, and generating segmentation labels from the retrieved videos to simulate strong supervision for semantic segmentation. During this process, we take advantage of image classification with discriminative localization technique to reject false alarms in retrieved videos and identify relevant spatio-temporal volumes within retrieved videos. Although the entire procedure does not require any additional supervision, the segmentation annotations obtained from videos are sufficiently strong to learn a model for semantic segmentation. The proposed algorithm substantially outperforms existing methods based on the same level of supervision and is even as competitive as the approaches relying on extra annotations.},
	author = {Hong, Seunghoon and Yeo, Donghun and Kwak, Suha and Lee, Honglak and Han, Bohyung},
	month = jan,
	year = {2017},
}

@article{shen_weakly_2017,
	title = {Weakly {Supervised} {Semantic} {Segmentation} {Based} on {Co}-segmentation},
	url = {http://arxiv.org/abs/1705.09052},
	abstract = {Training a Fully Convolutional Network (FCN) for semantic segmentation requires a large number of pixel-level masks, which involves a large amount of human labour and time for annotation. In contrast, image-level labels are much easier to obtain. In this work, we propose a novel method for weakly supervised semantic segmentation with only image-level labels. The method relies on a large scale co-segmentation framework that can produce object masks for a group of images containing objects belonging to the same semantic class. We first retrieve images from search engines, e.g. Flickr and Google, using semantic class names as queries, e.g. class names in PASCAL VOC 2012. We then use high quality masks produced by co-segmentation on the retrieved images as well as the target dataset images with image level labels to train segmentation networks. We obtain IoU 56.9 on test set of PASCAL VOC 2012, which reaches state of the art performance.},
	author = {Shen, Tong and Lin, Guosheng and Liu, Lingqiao and Shen, Chunhua and Reid, Ian},
	month = may,
	year = {2017},
}

@article{tai_pca-aided_2016,
	title = {{PCA}-aided {Fully} {Convolutional} {Networks} for {Semantic} {Segmentation} of {Multi}-channel {fMRI}},
	url = {http://arxiv.org/abs/1610.01732},
	abstract = {Semantic segmentation of functional magnetic resonance imaging (fMRI) makes great sense for pathology diagnosis and decision system of medical robots. The multi-channel fMRI provides more information of the pathological features. But the increased amount of data causes complexity in feature detections. This paper proposes a principal component analysis (PCA)-aided fully convolutional network to particularly deal with multi-channel fMRI. We transfer the learned weights of contemporary classification networks to the segmentation task by fine-tuning. The results of the convolutional network are compared with various methods e.g. k-NN. A new labeling strategy is proposed to solve the semantic segmentation problem with unclear boundaries. Even with a small-sized training dataset, the test results demonstrate that our model outperforms other pathological feature detection methods. Besides, its forward inference only takes 90 milliseconds for a single set of fMRI data. To our knowledge, this is the first time to realize pixel-wise labeling of multi-channel magnetic resonance image using FCN.},
	author = {Tai, Lei and Ye, Haoyang and Ye, Qiong and Liu, Ming},
	month = oct,
	year = {2016},
}

@article{ren_end--end_2016,
	title = {End-to-{End} {Instance} {Segmentation} with {Recurrent} {Attention}},
	url = {http://arxiv.org/abs/1605.09410},
	abstract = {While convolutional neural networks have gained impressive success recently in solving structured prediction problems such as semantic segmentation, it remains a challenge to differentiate individual object instances in the scene. Instance segmentation is very important in a variety of applications, such as autonomous driving, image captioning, and visual question answering. Techniques that combine large graphical models with low-level vision have been proposed to address this problem; however, we propose an end-to-end recurrent neural network (RNN) architecture with an attention mechanism to model a human-like counting process, and produce detailed instance segmentations. The network is jointly trained to sequentially produce regions of interest as well as a dominant object segmentation within each region. The proposed model achieves competitive results on the CVPPP, KITTI, and Cityscapes datasets.},
	author = {Ren, Mengye and Zemel, Richard S.},
	month = may,
	year = {2016},
}

@article{zeng_convolutional_2017,
	title = {A convolutional autoencoder approach for mining features in cellular electron cryo-tomograms and weakly supervised coarse segmentation},
	url = {http://arxiv.org/abs/1706.04970},
	abstract = {Cellular electron cryo-tomography enables the 3D visualization of cellular organization in a near-native state at submolecular resolution. However, the content of a cellular tomogram is often complex, making it difficult to automatically isolate different in situ cellular components. In this paper, we propose a convolutional autoencoder-based unsupervised approach to provide a coarse characterization of 3D patches extracted from tomograms. We demonstrate that the autoencoder can be used for the efficient and coarse characterizing of features that correspond to macromolecular complexes and surfaces, like membranes. In addition, it can be used to detect non-cellular features related to sample preparation and data collection like carbon edges from the grid, and tomogram boundaries. The autoencoder is also able to detect patterns that may indicate spatial interactions between cell components. Furthermore, we demonstrate that our autoencoder can be used for weakly supervised semantic segmentation of cellular components requiring very small amount of manual annotation.},
	author = {Zeng, Xiangrui and Leung, Miguel Ricardo and Zeev-Ben-Mordehai, Tzviya and Xu, Min},
	month = jun,
	year = {2017},
}

@article{simonyan_deep_2013,
	title = {Deep {Inside} {Convolutional} {Networks}: {Visualising} {Image} {Classification} {Models} and {Saliency} {Maps}},
	url = {http://arxiv.org/abs/1312.6034},
	abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = dec,
	year = {2013},
}

@article{kaiser_depthwise_2017,
	title = {Depthwise {Separable} {Convolutions} for {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1706.03059},
	abstract = {Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves new state-of-the-art results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new "super-separable" convolution operation that further reduces the number of parameters and computational cost for obtaining state-of-the-art results.},
	author = {Kaiser, Lukasz and Gomez, Aidan N. and Chollet, Francois},
	month = jun,
	year = {2017},
}

@article{howard_mobilenets_2017,
	title = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
	url = {http://arxiv.org/abs/1704.04861},
	abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	month = apr,
	year = {2017},
}

@article{clevert_fast_nodate,
	title = {{FAST} {AND} {ACCURATE} {DEEP} {NETWORK} {LEARNING} {BY} {EXPONENTIAL} {LINEAR} {UNITS} ({ELUS})},
	url = {https://arxiv.org/pdf/1511.07289.pdf},
	abstract = {We introduce the " exponential linear unit " (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like recti-fied linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PRe-LUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gra-dient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore ELUs code the degree of pres-ence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly bet-ter generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10\% classification error for a single crop, single model network.},
	author = {Clevert, Djork-Arné and Unterthiner, Thomas and Hochreiter, Sepp},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = jun,
	year = {2017},
}

@article{kaiser_one_2017,
	title = {One {Model} {To} {Learn} {Them} {All}},
	url = {http://arxiv.org/abs/1706.05137},
	abstract = {Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.},
	author = {Kaiser, Lukasz and Gomez, Aidan N. and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob},
	month = jun,
	year = {2017},
}

@article{galliani_learned_2017,
	title = {Learned {Spectral} {Super}-{Resolution}},
	url = {http://arxiv.org/abs/1703.09470},
	abstract = {We describe a novel method for blind, single-image spectral super-resolution. While conventional super-resolution aims to increase the spatial resolution of an input image, our goal is to spectrally enhance the input, i.e., generate an image with the same spatial resolution, but a greatly increased number of narrow (hyper-spectral) wave-length bands. Just like the spatial statistics of natural images has rich structure, which one can exploit as prior to predict high-frequency content from a low resolution image, the same is also true in the spectral domain: the materials and lighting conditions of the observed world induce structure in the spectrum of wavelengths observed at a given pixel. Surprisingly, very little work exists that attempts to use this diagnosis and achieve blind spectral super-resolution from single images. We start from the conjecture that, just like in the spatial domain, we can learn the statistics of natural image spectra, and with its help generate finely resolved hyper-spectral images from RGB input. Technically, we follow the current best practice and implement a convolutional neural network (CNN), which is trained to carry out the end-to-end mapping from an entire RGB image to the corresponding hyperspectral image of equal size. We demonstrate spectral super-resolution both for conventional RGB images and for multi-spectral satellite data, outperforming the state-of-the-art.},
	author = {Galliani, Silvano and Lanaras, Charis and Marmanis, Dimitrios and Baltsavias, Emmanuel and Schindler, Konrad},
	month = mar,
	year = {2017},
}

@article{goyal_something_2017,
	title = {The "something something" video database for learning and evaluating visual common sense},
	url = {http://arxiv.org/abs/1706.04261},
	abstract = {Neural networks trained on datasets such as ImageNet have led to major advances in visual object classification. One obstacle that prevents networks from reasoning more deeply about complex scenes and situations, and from integrating visual knowledge with natural language, like humans do, is their lack of common sense knowledge about the physical world. Videos, unlike still images, contain a wealth of detailed information about the physical world. However, most labelled video datasets represent high-level concepts rather than detailed physical aspects about actions and scenes. In this work, we describe our ongoing collection of the "something-something" database of video prediction tasks whose solutions require a common sense understanding of the depicted situation. The database currently contains more than 100,000 videos across 174 classes, which are defined as caption-templates. We also describe the challenges in crowd-sourcing this data at scale.},
	author = {Goyal, Raghav and Kahou, Samira Ebrahimi and Michalski, Vincent and Materzyńska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and Hoppe, Florian and Thurau, Christian and Bax, Ingo and Memisevic, Roland},
	month = jun,
	year = {2017},
}

@article{weigert_isotropic_2017,
	title = {Isotropic reconstruction of {3D} fluorescence microscopy images using convolutional neural networks},
	url = {http://arxiv.org/abs/1704.01510},
	abstract = {Fluorescence microscopy images usually show severe anisotropy in axial versus lateral resolution. This hampers downstream processing, i.e. the automatic extraction of quantitative biological data. While deconvolution methods and other techniques to address this problem exist, they are either time consuming to apply or limited in their ability to remove anisotropy. We propose a method to recover isotropic resolution from readily acquired anisotropic data. We achieve this using a convolutional neural network that is trained end-to-end from the same anisotropic body of data we later apply the network to. The network effectively learns to restore the full isotropic resolution by restoring the image under a trained, sample specific image prior. We apply our method to \$3\$ synthetic and \$3\$ real datasets and show that our results improve on results from deconvolution and state-of-the-art super-resolution techniques. Finally, we demonstrate that a standard 3D segmentation pipeline performs on the output of our network with comparable accuracy as on the full isotropic data.},
	author = {Weigert, Martin and Royer, Loic and Jug, Florian and Myers, Gene},
	month = apr,
	year = {2017},
}

@article{xia_dual_nodate,
	title = {Dual {Supervised} {Learning}},
	abstract = {Many supervised learning tasks are emerged in dual forms, e.g., English-to-French translation vs. French-to-English translation, speech recog-nition vs. text to speech, and image classification vs. image generation. Two dual tasks have intrin-sic connections with each other due to the prob-abilistic correlation between their models. This connection is, however, not effectively utilized today, since people usually train the models of two dual tasks separately and independently. In this work, we propose training the models of two dual tasks simultaneously, and explicitly exploit-ing the probabilistic correlation between them to regularize the training process. For ease of ref-erence, we call the proposed approach dual su-pervised learning. We demonstrate that dual su-pervised learning can improve the practical per-formances of both tasks, for various applications including machine translation, image processing, and sentiment analysis.},
	author = {Xia, Yingce and Qin, Tao and Chen, Wei and Bian, Jiang and Yu, Nenghai and Liu, Tie-Yan},
}

@article{beltramelli_pix2code_2017,
	title = {pix2code: {Generating} {Code} from a {Graphical} {User} {Interface} {Screenshot}},
	url = {http://arxiv.org/abs/1705.07962},
	abstract = {Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites and mobile applications. In this paper, we show that Deep Learning techniques can be leveraged to automatically generate code given a graphical user interface screenshot as input. Our model is able to generate code targeting three different platforms (i.e. iOS, Android and web-based technologies) from a single input image with over 77\% of accuracy.},
	author = {Beltramelli, Tony},
	month = may,
	year = {2017},
}

@article{donovan_unbiased_2016,
	title = {Unbiased {Rare} {Event} {Sampling} in {Spatial} {Stochastic} {Systems} {Biology} {Models} {Using} a {Weighted} {Ensemble} of {Trajectories}.},
	volume = {12},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/26845334},
	doi = {10.1371/journal.pcbi.1004611},
	abstract = {The long-term goal of connecting scales in biological simulation can be facilitated by scale-agnostic methods. We demonstrate that the weighted ensemble (WE) strategy, initially developed for molecular simulations, applies effectively to spatially resolved cell-scale simulations. The WE approach runs an ensemble of parallel trajectories with assigned weights and uses a statistical resampling strategy of replicating and pruning trajectories to focus computational effort on difficult-to-sample regions. The method can also generate unbiased estimates of non-equilibrium and equilibrium observables, sometimes with significantly less aggregate computing time than would be possible using standard parallelization. Here, we use WE to orchestrate particle-based kinetic Monte Carlo simulations, which include spatial geometry (e.g., of organelles, plasma membrane) and biochemical interactions among mobile molecular species. We study a series of models exhibiting spatial, temporal and biochemical complexity and show that although WE has important limitations, it can achieve performance significantly exceeding standard parallel simulation--by orders of magnitude for some observables.},
	number = {2},
	journal = {PLoS computational biology},
	author = {Donovan, Rory M and Tapia, Jose-Juan and Sullivan, Devin P and Faeder, James R and Murphy, Robert F and Dittrich, Markus and Zuckerman, Daniel M},
	month = feb,
	year = {2016},
	note = {Publisher: Public Library of Science},
	pages = {e1004611--e1004611},
}

@inproceedings{kim_non-rigid_2007,
	title = {Non-rigid temporal registration of {2D} and {3D} multi-channel microscopy image sequences of human cells},
	isbn = {1-4244-0672-2},
	doi = {10.1109/ISBI.2007.357105},
	abstract = {The analysis of fluorescently tagged proteins in live cells from multi-channel microscopy image sequences requires a registration to a reference frame to decouple the movement and deformation of cells from the movement of proteins. We have developed an intensity-based approach to register 2D and 3D multi-channel microscopy image sequences. This approach directly exploits the image intensities. We have compared the results of our approach with results based on segmented images. Also, we have performed a comparison between a direct registration scheme to the reference frame and an incremental scheme taking into account results from preceding time steps. We have validated our approach based on 3D synthetic images of a simulated cell with known deformation which has been calculated based on an analytic solution of the Navier equation given certain boundary conditions. We have also successfully applied our approach to 2D and 3D real microscopy image sequences. © 2007 IEEE.},
	author = {Kim, I. and Yang, S. and Le Baccon, P. and Heard, E. and Chen, Y.-C. and Spector, D. and Kappel, C. and Eils, R. and Rohr, K.},
	year = {2007},
	keywords = {Biomedical microscopy, Image sequences, Non-rigid registration},
}

@inproceedings{kim_non-rigid_2007-1,
	title = {Non-rigid temporal alignment of {2D} and {3D} multi-channel microscopy image sequences of human cells},
	isbn = {3-540-71090-6},
	abstract = {The analysis of fluroescence tagged proteins in live cells from multi-channel microscopy image sequences requires a registration to a reference frame to decouple the movement and deformation of cells from the movement of proteins. We have developed an intensity-based approach for the registration of 2D and 3D multi-channel microscopy image sequences. This approach can be directly applied to the intensity images or to the segmented images. Also, we have performed a comparison using a direct registration scheme to the reference frame and an incremental scheme taking into account results from preceding time steps. We have evaluated our approach based on 3D synthetic images of a simulated spherical cell with known deformation which has been calculated based on an analytic solution of the Navier equation given certain boundary conditions. We have also successfully applied our approach to 2D and 3D real microscopy image sequences.},
	author = {Kim, I. and Yang, S. and Le Baccon, P. and Heard, E. and Kappel, C. and Eils, R. and Rohr, K.},
	year = {2007},
}

@article{saito_photorealistic_nodate,
	title = {Photorealistic {Facial} {Texture} {Inference} {Using} {Deep} {Neural} {Networks}},
	url = {https://arxiv.org/pdf/1612.00523v1.pdf},
	abstract = {input picture output albedo map rendering rendering (zoom) rendering (zoom) rendering Figure 1: We present an inference framework based on deep neural networks for synthesizing photorealistic facial texture along with 3D geometry from a single unconstrained image. We can successfully digitize historic figures that are no longer available for scanning and produce high-fidelity facial texture maps with mesoscopic skin details. Abstract We present a data-driven inference method that can syn-thesize a photorealistic texture map of a complete 3D face model given a partial 2D view of a person in the wild. After an initial estimation of shape and low-frequency albedo, we compute a high-frequency partial texture map, without the shading component, of the visible face area. To extract the fine appearance details from this incomplete input, we intro-duce a multi-scale detail analysis technique based on mid-layer feature correlations extracted from a deep convolu-tional neural network. We demonstrate that fitting a convex combination of feature correlations from a high-resolution face database can yield a semantically plausible facial de-tail description of the entire face. A complete and photo-realistic texture map can then be synthesized by iteratively optimizing for the reconstructed feature correlations. Using these high-resolution textures and a commercial rendering framework, we can produce high-fidelity 3D renderings that are visually comparable to those obtained with state-of-the-art multi-view face capture systems. We demonstrate suc-cessful face reconstructions from a wide range of low reso-lution input images, including those of historical figures. In addition to extensive evaluations, we validate the realism of our results using a crowdsourced user study. -indicates equal contribution},
	author = {Saito, Shunsuke and Wei, Lingyu and Hu, Liwen and Nagano, Koki and Li, Hao},
}

@article{wallach_atomnet_nodate,
	title = {{AtomNet}: {A} {Deep} {Convolutional} {Neural} {Network} for {Bioactivity} {Prediction} in {Structure}-based {Drug} {Discovery}},
	url = {https://arxiv.org/pdf/1510.02855.pdf},
	abstract = {Deep convolutional neural networks comprise a subclass of deep neural networks (DNN) with a constrained architecture that leverages the spatial and temporal structure of the domain they model. Convolutional networks achieve the best pre-dictive performance in areas such as speech and image recognition by hierarchi-cally composing simple local features into complex models. Although DNNs have been used in drug discovery for QSAR and ligand-based bioactivity predictions, none of these models have benefited from this powerful convolutional architec-ture. This paper introduces AtomNet, the first structure-based, deep convolutional neural network designed to predict the bioactivity of small molecules for drug dis-covery applications. We demonstrate how to apply the convolutional concepts of feature locality and hierarchical composition to the modeling of bioactivity and chemical interactions. In further contrast to existing DNN techniques, we show that AtomNet's application of local convolutional filters to structural target infor-mation successfully predicts new active molecules for targets with no previously known modulators. Finally, we show that AtomNet outperforms previous docking approaches on a diverse set of benchmarks by a large margin, achieving an AUC greater than 0.9 on 57.8\% of the targets in the DUDE benchmark.},
	author = {Wallach, Izhar and Dzamba, Michael and Heifets, Abraham},
	file = {Full Text:/home/zwerg/Zotero/storage/IQDFBKEF/Wallach et al. - AtomNet A Deep Convolutional Neural Network for B.pdf:application/pdf},
}

@article{leung_machine_2016,
	title = {Machine {Learning} in {Genomic} {Medicine}: {A} {Review} of {Computational} {Problems} and {Data} {Sets}},
	volume = {104},
	url = {http://ieeexplore.ieee.org/document/7347331/},
	doi = {10.1109/JPROC.2015.2494198},
	number = {1},
	journal = {Proceedings of the IEEE},
	author = {Leung, Michael K. K. and Delong, Andrew and Alipanahi, Babak and Frey, Brendan J.},
	month = jan,
	year = {2016},
	pages = {176--197},
}

@article{williams_image_nodate,
	title = {The {Image} {Data} {Resource}: {A} {Scalable} {Platform} for {Biological} {Image} {Data} {Access}, {Integration}, and {Dissemination}},
	url = {www.openmicroscopy.org},
	doi = {10.1101/089359},
	abstract = {Williams et al. p. 2 Abstract Access to primary research data is vital for the advancement of science. To extend the data types supported by community repositories, we built a prototype Image Data Resource (IDR) that collects and integrates imaging data acquired across many different imaging modalities. IDR links high-content screening, super-resolution microscopy, time-lapse and digital pathology imaging experiments to public genetic or chemical databases, and to cell and tissue phenotypes expressed using controlled ontologies. Using this integration, IDR facilitates the analysis of gene networks and reveals functional interactions that are inaccessible to individual studies. To enable re-analysis, we also established a computational resource based on IPython notebooks that allows remote access to the entire IDR. IDR is also an open source platform that others can use to publish their own image data. Thus IDR provides both a novel on-line resource and a software infrastructure that promotes and extends publication and re-analysis of scientific image data. Much of the published research in the life sciences is based on image datasets that sample 3D space, time, and the spectral characteristics of detected signal (e.g., photons, electrons, proton relaxation, etc) to provide quantitative measures of cell, tissue and organismal processes and structures. The sheer size of biological image datasets makes data submission, handling and publication extremely challenging. An image-based genome-wide " high-content " screen (HCS) may contain over a million images, and new " virtual slide " and " light sheet " tissue imaging technologies generate individual images that contain gigapixels of data showing tissues or whole organisms at subcellular resolutions. At the same time, published versions of image data often are mere illustrations: they are presented in processed, compressed formats that cannot convey the measurements and multiple dimensions contained in the original image data and that can no longer be easily subjected to re-analysis. Furthermore, conventional publications neither include the metadata that define imaging protocols, biological systems and perturbations nor the processing and analytic outputs that convert the image data into quantitative measurements.},
	number = {1},
	author = {Williams, Eleanor and Moore, Josh and Li, Simon W and Rustici, Gabriella and Tarkowska, Aleksandra and Chessel, Anatole and Leo, Simone and Antal, Bálint and Ferguson, Richard K and Sarkans, Ugis and Brazma, Alvis and Salas, Rafael E Carazo and Swedlow, Jason R},
}

@article{you_privileged_nodate,
	title = {{PRIVILEGED} {MULTI}-{LABEL} {LEARNING}},
	url = {https://arxiv.org/pdf/1701.07194.pdf},
	abstract = {This paper presents privileged multi-label learning (PrML) to explore and exploit the relationship between labels in multi-label learning problems. We suggest that for each individual label, it cannot only be implicitly connected with other labels via the low-rank constraint over label predictors, but also its performance on examples can receive the explicit comments from other labels together acting as an Oracle teacher. We generate privileged label feature for each example and its individual label, and then integrate it into the framework of low-rank based multi-label learning. The proposed algorithm can therefore comprehensively explore and exploit label relationships by inheriting all the merits of privileged information and low-rank constraints. We show that PrML can be efficiently solved by dual coordinate descent algorithm using iterative optimization strategy with cheap updates. Experiments on benchmark datasets show that through privileged label features, the performance can be significantly improved and PrML is superior to several competing methods in most cases.},
	author = {You, Shan and Xu, Chang and Wang, Yunhe and Xu, Chao and Tao, Dacheng},
}

@article{pathak_fully_nodate,
	title = {{FULLY} {CONVOLUTIONAL} {MULTI}-{CLASS} {MULTIPLE} {INSTANCE} {LEARNING}},
	url = {https://arxiv.org/pdf/1412.7144.pdf},
	abstract = {Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learn-ing by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge.},
	author = {Pathak, Deepak and Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
}

@inproceedings{gladilin_determination_2009,
	title = {Determination of {Material} {Properties} of {Cellular} {Structures} {Using} {Time} {Series} of {Microscopic} {Images} and {Numerical} {Model} of {Cell} {Mechanics}},
	volume = {23},
	isbn = {978-3-540-92840-9},
	doi = {10.1007/978-3-540-92841-6_378},
	abstract = {Determination of material constants is indispensable for quantitative description of mechanical properties of cellular structures. Since conventional methods of experimental cell mechanics are based on application of forces onto the cell boundary, intra-cellular structures are not accessible for direct measurements. In this work, we present an approach for estimation of canonic material constants (i.e. stiffness and compressibility) using times series of microscopic images and numerical model of cell mechanics. The proposed method is based on optical monitoring of the cellular deformation which can be triggered by any appropriate contacting or noncontacting technique affecting mechanical cell integrity. The presented experimental studies give examples of determination of material parameters of optically monitored cells whose deformation was induced by conventional micromanipulation and completely contact-free, using chemical agents.},
	author = {Gladilin, E. and Schulz, M. and Kappel, C. and Eils, R.},
	year = {2009},
	keywords = {image analysis, cell mechanics, contact-free measuring techniques, material parameter estimation, numerical modeling},
}

@article{gladilin_contactless_2010,
	title = {Contactless determination of nuclear compressibility using {3D} image- and model-based analysis of drug-induced cellular deformation},
	volume = {240},
	doi = {10.1111/j.1365-2818.2010.03394.x},
	abstract = {Mechanical properties of the chromatin-bearing nucleus in normal and pathological cells are of general interest for epigenetics and medicine. Conventional techniques for quantitative measurements of material properties of cellular matter are based on application of controlled forces onto the cellular or nuclear boundary and do not allow probing intracellular structures that are not directly accessible for physical contact inside the living cell. In this work, we present a novel approach for contactless determination of the nuclear compressibility (i.e. the Poisson's ratio ν) in living cells by means of image- and model-based analysis of drug-induced cell deformation. The Poisson's ratio of the HeLa cell nucleus is determined from time-series of 3D images as a parameter of constitutive model that minimizes the dissimilarity between the numerically predicted and experimentally observed images. © 2010 The Authors Journal of Microscopy © 2010 The Royal Microscopical Society.},
	number = {3},
	journal = {Journal of Microscopy},
	author = {Gladilin, E. and Schulz, M. and Kappel, C. and Eils, R.},
	year = {2010},
	keywords = {3D image analysis, Cell nucleus, Compressibility, Confocal scanning laser microscopy, Contactless deformation induction, Finite element method, Parameter estimation},
}

@article{read_scalable_2012,
	title = {Scalable and efficient multi-label classification for evolving data streams},
	volume = {88},
	url = {http://download.springer.com/static/pdf/467/art%253A10.1007%252Fs10994-012-5279-6.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10994-012-5279-6&token2=exp=1496526895~acl=%2Fstatic%2Fpdf%2F467%2Fart%25253A10.1007%25252Fs10994-012-5279-6.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1007%252Fs10994-012-5279-6*~hmac=847a960c8207e6c1ead5c629a129584f10b7b170ef744331c5ea901e22541e9e},
	doi = {10.1007/s10994-012-5279-6},
	abstract = {Many challenging real world problems involve multi-label data streams. Efficient methods exist for multi-label classification in non-streaming scenarios. However, learning in evolving streaming scenarios is more challenging, as classifiers must be able to deal with huge numbers of examples and to adapt to change using limited time and memory while being ready to predict at any point. This paper proposes a new experimental framework for learning and evaluating on multi-label data streams, and uses it to study the performance of various methods. From this study, we develop a multi-label Hoeffding tree with multi-label classifiers at the leaves. We show empirically that this method is well suited to this challenging task. Using our new frame-work, which allows us to generate realistic multi-label data streams with concept drift (as well as real data), we compare with a selection of baseline methods, as well as new learn-ing methods from the literature, and show that our Hoeffding tree method achieves fast and more accurate performance.},
	number = {88},
	journal = {Mach Learn},
	author = {Read, Jesse and Bifet, Albert and Holmes, Geoff and Pfahringer, Bernhard and Tsoumakas, Grigorios and Zhang, Min-Ling and Zhou Read, Zhi-Hua J and Read, J and Bifet, A and Holmes, G and Pfahringer, B},
	year = {2012},
	keywords = {Data streams classification, Multi-label classification ·},
}

@inproceedings{worz_model-based_2007,
	title = {Model-based segmentation and quantification of fluorescent bacteria in {3D} microscopy live cell images},
	volume = {6512},
	isbn = {0-8194-6630-1},
	doi = {10.1117/12.709825},
	abstract = {We introduce a new model-based approach for segmenting and quantifying fluorescent bacteria in 3D microscopy live cell images. The approach is based on a new 3D superellipsoidal parametric intensity model, which is directly fitted to the image intensities within 3D regions-of-interest.Based on the fitting results, we can directly compute the total amount of intensity (fluorescence) of each cell. In addition, we introduce a method for automatic initialization of the model parameters, and we propose a method for simultaneously fitting clustered cells by using a superposition of 3D superellipsoids for model fitting. We demonstrate the applicability of our approach based on 3D synthetic and real 3D microscopy images.},
	author = {Wörz, S. and Kappel, C. and Eils, R. and Rohr, K.},
	year = {2007},
	note = {Issue: PART 3},
	keywords = {3D microscopy cell images, 3D parametric intensity model, Model fitting, Segmentation},
}

@inproceedings{gladilin_motion_2007,
	title = {Motion detection and pattern tracking in microscopical images using phase correlation approach},
	volume = {6512},
	isbn = {0-8194-6630-1},
	doi = {10.1117/12.698141},
	abstract = {High-throughput live-cell imaging is one of the important tools for the investigation of cellular structure and functions in modern experimental biology. Automatic processing of time series of microscopic images is hampered by a number of technical and natural factors such as permanent movements of cells in the optical field, alteration of optical cell appearance and high level of noise. Detection and compensation of global motion of groups of cells or relocation of a single cell within a dynamical multi-cell environment is the first indispensable step in the image analysis chain. This article presents an approach for detection of global image motion and single cell tracking in time series of confocal laser scanning microscopy images using an extended Fourier-phase correlation technique, which allows for analysis of non-uniform multi-body motion in partially-similar images. Our experimental results have shown that the developed approach is capable to perform cell tracking and registration in dynamical and noisy scenes, and provides a robust tool for fully-automatic registration of time-series of microscopic images.},
	author = {Gladilin, E. and Kappel, C. and Eils, R.},
	year = {2007},
	note = {Issue: PART 2},
	keywords = {Affine registration, FFT, Fourier-phase correlation, Large displacements, Motion detection, Pattern tracking},
}

@article{li_improving_2017,
	title = {Improving {Pairwise} {Ranking} for {Multi}-label {Image} {Classification}},
	url = {http://arxiv.org/abs/1704.03135},
	abstract = {Learning to rank has recently emerged as an attractive technique to train deep convolutional neural networks for various computer vision tasks. Pairwise ranking, in particular, has been successful in multi-label image classification, achieving state-of-the-art results on various benchmarks. However, most existing approaches use the hinge loss to train their models, which is non-smooth and thus is difficult to optimize especially with deep networks. Furthermore, they employ simple heuristics, such as top-k or thresholding, to determine which labels to include in the output from a ranked list of labels, which limits their use in the real-world setting. In this work, we propose two techniques to improve pairwise ranking based multi-label image classification: (1) we propose a novel loss function for pairwise ranking, which is smooth everywhere and thus is easier to optimize; and (2) we incorporate a label decision module into the model, estimating the optimal confidence thresholds for each visual concept. We provide theoretical analyses of our loss function in the Bayes consistency and risk minimization framework, and show its benefit over existing pairwise ranking formulations. We demonstrate the effectiveness of our approach on three large-scale datasets, VOC2007, NUS-WIDE and MS-COCO, achieving the best reported results in the literature.},
	author = {Li, Yuncheng and Song, Yale and Luo, Jiebo},
	month = apr,
	year = {2017},
}

@article{chen_learning_2017,
	title = {Learning with {Privileged} {Information} for {Multi}-{Label} {Classification}},
	url = {http://arxiv.org/abs/1703.09911},
	abstract = {In this paper, we propose a novel approach for learning multi-label classifiers with the help of privileged information. Specifically, we use similarity constraints to capture the relationship between available information and privileged information, and use ranking constraints to capture the dependencies among multiple labels. By integrating similarity constraints and ranking constraints into the learning process of classifiers, the privileged information and the dependencies among multiple labels are exploited to construct better classifiers during training. A maximum margin classifier is adopted, and an efficient learning algorithm of the proposed method is also developed. We evaluate the proposed method on two applications: multiple object recognition from images with the help of implicit information about object importance conveyed by the list of manually annotated image tags; and multiple facial action unit detection from low-resolution images augmented by high-resolution images. Experimental results demonstrate that the proposed method can effectively take full advantage of privileged information and dependencies among multiple labels for better object recognition and better facial action unit detection.},
	author = {Chen, Shiyu and Wang, Shangfei and Chen, Tanfang and Shi, Xiaoxiao},
	month = mar,
	year = {2017},
}

@article{gygli_deep_2017,
	title = {Deep {Value} {Networks} {Learn} to {Evaluate} and {Iteratively} {Refine} {Structured} {Outputs}},
	url = {http://arxiv.org/abs/1703.04363},
	abstract = {We approach structured output prediction by learning a deep value network (DVN) that evaluates different output structures for a given input. For example, when applied to image segmentation, the value network takes an image and a segmentation mask as inputs and predicts a scalar score evaluating the mask quality and its correspondence with the image. Once the value network is optimized, at inference, it finds output structures that maximize the score of the value net via gradient descent on continuous relaxations of structured outputs. Thus DVN takes advantage of the joint modeling of the inputs and outputs. Our framework applies to a wide range of structured output prediction problems. We conduct experiments on multi-label classification based on text data and on image segmentation problems. DVN outperforms several strong baselines and the state-of-the-art results on these benchmarks. In addition, on image segmentation, the proposed deep value network learns complex shape priors and effectively combines image information with the prior to obtain competitive segmentation results.},
	author = {Gygli, Michael and Norouzi, Mohammad and Angelova, Anelia},
	month = mar,
	year = {2017},
}

@article{murthy_center-focusing_2016,
	title = {Center-{Focusing} {Multi}-task {CNN} with {Injected} {Features} for {Classification} of {Glioma} {Nuclear} {Images}},
	url = {http://arxiv.org/abs/1612.06825},
	abstract = {Classifying the various shapes and attributes of a glioma cell nucleus is crucial for diagnosis and understanding the disease. We investigate automated classification of glioma nuclear shapes and visual attributes using Convolutional Neural Networks (CNNs) on pathology images of automatically segmented nuclei. We propose three methods that improve the performance of a previously-developed semi-supervised CNN. First, we propose a method that allows the CNN to focus on the most important part of an image- the image's center containing the nucleus. Second, we inject (concatenate) pre-extracted VGG features into an intermediate layer of our Semi-Supervised CNN so that during training, the CNN can learn a set of complementary features. Third, we separate the losses of the two groups of target classes (nuclear shapes and attributes) into a single-label loss and a multi-label loss so that the prior knowledge of inter-label exclusiveness can be incorporated. On a dataset of 2078 images, the proposed methods combined reduce the error rate of attribute and shape classification by 21.54\% and 15.07\% respectively compared to the existing state-of-the-art method on the same dataset.},
	author = {Murthy, Veda and Hou, Le and Samaras, Dimitris and Kurc, Tahsin M. and Saltz, Joel H.},
	month = dec,
	year = {2016},
}

@article{zhang_multi-label_2016,
	title = {Multi-{Label} {Image} {Classification} with {Regional} {Latent} {Semantic} {Dependencies}},
	url = {http://arxiv.org/abs/1612.01082},
	abstract = {Deep convolution neural networks (CNN) have demonstrated advanced performance on single-label image classification, and various progress also have been made to apply CNN methods on multi-label image classification, which requires to annotate objects, attributes, scene categories etc. in a single shot. Recent state-of-the-art approaches to multi-label image classification exploit the label dependencies in an image, at global level, largely improving the labeling capacity. However, predicting small objects and visual concepts is still challenging due to the limited discrimination of the global visual features. In this paper, we propose a Regional Latent Semantic Dependencies model (RLSD) to address this problem. The utilized model includes a fully convolutional localization architecture to localize the regions that may contain multiple highly-dependent labels. The localized regions are further sent to the recurrent neural networks (RNN) to characterize the latent semantic dependencies at the regional level. Experimental results on several benchmark datasets show that our proposed model achieves the best performance compared to the state-of-the-art models, especially for predicting small objects occurred in the images. In addition, we set up an upper bound model (RLSD+ft-RPN) using bounding box coordinates during training, the experimental results also show that our RLSD can approach the upper bound without using the bounding-box annotations, which is more realistic in the real world.},
	author = {Zhang, Junjie and Wu, Qi and Shen, Chunhua and Zhang, Jian and Lu, Jianfeng},
	month = dec,
	year = {2016},
}

@article{gong_deep_2013,
	title = {Deep {Convolutional} {Ranking} for {Multilabel} {Image} {Annotation}},
	url = {http://arxiv.org/abs/1312.4894},
	abstract = {Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, features based on Deep Neural Networks have shown potential to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key components that lead to better performances. Specifically, we show that a significant performance gain could be obtained by combining convolutional architectures with approximate top-\$k\$ ranking objectives, as thye naturally fit the multilabel tagging problem. Our experiments on the NUS-WIDE dataset outperforms the conventional visual features by about 10\%, obtaining the best reported performance in the literature.},
	author = {Gong, Yunchao and Jia, Yangqing and Leung, Thomas and Toshev, Alexander and Ioffe, Sergey},
	month = dec,
	year = {2013},
}

@article{zhuang_fast_2016,
	title = {Fast {Training} of {Triplet}-based {Deep} {Binary} {Embedding} {Networks}},
	url = {http://arxiv.org/abs/1603.02844},
	abstract = {In this paper, we aim to learn a mapping (or embedding) from images to a compact binary space in which Hamming distances correspond to a ranking measure for the image retrieval task. We make use of a triplet loss because this has been shown to be most effective for ranking problems. However, training in previous works can be prohibitively expensive due to the fact that optimization is directly performed on the triplet space, where the number of possible triplets for training is cubic in the number of training examples. To address this issue, we propose to formulate high-order binary codes learning as a multi-label classification problem by explicitly separating learning into two interleaved stages. To solve the first stage, we design a large-scale high-order binary codes inference algorithm to reduce the high-order objective to a standard binary quadratic problem such that graph cuts can be used to efficiently infer the binary code which serve as the label of each training datum. In the second stage we propose to map the original image to compact binary codes via carefully designed deep convolutional neural networks (CNNs) and the hashing function fitting can be solved by training binary CNN classifiers. An incremental/interleaved optimization strategy is proffered to ensure that these two steps are interactive with each other during training for better accuracy. We conduct experiments on several benchmark datasets, which demonstrate both improved training time (by as much as two orders of magnitude) as well as producing state-of-the-art hashing for various retrieval tasks.},
	author = {Zhuang, Bohan and Lin, Guosheng and Shen, Chunhua and Reid, Ian},
	month = mar,
	year = {2016},
}

@article{sandouk_multi-label_2016,
	title = {Multi-{Label} {Zero}-{Shot} {Learning} via {Concept} {Embedding}},
	url = {http://arxiv.org/abs/1606.00282},
	abstract = {Zero Shot Learning (ZSL) enables a learning model to classify instances of an unseen class during training. While most research in ZSL focuses on single-label classification, few studies have been done in multi-label ZSL, where an instance is associated with a set of labels simultaneously, due to the difficulty in modeling complex semantics conveyed by a set of labels. In this paper, we propose a novel approach to multi-label ZSL via concept embedding learned from collections of public users' annotations of multimedia. Thanks to concept embedding, multi-label ZSL can be done by efficiently mapping an instance input features onto the concept embedding space in a similar manner used in single-label ZSL. Moreover, our semantic learning model is capable of embedding an out-of-vocabulary label by inferring its meaning from its co-occurring labels. Thus, our approach allows both seen and unseen labels during the concept embedding learning to be used in the aforementioned instance mapping, which makes multi-label ZSL more flexible and suitable for real applications. Experimental results of multi-label ZSL on images and music tracks suggest that our approach outperforms a state-of-the-art multi-label ZSL model and can deal with a scenario involving out-of-vocabulary labels without re-training the semantics learning model.},
	author = {Sandouk, Ubai and Chen, Ke},
	month = jun,
	year = {2016},
}

@article{wei_cnn_nodate,
	title = {{CNN}: {Single}-label to {Multi}-label},
	url = {https://arxiv.org/pdf/1406.5726.pdf},
	abstract = {—Convolutional Neural Network (CNN) has demonstrated promising performance in single-label image classification tasks. However, how CNN best copes with multi-label images still remains an open problem, mainly due to the complex underlying object layouts and insufficient multi-label training images. In this work, we propose a flexible deep CNN infrastructure, called Hypotheses-CNN-Pooling (HCP), where an arbitrary number of object segment hypotheses are taken as the inputs, then a shared CNN is connected with each hypothesis, and finally the CNN output results from different hypotheses are aggregated with max pooling to produce the ultimate multi-label predictions. Some unique characteristics of this flexible deep CNN infrastructure include: 1) no ground-truth bounding box information is required for training; 2) the whole HCP infrastructure is robust to possibly noisy and/or redundant hypotheses; 3) no explicit hypothesis label is required; 4) the shared CNN may be well pre-trained with a large-scale single-label image dataset, e.g. ImageNet; and 5) it may naturally output multi-label prediction results. Experimental results on Pascal VOC2007 and VOC2012 multi-label image datasets well demonstrate the superiority of the proposed HCP infrastructure over other state-of-the-arts. In particular, the mAP reaches 84.2\% by HCP only and 90.3\% after the fusion with our complementary result in [47] based on hand-crafted features on the VOC2012 dataset, which significantly outperforms the state-of-the-arts with a large margin of more than 7\%.},
	author = {Wei, Yunchao and Xia, Wei and Huang, Junshi and Ni, Bingbing and Dong, Jian and Zhao, Yao and Yan, Shuicheng},
	keywords = {CNN, Index Terms—Deep Learning, Multi-label Classification},
}

@article{bouma_normalized_nodate,
	title = {Normalized ({Pointwise}) {Mutual} {Information} in {Collocation} {Extraction}},
	url = {https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/npmi-pfd.pdf},
	abstract = {In this paper, we discuss the related information theoreti-cal association measures of mutual information and pointwise mutual information, in the context of collocation extraction. We introduce nor-malized variants of these measures in order to make them more easily interpretable and at the same time less sensitive to occurrence frequency. We also provide a small empirical study to give more insight into the be-haviour of these new measures in a collocation extraction setup.},
	author = {Bouma, Gerlof},
}

@article{ren_multi-instance_2015,
	title = {Multi-{Instance} {Visual}-{Semantic} {Embedding}},
	url = {http://arxiv.org/abs/1512.06963},
	abstract = {Visual-semantic embedding models have been recently proposed and shown to be effective for image classification and zero-shot learning, by mapping images into a continuous semantic label space. Although several approaches have been proposed for single-label embedding tasks, handling images with multiple labels (which is a more general setting) still remains an open problem, mainly due to the complex underlying corresponding relationship between image and its labels. In this work, we present Multi-Instance visual-semantic Embedding model (MIE) for embedding images associated with either single or multiple labels. Our model discovers and maps semantically-meaningful image subregions to their corresponding labels. And we demonstrate the superiority of our method over the state-of-the-art on two tasks, including multi-label image annotation and zero-shot learning.},
	author = {Ren, Zhou and Jin, Hailin and Lin, Zhe and Fang, Chen and Yuille, Alan},
	month = dec,
	year = {2015},
}

@article{wang_cnn-rnn_2016,
	title = {{CNN}-{RNN}: {A} {Unified} {Framework} for {Multi}-label {Image} {Classification}},
	url = {http://arxiv.org/abs/1604.04573},
	abstract = {While deep convolutional neural networks (CNNs) have shown a great success in single-label image classification, it is important to note that real world images generally contain multiple labels, which could correspond to different objects, scenes, actions and attributes in an image. Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results. These techniques, although working well, fail to explicitly exploit the label dependencies in an image. In this paper, we utilize recurrent neural networks (RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNN framework learns a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance, and it can be trained end-to-end from scratch to integrate both information in a unified framework. Experimental results on public benchmark datasets demonstrate that the proposed architecture achieves better performance than the state-of-the-art multi-label classification model},
	author = {Wang, Jiang and Yang, Yi and Mao, Junhua and Huang, Zhiheng and Huang, Chang and Xu, Wei},
	month = apr,
	year = {2016},
}

@article{kraus_classifying_2016,
	title = {Classifying and segmenting microscopy images with deep multiple instance learning},
	volume = {32},
	url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btw252},
	doi = {10.1093/bioinformatics/btw252},
	number = {12},
	journal = {Bioinformatics},
	author = {Kraus, Oren Z. and Ba, Jimmy Lei and Frey, Brendan J.},
	month = jun,
	year = {2016},
	note = {Publisher: MIT Press, Cambridge,},
	pages = {i52--i59},
}

@article{chollet_information-theoretical_2016,
	title = {Information-theoretical label embeddings for large-scale image classification},
	url = {https://arxiv.org/pdf/1607.05691.pdf},
	abstract = {We present a method for training multi-label, massively multi-class im-age classification models, that is faster and more accurate than supervision via a sigmoid cross-entropy loss (logistic regression). Our method consists in embedding high-dimensional sparse labels onto a lower-dimensional dense sphere of unit-normed vectors, and treating the classification prob-lem as a cosine proximity regression problem on this sphere. We test our method on a dataset of 300 million high-resolution images with 17,000 labels, where it yields considerably faster convergence, as well as a 7\% higher mean average precision compared to logistic regression.},
	author = {Chollet, François},
	year = {2016},
}

@article{munck_sub-diffraction_nodate,
	title = {Sub-diffraction imaging on standard microscopes through photobleaching microscopy with non- linear processing},
	volume = {125},
	url = {http://jcs.biologists.org/content/joces/125/9/2257.full.pdf?with-ds=yes},
	doi = {10.1242/jcs.098939},
	abstract = {Visualization of organelles and molecules at nanometer resolution is revolutionizing the biological sciences. However, such technology is still limited for many cell biologists. We present here a novel approach using photobleaching microscopy with non-linear processing (PiMP) for sub-diffraction imaging. Bleaching of fluorophores both within the single-molecule regime and beyond allows visualization of stochastic representations of sub-populations of fluorophores by imaging the same region over time. Our method is based on enhancing the probable positions of the fluorophores underlying the images. The random nature of the bleached fluorophores is assessed by calculating the deviation of the local actual bleached fluorescence intensity to the average bleach expectation as given by the overall decay of intensity. Subtracting measured from estimated decay images yields differential images. Non-linear enhancement of maxima in these diffraction-limited differential images approximates the positions of the underlying structure. Summing many such processed differential images yields a super-resolution PiMP image. PiMP allows multi-color, three-dimensional sub-diffraction imaging of cells and tissues using common fluorophores and can be implemented on standard wide-field or confocal systems. Introduction Numerous super-resolution imaging technologies have recently emerged, including stimulated emission depletion (STED), photo-activated localization microscopy (PALM) and stochastic optical reconstruction microscopy (STORM) (Hell and Wichmann, 1994; Gustafsson et al., 1999; Gustafsson, 2000; Heintzmann et al., 2002; Betzig et al., 2006; Hess et al., 2006; Rust et al., 2006; Fölling et al., 2008; Heilemann et al., 2008; Dertinger et al., 2009; Gong et al., 2010). These technologies allow for the accurate imaging of labeled molecules by separating their emission in space or time, enabling researchers to bypass the resolution limit of conventional light microscopy (Abbe, 1873). Given the impact on biological research, super-resolution technology has gained great attention; however, assembling a super-resolution device or acquiring an integrated solution can be expensive and custom-built systems still outperform the commercial ones. Moreover, the aforementioned strategies are not applicable without additional hardware and/or imaging sensitivity. A different strategy that allows easy accessible super-resolution imaging is thus a very attractive alternative. Here we describe a novel approach to achieve sub-diffraction imaging beyond the point where emitters are in such close proximity that they form a single indiscernible diffraction pattern as described by the Abbe limit (Abbe, 1873). We exploit stochastic processes to generate multiple consecutive sub-images that can be reconstructed to yield a significantly improved image at sub-diffraction resolution. More specifically, we use bleaching to image sub-populations of fluorophores, hence the name 'photobleaching microscopy with non-linear processing' or PiMP. Pointillistic approaches (such as PALM and STORM) exploit the principle of decomposing the object over multiple images, but require only a small sub-population of isolated single molecules to be present per frame. In contrast to these, we analyze the positions at which the bleaching deviates from the expected (average) bleaching per frame. Such analyses are possible because the number of bleached fluorophores on densely labeled neighboring structures (that might be located at a distance below the diffraction limit) is in most cases not equal owing to the stochastic nature of bleaching. This stochastic effect can be used beyond the single-molecule regime to approximate probable fluorophore locations in the underlying structure. Hence, the combination of extracting and enhancing spatial information of sub-populations of fluorophores from differential images can yield a sub-diffraction image revealing information otherwise concealed by diffraction. Biological samples usually exhibit a redundancy of labeling with respect to the underlying structural information},
	journal = {Journal of Cell Science},
	author = {Munck, Sebastian and Miskiewicz, Katarzyna and Sannerud, Ragna and Menchon, Silvia A and Jose, Liya and Heintzmann, Rainer and Verstreken, Patrik and Annaert, Wim},
	keywords = {Bleaching, Method, PiMP, Super-resolution microscopy},
	pages = {2257--2266},
}

@article{sermanet_attention_2014,
	title = {Attention for {Fine}-{Grained} {Categorization}},
	url = {http://arxiv.org/abs/1412.7054},
	abstract = {This paper presents experiments extending the work of Ba et al. (2014) on recurrent neural models for attention into less constrained visual environments, specifically fine-grained categorization on the Stanford Dogs data set. In this work we use an RNN of the same structure but substitute a more powerful visual network and perform large-scale pre-training of the visual network outside of the attention RNN. Most work in attention models to date focuses on tasks with toy or more constrained visual environments, whereas we present results for fine-grained categorization better than the state-of-the-art GoogLeNet classification model. We show that our model learns to direct high resolution attention to the most discriminative regions without any spatial supervision such as bounding boxes, and it is able to discriminate fine-grained dog breeds moderately well even when given only an initial low-resolution context image and narrow, inexpensive glimpses at faces and fur patterns. This and similar attention models have the major advantage of being trained end-to-end, as opposed to other current detection and recognition pipelines with hand-engineered components where information is lost. While our model is state-of-the-art, further work is needed to fully leverage the sequential input.},
	author = {Sermanet, Pierre and Frome, Andrea and Real, Esteban},
	month = dec,
	year = {2014},
}

@article{freedman_image_2009,
	title = {Image and {Video} {Upscaling} from {Local} {Self}-{Examples}},
	volume = {11},
	url = {http://doi.acm.org/10.1145/XXXXXXX.YYYYYYY},
	doi = {10.1145/1559755.1559763},
	abstract = {We propose a new high-quality and efficient single-image upscaling tech-nique that extends existing example-based super-resolution frameworks. In our approach we do not rely on an external example database or use the whole input image as a source for example patches. Instead, we follow a local self-similarity assumption on natural images and extract patches from extremely localized regions in the input image. This allows us to reduce considerably the nearest-patch search time without compromising quality in most images. Tests, that we perform and report, show that the local-self similarity assumption holds better for small scaling factors where there are more example patches of greater relevance. We implement these small scalings using dedicated novel non-dyadic filter banks, that we de-rive based on principles that model the upscaling process. Moreover, the new filters are nearly-biorthogonal and hence produce high-resolution im-ages that are highly consistent with the input image without solving implicit back-projection equations. The local and explicit nature of our algorithm makes it simple, efficient and allows a trivial parallel implementation on a GPU. We demonstrate the new method ability to produce high-quality reso-lution enhancement, its application to video sequences with no algorithmic modification, and its efficiency to perform real-time enhancement of low-resolution video standard into recent high-definition formats.},
	author = {Freedman, Gilad and Fattal, Raanan},
	year = {2009},
	keywords = {super-resolution, Categories and Subject Descriptors, Enhancement—Image upscaling General Terms, I35 [Computer Graphics], I42 [Image Processing and Computer Vision], image and video upscaling, Image Upscaling Additional Key Words and Phrases, natural image modeling, non-dyadic filter banks, Pic-ture/Image Generation—Viewing algorithms, scale-invariance, wavelets},
}

@article{ulman_objective_2017,
	title = {An objective comparison of cell-tracking algorithms},
	volume = {14},
	url = {http://www.nature.com/doifinder/10.1038/nmeth.4473},
	doi = {10.1038/nmeth.4473},
	abstract = {This analysis describes the results of three Cell Tracking Challenge editions for examining the performance of cell segmentation and tracking algorithms and provides practical feedback for users and developers.},
	number = {12},
	journal = {Nature Methods},
	author = {Ulman, Vladimír and Maška, Martin and Magnusson, Klas E G and Ronneberger, Olaf and Haubold, Carsten and Harder, Nathalie and Matula, Pavel and Matula, Petr and Svoboda, David and Radojevic, Miroslav and Smal, Ihor and Rohr, Karl and Jaldén, Joakim and Blau, Helen M and Dzyubachyk, Oleh and Lelieveldt, Boudewijn and Xiao, Pengdong and Li, Yuexiang and Cho, Siu-Yeung and Dufour, Alexandre C and Olivo-Marin, Jean-Christophe and Reyes-Aldasoro, Constantino C and Solis-Lemus, Jose A and Bensch, Robert and Brox, Thomas and Stegmaier, Johannes and Mikut, Ralf and Wolf, Steffen and Hamprecht, Fred A and Esteves, Tiago and Quelhas, Pedro and Demirel, Ömer and Malmström, Lars and Jug, Florian and Tomancak, Pavel and Meijering, Erik and Muñoz-Barrutia, Arrate and Kozubek, Michal and Ortiz-de-Solorzano, Carlos},
	month = oct,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {Image processing, Cell migration},
	pages = {1141--1152},
}

@article{tschopp_connectome_2018,
	title = {A {Connectome} {Based} {Hexagonal} {Lattice} {Convolutional} {Network} {Model} of the {Drosophila} {Visual} {System}},
	url = {http://arxiv.org/abs/1806.04793},
	abstract = {What can we learn from a connectome? We constructed a simplified model of the first two stages of the fly visual system, the lamina and medulla. The resulting hexagonal lattice convolutional network was trained using backpropagation through time to perform object tracking in natural scene videos. Networks initialized with weights from connectome reconstructions automatically discovered well-known orientation and direction selectivity properties in T4 neurons and their inputs, while networks initialized at random did not. Our work is the first demonstration, that knowledge of the connectome can enable in silico predictions of the functional properties of individual neurons in a circuit, leading to an understanding of circuit function from structure alone.},
	author = {Tschopp, Fabian David and Reiser, Michael B. and Turaga, Srinivas C.},
	month = jun,
	year = {2018},
}

@article{leclerc_smallify_2018,
	title = {Smallify: {Learning} {Network} {Size} while {Training}},
	url = {http://arxiv.org/abs/1806.03723},
	abstract = {As neural networks become widely deployed in different applications and on different hardware, it has become increasingly important to optimize inference time and model size along with model accuracy. Most current techniques optimize model size, model accuracy and inference time in different stages, resulting in suboptimal results and computational inefficiency. In this work, we propose a new technique called Smallify that optimizes all three of these metrics at the same time. Specifically we present a new method to simultaneously optimize network size and model performance by neuron-level pruning during training. Neuron-level pruning not only produces much smaller networks but also produces dense weight matrices that are amenable to efficient inference. By applying our technique to convolutional as well as fully connected models, we show that Smallify can reduce network size by 35X with a 6X improvement in inference time with similar accuracy as models found by traditional training techniques.},
	author = {Leclerc, Guillaume and Vartak, Manasi and Fernandez, Raul Castro and Kraska, Tim and Madden, Samuel},
	month = jun,
	year = {2018},
}

@article{aitken_checkerboard_2017,
	title = {Checkerboard artifact free sub-pixel convolution: {A} note on sub-pixel convolution, resize convolution and convolution resize},
	url = {http://arxiv.org/abs/1707.02937},
	abstract = {The most prominent problem associated with the deconvolution layer is the presence of checkerboard artifacts in output images and dense labels. To combat this problem, smoothness constraints, post processing and different architecture designs have been proposed. Odena et al. highlight three sources of checkerboard artifacts: deconvolution overlap, random initialization and loss functions. In this note, we proposed an initialization method for sub-pixel convolution known as convolution NN resize. Compared to sub-pixel convolution initialized with schemes designed for standard convolution kernels, it is free from checkerboard artifacts immediately after initialization. Compared to resize convolution, at the same computational complexity, it has more modelling power and converges to solutions with smaller test errors.},
	author = {Aitken, Andrew and Ledig, Christian and Theis, Lucas and Caballero, Jose and Wang, Zehan and Shi, Wenzhe},
	month = jul,
	year = {2017},
}

@article{guerrero-pena_multiclass_2018,
	title = {Multiclass {Weighted} {Loss} for {Instance} {Segmentation} of {Cluttered} {Cells}},
	url = {http://arxiv.org/abs/1802.07465},
	abstract = {We propose a new multiclass weighted loss function for instance segmentation of cluttered cells. We are primarily motivated by the need of developmental biologists to quantify and model the behavior of blood T-cells which might help us in understanding their regulation mechanisms and ultimately help researchers in their quest for developing an effective immuno-therapy cancer treatment. Segmenting individual touching cells in cluttered regions is challenging as the feature distribution on shared borders and cell foreground are similar thus difficulting discriminating pixels into proper classes. We present two novel weight maps applied to the weighted cross entropy loss function which take into account both class imbalance and cell geometry. Binary ground truth training data is augmented so the learning model can handle not only foreground and background but also a third touching class. This framework allows training using U-Net. Experiments with our formulations have shown superior results when compared to other similar schemes, outperforming binary class models with significant improvement of boundary adequacy and instance detection. We validate our results on manually annotated microscope images of T-cells.},
	author = {Guerrero-Pena, Fidel A. and Fernandez, Pedro D. Marrero and Ren, Tsang Ing and Yui, Mary and Rothenberg, Ellen and Cunha, Alexandre},
	month = feb,
	year = {2018},
}

@article{izmailov_averaging_2018,
	title = {Averaging {Weights} {Leads} to {Wider} {Optima} and {Better} {Generalization}},
	url = {http://arxiv.org/abs/1803.05407},
	abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
	author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
	month = mar,
	year = {2018},
}

@article{shi_real-time_2016-1,
	title = {Real-{Time} {Single} {Image} and {Video} {Super}-{Resolution} {Using} an {Efficient} {Sub}-{Pixel} {Convolutional} {Neural} {Network}},
	url = {http://arxiv.org/abs/1609.05158},
	abstract = {Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.},
	author = {Shi, Wenzhe and Caballero, Jose and Huszár, Ferenc and Totz, Johannes and Aitken, Andrew P. and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
	month = sep,
	year = {2016},
}

@article{sun_pwc-net_nodate,
	title = {{PWC}-{Net}: {CNNs} for {Optical} {Flow} {Using} {Pyramid}, {Warping}, and {Cost} {Volume}},
	url = {http://research.nvidia.com/sites/default/files/pubs/2018-02_PWC-Net%3A-CNNs-for/PWC_Net_0.pdf},
	abstract = {We present a compact but effective CNN model for op-tical flow, called PWC-Net. PWC-Net has been designed according to simple and well-established principles: pyra-midal processing, warping, and the use of a cost volume. Cast in a learnable feature pyramid, PWC-Net uses the cur-rent optical flow estimate to warp the CNN features of the second image. It then uses the warped features and fea-tures of the first image to construct a cost volume, which is processed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller in size and easier to train than the recent FlowNet2 model. Moreover, it outperforms all pub-lished optical flow methods on the MPI Sintel final pass and KITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024×436) images. Our models are available on our project website.},
	author = {Sun, Deqing and Yang, Xiaodong and Liu, Ming-Yu and Kautz NVIDIA, Jan},
}

@article{ilg_flownet_2016,
	title = {{FlowNet} 2.0: {Evolution} of {Optical} {Flow} {Estimation} with {Deep} {Networks}},
	url = {http://arxiv.org/abs/1612.01925},
	abstract = {The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a sub-network specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50\%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.},
	author = {Ilg, Eddy and Mayer, Nikolaus and Saikia, Tonmoy and Keuper, Margret and Dosovitskiy, Alexey and Brox, Thomas},
	month = dec,
	year = {2016},
}

@article{braszus_swr2_nodate,
	title = {{SWR2} {MANUSKRIPT} {Hoffnung} für {Gelähmte} {Therapien} bei {Querschnittslähmung}},
	abstract = {Bitte beachten Sie: Das Manuskript ist ausschließlich zum persönlichen, privaten Gebrauch bestimmt. Jede weitere Vervielfältigung und Verbreitung bedarf der ausdrücklichen Genehmigung des Urhebers bzw. des SWR. Online-Teaser: Querschnittslähmungen galten bisher als unheilbar. Neue Forschungen zeigen jedoch, dass die Schädigung des Rückenmarks nicht immer irreversibel sein muss. MANUSKRIPT Atmo: Trenner, Klänge 1. O-Ton -Andreas Badke: Die Exoskelette sind Geräte, die über Sensoren merken, wenn der Patient einen Schritt auslösen will. Auch ein komplett gelähmter Patient kann mit einem solchen Gerät automatisch laufen. 2. O-Ton -Norbert Weidner: Was ich für unrealistisch halte ist tatsächlich, dass ein komplett Gelähmter durch einen therapeutischen Einsatz innerhalb von wenigen Wochen wieder zum Fußgänger wird. 3. O-Ton -Martin Schwab: Was wir erwarten ist, dass sehr wichtige Körperfunktionen, das betrifft die Blase, die Hand-und Arm-Funktion, auch die Bein-Funktion, dass solche Funktionen zurückgewonnen werden können. 2 4. O-Ton -Rüdiger Rupp: Die heutigen Anwendungen der Implantate im Gehirn konzentrieren sich hauptsächlich auf die Wiederherstellung der Handfunktion. Klavier spielen kriegt man auch mit diesen Implantaten nicht wirklich hin. Sprecher: " Hoffnung für Gelähmte – Therapien bei Querschnittslähmung " . Eine Sendung von Margrit Braszus. 1. Atmo: Geräusche aus dem Testlabor, darüber Autorin: Querschnittslähmung gilt als unheilbar. Trotzdem suchen Forscher weltweit nach Lösungen, damit Gelähmte wieder laufen können. Bei einem spektakulären Tier-Experiment in Lausanne schien das bislang Unmögliche gelungen. Schweizer Wissenschaftler zeigten im November 2016 ein Video in dem man sieht, wie ein zuvor hinkender Rhesusaffe mit gelähmtem Hinterbein plötzlich wieder normal läuft. Die Forscher hatten dem Affen einen Mikrochip ins Hirn verpflanzt und Elektroden am Rückenmark angebracht. (kurz 5. O-Ton -Affen-Video Anfang: aller!, aller! Juchu, Applaus blenden) Das Video ging durch die Medien: weiter 5. O-Ton -Affen-Video Sprecher: Bei einem Affen waren die Wissenschaftler erfolgreich: (französischer Originalton, overvoiced) Zum ersten Mal ist es gelungen, ein gelähmtes Bein quasi wiederzubeleben, das heißt, keine Technik vorher konnte eine gewollte Bewegung eines gelähmten Beins durchführen. Autorin: Bei gesunden Tieren oder Menschen schickt das Gehirn Befehle über Nervenfasern im Rückenmark an die Muskeln. Ist das Rückenmark verletzt, kommen die Signale nicht mehr im Bein oder im Arm an, sie sind gelähmt. Die Forscher in Lausanne haben die zerstörte Leitungsbahn im Rückenmark überbrückt: Der Chip im Gehirn des Rhesusaffen zeichnet die Befehle für einen Bewegungsablauf auf. Diese Signale werden an einen Empfänger weitergeleitet, der die Muskeln aktiviert – dann bewegt sich das Bein des Tieres. Das gelungene Rhesusaffen-Experiment gilt als bahnbrechend, doch es gibt auch Kritiker: Chips ins Gehirn zu implantieren ist grundsätzlich riskant. Es könnten dabei Areale verletzt werden, die die Bewegungsabläufe steuern, gibt Neuroingenieur Rüdiger Rupp aus Heidelberg zu Bedenken. Außerdem sind – anders als bei dem Labor-Affen – bei Menschen mit Querschnittslähmung immer beide Beine betroffen:},
	author = {Braszus, Margrit},
}

@article{lim_enhanced_2017,
	title = {Enhanced {Deep} {Residual} {Networks} for {Single} {Image} {Super}-{Resolution}},
	url = {http://arxiv.org/abs/1707.02921},
	abstract = {Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance. In this paper, we develop an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods. The significant performance improvement of our model is due to optimization by removing unnecessary modules in conventional residual networks. The performance is further improved by expanding the model size while we stabilize the training procedure. We also propose a new multi-scale deep super-resolution system (MDSR) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model. The proposed methods show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the NTIRE2017 Super-Resolution Challenge.},
	author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},
	month = jul,
	year = {2017},
}

@article{ellett_diagnosis_2018,
	title = {Diagnosis of sepsis from a drop of blood by measurement of spontaneous neutrophil motility in a microfluidic assay},
	volume = {2},
	url = {http://dx.doi.org/10.1038/s41551-018-0208-z},
	doi = {10.1038/s41551-018-0208-z},
	abstract = {Current methods for the diagnosis of sepsis have insufficient precision, causing regular misdiagnoses. Microbiological tests can help to diagnose sepsis, but are usually too slow to have an impact on timely clinical decision-making. Neutrophils have a high sensitivity to infections, yet measurements of neutrophil surface markers, genomic changes and phenotype alterations have had only a marginal effect on sepsis diagnosis. Here, we report a microfluidic assay that measures, from one droplet of diluted blood, the spontaneous motility of neutrophils in the presence of plasma. We measured the performance of the assay in two independent cohorts of critically ill patients suspected of sepsis. Using data from a first cohort, we developed a machine-learning-based scoring system (sepsis score) that segregated patients with sepsis from those without sepsis. We then validated the sepsis score in a double-blind, prospective case–control study. For the 42 patients across the two cohorts, the assay identified sepsis patients with 97\% sensitivity and 98\% specificity. The neutrophil assay could potentially be used to accurately diagnose and monitor sepsis in larger populations of at-risk patients.},
	number = {4},
	journal = {Nature Biomedical Engineering},
	author = {Ellett, Felix and Jorgensen, Julianne and Marand, Anika L. and Liu, Yuk Ming and Martinez, Myriam M. and Sein, Vicki and Butler, Kathryn L. and Lee, Jarone and Irimia, Daniel},
	year = {2018},
	note = {Publisher: Springer US},
	pages = {207--214},
}

@article{zhang_layered_2018,
	title = {Layered {Optical} {Flow} {Estimation} {Using} a {Deep} {Neural} {Network} with a {Soft} {Mask}},
	url = {http://arxiv.org/abs/1805.03596},
	abstract = {Using a layered representation for motion estimation has the advantage of being able to cope with discontinuities and occlusions. In this paper, we learn to estimate optical flow by combining a layered motion representation with deep learning. Instead of pre-segmenting the image to layers, the proposed approach automatically generates a layered representation of optical flow using the proposed soft-mask module. The essential components of the soft-mask module are maxout and fuse operations, which enable a disjoint layered representation of optical flow and more accurate flow estimation. We show that by using masks the motion estimate results in a quadratic function of input features in the output layer. The proposed soft-mask module can be added to any existing optical flow estimation networks by replacing their flow output layer. In this work, we use FlowNet as the base network to which we add the soft-mask module. The resulting network is tested on three well-known benchmarks with both supervised and unsupervised flow estimation tasks. Evaluation results show that the proposed network achieve better results compared with the original FlowNet.},
	author = {Zhang, Xi and Ma, Di and Ouyang, Xu and Jiang, Shanshan and Gan, Lin and Agam, Gady},
	month = may,
	year = {2018},
}

@article{fischer_flownet_2015,
	title = {{FlowNet}: {Learning} {Optical} {Flow} with {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1504.06852},
	abstract = {Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks where CNNs were successful. In this paper we construct appropriate CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.},
	author = {Fischer, Philipp and Dosovitskiy, Alexey and Ilg, Eddy and Häusser, Philip and Hazırbaş, Caner and Golkov, Vladimir and van der Smagt, Patrick and Cremers, Daniel and Brox, Thomas},
	month = apr,
	year = {2015},
}

@article{hipp_spatially_2011,
	title = {Spatially {Invariant} {Vector} {Quantization}: {A} pattern matching algorithm for multiple classes of image subject matter including pathology.},
	volume = {2},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21383936},
	doi = {10.4103/2153-3539.77175},
	abstract = {INTRODUCTION HISTORICALLY, EFFECTIVE CLINICAL UTILIZATION OF IMAGE ANALYSIS AND PATTERN RECOGNITION ALGORITHMS IN PATHOLOGY HAS BEEN HAMPERED BY TWO CRITICAL LIMITATIONS: 1) the availability of digital whole slide imagery data sets and 2) a relative domain knowledge deficit in terms of application of such algorithms, on the part of practicing pathologists. With the advent of the recent and rapid adoption of whole slide imaging solutions, the former limitation has been largely resolved. However, with the expectation that it is unlikely for the general cohort of contemporary pathologists to gain advanced image analysis skills in the short term, the latter problem remains, thus underscoring the need for a class of algorithm that has the concurrent properties of image domain (or organ system) independence and extreme ease of use, without the need for specialized training or expertise. RESULTS In this report, we present a novel, general case pattern recognition algorithm, Spatially Invariant Vector Quantization (SIVQ), that overcomes the aforementioned knowledge deficit. Fundamentally based on conventional Vector Quantization (VQ) pattern recognition approaches, SIVQ gains its superior performance and essentially zero-training workflow model from its use of ring vectors, which exhibit continuous symmetry, as opposed to square or rectangular vectors, which do not. By use of the stochastic matching properties inherent in continuous symmetry, a single ring vector can exhibit as much as a millionfold improvement in matching possibilities, as opposed to conventional VQ vectors. SIVQ was utilized to demonstrate rapid and highly precise pattern recognition capability in a broad range of gross and microscopic use-case settings. CONCLUSION With the performance of SIVQ observed thus far, we find evidence that indeed there exist classes of image analysis/pattern recognition algorithms suitable for deployment in settings where pathologists alone can effectively incorporate their use into clinical workflow, as a turnkey solution. We anticipate that SIVQ, and other related class-independent pattern recognition algorithms, will become part of the overall armamentarium of digital image analysis approaches that are immediately available to practicing pathologists, without the need for the immediate availability of an image analysis expert.},
	journal = {Journal of pathology informatics},
	author = {Hipp, Jason D and Cheng, Jerome Y and Toner, Mehmet and Tompkins, Ronald G and Balis, Ulysses J},
	month = feb,
	year = {2011},
	note = {Publisher: Wolters Kluwer -- Medknow Publications},
	keywords = {Bicubic interpolation, content-based image retrieval, continuous symmetry, digital whole slide imaging, image analysis, image vector, Nyquist sampling theory, pathology, pattern recognition, remote sensing, Spatially Invariant Vector Quantization, vector quantization},
	pages = {13--13},
}

@article{christiansen_silico_2018,
	title = {In {Silico} {Labeling}: {Predicting} {Fluorescent} {Labels} in {Unlabeled} {Images}},
	volume = {173},
	url = {https://doi.org/10.1016/j.cell.2018.03.040},
	doi = {10.1016/j.cell.2018.03.040},
	abstract = {Microscopy is a central method in life sciences. Many popular methods, such as antibody labeling, are used to add physical fluorescent labels to specific cellular constituents. However, these approaches have significant drawbacks, including inconsistency; limitations in the number of simultaneous labels because of spectral overlap; and necessary perturbations of the experiment, such as fixing the cells, to generate the measurement. Here, we show that a computational machine-learning approach, which we call “in silico labeling” (ISL), reliably predicts some fluorescent labels from transmitted-light images of unlabeled fixed or live biological samples. ISL predicts a range of labels, such as those for nuclei, cell type (e.g., neural), and cell state (e.g., cell death). Because prediction happens in silico, the method is consistent, is not limited by spectral overlap, and does not disturb the experiment. ISL generates biological measurements that would otherwise be problematic or impossible to acquire. In silico labeling, a machine-learning approach, reliably infers fluorescent measurements from transmitted-light images of unlabeled fixed or live biological samples.},
	number = {3},
	journal = {Cell},
	author = {Christiansen, Eric M. and Yang, Samuel J. and Ando, D. Michael and Javaherian, Ashkan and Skibinski, Gaia and Lipnick, Scott and Mount, Elliot and O'Neil, Alison and Shah, Kevan and Lee, Alicia K. and Goyal, Piyush and Fedus, William and Poplin, Ryan and Esteva, Andre and Berndl, Marc and Rubin, Lee L. and Nelson, Philip and Finkbeiner, Steven},
	year = {2018},
	note = {Publisher: Elsevier Inc.},
	keywords = {machine learning, deep learning, microscopy, cancer, computer vision, neuroscience, stem cells},
	pages = {792--803.e19},
}

@article{li_deepunet_2017,
	title = {{DeepUNet}: {A} {Deep} {Fully} {Convolutional} {Network} for {Pixel}-level {Sea}-{Land} {Segmentation}},
	url = {https://arxiv.org/pdf/1709.00201.pdf},
	abstract = {—Semantic segmentation is a fundamental research in remote sensing image processing. Because of the complex maritime environment, the sea-land segmentation is a challeng-ing task. Although the neural network has achieved excellent performance in semantic segmentation in the last years, there are a few of works using CNN for sea-land segmentation and the results could be further improved. This paper proposes a novel deep convolution neural network named DeepUNet. Like the U-Net, its structure has a contracting path and an expansive path to get high resolution output. But differently, the DeepUNet uses DownBlocks instead of convolution layers in the contracting path and uses UpBlock in the expansive path. The two novel blocks bring two new connections that are U-connection and Plus connection. They are promoted to get more precise segmentation results. To verify our network architecture, we made a new chal-lenging sea-land dataset and compare the DeepUNet on it with the SegNet and the U-Net. Experimental results show that DeepUNet achieved good performance compared with other architectures, especially in high-resolution remote sensing imagery.},
	author = {Li, Ruirui and Liu, Wenjie and Yang, Lei and Sun, Shihao and Hu, Wei and Zhang, Fan and Member, Senior and Li, Wei},
	year = {2017},
	keywords = {fully convolution network, Index Terms—sea-land segmentation, ResNet, satellite imagery process-ing, U-Net},
}

@article{ha_world_2018-1,
	title = {World {Models}},
	url = {http://arxiv.org/abs/1803.10122},
	doi = {10.5281/zenodo.1207631},
	abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
	author = {Ha, David and Schmidhuber, Jürgen},
	month = mar,
	year = {2018},
}

@article{stegmaier_fast_2014,
	title = {Fast segmentation of stained nuclei in terabyte-scale, time resolved {3D} microscopy image stacks.},
	volume = {9},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/24587204},
	doi = {10.1371/journal.pone.0090036},
	abstract = {Automated analysis of multi-dimensional microscopy images has become an integral part of modern research in life science. Most available algorithms that provide sufficient segmentation quality, however, are infeasible for a large amount of data due to their high complexity. In this contribution we present a fast parallelized segmentation method that is especially suited for the extraction of stained nuclei from microscopy images, e.g., of developing zebrafish embryos. The idea is to transform the input image based on gradient and normal directions in the proximity of detected seed points such that it can be handled by straightforward global thresholding like Otsu's method. We evaluate the quality of the obtained segmentation results on a set of real and simulated benchmark images in 2D and 3D and show the algorithm's superior performance compared to other state-of-the-art algorithms. We achieve an up to ten-fold decrease in processing times, allowing us to process large data sets while still providing reasonable segmentation results.},
	number = {2},
	journal = {PloS one},
	author = {Stegmaier, Johannes and Otte, Jens C and Kobitski, Andrei and Bartschat, Andreas and Garcia, Ariel and Nienhaus, G Ulrich and Strähle, Uwe and Mikut, Ralf},
	year = {2014},
	note = {Publisher: Public Library of Science},
	pages = {e90036--e90036},
}

@article{salehi_tversky_2017,
	title = {Tversky loss function for image segmentation using {3D} fully convolutional deep networks},
	url = {http://arxiv.org/abs/1706.05721},
	abstract = {Fully convolutional deep neural networks carry out excellent potential for fast and accurate image segmentation. One of the main challenges in training these networks is data imbalance, which is particularly problematic in medical imaging applications such as lesion segmentation where the number of lesion voxels is often much lower than the number of non-lesion voxels. Training with unbalanced data can lead to predictions that are severely biased towards high precision but low recall (sensitivity), which is undesired especially in medical applications where false negatives are much less tolerable than false positives. Several methods have been proposed to deal with this problem including balanced sampling, two step training, sample re-weighting, and similarity loss functions. In this paper, we propose a generalized loss function based on the Tversky index to address the issue of data imbalance and achieve much better trade-off between precision and recall in training 3D fully convolutional deep neural networks. Experimental results in multiple sclerosis lesion segmentation on magnetic resonance images show improved F2 score, Dice coefficient, and the area under the precision-recall curve in test data. Based on these results we suggest Tversky loss function as a generalized framework to effectively train deep neural networks.},
	author = {Salehi, Seyed Sadegh Mohseni and Erdogmus, Deniz and Gholipour, Ali},
	month = jun,
	year = {2017},
}

@article{nehme_deep-storm_2018,
	title = {Deep-{STORM}: super-resolution single-molecule microscopy by deep learning},
	volume = {5},
	url = {https://www.osapublishing.org/abstract.cfm?URI=optica-5-4-458},
	doi = {10.1364/OPTICA.5.000458},
	abstract = {We present an ultrafast, precise, parameter-free method, which we term Deep-STORM, for obtaining super-resolution images from stochastically blinking emitters, such as fluorescent molecules used for localization microscopy. Deep-STORM uses a deep convolutional neural network that can be trained on simulated data or experimental measurements, both of which are demonstrated. The method achieves state-of-the-art resolution under challenging signal-to-noise conditions and high emitter densities and is significantly faster than existing approaches. Additionally, no prior information on the shape of the underlying structure is required, making the method applicable to any blinking dataset. We validate our approach by super-resolution image reconstruction of simulated and experimentally obtained data.},
	number = {4},
	journal = {Optica},
	author = {Nehme, Elias and Weiss, Lucien E. and Michaeli, Tomer and Shechtman, Yoav},
	month = apr,
	year = {2018},
	note = {Publisher: Optical Society of America},
	keywords = {Image processing, Algorithms, Fluorescence microscopy, Superresolution},
	pages = {458--458},
}

@article{webb_deep_2018,
	title = {Deep learning for biology},
	volume = {554},
	url = {http://www.nature.com/doifinder/10.1038/d41586-018-02174-z},
	doi = {10.1038/d41586-018-02174-z},
	number = {7693},
	journal = {Nature},
	author = {Webb, Sarah},
	month = feb,
	year = {2018},
	pages = {555--557},
}

@article{salehi_auto-context_2017,
	title = {Auto-context {Convolutional} {Neural} {Network} ({Auto}-{Net}) for {Brain} {Extraction} in {Magnetic} {Resonance} {Imaging}},
	url = {http://arxiv.org/abs/1703.02083},
	abstract = {Brain extraction or whole brain segmentation is an important first step in many of the neuroimage analysis pipelines. The accuracy and robustness of brain extraction, therefore, is crucial for the accuracy of the entire brain analysis process. With the aim of designing a learning-based, geometry-independent and registration-free brain extraction tool in this study, we present a technique based on an auto-context convolutional neural network (CNN), in which intrinsic local and global image features are learned through 2D patches of different window sizes. In this architecture three parallel 2D convolutional pathways for three different directions (axial, coronal, and sagittal) implicitly learn 3D image information without the need for computationally expensive 3D convolutions. Posterior probability maps generated by the network are used iteratively as context information along with the original image patches to learn the local shape and connectedness of the brain, to extract it from non-brain tissue. The brain extraction results we have obtained from our algorithm are superior to the recently reported results in the literature on two publicly available benchmark datasets, namely LPBA40 and OASIS, in which we obtained Dice overlap coefficients of 97.42\% and 95.40\%, respectively. Furthermore, we evaluated the performance of our algorithm in the challenging problem of extracting arbitrarily-oriented fetal brains in reconstructed fetal brain magnetic resonance imaging (MRI) datasets. In this application our algorithm performed much better than the other methods (Dice coefficient: 95.98\%), where the other methods performed poorly due to the non-standard orientation and geometry of the fetal brain in MRI. Our CNN-based method can provide accurate, geometry-independent brain extraction in challenging applications.},
	author = {Salehi, Seyed Sadegh Mohseni and Erdogmus, Deniz and Gholipour, Ali},
	month = mar,
	year = {2017},
}

@article{webb_deep_2018-1,
	title = {Deep learning for biology},
	abstract = {A popular artificial-intelligence method provides a powerful tool for surveying and classifying biological data. But for the uninitiated, the technology poses significant difficulties. A popular artificial-intelligence method provides a powerful tool for surveying and classifying biological data. But for the uninitiated, the technology poses significant difficulties.},
	journal = {Nature 2018 554:7693},
	author = {Webb, Sarah},
	month = feb,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Computer science},
}

@article{wang_evolutionary_2018,
	title = {Evolutionary {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1803.00657},
	abstract = {Generative adversarial networks (GAN) have been effective for learning generative models for real-world data. However, existing GANs (GAN and its variants) tend to suffer from training problems such as instability and mode collapse. In this paper, we propose a novel GAN framework called evolutionary generative adversarial networks (E-GAN) for stable GAN training and improved generative performance. Unlike existing GANs, which employ a pre-defined adversarial objective function alternately training a generator and a discriminator, we utilize different adversarial training objectives as mutation operations and evolve a population of generators to adapt to the environment (i.e., the discriminator). We also utilize an evaluation mechanism to measure the quality and diversity of generated samples, such that only well-performing generator(s) are preserved and used for further training. In this way, E-GAN overcomes the limitations of an individual adversarial training objective and always preserves the best offspring, contributing to progress in and the success of GANs. Experiments on several datasets demonstrate that E-GAN achieves convincing generative performance and reduces the training problems inherent in existing GANs.},
	author = {Wang, Chaoyue and Xu, Chang and Yao, Xin and Tao, Dacheng},
	year = {2018},
}

@article{noauthor_full-text_nodate-3,
	title = {full-text},
}

@article{jiang_salient_nodate,
	title = {Salient {Object} {Detection}: {A} {Discriminative} {Regional} {Feature} {Integration} {Approach}},
	url = {https://arxiv.org/pdf/1410.5926.pdf},
	abstract = {—Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we formulate saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, utilizes the supervised learning approach to map the regional feature vector to a saliency score. Saliency scores across multiple layers are finally fused to produce the saliency map. The contributions lie in two-fold. One is that we propose a discriminate regional feature integration approach for salient object detection. Compared with existing heuristic models, our proposed method is able to automatically integrate high-dimensional regional saliency features and choose discriminative ones. The other is that by investigating standard generic region properties as well as two widely studied concepts for salient object detection, i.e., regional contrast and backgroundness, our approach significantly outperforms state-of-the-art methods on six benchmark datasets. Meanwhile, we demonstrate that our method runs as fast as most existing algorithms.},
	author = {Jiang, Huaizu and Yuan, Zejian and Cheng, Ming-Ming and Gong, Yihong and Zheng, Nanning and Wang, Jingdong},
}

@article{szegedy_inception-v4_nodate,
	title = {Inception-v4, {Inception}-{ResNet} and the {Impact} of {Residual} {Connections} on {Learning}},
	url = {https://arxiv.org/pdf/1602.07261.pdf},
	abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at rel-atively low computational cost. Recently, the introduction of residual connections in conjunction with a more tradi-tional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Incep-tion networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These varia-tions improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We fur-ther demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08\% top-5 error on the test set of the ImageNet classification (CLS) challenge.},
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
}

@article{osokin_gans_2017,
	title = {{GANs} for {Biological} {Image} {Synthesis}},
	url = {http://arxiv.org/abs/1708.04692},
	abstract = {In this paper, we propose a novel application of Generative Adversarial Networks (GAN) to the synthesis of cells imaged by fluorescence microscopy. Compared to natural images, cells tend to have a simpler and more geometric global structure that facilitates image generation. However, the correlation between the spatial pattern of different fluorescent proteins reflects important biological functions, and synthesized images have to capture these relationships to be relevant for biological applications. We adapt GANs to the task at hand and propose new models with casual dependencies between image channels that can generate multi-channel images, which would be impossible to obtain experimentally. We evaluate our approach using two independent techniques and compare it against sensible baselines. Finally, we demonstrate that by interpolating across the latent space we can mimic the known changes in protein localization that occur through time during the cell cycle, allowing us to predict temporal evolution from static images.},
	author = {Osokin, Anton and Chessel, Anatole and Salas, Rafael E. Carazo and Vaggi, Federico},
	month = aug,
	year = {2017},
}

@article{donahue_long-term_nodate,
	title = {Long-term {Recurrent} {Convolutional} {Networks} for {Visual} {Recognition} and {Description}},
	url = {https://arxiv.org/pdf/1411.4389.pdf},
	abstract = {— Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent are effective for tasks involving sequences, visual and otherwise. We describe a class of recurrent convolutional architectures which is end-to-end trainable and suitable for large-scale visual understanding tasks, and demonstrate the value of these models for activity recognition, image captioning, and video description. In contrast to previous models which assume a fixed visual representation or perform simple temporal averaging for sequential processing, recurrent convolutional models are " doubly deep " in that they learn compositional representations in space and time. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Differentiable recurrent models are appealing in that they can directly map variable-length inputs (e.g., videos) to variable-length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent sequence models are directly connected to modern visual convolutional network models and can be jointly trained to learn temporal dynamics and convolutional perceptual representations. Our results show that such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined or optimized.},
	author = {Donahue, Jeff and Hendricks, Lisa Anne and Rohrbach, Marcus and Venugopalan, Subhashini and Guadarrama, Sergio and Saenko, Kate and Darrell, Trevor},
}

@article{mnih_human-level_nodate,
	title = {Human-level control through deep reinforcement learning},
	url = {https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf},
	doi = {10.1038/nature14236},
	abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
}

@article{schmidhuber_deep_2014,
	title = {Deep {Learning} in {Neural} {Networks}: {An} {Overview}},
	url = {http://www.idsia.ch},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distin-guished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks. Preface This is the preprint of an invited Deep Learning (DL) overview. One of its goals is to assign credit to those who contributed to the present state of the art. I acknowledge the limitations of attempting to achieve this goal. The DL research community itself may be viewed as a continually evolving, deep network of scientists who have influenced each other in complex ways. Starting from recent DL results, I tried to trace back the origins of relevant ideas through the past half century and beyond, sometimes using " local search " to follow citations of citations backwards in time. Since not all DL publications properly acknowledge earlier relevant work, additional global search strategies were em-ployed, aided by consulting numerous neural network experts. As a result, the present preprint mostly consists of references. Nevertheless, through an expert selection bias I may have missed important work. A related bias was surely introduced by my special familiarity with the work of my own DL research group in the past quarter-century. For these reasons, this work should be viewed as merely a snapshot of an ongoing credit assignment process. To help improve it, please do not hesitate to send corrections and suggestions to juergen@idsia.ch.},
	author = {Schmidhuber, Jürgen},
	year = {2014},
}

@article{svoboda_generation_2009,
	title = {Generation of digital phantoms of cell nuclei and simulation of image formation in {3D} image cytometry},
	volume = {75},
	issn = {1552-4930 (Electronic){\textbackslash}n1552-4922 (Linking)},
	doi = {10.1002/cyto.a.20714},
	abstract = {Image cytometry still faces the problem of the quality of cell image analysis results. Degradations caused by cell preparation, optics, and electronics considerably affect most 2D and 3D cell image data acquired using optical microscopy. That is why image processing algorithms applied to these data typically offer imprecise and unreliable results. As the ground truth for given image data is not available in most experiments, the outputs of different image analysis methods can be neither verified nor compared to each other. Some papers solve this problem partially with estimates of ground truth by experts in the field (biologists or physicians). However, in many cases, such a ground truth estimate is very subjective and strongly varies between different experts. To overcome these difficulties, we have created a toolbox that can generate 3D digital phantoms of specific cellular components along with their corresponding images degraded by specific optics and electronics. The user can then apply image analysis methods to such simulated image data. The analysis results (such as segmentation or measurement results) can be compared with ground truth derived from input object digital phantoms (or measurements on them). In this way, image analysis methods can be compared with each other and their quality (based on the difference from ground truth) can be computed. We have also evaluated the plausibility of the synthetic images, measured by their similarity to real image data. We have tested several similarity criteria such as visual comparison, intensity histograms, central moments, frequency analysis, entropy, and 3D Haralick features. The results indicate a high degree of similarity between real and simulated image data.},
	number = {6},
	journal = {Cytometry Part A},
	author = {Svoboda, David and Kozubek, Michal and Stejskal, Stanislav},
	year = {2009},
	keywords = {3D image cytometry, Digital phantom, Fluorescence optical microscope, Point spread function, Procedural texture, Synthetic image},
	pages = {494--509},
}

@article{borlongan_age_2016,
	title = {Age of {PISCES}: stem-cell clinical trials in stroke.},
	volume = {388},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/27497863},
	doi = {10.1016/S0140-6736(16)31259-4},
	number = {10046},
	journal = {Lancet (London, England)},
	author = {Borlongan, Cesar V},
	month = aug,
	year = {2016},
	note = {Publisher: Elsevier},
	pages = {736--8},
}

@article{venhuizen_deep_nodate,
	title = {Deep learning approach for the detection and quantification of intraretinal cystoid fluid in multivendor optical coherence tomography},
	url = {https://www.osapublishing.org/DirectPDFAccess/6E4B4C22-B9D6-45BA-C446ACFF49BFDA5B_383190/boe-9-4-1545.pdf?da=1&id=383190&seq=0&mobile=no},
	doi = {10.1364/BOE.9.001545},
	abstract = {We developed a deep learning algorithm for the automatic segmentation and quan-tification of intraretinal cystoid fluid (IRC) in spectral domain optical coherence tomography (SD-OCT) volumes independent of the device used for acquisition. A cascade of neural networks was introduced to include prior information on the retinal anatomy, boosting performance significantly. The proposed algorithm approached human performance reaching an overall Dice coefficient of 0.754 ± 0.136 and an intraclass correlation coefficient of 0.936, for the task of IRC segmentation and quantification, respectively. The proposed method allows for fast quantitative IRC volume measurements that can be used to improve patient care, reduce costs, and allow fast and reliable analysis in large population studies.},
	author = {Venhuizen, Freerk G and Ginneken, Bram Van and Liefers, Bart and Asten, Freekje Van and Schreur, Vivian and Fauser, Sascha and Hoyng, Carel and Theelen, Thomas and Sánchez, Clara I},
}

@article{colabrese_machine_2015,
	title = {Machine learning approach for single molecule localisation microscopy},
	volume = {48},
	url = {https://www.osapublishing.org/DirectPDFAccess/6E4AC64D-0414-12FB-A8D000728879DF54_383467/boe-9-4-1680.pdf?da=1&id=383467&seq=0&mobile=no},
	doi = {10.1364/BOE.9.001680},
	abstract = {Single molecule localisation (SML) microscopy is a fundamental tool for biological discoveries; it provides sub-diffraction spatial resolution images by detecting and localizing "all" the fluorescent molecules labeling the structure of interest. For this reason, the effective resolution of SML microscopy strictly depends on the algorithm used to detect and localize the single molecules from the series of microscopy frames. To adapt to the different imaging conditions that can occur in a SML experiment, all current localisation algorithms request, from the microscopy users, the choice of different parameters. This choice is not always easy and their wrong selection can lead to poor performance. Here we overcome this weakness with the use of machine learning. We propose a parameter-free pipeline for SML learning based on support vector machine (SVM). This strategy requires a short supervised training that consists in selecting by the user few fluorescent molecules (∼ 10-20) from the frames under analysis. The algorithm has been extensively tested on both synthetic and real acquisitions. Results are qualitatively and quantitatively consistent with the state of the art in SML microscopy and demonstrate that the introduction of machine learning can lead to a new class of algorithms competitive and conceived from the user point of view. OCIS codes: (180.2520) Fluorescence microscopy; (100.0100) Image processing; (100.5010) Pattern recognition., " Imaging intracellular fluorescent proteins at nanometer resolution, " Science 313, 1642–1645 (2006). 6. M. J. Rust, M. Bates, and X. Zhuang, " Stochastic optical reconstruction microscopy (storm) provides sub-diffraction-limit image resolution, " Nat. Methods 3, 793 (2006). 7. E. Abbe, " Beiträge zur theorie des mikroskops und der mikroskopischen wahrnehmung, " Archiv für mikroskopische Anatomie 9, 413–418 (1873). " Machine learning 20, 273–297 (1995). 11. H. Guo, " A simple algorithm for fitting a gaussian function, " IEEE Signal Process. Mag. 28, 134–137 (2011). 12. S. Colabrese, M. Castello, G. Vicidomini, and A. Del Bue, " Learning-based approach to boost detection rate and localisation accuracy in single molecule localisation microscopy, " in " Image Processing (ICIP), 2016 IEEE International Conference on " (IEEE, 2016), pp. 3184–3188. 13. M.Oves y, P. Křížek, J. Borkovec, Z. Švindrych, and G. M. Hagen, " Thunderstorm: a comprehensive imagej plug-in for palm and storm data analysis and super-resolution imaging, " Bioinformatics 30, 2389–2390 (2014). 14. D. Sage, H. Kirshner, T. Pengo, N. Stuurman, J. Min, S. Manley, and M. Unser, " Quantitative evaluation of software packages for single-molecule localization microscopy, " Nat. Methods 12, 717–724 (2015).},
	number = {4},
	journal = {J. Phys. D: Appl. Phys Nat. Rev. Mol. Cell Biol. BIOMEDICAL OPTICS EXPRESS},
	author = {Colabrese, Silvia and Castello, Marco and Vicidomini, Giuseppe and Alessio, And and Bue, Del and Lindwasser, W and Olenych, S and Bonifacino, J S and Davidson, M W and Lippincott-Schwartz, J and Hess, H F and Cox, 15 S and Rosten, E and Monypenny, J and Jovanovic-Talisman, T and Burnette, D T and Jones, G E},
	year = {2015},
	pages = {685--701},
}

@article{nirschl_deep-learning_2018,
	title = {A deep-learning classifier identifies patients with clinical heart failure using whole-slide images of {H}\&{E} tissue},
	volume = {13},
	url = {http://dx.plos.org/10.1371/journal.pone.0192726},
	doi = {10.1371/journal.pone.0192726},
	abstract = {Over 26 million people worldwide suffer from heart failure annually. When the cause of heart failure cannot be identified, endomyocardial biopsy (EMB) represents the gold-standard for the evaluation of disease. However, manual EMB interpretation has high inter-rater variability. Deep convolutional neural networks (CNNs) have been successfully applied to detect cancer, diabetic retinopathy, and dermatologic lesions from images. In this study, we develop a CNN classifier to detect clinical heart failure from H\&E stained whole-slide images from a total of 209 patients, 104 patients were used for training and the remaining 105 patients for independent testing. The CNN was able to identify patients with heart failure or severe pathology with a 99\% sensitivity and 94\% specificity on the test set, outperforming conventional feature-engineering approaches. Importantly, the CNN outperformed two expert pathologists by nearly 20\%. Our results suggest that deep learning analytics of EMB can be used to predict cardiac outcome.},
	number = {4},
	journal = {PLOS ONE},
	author = {Nirschl, Jeffrey J. and Janowczyk, Andrew and Peyster, Eliot G. and Frank, Renee and Margulies, Kenneth B. and Feldman, Michael D. and Madabhushi, Anant},
	editor = {Marsden, Alison},
	month = apr,
	year = {2018},
	note = {Publisher: Public Library of Science},
	pages = {e0192726--e0192726},
}

@article{pelt_mixed-scale_2018,
	title = {A mixed-scale dense convolutional neural network for image analysis},
	volume = {115},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1715832114},
	doi = {10.1073/pnas.1715832114},
	abstract = {Deep convolutional neural networks have been successfully applied to many image-processing problems in recent works. Popular network architectures often add additional operations and connections to the standard architecture to enable training deeper networks. To achieve accurate results in practice, a large number of trainable parameters are often required. Here, we introduce a network architecture based on using dilated convolutions to capture features at different image scales and densely connecting all feature maps with each other. The resulting architecture is able to achieve accurate results with relatively few parameters and consists of a single set of operations, making it easier to implement, train, and apply in practice, and automatically adapts to different problems. We compare results of the proposed network architecture with popular existing architectures for several segmentation problems, showing that the proposed architecture is able to achieve accurate results with fewer parameters, with a reduced risk of overfitting the training data.},
	number = {2},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Pelt, Daniël M. and Sethian, James A.},
	year = {2018},
	pages = {254--259},
}

@article{chen_deep_2016,
	title = {Deep {Learning} in {Label}-free {Cell} {Classification}},
	volume = {6},
	url = {http://www.nature.com/articles/srep21471},
	doi = {10.1038/srep21471},
	abstract = {Deep Learning in Label-free Cell Classification},
	number = {1},
	journal = {Scientific Reports},
	author = {Chen, Claire Lifan and Mahjoubfar, Ata and Tai, Li-Chia and Blaby, Ian K. and Huang, Allen and Niazi, Kayvan Reza and Jalali, Bahram},
	month = aug,
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biophotonics, Cell biology, Electrical and electronic engineering},
	pages = {21471--21471},
}

@article{parthasarathy_rapid_2012-1,
	title = {Rapid, accurate particle tracking by calculation of radial symmetry centers},
	volume = {9},
	url = {http://www.nature.com/articles/nmeth.2071},
	doi = {10.1038/nmeth.2071},
	abstract = {An analytically exact approach that determines the radial symmetry center of the image of any radially symmetric particle allows faster localization than iterative methods while also giving localization accuracies approaching theoretical limits.},
	number = {7},
	journal = {Nature Methods},
	author = {Parthasarathy, Raghuveer},
	month = jul,
	year = {2012},
	note = {Publisher: Nature Publishing Group},
	keywords = {Microscopy, Biophysics, Imaging},
	pages = {724--726},
}

@article{culley_nanoj-squirrel_2017,
	title = {{NanoJ}-{SQUIRREL}: quantitative mapping and minimisation of super-resolution optical imaging artefacts},
	url = {https://www.biorxiv.org/content/early/2017/07/02/158279},
	doi = {10.1101/158279},
	abstract = {Most super-resolution microscopy methods depend on steps that contribute to the formation of image artefacts. Here we present NanoJ-SQUIRREL, an ImageJ-based analytical approach providing a quantitative assessment of super-resolution image quality. By comparing diffraction-limited images and super-resolution equivalents of the same focal volume, this approach generates a quantitative map of super-resolution defects, as well as methods for their correction. To illustrate its broad applicability to super-resolution approaches we apply our method to Localization Microscopy, STED and SIM images of a variety of in-cell structures including microtubules, poxviruses, neuronal actin rings and clathrin coated pits. We particularly focus on single-molecule localisation microscopy, where super-resolution reconstructions often feature imperfections not present in the original data. By showing the quantitative evolution of data quality over these varied sample preparation, acquisition and super-resolution methods we display the potential of NanoJ-SQUIRREL to guide optimization of super-resolution imaging parameters.},
	journal = {bioRxiv},
	author = {Culley, Siân and Albrecht, David and Jacobs, Caron and Pereira, Pedro Matos and Leterrier, Christophe and Mercer, Jason and Henriques, Ricardo},
	month = jul,
	year = {2017},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {158279--158279},
}

@article{diederich_using_2018,
	title = {Using machine-learning to optimize phase contrast in a low-cost cellphone microscope},
	volume = {13},
	url = {http://dx.plos.org/10.1371/journal.pone.0192937},
	doi = {10.1371/journal.pone.0192937},
	abstract = {Cellphones equipped with high-quality cameras and powerful CPUs as well as GPUs are widespread. This opens new prospects to use such existing computational and imaging resources to perform medical diagnosis in developing countries at a very low cost. Many relevant samples, like biological cells or waterborn parasites, are almost fully transparent. As they do not exhibit absorption, but alter the light’s phase only, they are almost invisible in brightfield microscopy. Expensive equipment and procedures for microscopic contrasting or sample staining often are not available. Dedicated illumination approaches, tailored to the sample under investigation help to boost the contrast. This is achieved by a programmable illumination source, which also allows to measure the phase gradient using the differential phase contrast (DPC) [1, 2] or even the quantitative phase using the derived qDPC approach [3]. By applying machine-learning techniques, such as a convolutional neural network (CNN), it is possible to learn a relationship between samples to be examined and its optimal light source shapes, in order to increase e.g. phase contrast, from a given dataset to enable real-time applications. For the experimental setup, we developed a 3D-printed smartphone microscope for less than 100 \$ using off-the-shelf components only such as a low-cost video projector. The fully automated system assures true Koehler illumination with an LCD as the condenser aperture and a reversed smartphone lens as the microscope objective. We show that the effect of a varied light source shape, using the pre-trained CNN, does not only improve the phase contrast, but also the impression of an improvement in optical resolution without adding any special optics, as demonstrated by measurements.},
	number = {3},
	journal = {PLOS ONE},
	author = {Diederich, Benedict and Wartmann, Rolf and Schadwinkel, Harald and Heintzmann, Rainer},
	editor = {Joo, Chulmin},
	month = mar,
	year = {2018},
	note = {Publisher: Public Library of Science},
	pages = {e0192937--e0192937},
}

@article{rivenson_deep_nodate,
	title = {Deep learning enhanced mobile-phone microscopy},
	url = {https://arxiv.org/ftp/arxiv/papers/1712/1712.04139.pdf},
	abstract = {Tel: +1(310)825-0915 Fax: +1(310)206-4685 Equal contributing authors. 2 Abstract Mobile-phones have facilitated the creation of field-portable, cost-effective imaging and sensing technologies that approach laboratory-grade instrument performance. However, the optical imaging interfaces of mobile-phones are not designed for microscopy and produce spatial and spectral distortions in imaging microscopic specimens. Here, we report on the use of deep learning to correct such distortions introduced by mobile-phone-based microscopes, facilitating the production of high-resolution, denoised and colour-corrected images, matching the performance of benchtop microscopes with high-end objective lenses, also extending their limited depth-of-field. After training a convolutional neural network, we successfully imaged various samples, including blood smears, histopathology tissue sections, and parasites, where the recorded images were highly compressed to ease storage and transmission for telemedicine applications. This method is applicable to other low-cost, aberrated imaging systems, and could offer alternatives for costly and bulky microscopes, while also providing a framework for standardization of optical images for clinical and biomedical applications.},
	author = {Rivenson, Yair and Koydemir, Hatice Ceylan and Wang, Hongda and Wei, Zhensong and Ren, Zhengshuang and Günaydın, Harun and Zhang, Yibo and Göröcs, Zoltán and Liang, Kyle and Tseng, Derek and Ozcan, Aydogan},
}

@article{kupyn_deblurgan_nodate,
	title = {{DeblurGAN}: {Blind} {Motion} {Deblurring} {Using} {Conditional} {Adversarial} {Networks}},
	url = {https://arxiv.org/pdf/1711.07064.pdf},
	abstract = {We present an end-to-end learning approach for motion deblurring, which is based on conditional GAN and con-tent loss – DeblurGAN. DeblurGAN achieves state-of-the art in structural similarity measure and by visual appear-ance. 1 The quality of the deblurring model is also evaluated in a novel way on a real-world problem – object detection on (de-)blurred images. The method is 5 times faster than the closest competitor. Second, we present a novel method of generating syn-thetic motion blurred images from the sharp ones, which allows realistic dataset augmentation. Model, training code and dataset are available at https://github.com/KupynOrest/DeblurGAN},
	author = {Kupyn, Orest and Budzan, Volodymyr and Mykhailych, Mykola and Mishkin, Dmytro and Matas, Jiři},
}

@article{noauthor_content-aware_nodate,
	title = {Content-{Aware} {Image} {Restoration}: {Pushing} the {Limits} of {Fluorescence} {Microscopy}},
	url = {https://www.biorxiv.org/content/biorxiv/suppl/2018/01/23/236463.DC3/236463-1.pdf},
}

@article{cheeseman_forget_2018,
	title = {Forget {Pixels}: {Adaptive} {Particle} {Representation} of {Fluorescence} {Microscopy} {Images}},
	url = {https://www.biorxiv.org/content/early/2018/02/09/263061},
	doi = {10.1101/263061},
	abstract = {Modern microscopy modalities create a data deluge with gigabytes of data generated each second, or terabytes per day. Storing and processing these data is a severe bottleneck. We argue that this is an artifact of the images being represented on pixels. To address the root of the problem, we here propose the Adaptive Particle Representation (APR) as an image-content-aware representation for fluorescence microscopy images. The APR replaces pixel images to overcome computational and memory bottlenecks in storage and processing pipelines for studying spatiotemporal processes in biology using fluorescence microscopy. We present the ideas, concepts, and algorithms and validate them using noisy 3D image data. We show how the APR spatially adapts to the information content of an image without reducing image quality. We then show that the adaptivity of the APR provides orders of magnitude benefits across a range of image processing tasks. Therefore, the APR provides a simple, extendable, and efficient content-adaptive representation of images that could be useful for many imaging modalities in order to relax the data and processing bottlenecks.},
	journal = {bioRxiv},
	author = {Cheeseman, Bevan L. and Günther, Ulrik and Susik, Mateusz and Gonciarz, Krzysztof and Sbalzarini, Ivo F.},
	month = feb,
	year = {2018},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {263061--263061},
}

@article{gustafsson_fast_2016,
	title = {Fast live-cell conventional fluorophore nanoscopy with {ImageJ} through super-resolution radial fluctuations},
	volume = {7},
	url = {http://www.nature.com/doifinder/10.1038/ncomms12471},
	doi = {10.1038/ncomms12471},
	abstract = {Super-resolution fluorescent imaging typically makes use of intense phototoxic illumination. Here the authors achieve live-cell super-resolution imaging using low-illumination standard microscopes with the aid of a new analytical approach called Super-Resolution Radial Fluctuations (SRRF), provided as an ImageJ plugin.},
	journal = {Nature Communications},
	author = {Gustafsson, Nils and Culley, Siân and Ashdown, George and Owen, Dylan M. and Pereira, Pedro Matos and Henriques, Ricardo},
	month = aug,
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cell biology, resolution microscopy, Software, Super},
	pages = {12471--12471},
}

@article{noauthor_full-text_nodate-4,
	title = {full-text},
}

@article{coleman_dawnbench_nodate,
	title = {{DAWNBench}: {An} {End}-to-{End} {Deep} {Learning} {Benchmark} and {Competition}},
	url = {http://dawn.cs.stanford.edu/benchmark},
	abstract = {Despite considerable research on systems, algorithms and hardware to speed up deep learning workloads, there is no standard means of evaluating end-to-end deep learning performance. Existing benchmarks measure proxy metrics, such as time to process one minibatch of data, that do not indicate whether the system as a whole will produce a high-quality result. In this work, we introduce DAWNBench, a benchmark and competition focused on end-to-end training time to achieve a state-of-the-art accuracy level, as well as inference time with that accuracy. Using time to accuracy as a target metric, we explore how different optimizations, including choice of optimizer, stochastic depth, and multi-GPU training, affect end-to-end training performance. Our results demonstrate that optimizations can interact in non-trivial ways when used in conjunction, producing lower speed-ups and less accurate models. We believe DAWNBench will provide a useful, reproducible means of evaluating the many trade-offs in deep learning systems.},
	author = {Coleman, Cody and Narayanan, Deepak and Kang, Daniel and Zhao, Tian and Zhang, Jian and Nardi, Luigi and Bailis, Peter and Olukotun, Kunle and Ré, Chris and Zaharia, Matei and Project, Stanford Dawn},
}

@article{ellenberg_public_nodate,
	title = {Public archives for biological image data},
	url = {https://arxiv.org/ftp/arxiv/papers/1801/1801.10189.pdf},
	abstract = {Public data archives are the backbone of modern biological and biomedical research. While archives for biological molecules and structures are well-established, resources for imaging data do not yet cover the full range of spatial and temporal scales or application domains used by the scientific community. In the last few years, the technical barriers to building such resources have been solved and the first examples of scientific outputs from public image data resources, often through linkage to existing molecular resources, have been published. Using the successes of existing biomolecular resources as a guide, we present the rationale and principles for the construction of image data archives and databases that will be the foundation of the next revolution in biological and biomedical informatics and discovery.},
	author = {Ellenberg, Jan and Swedlow, Jason R and Barlow, Mary and Cook, Charles E and Patwardhan, Ardan and Brazma, Alvis and Birney, Ewan and Affiliations, *},
}

@article{imaging_advances_1995,
	title = {Advances in {Acoustic} {Microscopy}},
	volume = {Vi},
	issn = {978-1-4613-5762-9},
	url = {http://link.springer.com/10.1007/978-1-4615-1873-0},
	doi = {10.1007/978-1-4615-1873-0},
	abstract = {Volume 1 of this timely new series contains key contributions from leading researchers in Europe, America, and Japan describing advanced techniques and applications of microscopy. The articles focus on technques and methods of analysis for quantitative measurements. This key reference includes chapters on interior imaging of materials and electronic devices, short fatigue cracks, surface wave measurements, and current applications of acoustic microscopy to problems in biology.},
	number = {May},
	author = {Imaging, D-sonographic and Technique, Precise and Tumor, Evaluate and View, Volume},
	year = {1995},
}

@article{saijo_recent_2008,
	title = {Recent progress of acoustic microscopy for medicine and biology},
	volume = {123},
	url = {http://scitation.aip.org/content/asa/journal/jasa/123/5/10.1121/1.2932556},
	doi = {10.1121/1.2932556},
	abstract = {Dramatic shortening of the calculation time provided by the recent progress of the computertechnology has made biomedical researchers possible to assess nonlinear acoustic phenomena in soft materials which had been assumed as acting linearly. Besides, the spread of Windows‐based personal computer and peripheral devices enabled easier and cheaper configuration of the whole acoustic microscope system such as pulse generation, analogue/digital conversion, mechanical scanning and image processing. According to these progresses, conventional acoustic microscopy with only C‐mode imaging has widened its data acquisition mode to B‐mode, C‐mode, surfaceacoustic impedance mode, and three‐dimensional (3D) imaging. The base of our acoustic microscope system was consisted of (1) PVDF transducer with the central frequency of 100 MHz, (2) ultrasonic pulser made of high speed semiconductor switching, (3) mechanical scanner using two linear servo motors, (4) high speed PCI card digitizer with the sampling frequency of 2 GHz, and (5) personal computer controlling the whole system. For skin imaging, 3D imaging of the fingerprint was reconstructed by consecutive B‐mode imaging.Surfaceacoustic impedanceimaging of the fingerprint can be obtained by just putting a finger on a thin plastic plate of the transducer. Conventional C‐mode imaging of thinly sliced skin sample presented quantitative values of thickness, attenuation and sound speed of the tissue.},
	number = {5},
	journal = {The Journal of the Acoustical Society of America},
	author = {Saijo, Yoshifumi and Kobayashi, Kazuto and Iwamoto, Takahiro and Okada, Nagaya and Tanaka, Akira and Hozumi, Naohiro},
	year = {2008},
	pages = {2998--2998},
}

@article{pleiss_memory-efficient_2017,
	title = {Memory-{Efficient} {Implementation} of {DenseNets}},
	url = {http://arxiv.org/abs/1707.06990},
	abstract = {The DenseNet architecture is highly computationally efficient as a result of feature reuse. However, a naive DenseNet implementation can require a significant amount of GPU memory: If not properly managed, pre-activation batch normalization and contiguous convolution operations can produce feature maps that grow quadratically with network depth. In this technical report, we introduce strategies to reduce the memory consumption of DenseNets during training. By strategically using shared memory allocations, we reduce the memory cost for storing feature maps from quadratic to linear. Without the GPU memory bottleneck, it is now possible to train extremely deep DenseNets. Networks with 14M parameters can be trained on a single GPU, up from 4M. A 264-layer DenseNet (73M parameters), which previously would have been infeasible to train, can now be trained on a single workstation with 8 NVIDIA Tesla M40 GPUs. On the ImageNet ILSVRC classification dataset, this large DenseNet obtains a state-of-the-art single-crop top-1 error of 20.26\%.},
	author = {Pleiss, Geoff and Chen, Danlu and Huang, Gao and Li, Tongcheng and van der Maaten, Laurens and Weinberger, Kilian Q.},
	month = jul,
	year = {2017},
}

@article{iglovikov_ternausnet_2018,
	title = {{TernausNet}: {U}-{Net} with {VGG11} {Encoder} {Pre}-{Trained} on {ImageNet} for {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1801.05746},
	abstract = {Pixel-wise image segmentation is demanding task in computer vision. Classical U-Net architectures composed of encoders and decoders are very popular for segmentation of medical images, satellite images etc. Typically, neural network initialized with weights from a network pre-trained on a large data set like ImageNet shows better performance than those trained from scratch on a small dataset. In some practical applications, particularly in medicine and traffic safety, the accuracy of the models is of utmost importance. In this paper, we demonstrate how the U-Net type architecture can be improved by the use of the pre-trained encoder. Our code and corresponding pre-trained weights are publicly available at https://github.com/ternaus/TernausNet. We compare three weight initialization schemes: LeCun uniform, the encoder with weights from VGG11 and full network trained on the Carvana dataset. This network architecture was a part of the winning solution (1st out of 735) in the Kaggle: Carvana Image Masking Challenge.},
	author = {Iglovikov, Vladimir and Shvets, Alexey},
	month = jan,
	year = {2018},
}

@article{eulenberg_reconstructing_2017,
	title = {Reconstructing cell cycle and disease progression using deep learning},
	volume = {8},
	url = {http://www.nature.com/articles/s41467-017-00623-3},
	doi = {10.1038/s41467-017-00623-3},
	abstract = {We show that deep convolutional neural networks combined with nonlinear dimension reduction enable reconstructing biological processes based on raw image data. We demonstrate this by reconstructing the cell cycle of Jurkat cells and disease progression in diabetic retinopathy. In further analysis of Jurkat cells, we detect and separate a subpopulation of dead cells in an unsupervised manner and, in classifying discrete cell cycle stages, we reach a sixfold reduction in error rate compared to a recent approach based on boosting on image features. In contrast to previous methods, deep learning based predictions are fast enough for on-the-fly analysis in an imaging flow cytometer.},
	number = {1},
	journal = {Nature Communications},
	author = {Eulenberg, Philipp and Köhler, Niklas and Blasi, Thomas and Filby, Andrew and Carpenter, Anne E. and Rees, Paul and Theis, Fabian J. and Wolf, F. Alexander},
	month = dec,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, multidisciplinary, Science},
	pages = {463--463},
}

@article{zheng_massively_2017,
	title = {Massively parallel digital transcriptional profiling of single cells},
	volume = {8},
	url = {http://www.nature.com/doifinder/10.1038/ncomms14049},
	doi = {10.1038/ncomms14049},
	abstract = {Single-cell gene expression analysis is challenging. This work describes a new droplet-based single cell RNA-seq platform capable of processing tens of thousands of cells across 8 independent samples in minutes, and demonstrates cellular subtypes and host–donor chimerism in transplant patients.},
	journal = {Nature Communications},
	author = {Zheng, Grace X. Y. and Terry, Jessica M. and Belgrader, Phillip and Ryvkin, Paul and Bent, Zachary W. and Wilson, Ryan and Ziraldo, Solongo B. and Wheeler, Tobias D. and McDermott, Geoff P. and Zhu, Junjie and Gregory, Mark T. and Shuga, Joe and Montesclaros, Luz and Underwood, Jason G. and Masquelier, Donald A. and Nishimura, Stefanie Y. and Schnall-Levin, Michael and Wyatt, Paul W. and Hindson, Christopher M. and Bharadwaj, Rajiv and Wong, Alexander and Ness, Kevin D. and Beppu, Lan W. and Deeg, H. Joachim and McFarland, Christopher and Loeb, Keith R. and Valente, William J. and Ericson, Nolan G. and Stevens, Emily A. and Radich, Jerald P. and Mikkelsen, Tarjei S. and Hindson, Benjamin J. and Bielas, Jason H.},
	month = jan,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cancer genomics, Transcriptomics},
	pages = {14049--14049},
}

@article{bray_workflow_2012,
	title = {Workflow and metrics for image quality control in large-scale high-content screens},
	issn = {1087-0571},
	doi = {10.1177/1087057111420292},
	abstract = {Automated microscopes have enabled the unprecedented collection of images at a rate that precludes visual inspection. Automated image analysis is required to identify interesting samples and extract quantitative information for high-content screening (HCS). However, researchers are impeded by the lack of metrics and software tools to identify image-based aberrations that pollute data, limiting experiment quality. The authors have developed and validated approaches to identify those image acquisition artifacts that prevent optimal extraction of knowledge from high-content microscopy experiments. They have implemented these as a versatile, open-source toolbox of algorithms and metrics readily usable by biologists to improve data quality in a wide variety of biological experiments.},
	journal = {Journal of Biomolecular Screening},
	author = {Bray, Mark Anthony and Fraser, Adam N. and Hasaka, Thomas P. and Carpenter, Anne E.},
	year = {2012},
	keywords = {microscopy, image analysis, cell-based assays, high-content screening},
}

@article{sengupta_going_2018,
	title = {Going {Deeper} in {Spiking} {Neural} {Networks}: {VGG} and {Residual} {Architectures}},
	url = {http://arxiv.org/abs/1802.02627},
	abstract = {Over the past few years, Spiking Neural Networks (SNNs) have become popular as a possible pathway to enable low-power event-driven neuromorphic hardware. However, their application in machine learning have largely been limited to very shallow neural network architectures for simple problems. In this paper, we propose a novel algorithmic technique for generating an SNN with a deep architecture, and demonstrate its effectiveness on complex visual recognition problems such as CIFAR-10 and ImageNet. Our technique applies to both VGG and Residual network architectures, with significantly better accuracy than the state-of-the-art. Finally, we present analysis of the sparse event-driven computations to demonstrate reduced hardware overhead when operating in the spiking domain.},
	author = {Sengupta, Abhronil and Ye, Yuting and Wang, Robert and Liu, Chiao and Roy, Kaushik},
	month = feb,
	year = {2018},
}

@article{real_regularized_2018,
	title = {Regularized {Evolution} for {Image} {Classifier} {Architecture} {Search}},
	url = {http://arxiv.org/abs/1802.01548},
	abstract = {The effort devoted to hand-crafting image classifiers has motivated the use of architecture search to discover them automatically. Reinforcement learning and evolution have both shown promise for this purpose. This study employs a regularized version of a popular asynchronous evolutionary algorithm. We rigorously compare it to the non-regularized form and to a highly-successful reinforcement learning baseline. Using the same hardware, compute effort and neural network training code, we conduct repeated experiments side-by-side, exploring different datasets, search spaces and scales. We show regularized evolution consistently produces models with similar or higher accuracy, across a variety of contexts without need for re-tuning parameters. In addition, evolution exhibits considerably better performance than reinforcement learning at early search stages, suggesting it may be the better choice when fewer compute resources are available. This constitutes the first controlled comparison of the two search algorithms in this context. Finally, we present new architectures discovered with evolution that we nickname AmoebaNets. These models set a new state of the art for CIFAR-10 (mean test error = 2.13\%) and mobile-size ImageNet (top-5 accuracy = 92.1\% with 5.06M parameters), and reach the current state of the art for ImageNet (top-5 accuracy = 96.2\%).},
	author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
	month = feb,
	year = {2018},
}

@incollection{van_tulder_why_2015,
	title = {Why does synthesized data improve multi-sequence classification},
	isbn = {978-3-319-24552-2},
	abstract = {The classification and registration of incomplete multi-modal medical images, such as multi-sequence MRI with missing sequences, can sometimes be improved by replacing the missing modalities with synthetic data. This may seem counter-intuitive: synthetic data is derived from data that is already available, so it does not add new information. Why can it still improve performance? In this paper we discuss possible explanations. If the synthesis model is more flexible than the classifier, the synthesis model can provide features that the classifier could not have extracted from the original data. In addition, using synthetic information to complete incomplete samples increases the size of the training set. We present experiments with two classifiers, linear support vector machines (SVMs) and random forests, together with two synthesis methods that can replace missing data in an image classification problem: neural networks and restricted Boltzmann machines (RBMs). We used data from the BRATS 2013 brain tumor segmentation challenge, which includes multi-modal MRI scans with T1, T1 post-contrast, T2 and FLAIR sequences. The linear SVMs appear to benefit from the complex transformations offered by the synthesis models, whereas the random forests mostly benefit from having more training data. Training on the hidden representation from the RBM brought the accuracy of the linear SVMs close to that of random forests.},
	booktitle = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	author = {van Tulder, Gijs and de Bruijne, Marleen},
	year = {2015},
	doi = {10.1007/978-3-319-24553-9_65},
}

@inproceedings{van_tulder_representation_2017,
	title = {Representation learning for cross-modality classification},
	isbn = {978-3-319-61187-7},
	doi = {10.1007/978-3-319-61188-4_12},
	abstract = {© Springer International Publishing AG 2017. Differences in scanning parameters or modalities can complicate image analysis based on supervised classification. This paper presents two representation learning approaches, based on autoencoders, that address this problem by learning representations that are similar across domains. Both approaches use, next to the data representation objective, a similarity objective to minimise the difference between representations of corresponding patches from each domain. We evaluated the methods in transfer learning experiments on multi-modal brain MRI data and on synthetic data. After transforming training and test data from different modalities to the common representations learned by our methods, we trained classifiers for each of pair of modalities. We found that adding the similarity term to the standard objective can produce representations that are more similar and can give a higher accuracy in these cross-modality classification experiments.},
	author = {van Tulder, Gijs and de Bruijne, Marleen},
	year = {2017},
	keywords = {Deep learning, Autoencoders, Multi-modal image analysis, Representation learning, Transfer learning},
}

@article{parr_matrix_2018,
	title = {The {Matrix} {Calculus} {You} {Need} {For} {Deep} {Learning}},
	abstract = {This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don't worry if you get stuck at some point along the way—just go back and reread the previous section, and try writing down and working through some examples. And if you're still stuck, we're happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here.},
	author = {Parr, Terence and Howard, Jeremy},
	year = {2018},
}

@article{wolf_scanpy_2018,
	title = {{SCANPY}: large-scale single-cell gene expression data analysis},
	volume = {19},
	url = {https://genomebiology.biomedcentral.com/articles/10.1186/s13059-017-1382-0},
	doi = {10.1186/s13059-017-1382-0},
	abstract = {Scanpy is a scalable toolkit for analyzing single-cell gene expression data. It includes methods for preprocessing, visualization, clustering, pseudotime and trajectory inference, differential expression testing, and simulation of gene regulatory networks. Its Python-based implementation efficiently deals with data sets of more than one million cells (
https://github.com/theislab/Scanpy
). Along with Scanpy, we present AnnData, a generic class for handling annotated data matrices (
https://github.com/theislab/anndata
).},
	number = {1},
	journal = {Genome Biology},
	author = {Wolf, F. Alexander and Angerer, Philipp and Theis, Fabian J.},
	month = dec,
	year = {2018},
	note = {Publisher: BioMed Central},
	keywords = {Bioinformatics, Animal Genetics and Genomics, Evolutionary Biology, Human Genetics, Microbial Genetics and Genomics, Plant Genetics and Genomics},
	pages = {15--15},
}

@article{parker_scanning_2009,
	title = {Scanning acoustic microscopy for mapping the microstructure of soft materials},
	url = {http://arxiv.org/abs/0904.4832},
	abstract = {Acoustics provides a powerful modality with which to 'see' the mechanical properties of a wide range of elastic materials. It is particularly adept at probing soft materials where excellent contrast and propagation distance can be achieved. We have constructed a scanning acoustic microscope capable of mapping the microstructure of such materials. We review the general principles of scanning acoustic microscopy and present new examples of its application in imaging biological matter, industrial materials and particulate systems.},
	author = {Parker, N. G. and Povey, M. J. W.},
	month = apr,
	year = {2009},
}

@article{meyer_watershed_2012,
	title = {The watershed concept and its use in segmentation : a brief history},
	url = {https://arxiv.org/pdf/1202.0216.pdf},
	author = {Meyer, Fernand},
	year = {2012},
	keywords = {()},
}

@article{van_tulder_combining_2016,
	title = {Combining {Generative} and {Discriminative} {Representation} {Learning} for {Lung} {CT} {Analysis} {With} {Convolutional} {Restricted} {Boltzmann} {Machines}},
	issn = {1558-254X (Electronic) 0278-0062 (Linking)},
	doi = {10.1109/TMI.2016.2526687},
	abstract = {The choice of features greatly influences the performance of a tissue classification system. Despite this, many systems are built with standard, predefined filter banks that are not optimized for that particular application. Representation learning methods such as restricted Boltzmann machines may outperform these standard filter banks because they learn a feature description directly from the training data. Like many other representation learning methods, restricted Boltzmann machines are unsupervised and are trained with a generative learning objective; this allows them to learn representations from unlabeled data, but does not necessarily produce features that are optimal for classification. In this paper we propose the convolutional classification restricted Boltzmann machine, which combines a generative and a discriminative learning objective. This allows it to learn filters that are good both for describing the training data and for classification. We present experiments with feature learning for lung texture classification and airway detection in CT images. In both applications, a combination of learning objectives outperformed purely discriminative or generative learning, increasing, for instance, the lung tissue classification accuracy by 1 to 8 percentage points. This shows that discriminative learning can help an otherwise unsupervised feature learner to learn filters that are optimized for classification.},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Van Tulder, Gijs and De Bruijne, Marleen},
	year = {2016},
	keywords = {machine learning, Deep learning, neural network, segmentation, lung, pattern recognition and classification, representation learning, restricted Boltzmann machine, X-ray imaging and computed tomography},
}

@article{sankaranarayanan_unsupervised_2017,
	title = {Unsupervised {Domain} {Adaptation} for {Semantic} {Segmentation} with {GANs}},
	abstract = {Visual Domain Adaptation is a problem of immense importance in computer vision. Previous approaches showcase the inability of even deep neural networks to learn informative representations across domain shift. This problem is more severe for tasks where acquiring hand labeled data is extremely hard and tedious. In this work, we focus on adapting the representations learned by segmentation networks across synthetic and real domains. Contrary to previous approaches that use a simple adversarial objective or superpixel information to aid the process, we propose an approach based on Generative Adversarial Networks (GANs) that brings the embeddings closer in the learned feature space. To showcase the generality and scalability of our approach, we show that we can achieve state of the art results on two challenging scenarios of synthetic to real domain adaptation. Additional exploratory experiments show that our approach: (1) generalizes to unseen domains and (2) results in improved alignment of source and target distributions.},
	author = {Sankaranarayanan, Swami and Balaji, Yogesh and Jain, Arpit and Lim, Ser Nam and Chellappa, Rama},
	year = {2017},
}

@article{noauthor_full-text_nodate-5,
	title = {full-text},
}

@article{xu_performance_nodate,
	title = {Performance {Evaluation} of {Deep} {Learning} {Tools} in {Docker} {Containers}},
	url = {https://arxiv.org/pdf/1711.03386.pdf},
	abstract = {—With the success of deep learning techniques in a broad range of application domains, many deep learning software frameworks have been developed and are being updated frequently to adapt to new hardware features and software libraries, which bring a big challenge for end users and system administrators. To address this problem, container techniques are widely used to simplify the deployment and management of deep learning software. However, it remains unknown whether con-tainer techniques bring any performance penalty to deep learning applications. The purpose of this work is to systematically evaluate the impact of docker container on the performance of deep learning applications. We first benchmark the performance of system components (IO, CPU and GPU) in a docker container and the host system and compare the results to see if there's any difference. According to our results, we find that computational intensive jobs, either running on CPU or GPU, have small overhead indicating docker containers can be applied to deep learning programs. Then we evaluate the performance of some popular deep learning tools deployed in a docker container and the host system. It turns out that the docker container will not cause noticeable drawbacks while running those deep learning tools. So encapsulating deep learning tool in a container is a feasible solution.},
	author = {Xu, Pengfei and Shi, Shaohuai and Chu, Xiaowen},
}

@article{xia_w-net_2017,
	title = {W-{Net}: {A} {Deep} {Model} for {Fully} {Unsupervised} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1711.08506},
	abstract = {While significant attention has been recently focused on designing supervised deep semantic segmentation algorithms for vision tasks, there are many domains in which sufficient supervised pixel-level labels are difficult to obtain. In this paper, we revisit the problem of purely unsupervised image segmentation and propose a novel deep architecture for this problem. We borrow recent ideas from supervised semantic segmentation methods, in particular by concatenating two fully convolutional networks together into an autoencoder--one for encoding and one for decoding. The encoding layer produces a k-way pixelwise prediction, and both the reconstruction error of the autoencoder as well as the normalized cut produced by the encoder are jointly minimized during training. When combined with suitable postprocessing involving conditional random field smoothing and hierarchical segmentation, our resulting algorithm achieves impressive results on the benchmark Berkeley Segmentation Data Set, outperforming a number of competing methods.},
	author = {Xia, Xide and Kulis, Brian},
	month = nov,
	year = {2017},
}

@article{yang_semantic_nodate,
	title = {Semantic {Segmentation} via {Highly} {Fused} {Convolutional} {Network} with {Multiple} {Soft} {Cost} {Functions}},
	url = {https://arxiv.org/ftp/arxiv/papers/1801/1801.01317.pdf},
	abstract = {Semantic image segmentation is one of the most challenged tasks in computer vision. In this paper, we propose a highly fused convolutional network, which consists of three parts: feature downsampling, combined feature upsampling and multiple predictions. We adopt a strategy of multiple steps of upsampling and combined feature maps in pooling layers with its corresponding unpooling layers. Then we bring out multiple pre-outputs, each pre-output is generated from an unpooling layer by one-step upsampling. Finally, we concatenate these pre-outputs to get the final output. As a result, our proposed network makes highly use of the feature information by fusing and reusing feature maps. In addition, when training our model, we add multiple soft cost functions on pre-outputs and final outputs. In this way, we can reduce the loss reduction when the loss is back propagated. We evaluate our model on three major segmentation datasets: CamVid, PASCAL VOC and ADE20K. We achieve a state-of-the-art performance on CamVid dataset, as well as considerable improvements on PASCAL VOC dataset and ADE20K dataset.},
	author = {Yang, Tao and Wu, Yan and Zhao, Junqiao and Guan, Linting},
	keywords = {highly fused convolutional network, multiple soft cost functions, semantic segmentation},
}

@article{krizek_celltracker_nodate,
	title = {Celltracker: {Cell} tracking challenge {ISBI} 2013},
	url = {http://www.codesolorzano.com/Challenges/CTC/CUNI-CZ_2013_files/isbi496.pdf},
	abstract = {Automated segmentation of objects like cells or cell nuclei and their tracking in time is an important procedure for biological research. For segmentation, we propose a simple method based on the k-means threshold selection algorithm combined with a sliding neighborhood approach. For tracking of segmented objects, we applied a simple method based on frame-by-frame association of nearest neighbors. The algorithm was originally designed for data from our Leica SP5 confocal and Andor spinning disk microscopes and is based on the experience the authors developed in previous projects. The code is written in Matlab as a single threaded application. Algorithm description},
	author = {Křížek, Pavel and Hagen, Guy M},
}

@article{noauthor_human_2017,
	title = {{THE} {HUMAN} {CELL} {ATLAS}},
	url = {https://www.humancellatlas.org/files/HCA_WhitePaper_18Oct2017.pdf},
	year = {2017},
}

@article{orloff_cell_2013,
	title = {The cell: an image library-{CCDB}: a curated repository of microscopy data.},
	volume = {41},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/23203874},
	doi = {10.1093/nar/gks1257},
	abstract = {The cell: an image library-CCDB (CIL-CCDB) (http://www.cellimagelibrary.org) is a searchable database and archive of cellular images. As a repository for microscopy data, it accepts all forms of cell imaging from light and electron microscopy, including multi-dimensional images, Z- and time stacks in a broad variety of raw-data formats, as well as movies and animations. The software design of CIL-CCDB was intentionally designed to allow easy incorporation of new technologies and image formats as they are developed. Currently, CIL-CCDB contains over 9250 images from 358 different species. Images are evaluated for quality and annotated with terms from 14 different ontologies in 16 different fields as well as a basic description and technical details. Since its public launch on 9 August 2010, it has been designed to serve as not only an archive but also an active site for researchers and educators.},
	number = {Database issue},
	journal = {Nucleic acids research},
	author = {Orloff, David N and Iwasa, Janet H and Martone, Maryann E and Ellisman, Mark H and Kane, Caroline M},
	month = jan,
	year = {2013},
	note = {Publisher: Oxford University Press},
	pages = {D1241--50},
}

@article{wang_tienet_2018,
	title = {{TieNet}: {Text}-{Image} {Embedding} {Network} for {Common} {Thorax} {Disease} {Classification} and {Reporting} in {Chest} {X}-rays},
	url = {http://arxiv.org/abs/1801.04334},
	abstract = {Chest X-rays are one of the most common radiological examinations in daily clinical routines. Reporting thorax diseases using chest X-rays is often an entry-level task for radiologist trainees. Yet, reading a chest X-ray image remains a challenging job for learning-oriented machine intelligence, due to (1) shortage of large-scale machine-learnable medical image datasets, and (2) lack of techniques that can mimic the high-level reasoning of human radiologists that requires years of knowledge accumulation and professional training. In this paper, we show the clinical free-text radiological reports can be utilized as a priori knowledge for tackling these two key problems. We propose a novel Text-Image Embedding network (TieNet) for extracting the distinctive image and text representations. Multi-level attention models are integrated into an end-to-end trainable CNN-RNN architecture for highlighting the meaningful text words and image regions. We first apply TieNet to classify the chest X-rays by using both image features and text embeddings extracted from associated reports. The proposed auto-annotation framework achieves high accuracy (over 0.9 on average in AUCs) in assigning disease labels for our hand-label evaluation dataset. Furthermore, we transform the TieNet into a chest X-ray reporting system. It simulates the reporting process and can output disease classification and a preliminary report together. The classification results are significantly improved (6\% increase on average in AUCs) compared to the state-of-the-art baseline on an unseen and hand-labeled dataset (OpenI).},
	author = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Summers, Ronald M.},
	month = jan,
	year = {2018},
}

@article{caelles_one-shot_nodate,
	title = {One-{Shot} {Video} {Object} {Segmentation}},
	url = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Caelles_One-Shot_Video_Object_CVPR_2017_paper.pdf},
	abstract = {Figure 1. Example result of our technique: The segmentation of the first frame (red) is used to learn the model of the specific object to track, which is segmented in the rest of the frames independently (green). One every 20 frames shown of 90 in total. Abstract This paper tackles the task of semi-supervised video ob-ject segmentation, i.e., the separation of an object from the background in a video, given the mask of the first frame. We present One-Shot Video Object Segmentation (OSVOS), based on a fully-convolutional neural network architecture that is able to successively transfer generic semantic infor-mation, learned on ImageNet, to the task of foreground seg-mentation, and finally to learning the appearance of a sin-gle annotated object of the test sequence (hence one-shot). Although all frames are processed independently, the re-sults are temporally coherent and stable. We perform ex-periments on two annotated video segmentation databases, which show that OSVOS is fast and improves the state of the art by a significant margin (79.8\% vs 68.0\%).},
	author = {Caelles, S and Maninis, K.-K and Pont-Tuset, J and Leal-Taixé, L and Cremers, D and Gool, L Van},
}

@article{ching_opportunities_2017,
	title = {Opportunities {And} {Obstacles} {For} {Deep} {Learning} {In} {Biology} {And} {Medicine}},
	url = {https://www.biorxiv.org/content/early/2017/05/28/142760},
	doi = {10.1101/142760},
	abstract = {Deep learning, which describes a class of machine learning algorithms, has recently showed impressive results across a variety of domains. Biology and medicine are data rich, but the data are complex and often ill-understood. Problems of this nature may be particularly well-suited to deep learning techniques. We examine applications of deep learning to a variety of biomedical problems -- patient classification, fundamental biological processes, and treatment of patients -- to predict whether deep learning will transform these tasks or if the biomedical sphere poses unique challenges. We find that deep learning has yet to revolutionize or definitively resolve any of these problems, but promising advances have been made on the prior state of the art. Even when improvement over a previous baseline has been modest, we have seen signs that deep learning methods may speed or aid human investigation. More work is needed to address concerns related to interpretability and how to best model each problem. Furthermore, the limited amount of labeled data for training presents problems in some domains, as can legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning powering changes at the bench and bedside with the potential to transform several areas of biology and medicine.},
	journal = {bioRxiv},
	author = {Ching, Travers and Himmelstein, Daniel S. and Beaulieu-Jones, Brett K. and Kalinin, Alexandr A. and Do, Brian T. and Way, Gregory P. and Ferrero, Enrico and Agapow, Paul-Michael and Xie, Wei and Rosen, Gail L. and Lengerich, Benjamin J. and Israeli, Johnny and Lanchantin, Jack and Woloszynek, Stephen and Carpenter, Anne E. and Shrikumar, Avanti and Xu, Jinbo and Cofer, Evan M. and Harris, David J. and DeCaprio, Dave and Qi, Yanjun and Kundaje, Anshul and Peng, Yifan and Wiley, Laura K. and Segler, Marwin H. S. and Gitter, Anthony and Greene, Casey S.},
	month = may,
	year = {2017},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {142760--142760},
}

@article{zhang_understanding_2016,
	title = {Understanding deep learning requires rethinking generalization},
	url = {http://arxiv.org/abs/1611.03530},
	abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	month = nov,
	year = {2016},
}

@article{micikevicius_mixed_2017,
	title = {Mixed {Precision} {Training}},
	url = {http://arxiv.org/abs/1710.03740},
	abstract = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
	author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
	month = oct,
	year = {2017},
}

@article{pietzsch_imglib2generic_2012,
	title = {{ImgLib2}—generic image processing in {Java}},
	volume = {28},
	url = {http://imglib2.net},
	doi = {10.1093/bioinformatics/bts543},
	abstract = {ImgLib2 is an open-source Java library for n-dimensional data representation and manipulation with focus on image processing. It aims at minimizing code duplication by cleanly separating pixel-algebra, data access and data representation in memory. Algorithms can be implemented for classes of pixel types and generic access patterns by which they become independent of the specific dimen-sionality, pixel type and data representation. ImgLib2 illustrates that an elegant high-level programming interface can be achieved without sacrificing performance. It provides efficient implementations of common data types, storage layouts and algorithms. It is the data model underlying ImageJ2, the KNIME Image Processing toolbox and an increasing number of Fiji-Plugins.},
	number = {22},
	author = {Pietzsch, Tobias and Preibisch, Stephan and Tomanč, Pavel and Saalfeld, Stephan},
	year = {2012},
	pages = {3009--3011},
}

@article{fang_creatism_2017,
	title = {Creatism: {A} deep-learning photographer capable of creating professional work},
	url = {http://arxiv.org/abs/1707.03491},
	abstract = {Machine-learning excels in many areas with well-defined goals. However, a clear goal is usually not available in art forms, such as photography. The success of a photograph is measured by its aesthetic value, a very subjective concept. This adds to the challenge for a machine learning approach. We introduce Creatism, a deep-learning system for artistic content creation. In our system, we break down aesthetics into multiple aspects, each can be learned individually from a shared dataset of professional examples. Each aspect corresponds to an image operation that can be optimized efficiently. A novel editing tool, dramatic mask, is introduced as one operation that improves dramatic lighting for a photo. Our training does not require a dataset with before/after image pairs, or any additional labels to indicate different aspects in aesthetics. Using our system, we mimic the workflow of a landscape photographer, from framing for the best composition to carrying out various post-processing operations. The environment for our virtual photographer is simulated by a collection of panorama images from Google Street View. We design a "Turing-test"-like experiment to objectively measure quality of its creations, where professional photographers rate a mixture of photographs from different sources blindly. Experiments show that a portion of our robot's creation can be confused with professional work.},
	author = {Fang, Hui and Zhang, Meng},
	month = jul,
	year = {2017},
}

@article{micikevicius_mixed_2017-1,
	title = {Mixed {Precision} {Training}},
	url = {http://arxiv.org/abs/1710.03740},
	abstract = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
	author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
	month = oct,
	year = {2017},
}

@article{li_universal_2017,
	title = {Universal {Style} {Transfer} via {Feature} {Transforms}},
	url = {http://arxiv.org/abs/1705.08086},
	author = {Li, Yijun and Fang, Chen and Yang, Jimei and Wang, Zhaowen and Lu, Xin and Yang, Ming-Hsuan},
	month = may,
	year = {2017},
}

@article{weigert_content-aware_2017,
	title = {Content-{Aware} {Image} {Restoration}: {Pushing} the {Limits} of {Fluorescence} {Microscopy}},
	url = {http://dx.doi.org/10.1101/236463},
	doi = {10.1101/236463},
	abstract = {Fluorescence microscopy is a key driver of discoveries in the life-sciences, with observable phenomena being limited by the optics of the microscope, the chemistry of the fluorophores, and the maximum photon exposure tolerated by the sample. These limits necessitate trade-offs between imaging speed, spatial resolution, light exposure, and imaging depth. In this work we show how deep learning enables biological observations beyond the physical lim-itations of microscopes. On seven concrete examples we illustrate how microscopy images can be restored even if 60-fold fewer photons are used during acquisition, how isotropic res-olution can be achieved even with a 10-fold under-sampling along the axial direction, and how diffraction-limited structures can be resolved at 20-times higher frame-rates compared to state-of-the-art methods. All developed image restoration methods are freely available as open source software.},
	author = {Weigert, Martin and Schmidt, Uwe and Boothe, Tobias and Müller, Andreas and Dibrov, Alexandr and Jain, Akanksha and Wilhelm, Benjamin and Schmidt, Deborah and Broaddus, Coleman and Culley, Siân and Rocha-Martins, Mauricio and Segovia-Miranda, Fabián and Norden, Caren and Henriques, Ricardo and Zerial, Marino and Solimena, Michele and Rink, Jochen and Tomancak, Pavel and Royer, Loic and Jug, Florian and Myers, Eugene W},
	year = {2017},
}

@article{steinberg_preliminary_2016,
	title = {Preliminary {Safety} of {Intracranial} {Implantation} of {Modified} {Bone} {Marrow} {Mesenchymal} {Stem} {Cells} ({SB623}) in {Stroke} {Patients}: {A} {Phase} 1/{2A} {Study}},
	volume = {97},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S000399931630990X},
	doi = {10.1016/j.apmr.2016.09.004},
	number = {12},
	journal = {Archives of Physical Medicine and Rehabilitation},
	author = {Steinberg, Gary and Liu, Wenzhong Jerry and Bates, Damien and Kondziolka, Douglas and Wechsler, Lawrence and Lunsford, L. Dade and Kim, Anthony S. and Johnson, Jeremiah N. and Schwartz, Neil E.},
	month = dec,
	year = {2016},
	pages = {e2--e2},
}

@article{jouppi_-datacenter_2017,
	title = {In-{Datacenter} {Performance} {Analysis} of a {Tensor} {Processing} {Unit}},
	url = {http://arxiv.org/abs/1704.04760},
	abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU)---deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs (caches, out-of-order execution, multithreading, multiprocessing, prefetching, ...) that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X - 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X - 80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
	author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
	month = apr,
	year = {2017},
}

@article{wang_high-resolution_2017,
	title = {High-{Resolution} {Image} {Synthesis} and {Semantic} {Manipulation} with {Conditional} {GANs}},
	url = {http://arxiv.org/abs/1711.11585},
	author = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
	month = nov,
	year = {2017},
}

@article{steinberg_improvement_2016,
	title = {Improvement in {Motor} {Function} {Following} {Intracranial} {Implantation} of {Modified} {Bone} {Marrow} {Mesenchymal} {Stem} {Cells} ({SB623}) for {Chronic} {Ischemic} {Stroke}},
	volume = {97},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0003999316309984},
	doi = {10.1016/j.apmr.2016.09.012},
	number = {12},
	journal = {Archives of Physical Medicine and Rehabilitation},
	author = {Steinberg, Gary and Bates, Damien and Liu, Wenzhong Jerry and Kondziolka, Douglas and Wechsler, Lawrence and Lunsford, L. Dade and Kim, Anthony S. and Johnson, Jeremiah N. and Schwartz, Neil E.},
	month = dec,
	year = {2016},
	note = {Publisher: Elsevier},
	pages = {e5--e5},
}

@article{steinberg_clinical_2016,
	title = {Clinical {Outcomes} of {Transplanted} {Modified} {Bone} {Marrow}-{Derived} {Mesenchymal} {Stem} {Cells} in {Stroke}: {A} {Phase} 1/2a {Study}.},
	volume = {47},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/27256670},
	doi = {10.1161/STROKEAHA.116.012995},
	abstract = {BACKGROUND AND PURPOSE Preclinical data suggest that cell-based therapies have the potential to improve stroke outcomes. METHODS Eighteen patients with stable, chronic stroke were enrolled in a 2-year, open-label, single-arm study to evaluate the safety and clinical outcomes of surgical transplantation of modified bone marrow-derived mesenchymal stem cells (SB623). RESULTS All patients in the safety population (N=18) experienced at least 1 treatment-emergent adverse event. Six patients experienced 6 serious treatment-emergent adverse events; 2 were probably or definitely related to surgical procedure; none were related to cell treatment. All serious treatment-emergent adverse events resolved without sequelae. There were no dose-limiting toxicities or deaths. Sixteen patients completed 12 months of follow-up at the time of this analysis. Significant improvement from baseline (mean) was reported for: (1) European Stroke Scale: mean increase 6.88 (95\% confidence interval, 3.5-10.3; P{\textless}0.001), (2) National Institutes of Health Stroke Scale: mean decrease 2.00 (95\% confidence interval, -2.7 to -1.3; P{\textless}0.001), (3) Fugl-Meyer total score: mean increase 19.20 (95\% confidence interval, 11.4-27.0; P{\textless}0.001), and (4) Fugl-Meyer motor function total score: mean increase 11.40 (95\% confidence interval, 4.6-18.2; P{\textless}0.001). No changes were observed in modified Rankin Scale. The area of magnetic resonance T2 fluid-attenuated inversion recovery signal change in the ipsilateral cortex 1 week after implantation significantly correlated with clinical improvement at 12 months (P{\textless}0.001 for European Stroke Scale). CONCLUSIONS In this interim report, SB623 cells were safe and associated with improvement in clinical outcome end points at 12 months. CLINICAL TRIAL REGISTRATION URL: https://www.clinicaltrials.gov. Unique identifier: NCT01287936.},
	number = {7},
	journal = {Stroke},
	author = {Steinberg, Gary K and Kondziolka, Douglas and Wechsler, Lawrence R and Lunsford, L Dade and Coburn, Maria L and Billigen, Julia B and Kim, Anthony S and Johnson, Jeremiah N and Bates, Damien and King, Bill and Case, Casey and McGrogan, Michael and Yankee, Ernest W and Schwartz, Neil E},
	month = jul,
	year = {2016},
	note = {Publisher: American Heart Association, Inc.},
	keywords = {stem cells, allogeneic transplantation, mesenchymal stromal cells, Notch 1, phase 1 clinical trial, stereotactic techniques, stroke},
	pages = {1817--24},
}

@article{delcroix_adult_2010,
	title = {Adult cell therapy for brain neuronal damages and the role of tissue engineering},
	volume = {31},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0142961209013283},
	doi = {10.1016/j.biomaterials.2009.11.084},
	number = {8},
	journal = {Biomaterials},
	author = {Delcroix, Gaëtan J.-R. and Schiller, Paul C. and Benoit, Jean-Pierre and Montero-Menei, Claudia N.},
	month = mar,
	year = {2010},
	pages = {2105--2120},
}

@article{liu_wide_nodate,
	title = {Wide {Inference} {Network} for {Image} {Denoising} via {Learning} {Pixel}-distribution {Prior}},
	url = {https://arxiv.org/pdf/1707.05414.pdf},
	abstract = {We explore an innovative strategy for image denoising by using convolutional neural networks (CNN) to learn similar pixel-distribution features from noisy images. Many types of image noise follow a certain pixel-distribution in common, such as additive white Gaussian noise (AWGN). By increasing CNN's width with larger reception fields and more channels in each layer, CNNs can reveal the ability to extract more accurate pixel-distribution features. The key to our approach is a discovery that wider CNNs with more convolutions tend to learn the similar pixel-distribution features, which reveals a new strategy to solve low-level vision problems effectively that the inference mapping primarily relies on the priors behind the noise property instead of deeper CNNs with more stacked nonlinear layers. We evaluate our work, Wide inference Networks (WIN), on AWGN and demonstrate that by learning pixel-distribution features from images, WIN-based network consistently achieves significantly better performance than current state-of-the-art deep CNN-based methods in both quantitative and visual evaluations. Code and models are available at https:// github.com/ cswin/ WIN.},
	author = {Liu, Peng and Fang, Ruogu},
}

@article{wang_ischemic_2014,
	title = {Ischemic stroke and repair: current trends in research and tissue engineering treatments},
	volume = {2},
	url = {http://regenmedres.biomedcentral.com/articles/10.1186/2050-490X-2-3},
	doi = {10.1186/2050-490X-2-3},
	abstract = {Stroke, the third leading cause of mortality, is usually associated with severe disabilities, high recurrence rate and other poor outcomes. Currently, there are no long-term effective treatments for stroke. Cell and cytokine therapies have been explored previously. However, the therapeutic outcomes are often limited by poor survival of transplanted cells, uncontrolled cell differentiation, ineffective engraftment with host tissues and non-sustained delivery of growth factors. A tissue-engineering approach provides an alternative for treating ischemic stroke. The key design considerations for the tissue engineering approach include: choice of scaffold materials, choice of cells and cytokines and delivery methods. Here, we review current cell and biomaterial based therapies available for ischemic stroke, with a special focus on tissue-engineering strategies for regeneration of stroke-affected neuronal tissue.},
	number = {1},
	journal = {Regenerative Medicine Research},
	author = {Wang, Jian and Yang, Wen and Xie, Hongjian and Song, Yu and Li, Yongkui and Wang, Lin},
	month = feb,
	year = {2014},
	note = {Publisher: BioMed Central},
	keywords = {Cell Biology, Stem Cells},
	pages = {3--3},
}

@article{hawkins_theory_2017,
	title = {A {Theory} of {How} {Columns} in the {Neocortex} {Enable} {Learning} the {Structure} of the {World}},
	doi = {10.3389/fncir.2017.00081},
	abstract = {Neocortical regions are organized into columns and layers. Connections between layers run mostly perpendicular to the surface suggesting a columnar functional organization. Some layers have long-range excitatory lateral connections suggesting interactions between columns. Similar patterns of connectivity exist in all regions but their exact role remain a mystery. In this paper, we propose a network model composed of columns and layers that performs robust object learning and recognition. Each column integrates its changing input over time to learn complete predictive models of observed objects. Excitatory lateral connections across columns allow the network to more rapidly infer objects based on the partial knowledge of adjacent columns. Because columns integrate input over time and space, the network learns models of complex objects that extend well beyond the receptive field of individual cells. Our network model introduces a new feature to cortical columns. We propose that a representation of location relative to the object being sensed is calculated within the sub-granular layers of each column. The location signal is provided as an input to the network, where it is combined with sensory data. Our model contains two layers and one or more columns. Simulations show that using Hebbian-like learning rules small single-column networks can learn to recognize hundreds of objects, with each object containing tens of features. Multi-column networks recognize objects with significantly fewer movements of the sensory receptors. Given the ubiquity of columnar and laminar connectivity patterns throughout the neocortex, we propose that columns and regions have more powerful recognition and modeling capabilities than previously assumed.},
	journal = {Frontiers in Neural Circuits},
	author = {Hawkins, Jeff and Ahmad, Subutai and Cui, Yuwei},
	year = {2017},
}

@article{bae_beyond_nodate,
	title = {Beyond {Deep} {Residual} {Learning} for {Image} {Restoration}: {Persistent} {Homology}-{Guided} {Manifold} {Simplification}},
	url = {https://arxiv.org/pdf/1611.06345.pdf},
	abstract = {The latest deep learning approaches perform better than the state-of-the-art signal processing approaches in vari-ous image restoration tasks. However, if an image contains many patterns and structures, the performance of these CNNs is still inferior. To address this issue, here we pro-pose a novel feature space deep residual learning algo-rithm that outperforms the existing residual learning. The main idea is originated from the observation that the perfor-mance of a learning algorithm can be improved if the input and/or label manifolds can be made topologically simpler by an analytic mapping to a feature space. Our extensive numerical studies using denoising experiments and NTIRE single-image super-resolution (SISR) competition demon-strate that the proposed feature space residual learning out-performs the existing state-of-the-art approaches. More-over, our algorithm was ranked third in NTIRE competition with 5-10 times faster computational time compared to the top ranked teams. The source code is available on page : https://github.com/iorism/CNN.git},
	author = {Bae, Woong and Yoo, Jaejun and Ye, Jong Chul},
}

@book{celko_graph_2014,
	series = {Joe {Celko}’s {Complete} {Guide} to {NoSQL}},
	title = {Graph {Databases}},
	isbn = {978-0-12-407192-6},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780124071926000030},
	abstract = {Abstract Graph databases model relationships, not data. Just as SQL and RDBMS are based on logic and set theory, graph databases are based on graph theory. The classic example of a popular graph problem is called the “Kevin Bacon problem” in the literature. Given actor Kevin Bacon, a graph database links him to anyone else in the movie industry either by a direct link (had some role in a movie with Kevin Bacon) or by a chain of links (had some role in a movie with someone who had some role in a movie with Kevin Bacon). However, other graph problems look for the shortest paths among a set of paths in a transportation network, the smallest set of nodes that cover a graph, and so forth. There is no ANSI/ISO standard language for graphs, but the most popular ones are Gremlin, Neo4j, and SPARQL. They tend to model their syntax after SQL and/or mathematics.},
	author = {Celko, Joe},
	year = {2014},
	doi = {10.1016/B978-0-12-407192-6.00003-0},
	note = {Pages: 46},
}

@article{karras_progressive_2017,
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	url = {http://arxiv.org/abs/1710.10196},
	abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024{\textasciicircum}2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	month = oct,
	year = {2017},
}

@article{karras_progressive_2017-1,
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	url = {http://arxiv.org/abs/1710.10196},
	abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024{\textasciicircum}2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	month = oct,
	year = {2017},
}

@article{sabour_dynamic_2017-1,
	title = {Dynamic {Routing} {Between} {Capsules}},
	url = {http://arxiv.org/abs/1710.09829},
	abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation paramters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
	month = oct,
	year = {2017},
}

@article{christiano_deep_nodate,
	title = {Deep {Reinforcement} {Learning} from {Human} {Preferences}},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1\% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
	author = {Christiano, Paul F and Leike, Jan and Brown, Tom B and Martic, Miljan and Legg, Shane and Amodei, Dario},
}

@article{noauthor_toward_nodate,
	title = {Toward a {Thinking} {Microscope}: {Deep} {Learning} in {Optical} {Microscopy} and {Image} {Reconstruction}},
	url = {http://innovate.ee.ucla.edu/welcome.html},
	abstract = {We discuss recently emerging applications of the state-of-art deep learning methods on optical microscopy and microscopic image reconstruction, which enable new transformations among different modes and modalities of microscopic imaging, driven entirely by image data. We believe that deep learning will fundamentally change both the hardware and image reconstruction methods used in optical microscopy in a holistic manner.},
	keywords = {Aydogan Ozcan, Yair Rivenson},
	pages = {1--11},
}

@techreport{karpathy_automated_nodate,
	title = {Automated {Image} {Captioning} with {ConvNets} and {Recurrent} {Nets}},
	url = {https://cs.stanford.edu/people/karpathy/sfmltalk.pdf},
	author = {Karpathy, Andrej and Li, Fei-Fei},
}

@article{bedi_automated_2015,
	title = {Automated analysis of free speech predicts psychosis onset in high-risk youths},
	volume = {1},
	url = {http://www.nature.com/articles/npjschz201530},
	doi = {10.1038/npjschz.2015.30},
	abstract = {A computer program that analyses natural speech could help predict the onset of psychosis in young people at risk. People with schizophrenia have subtle disorganization in speech, even before they first develop psychosis. In a collaboration between IBM, Columbia University Medical Center, and researchers in South America, an automated program that simulates how the human brain understands language was used to analyze interview transcripts from 34 ‘at risk’ youths. Decrease in the flow of meaning from one spoken phrase to the next, and grammatical markers of speech complexity, identified the five individuals who later developed psychosis. The computer program outperformed clinical assessments in predicting psychosis. While numbers are small in this proof-of-principle study, the authors suggest automated analysis could lay the foundation for a simple clinical test of emerging schizophrenia, which would inform early intervention.},
	number = {1},
	journal = {npj Schizophrenia},
	author = {Bedi, Gillinder and Carrillo, Facundo and Cecchi, Guillermo A and Slezak, Diego Fernández and Sigman, Mariano and Mota, Natália B and Ribeiro, Sidarta and Javitt, Daniel C and Copelli, Mauro and Corcoran, Cheryl M},
	month = dec,
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	keywords = {Neuroscience, Schizophrenia},
	pages = {15030--15030},
}

@article{cruz-roa_accurate_2017,
	title = {Accurate and reproducible invasive breast cancer detection in whole-slide images: {A} {Deep} {Learning} approach for quantifying tumor extent},
	volume = {7},
	url = {http://www.nature.com/articles/srep46450},
	doi = {10.1038/srep46450},
	abstract = {Accurate and reproducible invasive breast cancer detection in whole-slide images: A Deep Learning approach for quantifying tumor extent},
	number = {1},
	journal = {Scientific Reports},
	author = {Cruz-Roa, Angel and Gilmore, Hannah and Basavanhally, Ajay and Feldman, Michael and Ganesan, Shridar and Shih, Natalie N.C. and Tomaszewski, John and González, Fabio A. and Madabhushi, Anant},
	month = dec,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {Breast cancer, Biomedical engineering, Machine learning, Medical imaging, Computer science},
	pages = {46450--46450},
}

@techreport{yang_image_nodate,
	title = {Image {Captioning} with {Object} {Detection} and {Localization}},
	url = {https://arxiv.org/ftp/arxiv/papers/1706/1706.02430.pdf},
	abstract = {Automatically generating a natural language description of an image is a task close to the heart of image understanding. In this paper, we present a multi-model neural network method closely related to the human visual system that automatically learns to describe the content of images. Our model consists of two sub-models: an object detection and localization model, which extract the information of objects and their spatial relationship in images respectively; Besides, a deep recurrent neural network (RNN) based on long short-term memory (LSTM) units with attention mechanism for sentences generation. Each word of the description will be automatically aligned to different objects of the input image when it is generated. This is similar to the attention mechanism of the human visual system. Experimental results on the COCO dataset showcase the merit of the proposed method, which outperform previous benchmark models.},
	author = {Yang, Zhongliang and Zhang, Yu-Jin and Huang, Yongfeng},
	keywords = {Deep learning, Neural networks, Object detection, Image caption},
}

@article{noauthor_pubmed_nodate,
	title = {{PubMed}},
	url = {https://www.ncbi.nlm.nih.gov/pubmed},
	abstract = {PubMed - NCBI},
}

@article{noauthor_genbank_nodate,
	title = {Genbank},
	url = {https://www.ncbi.nlm.nih.gov/nucleotide/},
	abstract = {Nucleotide - NCBI},
}

@article{noauthor_ddbj_nodate,
	title = {{DDBJ} {Center}},
	url = {https://www.ddbj.nig.ac.jp/index-e.html},
}

@article{noauthor_uniprot_nodate,
	title = {{UniProt}},
	url = {https://www.uniprot.org/},
}

@article{chen_learning_2018,
	title = {Learning to {See} in the {Dark}},
	url = {http://arxiv.org/abs/1805.01934},
	abstract = {Imaging in low light is challenging due to low photon count and low SNR. Short-exposure images suffer from noise, while long exposure can induce blur and is often impractical. A variety of denoising, deblurring, and enhancement techniques have been proposed, but their effectiveness is limited in extreme conditions, such as video-rate imaging at night. To support the development of learning-based pipelines for low-light image processing, we introduce a dataset of raw short-exposure low-light images, with corresponding long-exposure reference images. Using the presented dataset, we develop a pipeline for processing low-light images, based on end-to-end training of a fully-convolutional network. The network operates directly on raw sensor data and replaces much of the traditional image processing pipeline, which tends to perform poorly on such data. We report promising results on the new dataset, analyze factors that affect performance, and highlight opportunities for future work. The results are shown in the supplementary video at https://youtu.be/qWKUFK7MWvg},
	author = {Chen, Chen and Chen, Qifeng and Xu, Jia and Koltun, Vladlen},
	month = may,
	year = {2018},
}

@article{noauthor_ena_nodate,
	title = {{ENA}},
	url = {https://www.ebi.ac.uk/ena},
	abstract = {European Nucleotide Archive {\textless} EMBL-EBI},
}

@article{wang_supplemental_nodate,
	title = {Supplemental {Material}},
	author = {Wang, Yifan and Perazzi, Federico and Mcwilliams, Brian and Olga, Alexander Sorkine-hornung and Schroers, Sorkine-hornung Christopher and Onv, C and Onv, C and Lu, E and Onv, C and Ool, E Lu-a V G P and Onv, C and Ool, E Lu-a V G P and Onv, C and Ool, E Lu-a V G P and Onv, C and Lu, E},
}

@article{wang_fully_2018,
	title = {A {Fully} {Progressive} {Approach} to {Single}-{Image} {Super}-{Resolution}},
	url = {http://arxiv.org/abs/1804.02900},
	abstract = {Recent deep learning approaches to single image super-resolution have achieved impressive results in terms of traditional error measures and perceptual quality. However, in each case it remains challenging to achieve high quality results for large upsampling factors. To this end, we propose a method (ProSR) that is progressive both in architecture and training: the network upsamples an image in intermediate steps, while the learning process is organized from easy to hard, as is done in curriculum learning. To obtain more photorealistic results, we design a generative adversarial network (GAN), named ProGanSR, that follows the same progressive multi-scale design principle. This not only allows to scale well to high upsampling factors (e.g., 8x) but constitutes a principled multi-scale approach that increases the reconstruction quality for all upsampling factors simultaneously. In particular ProSR ranks 2nd in terms of SSIM and 4th in terms of PSNR in the NTIRE2018 SISR challenge [34]. Compared to the top-ranking team, our model is marginally lower, but runs 5 times faster.},
	author = {Wang, Yifan and Perazzi, Federico and McWilliams, Brian and Sorkine-Hornung, Alexander and Sorkine-Hornung, Olga and Schroers, Christopher},
	month = apr,
	year = {2018},
}

@article{sullivan_deep_2018,
	title = {Deep learning is combined with massive-scale citizen science to improve large-scale image classification},
	url = {http://www.nature.com/doifinder/10.1038/nbt.4225},
	doi = {10.1038/nbt.4225},
	abstract = {Pattern recognition in imaging data by {\textgreater}300,000 players of a global, online, commercial computer game is combined with deep learning to improve the accuracy of annotation of subcellular protein localization.},
	journal = {Nature Biotechnology},
	author = {Sullivan, Devin P and Winsnes, Casper F and Åkesson, Lovisa and Hjelmare, Martin and Wiking, Mikaela and Schutten, Rutger and Campbell, Linzi and Leifsson, Hjalti and Rhodes, Scott and Nordgren, Andie and Smith, Kevin and Revaz, Bernard and Finnbogason, Bergur and Szantner, Attila and Lundberg, Emma},
	year = {2018},
	file = {Full Text:/home/zwerg/Zotero/storage/MKVFVPQK/Sullivan et al. - 2018 - Deep learning is combined with massive-scale citiz.pdf:application/pdf},
}

@article{li_protein_nodate,
	title = {Protein remote homology detection based on bidirectional long short-term memory},
	url = {http://www.bioinf.jku.at/software/LSTM_protein/.},
	doi = {10.1186/s12859-017-1842-2},
	abstract = {Background: Protein remote homology detection plays a vital role in studies of protein structures and functions. Almost all of the traditional machine leaning methods require fixed length features to represent the protein sequences. However, it is never an easy task to extract the discriminative features with limited knowledge of proteins. On the other hand, deep learning technique has demonstrated its advantage in automatically learning representations. It is worthwhile to explore the applications of deep learning techniques to the protein remote homology detection.},
	author = {Li, Shumin and Chen, Junjie and Liu, Bin},
}

@article{liu_deep_2017,
	title = {Deep {Recurrent} {Neural} {Network} for {Protein} {Function} {Prediction} from {Sequence}},
	url = {https://arxiv.org/abs/1701.08318},
	author = {Liu, Xueliang},
	month = jan,
	year = {2017},
}

@article{hochreiter_fast_2007,
	title = {Fast model-based protein homology detection without alignment},
	volume = {23},
	url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btm247},
	doi = {10.1093/bioinformatics/btm247},
	number = {14},
	journal = {Bioinformatics},
	author = {Hochreiter, S. and Heusel, M. and Obermayer, K.},
	month = jul,
	year = {2007},
	note = {Publisher: Oxford University Press},
	pages = {1728--1736},
}

@article{li_protein_2017,
	title = {Protein remote homology detection based on bidirectional long short-term memory.},
	volume = {18},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/29017445},
	doi = {10.1186/s12859-017-1842-2},
	abstract = {BACKGROUND Protein remote homology detection plays a vital role in studies of protein structures and functions. Almost all of the traditional machine leaning methods require fixed length features to represent the protein sequences. However, it is never an easy task to extract the discriminative features with limited knowledge of proteins. On the other hand, deep learning technique has demonstrated its advantage in automatically learning representations. It is worthwhile to explore the applications of deep learning techniques to the protein remote homology detection. RESULTS In this study, we employ the Bidirectional Long Short-Term Memory (BLSTM) to learn effective features from pseudo proteins, also propose a predictor called ProDec-BLSTM: it includes input layer, bidirectional LSTM, time distributed dense layer and output layer. This neural network can automatically extract the discriminative features by using bidirectional LSTM and the time distributed dense layer. CONCLUSION Experimental results on a widely-used benchmark dataset show that ProDec-BLSTM outperforms other related methods in terms of both the mean ROC and mean ROC50 scores. This promising result shows that ProDec-BLSTM is a useful tool for protein remote homology detection. Furthermore, the hidden patterns learnt by ProDec-BLSTM can be interpreted and visualized, and therefore, additional useful information can be obtained.},
	number = {1},
	journal = {BMC bioinformatics},
	author = {Li, Shumin and Chen, Junjie and Liu, Bin},
	month = oct,
	year = {2017},
	note = {Publisher: BioMed Central},
	keywords = {Bidirectional Long Short-Term Memory, Neural network, Protein remote homology detection, Protein sequence analysis},
	pages = {443--443},
}

@article{hochreiter_fast_2007-1,
	title = {Fast model-based protein homology detection without alignment},
	volume = {23},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/17488755},
	doi = {10.1093/bioinformatics/btm247},
	abstract = {MOTIVATION As more genomes are sequenced, the demand for fast gene classification techniques is increasing. To analyze a newly sequenced genome, first the genes are identified and translated into amino acid sequences which are then classified into structural or functional classes. The best-performing protein classification methods are based on protein homology detection using sequence alignment methods. Alignment methods have recently been enhanced by discriminative methods like support vector machines (SVMs) as well as by position-specific scoring matrices (PSSM) as obtained from PSI-BLAST. However, alignment methods are time consuming if a new sequence must be compared to many known sequences-the same holds for SVMs. Even more time consuming is to construct a PSSM for the new sequence. The best-performing methods would take about 25 days on present-day computers to classify the sequences of a new genome (20,000 genes) as belonging to just one specific class--however, there are hundreds of classes. Another shortcoming of alignment algorithms is that they do not build a model of the positive class but measure the mutual distance between sequences or profiles. Only multiple alignments and hidden Markov models are popular classification methods which build a model of the positive class but they show low classification performance. The advantage of a model is that it can be analyzed for chemical properties common to the class members to obtain new insights into protein function and structure. We propose a fast model-based recurrent neural network for protein homology detection, the 'Long Short-Term Memory' (LSTM). LSTM automatically extracts indicative patterns for the positive class, but in contrast to profile methods it also extracts negative patterns and uses correlations between all detected patterns for classification. LSTM is capable to automatically extract useful local and global sequence statistics like hydrophobicity, polarity, volume, polarizability and combine them with a pattern. These properties make LSTM complementary to alignment-based approaches as it does not use predefined similarity measures like BLOSUM or PAM matrices. RESULTS We have applied LSTM to a well known benchmark for remote protein homology detection, where a protein must be classified as belonging to a SCOP superfamily. LSTM reaches state-of-the-art classification performance but is considerably faster for classification than other approaches with comparable classification performance. LSTM is five orders of magnitude faster than methods which perform slightly better in classification and two orders of magnitude faster than the fastest SVM-based approaches (which, however, have lower classification performance than LSTM). Only PSI-BLAST and HMM-based methods show comparable time complexity as LSTM, but they cannot compete with LSTM in classification performance. To test the modeling capabilities of LSTM, we applied LSTM to PROSITE classes and interpreted the extracted patterns. In 8 out of 15 classes, LSTM automatically extracted the PROSITE motif. In the remaining 7 cases alternative motifs are generated which give better classification results on average than the PROSITE motifs. AVAILABILITY The LSTM algorithm is available from http://www.bioinf.jku.at/software/LSTM\_protein/.},
	number = {14},
	journal = {Bioinformatics},
	author = {Hochreiter, S. and Heusel, M. and Obermayer, K.},
	month = jul,
	year = {2007},
	pages = {1728--1736},
}

@techreport{hsieh_identifying_nodate,
	title = {Identifying {Protein}-protein {Interactions} in {Biomedical} {Literature} using {Recurrent} {Neural} {Networks} with {Long} {Short}-{Term} {Memory}},
	url = {https://pdfs.semanticscholar.org/fd36/df6aff2aaeb055c47f1e93231e661f0fc8d7.pdf},
	abstract = {Accurate identification of protein-protein interaction (PPI) helps biomedical researchers to quickly capture crucial information in literatures. This work proposes a recurrent neural network (RNN) model to identify PPIs. Experiments on two largest public benchmark datasets, AIMed and BioInfer, demonstrate that RNN outperforms state-of-the-art methods with relative improvements of 10\% and 18\%, respectively. Cross-corpus evaluation also indicates that RNN is robust even when trained on data from different domains. These results suggest that RNN effectively captures semantic relationships among proteins without any feature engineering .},
	author = {Hsieh, Yu-Lun and Chang, Yung-Chun and Chang, Nai-Wen and Hsu, Wen-Lian},
	pages = {240--245},
}

@article{smith_disciplined_2018,
	title = {A disciplined approach to neural network hyper-parameters: {Part} 1 -- learning rate, batch size, momentum, and weight decay},
	url = {https://arxiv.org/abs/1803.09820},
	author = {Smith, Leslie N.},
	month = mar,
	year = {2018},
}

@article{bauckhage_numpy_2016,
	title = {{NumPy} / {SciPy} / {NetworkX} {Recipies} for {Data} {Science}: {Spectral} {Clustering}},
	url = {https://www.researchgate.net/publication/309153198_NumPy_SciPy_NetworkX_Recipes_for_Data_Science_Spectral_Clustering},
	doi = {10.13140/RG.2.1.4602.0564},
	abstract = {We revisit the idea of relational clustering and look at NumPy code for spectral clustering that allows us to cluster graphs or networks. In addition, our topic in this note provides us with the opportunity to study the use of NetworkX functions.},
	number = {October},
	author = {Bauckhage, Christian},
	year = {2016},
	pages = {1--3},
}

@article{buggenthin_prospective_2017,
	title = {Prospective identification of hematopoietic lineage choice by deep learning},
	volume = {14},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/28218899},
	doi = {10.1038/nmeth.4182},
	abstract = {Differentiation alters molecular properties of stem and progenitor cells, leading to changes in their shape and movement characteristics. We present a deep neural network that prospectively predicts lineage choice in differentiating primary hematopoietic progenitors using image patches from brightfield microscopy and cellular movement. Surprisingly, lineage choice can be detected up to three generations before conventional molecular markers are observable. Our approach allows identification of cells with differentially expressed lineage-specifying genes without molecular labeling.},
	number = {4},
	journal = {Nature Methods},
	author = {Buggenthin, Felix and Buettner, Florian and Hoppe, Philipp S and Endele, Max and Kroiss, Manuel and Strasser, Michael and Schwarzfischer, Michael and Loeffler, Dirk and Kokkaliaris, Konstantinos D and Hilsenbeck, Oliver and Schroeder, Timm and Theis, Fabian J and Marr, Carsten},
	month = apr,
	year = {2017},
	pages = {403--406},
}

@article{huy_predicting_2018,
	title = {Predicting {Protein} {Transmembrane} {Regionsby} {Using} {LSTM} {Model}},
	url = {http://crimsonpublishers.com/sbb/pdf/SBB.000510.pdf},
	doi = {10.31031/SBB.2018.01.000510},
	author = {Huy, Vu Thanh and Nguyen, Manh and Duy, Tran and Tuan, Anh and Bao, Phamthe},
	year = {2018},
}

@techreport{cravens_annotating_nodate,
	title = {Annotating protein secondary structure from sequence},
	url = {https://cs224d.stanford.edu/reports/CravensAaron.pdf},
	abstract = {Proteins are linear chain biomolecules composed of 20 different amino acids which form secondary structures such as helixes and loops [8]. Here, we use deep recurrent memory networks and deep convectional networks for predicting secondary structure annotations on sequences from the Uniprot database. We compare different network architectures and memory models, and discuss possible improvements. Our results on several tasks are significantly better than current methods that use SVMs and feed-forward neural networks.},
	author = {Cravens, Aaron and Probert, Christopher},
}

@article{weigert_biobeammultiplexed_2018,
	title = {Biobeam—{Multiplexed} wave-optical simulations of light-sheet microscopy},
	volume = {14},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1006079},
	doi = {10.1371/journal.pcbi.1006079},
	abstract = {Sample-induced image-degradation remains an intricate wave-optical problem in light-sheet microscopy. Here we present biobeam, an open-source software package that enables simulation of operational light-sheet microscopes by combining data from 105–106 multiplexed and GPU-accelerated point-spread-function calculations. The wave-optical nature of these simulations leads to the faithful reproduction of spatially varying aberrations, diffraction artifacts, geometric image distortions, adaptive optics, and emergent wave-optical phenomena, and renders image-formation in light-sheet microscopy computationally tractable.},
	number = {4},
	journal = {PLOS Computational Biology},
	author = {Weigert, Martin and Subramanian, Kaushikaram and Bundschuh, Sebastian T. and Myers, Eugene W. and Kreysing, Moritz},
	editor = {Jbabdi, Saad},
	month = apr,
	year = {2018},
	note = {Publisher: Public Library of Science},
	pages = {e1006079--e1006079},
}

@article{orth_microscopy_2017,
	title = {Microscopy, {Meet} {Big} {Data}},
	volume = {4},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S2405471217300947},
	doi = {10.1016/j.cels.2017.03.009},
	number = {3},
	journal = {Cell Systems},
	author = {Orth, Antony and Schaak, Diane and Schonbrun, Ethan},
	month = mar,
	year = {2017},
	pages = {260--261},
}

@inproceedings{cao_toward_2015,
	title = {Toward a new approach for massive {LiDAR} data processing},
	isbn = {978-1-4799-7748-2},
	url = {http://ieeexplore.ieee.org/document/7298040/},
	doi = {10.1109/ICSDM.2015.7298040},
	publisher = {IEEE},
	author = {Cao, V-H. and Chu, K-X. and Le-Khac, N-A. and Kechadi, M-T. and Laefer, D. and Truong-Hong, L.},
	month = jul,
	year = {2015},
	pages = {135--140},
}

@article{bauckhage_numpy_2015,
	title = {{NumPy} / {SciPy} {Recipes} for {Data} {Science}: {Computing} the {Kullback}-{Leibler} {Divergence} between {Generalized} {Gamma} {Distributions} {NumPy} / {SciPy} {Recipes} for {Data} {Science}: {Computing} the {Kullback} {Leibler} {Divergence} between {Generalized} {Gamma} {Distributions}},
	url = {https://www.researchgate.net/publication/278158089},
	doi = {10.13140/RG.2.1.1588.2409},
	abstract = {In this note, we familiarize ourselves with the use of special functions in data science. In particular, we look at the gamma-and digamma function and study SciPy methods to compute them. As an application example, we consider the problem of computing the Kullback-Leibler divergence between two generalized gamma distributions.},
	author = {Bauckhage, Christian},
	year = {2015},
	file = {Full Text:/home/zwerg/Zotero/storage/B44APMMG/Bauckhage - 2015 - NumPy  SciPy Recipes for Data Science Computing .pdf:application/pdf},
}

@article{dauphin_identifying_2014,
	title = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
	url = {https://papers.nips.cc/paper/5486-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization},
	author = {Dauphin, Yann N. and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
	year = {2014},
	pages = {2933--2941},
}

@article{goldstein_potential_1998,
	title = {Potential {Effects} of {Common} {Drugs} on {Stroke} {Recovery}},
	volume = {55},
	url = {http://archneur.jamanetwork.com/article.aspx?doi=10.1001/archneur.55.4.454},
	doi = {10.1001/archneur.55.4.454},
	abstract = {{\textless}p{\textgreater}Studies in laboratory animals clearly show that the rate and extent of functional recovery after focal brain injury can be modulated by drugs affecting certain neurotransmitters in the central nervous system. Preliminary clinical studies suggest that similar drug effects occur in humans recovering from stroke. Understanding these pharmacological effects is important because several of the classes of drugs that impair recovery in laboratory experiments are used to treat coincident medical problems in patients who have had a stroke.{\textless}/p{\textgreater}},
	number = {4},
	journal = {Archives of Neurology},
	author = {Goldstein, Larry B.},
	month = apr,
	year = {1998},
	note = {Publisher: American Medical Association},
	keywords = {cerebrovascular accident, ischemic stroke},
	pages = {454--454},
}

@article{lodder_diazepam_2006,
	title = {Diazepam to improve acute stroke outcome: results of the early {GABA}-{Ergic} activation study in stroke trial. a randomized double-blind placebo-controlled trial.},
	volume = {21},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/16340187},
	doi = {10.1159/000090210},
	abstract = {BACKGROUND We tested whether diazepam, a GABA-ergic drug that also inhibits brain nitric monoxide formation, improves acute stroke prognosis. METHODS 880 patients, randomized within 12 h of acute stroke, received diazepam 10 mg or placebo by rectiole, as soon as possible, followed by 10-mg tablets twice daily for 3 days. Primary outcome was independence (Rankin score {\textless}3) at 3 months; secondary outcome was complete recovery (Barthel index {\textgreater}or=95 or Rankin score {\textless}or=1). RESULTS Intention-to-treat analyses on all 849 patients with full follow-up (50.4\% on diazepam): odds ratio (OR) 1.14, 95\% CI 0.87-1.49 for primary endpoint, and an OR of 1.26 (0.90-1.76) for complete recovery, both favoring diazepam. Adjusted analyses for all stroke patients (843): OR 1.20 (0.87-1.65), and 1.25 (0.89-1.74), respectively, and for all infarct patients (748): OR 1.31 (0.93-1.85), and 1.46 (1.02-2.09; p=0.037), respectively. Analyses restricted to cardioembolic infarct patients (200) showed treatment benefit for the primary outcome: OR 2.26, 95\% CI 1.07-4.76, p=0.032, and complete recovery: OR 2.65, 95\% CI 1.06-6.59, p=0.037. About one third of ischemic stroke patients had 'any adverse event', without any difference between treatment groups. In 95 intracerebral hemorrhage patients, frequency of pneumonia and death were higher in the diazepam group than in the placebo group: 35 and 10\%, 22 and 12\%, respectively. CONCLUSIONS Although point estimates favored diazepam treatment in various analyses, our data did not confirm our primary hypothesis. Diazepam treatment seems beneficial in cardioembolic infarct patients, is safe in acute ischemic stroke, but may better be avoided in intracerebral hemorrhage.},
	number = {1-2},
	journal = {Cerebrovascular diseases (Basel, Switzerland)},
	author = {Lodder, J and van Raak, L and Hilton, A and Hardy, E and Kessels, A and {EGASIS Study Group}},
	year = {2006},
	note = {Publisher: Karger Publishers},
	keywords = {Cardioembolic infarction, Diazepam, Neuroprotection, Stroke outcome},
	pages = {120--7},
}

@article{ferrand_signal--noise_nodate,
	title = {Signal-to-{Noise} ratio made easy: {A} tool to assess your confocal performance},
	url = {http://dx.doi.org/10.1101/291500},
	doi = {10.1101/291500},
	abstract = {Confocal microscopy is used today on a daily basis in life science labs. This "routine" technique contributes to the progress of scientific projects across many fields by revealing structural details and protein localization, but researchers need to be aware that detector efficiency and performance is of major influence in the confocal image quality. By design, a large portion of the signal is discarded in confocal imaging, leading to a decreased signal-to-noise ratio (SNR) which in turn limits resolution. A well-aligned system and high performance detectors are needed in order to generate an image of best quality. However, a convenient method to address system status and detectors performance is still lacking. Here we present a complete method to assess microscope and detector performance in terms of their SNR, with a comprehensive protocol alongside NoiSee, an easy-to-use macro for Fiji (available via the corresponding update site). We used this method to compare several confocal systems in our facility on biological samples under realistic imaging conditions. Our method reveals differences in detector performance that reflect their respective types (multialkali photomultiplier tube (PMT), gallium arsenide phosphide (GaAsP) PMT, and Hybrid detector). Altogether, our method will provide useful information to research groups and facilities to diagnose their confocal microscopes.},
	author = {Ferrand, Alexia and Schleicher, Kai D and Ehrenfeuchter, Nikolaus and Heusermann, Wolf and Biehlmaier, Oliver},
	keywords = {Confocal microscopy, signal-to-noise ratio},
}

@article{bauckhage_numpy_2015-1,
	title = {{NumPy} / {SciPy} {Recipes} for {Data} {Science}: k-{Medoids} {Clustering}},
	url = {https://www.researchgate.net/publication/272351873},
	doi = {10.13140/2.1.4453.2009},
	abstract = {In this note, we study k-medoids clustering and show how to implement the algorithm using NumPy. To illustrate potential and practical use of this lesser known clustering method, we discuss an application example where we cluster a data set of strings based on bi-gram distances.},
	author = {Bauckhage, Christian},
	year = {2015},
	file = {Full Text:/home/zwerg/Zotero/storage/UT2IZ9JA/Bauckhage - 2015 - NumPy  SciPy Recipes for Data Science k-Medoids .pdf:application/pdf},
}

@article{report_numpy_2015,
	title = {{NumPy} / {SciPy} {Recipes} for {Image} {Processing}: {Drawing} the {Dragon} (1)},
	doi = {10.13140/2.1.3323.7289},
	number = {February},
	author = {Report, Technical},
	year = {2015},
}

@article{bauckhage_numpy_2015-2,
	title = {{NumPy} / {SciPy} {Recipes} for {Image} {Processing}: "{Simple}" {Intensity} {Transformations}},
	url = {https://www.youtube.com/playlist?list=},
	doi = {10.13140/RG.2.1.4125.3606},
	abstract = {This is the first in a series of notes on intensity transformations of digital images. We introduce basic terms and concepts and show how to realize several arguably simple transformations using NumPy.},
	author = {Bauckhage, Christian},
	year = {2015},
	file = {Full Text:/home/zwerg/Zotero/storage/9LW98E4R/Bauckhage - 2015 - NumPy  SciPy Recipes for Image Processing Simpl.pdf:application/pdf},
}

@article{bauckhage_lecture_2015,
	title = {Lecture {Notes} on {Data} {Science} : k - {Means} {Clustering}},
	doi = {10.13140/RG.2.1.2829.4886},
	abstract = {See, stats, and : https : / / www . researchgate . net / publication / 281176692 Lecture : k - Means Clustering Technical DOI : 10 . 13140 / RG . 2 . 1 . 2829 . 4886 CITATIONS 5 READS 258 1 : Christian University 263 , 729 SEE All - text , letting . Available : Christian Retrieved : 13 This is the first in a series of lecture notes on k - means clustering , its variants , and applications . We discuss the basic ideas behind k - means clustering and study the classical algorithm .},
	number = {August},
	author = {Bauckhage, Christian},
	year = {2015},
	file = {Full Text:/home/zwerg/Zotero/storage/VZ54DXHA/Bauckhage - 2015 - Lecture Notes on Data Science  k - Means Clusteri.pdf:application/pdf},
}

@article{wiskott_lecture_2006,
	title = {Lecture {Notes} on the {Kernel} {Trick}},
	doi = {10.13140/RG.2.1.2471.6322},
	number = {April},
	journal = {Neuroscience},
	author = {Wiskott, Laurenz},
	year = {2006},
	pages = {1--4},
}

@article{bauckhage_k-means_2018,
	title = {k-{Means} and {Fisher} ' s {Analysis} of {Variance} k -{Means} and {Fisher} ’ s {Analysis} of {Variance}},
	number = {May},
	author = {Bauckhage, Christian},
	year = {2018},
}

@article{bauckhage_tutorial_2018,
	title = {A {Tutorial} on {Principal} {Component} {Analysis} {Part} 1 : {Motivation} {A} {Tutorial} on {Principal} {Component} {Analysis} {Part} 1 : {Motivation}},
	number = {June},
	author = {Bauckhage, Christian and Dong, Tiansi},
	year = {2018},
}

@article{bauckhage_gentle_2018,
	title = {A {Gentle} {Tutorial} on {Reinforcement} {Learning} {Part} 1 : {Motivation} and {Basic} {Terminology} {A} {Gentle} {Tutorial} on {Reinforcement} {Learning} {Part} 1 : {Motivation} and {Basic} {Terminology}},
	number = {June},
	author = {Bauckhage, Christian and Speicher, Daniel},
	year = {2018},
}

@article{bauckhage_numpy_2018,
	title = {{NumPy} / {SciPy} {Recipes} for {Data} {NumPy} / {SciPy} {Recipes} for {Data} {Science} : {Training} {Neural} {Networks} {Without} {Backpropagation}},
	doi = {10.13140/RG.2.2.12912.76800},
	number = {January},
	author = {Bauckhage, Christian},
	year = {2018},
}

@article{bauckhage_tutorial_2018-1,
	title = {A {Tutorial} on {Principal} {Component} {Analysis} {Part} 1 : {Motivation} {A} {Tutorial} on {Principal} {Component} {Analysis} {Part} 1 : {Motivation}},
	number = {June},
	author = {Bauckhage, Christian and Dong, Tiansi},
	year = {2018},
}

@article{bauckhage_numpy_2017,
	title = {{NumPy} / {SciPy} {Recipes} for {Image} {Processing}: {Binary} {Images} and {Morphological} {Operations}},
	url = {https://www.researchgate.net/publication/312020813},
	doi = {10.13140/RG.2.2.16622.00324},
	abstract = {This note discusses several morphological operations for binary image processing and demonstrates how to use readily available SciPy functions for this purpose.},
	author = {Bauckhage, Christian},
	year = {2017},
}

@article{bauckhage_numpy_2016-1,
	title = {{NumPy} / {SciPy} {Recipes} for {Data} {Science} : {Eigenvalues} / {Eigenvectors} of {Covariance} {Matrices} {NumPy} / {SciPy} {Recipes} for {Data} {Science} : {Eigenvalues} / {Eigenvectors} of {Covariance} {Matrices}},
	doi = {10.13140/RG.2.1.2307.5046},
	number = {April},
	author = {Bauckhage, Christian and Bauckhage, Christian},
	year = {2016},
}

@article{shlens_tutorial_2014,
	title = {A {Tutorial} on {Principal} {Component} {Analysis}},
	volume = {1},
	issn = {9781457705052},
	url = {http://arxiv.org/abs/1404.1100},
	abstract = {Principal component analysis (PCA) is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind PCA. This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of PCA as well as the when, the how and the why of applying this technique.},
	number = {c},
	author = {Shlens, Jonathon},
	year = {2014},
	pages = {2--5},
}

@article{bauckhage_numpy_nodate,
	title = {{NumPy} / {SciPy} {Recipes} for {Image} {Processing}: {Intensity} {Normalization} and {Histogram} {Equalization}},
	url = {https://www.researchgate.net/publication/281118372},
	doi = {10.13140/RG.2.1.3364.0164},
	abstract = {In this note, we study NumPy recipes that allow for contrast enhancement of overly dark or overly bright images. In particular, we look at the ideas of intensity normalization and histogram equalization and discuss efficient, properly vectorized implementations.},
	author = {Bauckhage, Christian},
}

@article{bauckhage_numpy_2015-3,
	title = {{NumPy} / {SciPy} {Recipes} for {Data} {Science}: {Regularized} {Least} {Squares} {Optimization}},
	issn = {0521546737},
	doi = {10.13140/RG.2.1.2565.3286},
	abstract = {—In this note, we study k-medoids clustering and show how to implement the algorithm using NumPy. To illustrate potential and practical use of this lesser known clustering method, we discuss an application example where we cluster a data set of strings based on bi-gram distances.},
	number = {July},
	author = {Bauckhage, Christian},
	year = {2015},
	pages = {1--4},
	file = {Full Text:/home/zwerg/Zotero/storage/N3QHE3KK/Bauckhage - 2015 - NumPy  SciPy Recipes for Data Science Regularize.pdf:application/pdf},
}

@techreport{lanza_second-order_nodate,
	title = {Second-{Order} {Polynomial} {Models} for {Background} {Subtraction}},
	url = {https://pdfs.semanticscholar.org/db72/037e83d24bdc54fa41b0a532e44f93164e28.pdf},
	abstract = {This paper is aimed at investigating background subtraction based on second-order polynomial models. Recently, preliminary results suggested that quadratic models hold the potential to yield superior performance in handling common disturbance factors, such as noise, sudden illumination changes and variations of camera parameters, with respect to state-of-the-art background subtraction methods. Therefore, based on the formalization of background subtraction as Bayesian regression of a second-order polynomial model, we propose here a thorough theoretical analysis aimed at identifying a family of suitable models and deriving the closed-form solutions of the associated regression problems. In addition , we present a detailed quantitative experimental evaluation aimed at comparing the different background subtraction algorithms resulting from theoretical analysis, so as to highlight those more favorable in terms of accuracy, speed and speed-accuracy tradeoff.},
	author = {Lanza, Alessandro and Tombari, Federico and Stefano, Luigi Di},
}

@article{pham_efficient_2018-1,
	title = {Efficient {Neural} {Architecture} {Search} via {Parameter} {Sharing}},
	abstract = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89\%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65\%.},
	author = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
	year = {2018},
}

@techreport{tzeng_adversarial_nodate,
	title = {Adversarial {Discriminative} {Domain} {Adaptation}},
	abstract = {Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They also can improve recognition despite the presence of domain shift or dataset bias: several adversarial approaches to unsupervised domain adaptation have recently been introduced, which reduce the difference between the training and test domain distributions and thus improve generalization performance. Prior generative approaches show compelling visualizations, but are not optimal on discriminative tasks and can be limited to smaller shifts. Prior discriminative approaches could handle larger domain shifts, but imposed tied weights on the model and did not exploit a GAN-based loss. We first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and we use this generalized view to better relate the prior approaches. We propose a previously unexplored instance of our general framework which combines discrim-inative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard cross-domain digit classification tasks and a new more difficult cross-modality object classification task.},
	author = {Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
}

@techreport{novikov_fully_nodate,
	title = {Fully {Convolutional} {Architectures} for {Multi}-{Class} {Segmentation} in {Chest} {Radiographs}},
	url = {https://arxiv.org/pdf/1701.08816.pdf},
	abstract = {The success of deep convolutional neural networks on image classification and recognition tasks has led to new applications in very diversified contexts, including the field of medical imaging. In this paper we investigate and propose neural network architectures for automated multi-class segmentation of anatomical organs in chest radiographs, namely for lungs, clavicles and heart. We address several open challenges including model overfit-ting, reducing number of parameters and handling of severely imbalanced data in CXR by fusing recent concepts in convolu-tional networks and adapting them to the segmentation problem task in CXR. We demonstrate that our architecture combining delayed subsampling, exponential linear units, highly restrictive regularization and a large number of high resolution low level abstract features outperforms state-of-the-art methods on all considered organs, as well as the human observer on lungs and heart. The models use a multi-class configuration with three target classes and are trained and tested on the publicly available JSRT database, consisting of 247 X-ray images the ground-truth masks for which are available in the SCR database. Our best performing model, trained with the loss function based on the Dice coefficient, reached mean Jaccard overlap scores of 95.0\% for lungs, 86.8\% for clavicles and 88.2\% for heart. This architecture outperformed the human observer results for lungs and heart.},
	author = {Novikov, Alexey A and Lenis, Dimitrios and Major, David and Hladůvka, Jiří and Wimmer, Maria and Bühler, Katja},
	keywords = {regularization, chest radiographs, clavicle segmentation, fully convolutional network, heart segmentation, imbal-anced data, Index Terms-Lung segmentation, JSRT dataset, multi-class segmentation},
}

@techreport{oktay_anatomically_2017,
	title = {Anatomically {Constrained} {Neural} {Networks} ({ACNN}): {Application} to {Cardiac} {Image} {Enhancement} and {Segmentation}},
	url = {https://arxiv.org/pdf/1705.08302.pdf},
	abstract = {Incorporation of prior knowledge about organ shape and location is key to improve performance of image analysis approaches. In particular, priors can be useful in cases where images are corrupted and contain artefacts due to limitations in image acquisition. The highly constrained nature of anatomical objects can be well captured with learning based techniques. However, in most recent and promising techniques such as CNN based segmentation it is not obvious how to incorporate such prior knowledge. State-of-the-art methods operate as pixel-wise classifiers where the training objectives do not incorporate the structure and inter-dependencies of the output. To overcome this limitation, we propose a generic training strategy that incorporates anatomical prior knowledge into CNNs through a new regularisation model, which is trained end-to-end. The new framework encourages models to follow the global anatomical properties of the underlying anatomy (e.g. shape, label structure) via learnt non-linear representations of the shape. We show that the proposed approach can be easily adapted to different analysis tasks (e.g. image enhancement, segmentation) and improve the prediction accuracy of the state-of-the-art models. The applicability of our approach is shown on multi-modal cardiac datasets and public benchmarks. Additionally, we demonstrate how the learnt deep models of 3D shapes can be interpreted and used as biomarkers for classification of cardiac pathologies},
	author = {Oktay, Ozan and Ferrante, Enzo and Kamnitsas, Konstantinos and Heinrich, Mattias and Bai, Wenjia and Caballero, Jose and Cook, Stuart and De Marvao, Antonio and Dawes, Timothy and O'regan, Declan and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
	year = {2017},
	keywords = {Convolutional Neural Network, Image Super-Resolution, Index Terms-Shape Prior, Medical Image Segmentation},
}

@article{report_numpy_2014,
	title = {{NumPy} / {SciPy} {Recipes} for {Data} {Science}: {Squared} {Euclidean} {Distance} {Matrices}},
	doi = {10.13140/2.1.4426.1127},
	number = {October},
	author = {Report, Technical},
	year = {2014},
	pages = {1--5},
}

@article{bauckhage_numpy_2015-4,
	title = {{NumPy} / {SciPy} {Recipes} for {Data} {Science} : {Ordinary} {Least} {Squares} {Optimization} {NumPy} / {SciPy} {Recipes} for {Data} {Science} : {Ordinary} {Least} {Squares} {Optimization}},
	doi = {10.13140/2.1.3370.3209/1},
	number = {July},
	author = {Bauckhage, Christian and Bauckhage, Christian},
	year = {2015},
	file = {Full Text:/home/zwerg/Zotero/storage/J9R94NBV/Bauckhage and Bauckhage - 2015 - NumPy  SciPy Recipes for Data Science  Ordinary .pdf:application/pdf},
}

@article{ouyang_deep_2018-1,
	title = {Deep learning massively accelerates super-resolution localization microscopy},
	volume = {36},
	url = {http://www.nature.com/doifinder/10.1038/nbt.4106},
	doi = {10.1038/nbt.4106},
	abstract = {Accelerating PALM/STORM microscopy with deep learning allows super-resolution imaging of {\textgreater}1,000 cells in a few hours.},
	number = {5},
	journal = {Nature Biotechnology},
	author = {Ouyang, Wei and Aristov, Andrey and Lelek, Mickaël and Hao, Xian and Zimmer, Christophe},
	month = apr,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Image processing, Microscopy, Machine learning, Fluorescence imaging, resolution microscopy, Super},
	pages = {460--468},
}

@article{zhang_3d_2017,
	title = {{3D} imaging of optically cleared tissue using a simplified {CLARITY} method and on-chip microscopy},
	volume = {3},
	url = {http://advances.sciencemag.org/lookup/doi/10.1126/sciadv.1700553},
	doi = {10.1126/sciadv.1700553},
	abstract = {High-throughput sectioning and optical imaging of tissue samples using traditional immunohistochemical techniques can be costly and inaccessible in resource-limited areas. We demonstrate three-dimensional (3D) imaging and phenotyping in optically transparent tissue using lens-free holographic on-chip microscopy as a low-cost, simple, and high-throughput alternative to conventional approaches. The tissue sample is passively cleared using a simplified CLARITY method and stained using 3,3′-diaminobenzidine to target cells of interest, enabling bright-field optical imaging and 3D sectioning of thick samples. The lens-free computational microscope uses pixel super-resolution and multi-height phase recovery algorithms to digitally refocus throughout the cleared tissue and obtain a 3D stack of complex-valued images of the sample, containing both phase and amplitude information. We optimized the tissue-clearing and imaging system by finding the optimal illumination wavelength, tissue thickness, sample preparation parameters, and the number of heights of the lens-free image acquisition and implemented a sparsity-based denoising algorithm to maximize the imaging volume and minimize the amount of the acquired data while also preserving the contrast-to-noise ratio of the reconstructed images. As a proof of concept, we achieved 3D imaging of neurons in a 200-μm-thick cleared mouse brain tissue over a wide field of view of 20.5 mm2. The lens-free microscope also achieved more than an order-of-magnitude reduction in raw data compared to a conventional scanning optical microscope imaging the same sample volume. Being low cost, simple, high-throughput, and data-efficient, we believe that this CLARITY-enabled computational tissue imaging technique could find numerous applications in biomedical diagnosis and research in low-resource settings.},
	number = {8},
	journal = {Science Advances},
	author = {Zhang, Yibo and Shin, Yoonjung and Sung, Kevin and Yang, Sam and Chen, Harrison and Wang, Hongda and Teng, Da and Rivenson, Yair and Kulkarni, Rajan P. and Ozcan, Aydogan},
	month = aug,
	year = {2017},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {e1700553--e1700553},
}

@article{loshchilov_fixing_2017,
	title = {Fixing {Weight} {Decay} {Regularization} in {Adam}},
	url = {http://arxiv.org/abs/1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common deep learning frameworks of these algorithms implement L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by decoupling the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code is available at https://github.com/loshchil/AdamW-and-SGDW},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = nov,
	year = {2017},
}

@article{zou_ai_2018,
	title = {{AI} can be sexist and racist — it’s time to make it fair},
	volume = {559},
	url = {http://www.nature.com/articles/d41586-018-05707-8},
	doi = {10.1038/d41586-018-05707-8},
	abstract = {Computer scientists must identify sources of bias, de-bias training data and develop artificial-intelligence algorithms that are robust to skews in the data, argue James Zou and Londa Schiebinger. Computer scientists must identify sources of bias, de-bias training data and develop artificial-intelligence algorithms that are robust to skews in the data.},
	number = {7714},
	journal = {Nature},
	author = {Zou, James and Schiebinger, Londa},
	month = jul,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Information technology, Society},
	pages = {324--326},
}

@article{wang_deep_2018-2,
	title = {Deep learning achieves super-resolution in fluorescence microscopy},
	url = {https://www.biorxiv.org/content/early/2018/04/27/309641},
	doi = {10.1101/309641},
	abstract = {We present a deep learning-based method for achieving super-resolution in fluorescence microscopy. This data-driven approach does not require any numerical models of the imaging process or the estimation of a point spread function, and is solely based on training a generative adversarial network, which statistically learns to transform low resolution input images into super-resolved ones. Using this method, we super-resolve wide-field images acquired with low numerical aperture objective lenses, matching the resolution that is acquired using high numerical aperture objectives. We also demonstrate that diffraction-limited confocal microscopy images can be transformed by the same framework into super-resolved fluorescence images, matching the image resolution acquired with a stimulated emission depletion (STED) microscope. The deep network rapidly outputs these super-resolution images, without any iterations or parameter search, and even works for types of samples that it was not trained for.},
	journal = {bioRxiv},
	author = {Wang, Hongda and Rivenson, Yair and Jin, Yiyin and Wei, Zhensong and Gao, Ronald and Gunaydin, Harun and Bentolila, Laurent and Ozcan, Aydogan},
	month = apr,
	year = {2018},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {309641--309641},
}

@article{pedamonti_comparison_nodate,
	title = {Comparison of non-linear activation functions for deep neural networks on {MNIST} classification task},
	url = {https://arxiv.org/pdf/1804.02763.pdf},
	abstract = {Activation functions play a key role in neural networks so it becomes fundamental to understand their advantages and disadvantages in order to achieve better performances. This paper will first introduce common types of non linear activation functions that are alternative to the well known sigmoid function and then evaluate their characteristics. Moreover deeper neural networks will be analysed because they positively influence the final performances compared to shallower networks. They also strictly depend on the weight initialisation hence the effect of drawing weights from Gaussian and uniform distribution will be analysed making particular attention on how the number of incoming and outgoing connection to a node influence the whole network.},
	author = {Pedamonti, Dabal},
}

@article{zoph_neural_nodate,
	title = {{NEURAL} {ARCHITECTURE} {SEARCH} {WITH} {REINFORCEMENT} {LEARNING}},
	url = {https://arxiv.org/pdf/1611.01578.pdf},
	abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that out-performs the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplex-ity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
	author = {Zoph, Barret and Le Google Brain, Quoc V},
}

@article{smith_super-convergence_2017,
	title = {Super-{Convergence}: {Very} {Fast} {Training} of {Neural} {Networks} {Using} {Large} {Learning} {Rates}},
	url = {http://arxiv.org/abs/1708.07120},
	abstract = {In this paper, we describe a phenomenon, which we named "super-convergence", where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence. See http://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of super-convergence to win the DAWNBench challenge (see https://dawn.cs.stanford.edu/benchmark/).},
	author = {Smith, Leslie N. and Topin, Nicholay},
	month = aug,
	year = {2017},
	file = {Full Text:/home/zwerg/Zotero/storage/DIQCACUX/Smith and Topin - 2017 - Super-Convergence Very Fast Training of Neural Ne.pdf:application/pdf},
}

@article{saqur_capsgan_2018,
	title = {{CapsGAN}: {Using} {Dynamic} {Routing} for {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1806.03968},
	abstract = {In this paper, we propose a novel technique for generating images in the 3D domain from images with high degree of geometrical transformations. By coalescing two popular concurrent methods that have seen rapid ascension to the machine learning zeitgeist in recent years: GANs (Goodfellow et. al.) and Capsule networks (Sabour, Hinton et. al.) - we present: {\textbackslash}textbf\{CapsGAN\}. We show that CapsGAN performs better than or equal to traditional CNN based GANs in generating images with high geometric transformations using rotated MNIST. In the process, we also show the efficacy of using capsules architecture in the GANs domain. Furthermore, we tackle the Gordian Knot in training GANs - the performance control and training stability by experimenting with using Wasserstein distance (gradient clipping, penalty) and Spectral Normalization. The experimental findings of this paper should propel the application of capsules and GANs in the still exciting and nascent domain of 3D image generation, and plausibly video (frame) generation.},
	author = {Saqur, Raeid and Vivona, Sal},
	month = jun,
	year = {2018},
}

@article{nguyen_halo-free_2017,
	title = {Halo-free {Phase} {Contrast} {Microscopy}},
	volume = {7},
	url = {http://www.nature.com/articles/srep44034},
	doi = {10.1038/srep44034},
	abstract = {Halo-free Phase Contrast Microscopy},
	number = {1},
	journal = {Scientific Reports},
	author = {Nguyen, Tan H. and Kandel, Mikhail and Shakir, Haadi M. and Best-Popescu, Catherine and Arikkath, Jyothi and Do, Minh N. and Popescu, Gabriel},
	month = dec,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biomedical engineering, Applied optics, Micro, optics},
	pages = {44034--44034},
}

@article{li_deepunet_nodate,
	title = {{DeepUNet}: {A} {Deep} {Fully} {Convolutional} {Network} for {Pixel}-level {Sea}-{Land} {Segmentation}},
	url = {https://arxiv.org/pdf/1709.00201.pdf},
	abstract = {Semantic segmentation is a fundamental research in remote sensing image processing. Because of the complex maritime environment, the sea-land segmentation is a challenging task. Although the neural network has achieved excellent performance in semantic segmentation in the last years, there are a few of works using CNN for sea-land segmentation and the results could be further improved. This paper proposes a novel deep convolution neural network named DeepUNet. Like the U-Net, its structure has a contracting path and an expansive path to get high resolution output. But differently, the DeepUNet uses DownBlocks instead of convolution layers in the contracting path and uses UpBlock in the expansive path. The two novel blocks bring two new connections that are U-connection and Plus connection. They are promoted to get more precise segmentation results. To verify our network architecture, we made a new challenging sea-land dataset and compare the DeepUNet on it with the SegNet and the U-Net. Experimental results show that DeepUNet achieved good performance compared with other architectures, especially in high-resolution remote sensing imagery.},
	author = {Li, Ruirui and Liu, Wenjie and Yang, Lei and Sun, Shihao and Hu, Wei and Zhang, Fan and Member, Senior and Li, Wei},
	keywords = {fully convolution network, ResNet, satellite imagery process-ing, U-Net, Index Terms-sea-land segmentation},
}

@article{haloi_rethinking_nodate,
	title = {Rethinking {Convolutional} {Semantic} {Segmentation} {Learning}},
	url = {https://arxiv.org/pdf/1710.07991.pdf},
	abstract = {Deep convolutional semantic segmentation (DCSS) learning doesn't converge to an optimal local minimum with random parameters initializations; a pre-trained model on the same domain becomes necessary to achieve convergence.In this work, we propose a joint cooperative end-to-end learning method for DCSS. It addresses many drawbacks with existing deep semantic segmentation learning; the proposed approach simultaneously learn both segmentation and classification; taking away the essential need of the pre-trained model for learning convergence. We present an improved inception based architecture with partial attention gating (PAG) over encoder information. The PAG also adds to achieve faster convergence and better accuracy for segmentation task. We will show the effectiveness of this learning on a diabetic retinopathy classification and segmentation dataset.},
	author = {Haloi, Mrinal},
}

@article{ren_sbnet_nodate,
	title = {{SBNet}: {Sparse} {Blocks} {Network} for {Fast} {Inference}},
	url = {https://github.com/uber/sbnet},
	abstract = {Conventional deep convolutional neural networks (CNNs) apply convolution operators uniformly in space across all feature maps for hundreds of layers-this incurs a high computational cost for real-time applications. For many problems such as object detection and semantic segmentation, we are able to obtain a low-cost computation mask, either from a priori problem knowledge, or from a low-resolution segmentation network. We show that such computation masks can be used to reduce computation in the high-resolution main network. Variants of sparse activation CNNs have previously been explored on small-scale tasks and showed no degradation in terms of object classification accuracy, but often measured gains in terms of theoretical FLOPs without realizing a practical speed-up when compared to highly optimized dense convolution implementations. In this work, we leverage the sparsity structure of computation masks and propose a novel tiling-based sparse convolution algorithm. We verified the effectiveness of our sparse CNN on LiDAR-based 3D object detection, and we report significant wall-clock speed-ups compared to dense convolution without noticeable loss of accuracy.},
	author = {Ren, Mengye and Pokrovsky, Andrei and Yang, Bin and Urtasun, Raquel},
}

@article{upadhyay_generative_2018,
	title = {Generative {Adversarial} {Network} {Architectures} {For} {Image} {Synthesis} {Using} {Capsule} {Networks}},
	url = {http://arxiv.org/abs/1806.03796},
	abstract = {In this paper, we propose Generative Adversarial Network (GAN) architectures using dynamically-routed Capsule Networks for conditional and random image-synthesis. The architectures benefit from the fact that Capsule Networks encode the properties and spatial relationships of the features in the images. Capsule Networks being a more effective critic provide multiple benefits when replacing Convolutional Neural Network discriminators being used in the current work-horses for image synthesis - DCGANs. Our architectures use a loss analogous to Wasserstein loss and demonstrate that they can encode a dataset's representation faster than currently existing GANs resulting in lesser number of training samples required. Our experiments show that the generator is pushed to give better and more diverse results in significantly lesser number of epochs without over-fitting. We have used MNIST, Fashion-MNIST and their variants for demonstrating the results achieved from this architecture.},
	author = {Upadhyay, Yash and Schrater, Paul},
	month = jun,
	year = {2018},
}

@article{iqbal_capsule_2018,
	title = {Capsule {Routing} for {Sound} {Event} {Detection}},
	url = {http://arxiv.org/abs/1806.04699},
	abstract = {The detection of acoustic scenes is a challenging problem in which environmental sound events must be detected from a given audio signal. This includes classifying the events as well as estimating their onset and offset times. We approach this problem with a neural network architecture that uses the recently-proposed capsule routing mechanism. A capsule is a group of activation units representing a set of properties for an entity of interest, and the purpose of routing is to identify part-whole relationships between capsules. That is, a capsule in one layer is assumed to belong to a capsule in the layer above in terms of the entity being represented. Using capsule routing, we wish to train a network that can learn global coherence implicitly, thereby improving generalization performance. Our proposed method is evaluated on Task 4 of the DCASE 2017 challenge. Results show that classification performance is state-of-the-art, achieving an F-score of 58.6\%. In addition, overfitting is reduced considerably compared to other architectures.},
	author = {Iqbal, Turab and Xu, Yong and Kong, Qiuqiang and Wang, Wenwu},
	month = jun,
	year = {2018},
}

@article{mobiny_fast_2018,
	title = {Fast {CapsNet} for {Lung} {Cancer} {Screening}},
	url = {https://arxiv.org/pdf/1806.07416.pdf},
	abstract = {Lung cancer is the leading cause of cancer-related deaths in the past several years. A major challenge in lung cancer screening is the detection of lung nodules from computed tomography (CT) scans. State-of-the-art approaches in automated lung nodule classification use deep convolutional neural networks (CNNs). However, these networks require a large number of training samples to generalize well. This paper investigates the use of capsule networks (CapsNets) as an alternative to CNNs. We show that CapsNets significantly outperforms CNNs when the number of training samples is small. To increase the computational efficiency, our paper proposes a consistent dynamic routing mechanism that results in 3× speedup of CapsNet. Finally, we show that the original image reconstruction method of CapNets performs poorly on lung nodule data. We propose an efficient alternative, called convolutional decoder, that yields lower reconstruction error and higher classification accuracy.},
	author = {Mobiny, Aryan and Nguyen, Hien Van},
	year = {2018},
}

@article{kanade_cell_nodate,
	title = {Cell {Image} {Analysis}: {Algorithms}, {System} and {Applications}},
	url = {http://www.cs.cmu.edu/~zhaozhen/Papers/WACV2011_CellImageAnalysis.pdf},
	abstract = {We present several algorithms for cell image analysis including microscopy image restoration, cell event detection and cell tracking in a large population. The algorithms are integrated into an automated system capable of quantifying cell proliferation metrics in vitro in real-time. This offers unique opportunities for biological applications such as efficient cell behavior discovery in response to different cell culturing conditions and adaptive experiment control. We quantitatively evaluated our system's performance on 16 microscopy image sequences with satisfactory accuracy for biologists' need. We have also developed a public web-site compatible to the system's local user interface, thereby allowing biologists to conveniently check their experiment progress online. The website will serve as a community resource that allows other research groups to upload their cell images for analysis and comparison.},
	author = {Kanade, Takeo and Yin, Zhaozheng and Bise, Ryoma and Huh, Seungil and Eom, Sungeun and Sandbothe, Michael F and Chen, Mei},
}

@article{hong_decoupled_2015,
	title = {Decoupled {Deep} {Neural} {Network} for {Semi}-supervised {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1506.04924},
	abstract = {We propose a novel deep neural network architecture for semi-supervised semantic segmentation using heterogeneous annotations. Contrary to existing approaches posing semantic segmentation as a single task of region-based classification, our algorithm decouples classification and segmentation, and learns a separate network for each task. In this architecture, labels associated with an image are identified by classification network, and binary segmentation is subsequently performed for each identified label in segmentation network. The decoupled architecture enables us to learn classification and segmentation networks separately based on the training data with image-level and pixel-wise class labels, respectively. It facilitates to reduce search space for segmentation effectively by exploiting class-specific activation maps obtained from bridging layers. Our algorithm shows outstanding performance compared to other semi-supervised approaches even with much less training images with strong annotations in PASCAL VOC dataset.},
	author = {Hong, Seunghoon and Noh, Hyeonwoo and Han, Bohyung},
	month = jun,
	year = {2015},
}

@article{hipp_sivq-aided_2011,
	title = {{SIVQ}-aided laser capture microdissection: {A} tool for high-throughput expression profiling.},
	volume = {2},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21572509},
	doi = {10.4103/2153-3539.78500},
	abstract = {INTRODUCTION Laser capture microdissection (LCM) facilitates procurement of defined cell populations for study in the context of histopathology. The morphologic assessment step in the LCM procedure is time consuming and tedious, thus restricting the utility of the technology for large applications. RESULTS Here, we describe the use of Spatially Invariant Vector Quantization (SIVQ) for histological analysis and LCM. Using SIVQ, we selected vectors as morphologic predicates that were representative of normal epithelial or cancer cells and then searched for phenotypically similar cells across entire tissue sections. The selected cells were subsequently auto-microdissected and the recovered RNA was analyzed by expression microarray. Gene expression profiles from SIVQ-LCM and standard LCM-derived samples demonstrated highly congruous signatures, confirming the equivalence of the differing microdissection methods. CONCLUSION SIVQ-LCM improves the work-flow of microdissection in two significant ways. First, the process is transformative in that it shifts the pathologist's role from technical execution of the entire microdissection to a limited-contact supervisory role, enabling large-scale extraction of tissue by expediting subsequent semi-autonomous identification of target cell populations. Second, this work-flow model provides an opportunity to systematically identify highly constrained cell populations and morphologically consistent regions within tissue sections. Integrating SIVQ with LCM in a single environment provides advanced capabilities for efficient and high-throughput histological-based molecular studies.},
	journal = {Journal of pathology informatics},
	author = {Hipp, Jason and Cheng, Jerome and Hanson, Jeffrey C and Yan, Wusheng and Taylor, Phil and Hu, Nan and Rodriguez-Canales, Jaime and Hipp, Jennifer and Tangrea, Michael A and Emmert-Buck, Michael R and Balis, Ulysses and Response, the Inflammation {and} the Host and Program, Large-Scale Collaborative Research},
	month = mar,
	year = {2011},
	note = {Publisher: Wolters Kluwer -- Medknow Publications},
	keywords = {Spatially Invariant Vector Quantization, Laser capture microdissection, microarray},
	pages = {19--19},
}

@article{hipp_sivq-lcm_2014,
	title = {{SIVQ}-{LCM} protocol for the {ArcturusXT} instrument.},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/25078867},
	doi = {10.3791/51662},
	abstract = {SIVQ-LCM is a new methodology that automates and streamlines the more traditional, user-dependent laser dissection process. It aims to create an advanced, rapidly customizable laser dissection platform technology. In this report, we describe the integration of the image analysis software Spatially Invariant Vector Quantization (SIVQ) onto the ArcturusXT instrument. The ArcturusXT system contains both an infrared (IR) and ultraviolet (UV) laser, allowing for specific cell or large area dissections. The principal goal is to improve the speed, accuracy, and reproducibility of the laser dissection to increase sample throughput. This novel approach facilitates microdissection of both animal and human tissues in research and clinical workflows.},
	number = {89},
	journal = {Journal of visualized experiments : JoVE},
	author = {Hipp, Jason D and Cheng, Jerome and Hanson, Jeffrey C and Rosenberg, Avi Z and Emmert-Buck, Michael R and Tangrea, Michael A and Balis, Ulysses J},
	month = jul,
	year = {2014},
	note = {Publisher: MyJoVE Corporation},
}

@article{hanson_expression_2011,
	title = {Expression microdissection adapted to commercial laser dissection instruments.},
	volume = {6},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/21412274},
	doi = {10.1038/nprot.2010.202},
	abstract = {Laser-based microdissection facilitates the isolation of specific cell populations from clinical or animal model tissue specimens for molecular analysis. Expression microdissection (xMD) is a second-generation technology that offers considerable advantages in dissection capabilities; however, until recently the method has not been accessible to investigators. This protocol describes the adaptation of xMD to commonly used laser microdissection instruments and to a commercially available handheld laser device in order to make the technique widely available to the biomedical research community. The method improves dissection speed for many applications by using a targeting probe for cell procurement in place of an operator-based, cell-by-cell selection process. Moreover, xMD can provide improved dissection precision because of the unique characteristics of film activation. The time to complete the protocol is highly dependent on the target cell population and the number of cells needed for subsequent molecular analysis.},
	number = {4},
	journal = {Nature protocols},
	author = {Hanson, Jeffrey C and Tangrea, Michael A and Kim, Skye and Armani, Michael D and Pohida, Thomas J and Bonner, Robert F and Rodriguez-Canales, Jaime and Emmert-Buck, Michael R},
	month = apr,
	year = {2011},
	note = {Publisher: NIH Public Access},
	pages = {457--67},
}

@article{aberman_neural_2018,
	title = {Neural {Best}-{Buddies}: {Sparse} {Cross}-{Domain} {Correspondence}},
	url = {http://arxiv.org/abs/1805.04140},
	doi = {10.1145/3197517.3201332},
	abstract = {Correspondence between images is a fundamental problem in computer vision, with a variety of graphics applications. This paper presents a novel method for sparse cross-domain correspondence. Our method is designed for pairs of images where the main objects of interest may belong to different semantic categories and differ drastically in shape and appearance, yet still contain semantically related or geometrically similar parts. Our approach operates on hierarchies of deep features, extracted from the input images by a pre-trained CNN. Specifically, starting from the coarsest layer in both hierarchies, we search for Neural Best Buddies (NBB): pairs of neurons that are mutual nearest neighbors. The key idea is then to percolate NBBs through the hierarchy, while narrowing down the search regions at each level and retaining only NBBs with significant activations. Furthermore, in order to overcome differences in appearance, each pair of search regions is transformed into a common appearance. We evaluate our method via a user study, in addition to comparisons with alternative correspondence approaches. The usefulness of our method is demonstrated using a variety of graphics applications, including cross-domain image alignment, creation of hybrid images, automatic image morphing, and more.},
	author = {Aberman, Kfir and Liao, Jing and Shi, Mingyi and Lischinski, Dani and Chen, Baoquan and Cohen-Or, Daniel},
	month = may,
	year = {2018},
}

@article{luan_deep_2018,
	title = {Deep {Painterly} {Harmonization}},
	url = {http://arxiv.org/abs/1804.03189},
	abstract = {Copying an element from a photo and pasting it into a painting is a challenging task. Applying photo compositing techniques in this context yields subpar results that look like a collage --- and existing painterly stylization algorithms, which are global, perform poorly when applied locally. We address these issues with a dedicated algorithm that carefully determines the local statistics to be transferred. We ensure both spatial and inter-scale statistical consistency and demonstrate that both aspects are key to generating quality results. To cope with the diversity of abstraction levels and types of paintings, we introduce a technique to adjust the parameters of the transfer depending on the painting. We show that our algorithm produces significantly better results than photo compositing or global stylization techniques and that it enables creative painterly edits that would be otherwise difficult to achieve.},
	author = {Luan, Fujun and Paris, Sylvain and Shechtman, Eli and Bala, Kavita},
	month = apr,
	year = {2018},
}

@article{jaccard_segmentation_2017,
	title = {Segmentation of phase contrast microscopy images based on multi-scale local {Basic} {Image} {Features} histograms.},
	volume = {5},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/28815155},
	doi = {10.1080/21681163.2015.1016243},
	abstract = {Phase contrast microscopy (PCM) is routinely used for the inspection of adherent cell cultures in all fields of biology and biomedicine. Key decisions for experimental protocols are often taken by an operator based on typically qualitative observations. However, automated processing and analysis of PCM images remain challenging due to the low contrast between foreground objects (cells) and background as well as various imaging artefacts. We propose a trainable pixel-wise segmentation approach whereby image structures and symmetries are encoded in the form of multi-scale Basic Image Features local histograms, and classification of them is learned by random decision trees. This approach was validated for segmentation of cell versus background, and discrimination between two different cell types. Performance close to that of state-of-the-art specialised algorithms was achieved despite the general nature of the method. The low processing time ( {\textless} 4 s per 1280 × 960 pixel images) is suitable for batch processing of experimental data as well as for interactive segmentation applications.},
	number = {5},
	journal = {Computer methods in biomechanics and biomedical engineering. Imaging \& visualization},
	author = {Jaccard, N and Szita, N and Griffin, L D},
	month = sep,
	year = {2017},
	note = {Publisher: Taylor \& Francis},
	keywords = {segmentation, Basic Image Features, local feature histograms, phase contrast microscopy, random forest, trainable segmentation},
	pages = {359--367},
}

@article{yin_understanding_nodate,
	title = {Understanding the {Optics} to {Aid} {Microscopy} {Image} {Segmentation}},
	url = {https://www.ri.cmu.edu/pub_files/2010/9/MICCAI2010_opticsseg.pdf},
	abstract = {Image segmentation is essential for many automated mi-croscopy image analysis systems. Rather than treating microscopy im-ages as general natural images and rushing into the image processing warehouse for solutions, we propose to study a microscope's optical prop-erties to model its image formation process first using phase contrast microscopy as an exemplar. It turns out that the phase contrast imag-ing system can be relatively well explained by a linear imaging model. Using this model, we formulate a quadratic optimization function with sparseness and smoothness regularizations to restore the " authentic " phase contrast image that directly corresponds to specimen's optical path length without phase contrast artifacts such as halo and shade-off. With artifacts removed, high quality segmentation can be achieved by sim-ply thresholding the restored image. The imaging model and restoration method are quantitatively evaluated on two sequences with thousands of cells captured over several days.},
	author = {Yin, Zhaozheng and Li, Kang and Kanade, Takeo and Chen, Mei},
}

@article{lukin_image_2006,
	title = {Image {Interpolation} by {Super}-{Resolution}},
	url = {http://www.graphicon.ru/},
	abstract = {Term " super-resolution " is typically used for a high-resolution image produced from several low-resolution noisy observations. In this paper, we consider the problem of high-quality interpolation of a single noise-free image. Several aspects of the corresponding super-resolution algorithm are investigated: choice of regularization term, dependence of the result on initial approximation, convergence speed, and heuristics to facilitate convergence and improve the visual quality of the resulting image.},
	journal = {International Conference Graphicon},
	author = {Lukin, Alexey and Krylov, Andrey S and Nasonov, Andrey},
	year = {2006},
	keywords = {super-resolution, image interpolation, regularization},
}

@article{parwani_sivq-aided_2011,
	title = {{SIVQ}-aided laser capture microdissection: {A} tool for high- throughput expression profiling {SIVQ}-aided laser capture microdissection: {A} tool for high-throughput expression profiling},
	volume = {22},
	url = {http://www.jpathinformatics.org/content/2/1/19},
	doi = {10.4103/2153-3539.78500},
	abstract = {Introduction: Laser capture microdissection (LCM) facilitates procurement of defined cell populations for study in the context of histopathology. The morphologic assessment step in the LCM procedure is time consuming and tedious, thus restricting the utility of the technology for large applications. Results: Here, we describe the use of Spatially Invariant Vector Quantization (SIVQ) for histological analysis within LCM. Using SIVQ, we selected vectors as morphologic predicates that were representative of normal epithelial or cancer cells and then searched for phenotypically similar cells across entire tissue sections. The selected cells were subsequently auto-microdissected and the recovered RNA was analyzed by expression microarray. Gene expression profiles from SIVQ– LCM and standard LCM-derived samples demonstrated highly congruous signatures, confirming the equivalence of the differing microdissection methods. Conclusion: SIVQ–LCM improves the work-flow of microdissection in two significant ways. First, the process is transformative in that it shifts the pathologist's role from technical execution of the entire microdissection to a limited-contact supervisory role, enabling large-scale extraction of tissue by expediting subsequent semi-autonomous identification of target cell populations. Second, this work-flow model provides an opportunity to systematically identify highly constrained cell populations and morphologically consistent regions within tissue sections. Integrating SIVQ with LCM in a single environment provides advanced capabilities for efficient and high-throughput histological-based molecular studies.},
	number = {19},
	journal = {J Pathol Inform},
	author = {Parwani, Anil V and Hipp, Jason and Cheng, Jerome and Hanson, Jeffrey C and Yan, Wusheng and Taylor, Phil and Hu, Nan and Rodriguez-Canales, Jaime and Hipp, Jennifer and Tangrea, Michael A and Emmert-Buck, Michael R and Balis, Ulysses},
	year = {2011},
	keywords = {Spatially Invariant Vector Quantization, Laser capture microdissection, microarray},
}

@article{aberman_neural_2018-1,
	title = {Neural {Best}-{Buddies}: {Sparse} {Cross}-{Domain} {Correspondence}},
	url = {http://arxiv.org/abs/1805.04140},
	doi = {10.1145/3197517.3201332},
	abstract = {Correspondence between images is a fundamental problem in computer vision, with a variety of graphics applications. This paper presents a novel method for sparse cross-domain correspondence. Our method is designed for pairs of images where the main objects of interest may belong to different semantic categories and differ drastically in shape and appearance, yet still contain semantically related or geometrically similar parts. Our approach operates on hierarchies of deep features, extracted from the input images by a pre-trained CNN. Specifically, starting from the coarsest layer in both hierarchies, we search for Neural Best Buddies (NBB): pairs of neurons that are mutual nearest neighbors. The key idea is then to percolate NBBs through the hierarchy, while narrowing down the search regions at each level and retaining only NBBs with significant activations. Furthermore, in order to overcome differences in appearance, each pair of search regions is transformed into a common appearance. We evaluate our method via a user study, in addition to comparisons with alternative correspondence approaches. The usefulness of our method is demonstrated using a variety of graphics applications, including cross-domain image alignment, creation of hybrid images, automatic image morphing, and more.},
	author = {Aberman, Kfir and Liao, Jing and Shi, Mingyi and Lischinski, Dani and Chen, Baoquan and Cohen-Or, Daniel},
	month = may,
	year = {2018},
}

@article{yin_understanding_2012,
	title = {Understanding the phase contrast optics to restore artifact-free microscopy images for segmentation.},
	volume = {16},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/22386070},
	doi = {10.1016/j.media.2011.12.006},
	abstract = {Phase contrast, a noninvasive microscopy imaging technique, is widely used to capture time-lapse images to monitor the behavior of transparent cells without staining or altering them. Due to the optical principle, phase contrast microscopy images contain artifacts such as the halo and shade-off that hinder image segmentation, a critical step in automated microscopy image analysis. Rather than treating phase contrast microscopy images as general natural images and applying generic image processing techniques on them, we propose to study the optical properties of the phase contrast microscope to model its image formation process. The phase contrast imaging system can be approximated by a linear imaging model. Based on this model and input image properties, we formulate a regularized quadratic cost function to restore artifact-free phase contrast images that directly correspond to the specimen's optical path length. With artifacts removed, high quality segmentation can be achieved by simply thresholding the restored images. The imaging model and restoration method are quantitatively evaluated on microscopy image sequences with thousands of cells captured over several days. We also demonstrate that accurate restoration lays the foundation for high performance in cell detection and tracking.},
	number = {5},
	journal = {Medical image analysis},
	author = {Yin, Zhaozheng and Kanade, Takeo and Chen, Mei},
	month = jul,
	year = {2012},
	note = {Publisher: NIH Public Access},
	pages = {1047--62},
}

@article{dong_tensorlayer_2017,
	title = {{TensorLayer}: {A} {Versatile} {Library} for {Efficient} {Deep} {Learning} {Development}},
	url = {https://doi.org/10.1145/3123266.3129391},
	doi = {10.1145/3123266.3129391},
	abstract = {Recently we have observed emerging uses of deep learning tech-niques in multimedia systems. Developing a practical deep learning system is arduous and complex. It involves labor-intensive tasks for constructing sophisticated neural networks, coordinating mul-tiple network models, and managing a large amount of training-related data. To facilitate such a development process, we propose TensorLayer which is a Python-based versatile deep learning library. TensorLayer provides high-level modules that abstract sophisticated operations towards neuron layers, network models, training data and dependent training jobs. In spite of offering simplicity, it has transparent module interfaces that allows developers to flexibly embed low-level controls within a backend engine, with the aim of supporting fine-grain tuning towards training. Real-world cluster experiment results show that TensorLayer is able to achieve com-petitive performance and scalability in critical deep learning tasks. TensorLayer was released in September 2016 on GitHub. Since after, it soon become one of the most popular open-sourced deep learning library used by researchers and practitioners.},
	author = {Dong, Hao and Supratak, Akara and Mai, Luo and Liu, Fangde and Oehmichen, Axel and Yu, Simiao and Guo, Yike},
	year = {2017},
	keywords = {Deep Learning, Computer Vision, Data Management, Natural Language Processing, Parallel Computation, Reinforcement Learning},
}

@article{dai_jointly_2015,
	title = {Jointly {Optimized} {Regressors} for {Image} {Super}-resolution},
	volume = {34},
	url = {https://pdfs.semanticscholar.org/fc3a/e73f825259131c2259d0f0fab435594a9e69.pdf},
	abstract = {Learning regressors from low-resolution patches to high-resolution patches has shown promising results for im-age super-resolution. We observe that some regressors are better at dealing with certain cases, and others with different cases. In this paper, we jointly learn a collection of regressors, which collectively yield the smallest super-resolving error for all training data. After training, each training sample is associated with a label to indicate its 'best' regressor, the one yielding the smallest error. During testing, our method bases on the concept of 'adaptive selection' to select the most appropriate regressor for each input patch. We assume that similar patches can be super-resolved by the same regressor and use a fast, approximate kNN approach to transfer the labels of train-ing patches to test patches. The method is conceptually simple and computationally efficient, yet very effective. Experiments on four datasets show that our method outperforms competing methods.},
	number = {2},
	author = {Dai, D and Timofte, R and Gool, L Van},
	year = {2015},
	keywords = {Enhancement—Sharpening and deblurring, I33 [Computer Graphics]},
}

@article{chang_super-resolution_nodate,
	title = {Super-{Resolution} {Through} {Neighbor} {Embedding}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.330.2972&rep=rep1&type=pdf},
	abstract = {In this paper, we propose a novel method for solv-ing single-image super-resolution problems. Given a low-resolution image as input, we recover its high-resolution counterpart using a set of training exam-ples. While this formulation resembles other learning-based methods for super-resolution, our method has been inspired by recent manifold learning methods, par-ticularly locally linear embedding (LLE). Specifically, small image patches in the low-and high-resolution images form manifolds with similar local geometry in two distinct feature spaces. As in LLE, local geometry is characterized by how a feature vector correspond-ing to a patch can be reconstructed by its neighbors in the feature space. Besides using the training image pairs to estimate the high-resolution embedding, we also enforce local compatibility and smoothness con-straints between patches in the target high-resolution image through overlapping. Experiments show that our method is very flexible and gives good empirical results.},
	author = {Chang, Hong and Yeung, Dit-Yan and Xiong, Yimin},
}

@article{ma_accelerated_2017,
	title = {Accelerated {Image} {Reconstruction} for {Nonlinear} {Diffractive} {Imaging}},
	url = {http://arxiv.org/abs/1708.01663},
	abstract = {The problem of reconstructing an object from the measurements of the light it scatters is common in numerous imaging applications. While the most popular formulations of the problem are based on linearizing the object-light relationship, there is an increased interest in considering nonlinear formulations that can account for multiple light scattering. In this paper, we propose an image reconstruction method, called CISOR, for nonlinear diffractive imaging, based on a nonconvex optimization formulation with total variation (TV) regularization. The nonconvex solver used in CISOR is our new variant of fast iterative shrinkage/thresholding algorithm (FISTA). We provide fast and memory-efficient implementation of the new FISTA variant and prove that it reliably converges for our nonconvex optimization problem. In addition, we systematically compare our method with other state-of-the-art methods on simulated as well as experimentally measured data in both 2D and 3D settings.},
	author = {Ma, Yanting and Mansour, Hassan and Liu, Dehong and Boufounos, Petros T. and Kamilov, Ulugbek S.},
	month = aug,
	year = {2017},
}

@article{mansour_sparse_2018,
	title = {Sparse {Blind} {Deconvolution} for {Distributed} {Radar} {Autofocus} {Imaging}},
	url = {http://arxiv.org/abs/1805.03269},
	abstract = {A common problem that arises in radar imaging systems, especially those mounted on mobile platforms, is antenna position ambiguity. Approaches to resolve this ambiguity and correct position errors are generally known as radar autofocus. Common techniques that attempt to resolve the antenna ambiguity generally assume an unknown gain and phase error afflicting the radar measurements. However, ensuring identifiability and tractability of the unknown error imposes strict restrictions on the allowable antenna perturbations. Furthermore, these techniques are often not applicable in near-field imaging, where mapping the position ambiguity to phase errors breaks down. In this paper, we propose an alternate formulation where the position error of each antenna is mapped to a spatial shift operator in the image-domain. Thus, the radar autofocus problem becomes a multichannel blind deconvolution problem, in which the radar measurements correspond to observations of a static radar image that is convolved with the spatial shift kernel associated with each antenna. To solve the reformulated problem, we also develop a block coordinate descent framework that leverages the sparsity and piece-wise smoothness of the radar scene, as well as the one-sparse property of the two dimensional shift kernels. We evaluate the performance of our approach using both simulated and experimental radar measurements, and demonstrate its superior performance compared to state-of-the-art methods.},
	author = {Mansour, Hassan and Liu, Dehong and Kamilov, Ulugbek S. and Boufounos, Petros T.},
	month = may,
	year = {2018},
}

@article{sun_stability_2018,
	title = {Stability of {Scattering} {Decoder} {For} {Nonlinear} {Diffractive} {Imaging}},
	url = {http://arxiv.org/abs/1806.08015},
	abstract = {The problem of image reconstruction under multiple light scattering is usually formulated as a regularized non-convex optimization. A deep learning architecture, Scattering Decoder (ScaDec), was recently proposed to solve this problem in a purely data-driven fashion. The proposed method was shown to substantially outperform optimization-based baselines and achieve state-of-the-art results. In this paper, we thoroughly test the robustness of ScaDec to different permittivity contrasts, number of transmissions, and input signal-to-noise ratios. The results on high-fidelity simulated datasets show that the performance of ScaDec is stable in different settings.},
	author = {Sun, Yu and Kamilov, Ulugbek S.},
	month = jun,
	year = {2018},
}

@article{odena_deconvolution_2016,
	title = {Deconvolution and {Checkerboard} {Artifacts}},
	url = {https://ai.google/research/pubs/pub46191},
	author = {Odena, Augustus and Dumoulin, Vincent and Olah, Chris},
	year = {2016},
}

@article{yang_learning_2017,
	title = {Learning {Face} {Age} {Progression}: {A} {Pyramid} {Architecture} of {GANs}},
	url = {http://arxiv.org/abs/1711.10352},
	abstract = {The two underlying requirements of face age progression, i.e. aging accuracy and identity permanence, are not well handled in the literature. In this paper, we present a novel generative adversarial network based approach. It separately models the constraints for the intrinsic subject-specific characteristics and the age-specific facial changes with respect to the elapsed time, ensuring that the generated faces present desired aging effects while simultaneously keeping personalized properties stable. Further, to generate more lifelike facial details, high-level age-specific features conveyed by the synthesized face are estimated by a pyramidal adversarial discriminator at multiple scales, which simulates the aging effects in a finer manner. The proposed method is applicable for diverse face samples in the presence of variations in pose, expression, makeup, etc., and remarkably vivid aging effects are achieved. Both visual fidelity and quantitative evaluations show that the approach advances the state-of-the-art.},
	author = {Yang, Hongyu and Huang, Di and Wang, Yunhong and Jain, Anil K.},
	month = nov,
	year = {2017},
}

@article{timofte__nodate,
	title = {A+: {Adjusted} {Anchored} {Neighborhood} {Regression} for {Fast} {Super}-{Resolution}},
	url = {http://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_01165.pdf},
	abstract = {We address the problem of image upscaling in the form of single image super-resolution based on a dictionary of low-and high-resolution exemplars. Two recently proposed methods, Anchored Neigh-borhood Regression (ANR) and Simple Functions (SF), provide state-of-the-art quality performance. Moreover, ANR is among the fastest known super-resolution methods. ANR learns sparse dictionaries and regressors anchored to the dictionary atoms. SF relies on clusters and corresponding learned functions. We propose A+, an improved variant of ANR, which combines the best qualities of ANR and SF. A+ builds on the features and anchored regressors from ANR but instead of learning the regressors on the dictionary it uses the full training material, similar to SF. We val-idate our method on standard images and compare with state-of-the-art methods. We obtain improved quality (i.e. 0.2-0.7dB PSNR better than ANR) and excellent time complexity, rendering A+ the most efficient dictionary-based super-resolution method to date.},
	author = {Timofte, Radu and De Smet, Vincent and Gool, Luc Van},
}

@article{bevilacqua_low-complexity_nodate,
	title = {Low-{Complexity} {Single}-{Image} {Super}-{Resolution} based on {Nonnegative} {Neighbor} {Embedding}},
	url = {http://people.rennes.inria.fr/Aline.Roumy/publi/12bmvc_Bevilacqua_lowComplexitySR.pdf},
	abstract = {This paper describes a single-image super-resolution (SR) algorithm based on non-negative neighbor embedding. It belongs to the family of single-image example-based SR algorithms, since it uses a dictionary of low resolution (LR) and high resolution (HR) trained patch pairs to infer the unknown HR details. Each LR feature vector in the input image is expressed as the weighted combination of its K nearest neighbors in the dictio-nary; the corresponding HR feature vector is reconstructed under the assumption that the local LR embedding is preserved. Three key aspects are introduced in order to build a low-complexity competitive algorithm: (i) a compact but efficient representation of the patches (feature representation) (ii) an accurate estimation of the patches by their near-est neighbors (weight computation) (iii) a compact and already built (therefore external) dictionary, which allows a one-step upscaling. The neighbor embedding SR algorithm so designed is shown to give good visual results, comparable to other state-of-the-art methods, while presenting an appreciable reduction of the computational time.},
	author = {Bevilacqua, Marco and Roumy, Aline and Guillemot, Christine and Alberi Morel, Marie-Line},
}

@incollection{svoboda_multimodal_2017,
	title = {Multimodal {Simulations} in {Live} {Cell} {Imaging}},
	url = {http://link.springer.com/10.1007/978-3-319-68127-6_10},
	publisher = {Springer, Cham},
	author = {Svoboda, David and Kozubek, Michal},
	year = {2017},
	doi = {10.1007/978-3-319-68127-6_10},
	pages = {89--98},
}

@inproceedings{huang_single_2015,
	title = {Single image super-resolution from transformed self-exemplars},
	volume = {07-12-June},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7299156/},
	doi = {10.1109/CVPR.2015.7299156},
	abstract = {Self-similarity based super-resolution (SR) algorithms are able to produce visually pleasing results without extensive training on external databases. Such algorithms exploit the statistical prior that patches in a natural image tend to recur within and across scales of the same image. However, the internal dictionary obtained from the given image may not always be sufficiently expressive to cover the textural appearance variations in the scene. In this paper, we extend self-similarity based SR to overcome this drawback. We ex- pand the internal patch search space by allowing geometric variations. We do so by explicitly localizing planes in the scene and using the detected perspective geometry to guide the patch search process. We also incorporate additional affine transformations to accommodate local shape variations. We propose a compositional model to simultaneously handle both types of transformations. We extensively evalu- ate the performance in both urban and natural scenes. Even without using any external training databases, we achieve significantly superior results on urban scenes, while main- taining comparable performance on natural scenes as other state-of-the-art SR algorithms.},
	publisher = {IEEE},
	author = {Huang, Jia Bin and Singh, Abhishek and Ahuja, Narendra},
	month = jun,
	year = {2015},
	pages = {5197--5206},
}

@techreport{jacobsen_excessive_nodate,
	title = {{EXCESSIVE} {INVARIANCE} {CAUSES} {ADVERSARIAL} {VULNERABILITY}},
	url = {https://openreview.net/pdf?id=BkfbpsAcF7},
	abstract = {Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.},
	author = {Jacobsen, Jörn-Henrik and Behrmann, Jens and Zemel, Richard and Bethge, Matthias},
}

@article{wu_eulerian_2012,
	title = {Eulerian video magnification for revealing subtle changes in the world},
	volume = {31},
	url = {http://dl.acm.org/citation.cfm?doid=2185520.2335416},
	doi = {10.1145/2185520.2335416},
	abstract = {Our goal is to reveal temporal variations in videos that are difficult or impossible to see with the naked eye and display them in an indicative manner. Our method, which we call Eulerian Video Magnification, takes a standard video sequence as input, and applies spatial decomposition, followed by temporal filtering to the frames. The resulting signal is then amplified to reveal hidden information. Using our method, we are able to visualize the flow of blood as it fills the face and also to amplify and reveal small motions. Our technique can run in real time to show phenomena occurring at temporal frequencies selected by the user.},
	number = {4},
	journal = {ACM Transactions on Graphics},
	author = {Wu, Hao-Yu and Rubinstein, Michael and Shih, Eugene and Guttag, John and Durand, Frédo and Freeman, William},
	month = jul,
	year = {2012},
	pages = {1--8},
}

@article{yao_net-flics_nodate,
	title = {Net-{FLICS}: fast quantitative wide-field fluorescence lifetime imaging with compressed sensing-a deep learning approach},
	url = {https://doi.org/10.1038/s41377-019-0138-x},
	doi = {10.1038/s41377-019-0138-x},
	abstract = {Macroscopic fluorescence lifetime imaging (MFLI) via compressed sensed (CS) measurements enables efficient and accurate quantification of molecular interactions in vivo over a large field of view (FOV). However, the current data-processing workflow is slow, complex and performs poorly under photon-starved conditions. In this paper, we propose Net-FLICS, a novel image reconstruction method based on a convolutional neural network (CNN), to directly reconstruct the intensity and lifetime images from raw time-resolved CS data. By carefully designing a large simulated dataset, Net-FLICS is successfully trained and achieves outstanding reconstruction performance on both in vitro and in vivo experimental data and even superior results at low photon count levels for lifetime quantification. Lifetime, which is an intrinsic property of fluorescent molecules, describes the time that a fluorophore spends in the excited state before returning to the ground state. Fluorescence lifetime imaging (FLI) provides unique additional information to intensity imaging with the advantages of being independent of the fluorophore concentration (before quenching) and minimal susceptibility to the changes in optical properties. Fluorescence lifetime imaging microscopy (FLIM) is widely applied to increase the multiplexing power, sense changes in microenvironment (pH, pO 2 , etc.) 1 , and monitor molecular interactions such as performed via Förster resonance energy transfer (FRET) studies. Coupled with multispectral or hyperspectral imaging, FLIM can unveil enriched information in the lifetime and intensity spectra 2. Recently, we reported the development of a hyper-spectral time-resolved wide-field system for macroscopic fluorescence lifetime imaging (MFLI) 3. The system supports structured light illumination and single-pixel detection over an 8 × 6 cm 2 field of view (FOV) via two digital micromirror devices (DMDs) and hyperspectral (80 nm range in 16 wavelength channels) data acquisition via a Photomultiplier Tube based Time-Correlated Single Photon Counting (PMT-TCSPC) spectrophotometer, which enables efficient whole-body FLI of live subjects. With time-resolved CS signals S collected with patterns P, both intensity image I A and lifetime image I τ can be reconstructed. However, the data-processing workflow necessesary to generate the spatially resolved quantitative FLI images can be time-consuming, susceptible to noise and requires user inputs to define the key parameters of the core iterative procedures. More precisely, the classical workflow recovers I A and I τ from three steps: (1) Solve the inverse problems PI A (t) = S(t) with CS-based 4 solvers for each time gate t, where P is the sensitivity matrix formed by the pattern weights. (2) Retrieve lifetime values τ i,j from the time point spread function (TPSF) curve C i,j with the least squares method (LSM)-based fitting for every pixel of interest, where i and j are the row and column indices of the image. (3) Repeat (1)-(2) for each wavelength channel.},
	journal = {Official journal of the CIOMP},
	author = {Yao, Ruoyang and Ochoa, Marien and Yan, Pingkun and Intes, Xavier},
	pages = {2047--7538},
}

@techreport{loshchilov_decoupled_nodate,
	title = {{DECOUPLED} {WEIGHT} {DECAY} {REGULARIZATION}},
	url = {https://github.com/loshchil/AdamW-and-SGDW},
	abstract = {L 2 regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is not the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L 2 regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by decoupling the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	author = {Loshchilov, Ilya and Hutter, Frank},
}

@article{goyal_accurate_2017,
	title = {Accurate, {Large} {Minibatch} {SGD}: {Training} {ImageNet} in 1 {Hour}},
	url = {http://arxiv.org/abs/1706.02677},
	abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves {\textasciitilde}90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
	author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	month = jun,
	year = {2017},
}

@techreport{brendel_approximating_nodate,
	title = {{APPROXIMATING} {CNNS} {WITH} {BAG}-{OF}-{LOCAL}-{FEATURES} {MODELS} {WORKS} {SURPRISINGLY} {WELL} {ON} {IMAGENET}},
	url = {https://openreview.net/pdf?id=SkfMWhAqYQ},
	abstract = {Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6\% top-5 for 33 × 33 px features and Alexnet performance for 17 × 17 px features). The constraint on local features makes it straightforward to analyse how exactly each part of the image influences the classification. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.},
	author = {Brendel, Wieland and Bethge, Matthias and Karls, Eberhard},
}

@article{geirhos_imagenet-trained_2018-1,
	title = {{ImageNet}-trained {CNNs} are biased towards texture; increasing shape bias improves accuracy and robustness},
	url = {https://openreview.net/forum?id=Bygh9j09KX},
	author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
	month = sep,
	year = {2018},
}

@article{zisimopoulos_can_2017,
	title = {Can surgical simulation be used to train detection and classification of neural networks?},
	volume = {4},
	doi = {10.1049/htl.2017.0064},
	abstract = {Computer-assisted interventions (CAI) aim to increase the effectiveness, precision and repeatability of procedures to improve surgical outcomes. The presence and motion of surgical tools is a key information input for CAI surgical phase recognition algorithms. Vision-based tool detection and recognition approaches are an attractive solution and can be designed to take advantage of the powerful deep learning paradigm that is rapidly advancing image recognition and classification. The challenge for such algorithms is the availability and quality of labelled data used for training. In this Letter, surgical simulation is used to train tool detection and segmentation based on deep convolutional neural networks and generative adversarial networks. The authors experiment with two network architectures for image segmentation in tool classes commonly encountered during cataract surgery. A commercially-available simulator is used to create a simulated cataract dataset for training models prior to performing transfer learning on real surgical data. To the best of authors' knowledge, this is the first attempt to train deep learning models for surgical instrument detection on simulated data while demonstrating promising results to generalise on real data. Results indicate that simulated data does have some potential for training advanced classification methods for CAI systems.},
	number = {5},
	journal = {Healthcare Technology Letters},
	author = {Zisimopoulos, Odysseas and Flouty, Evangello and Stacey, Mark and Muscroft, Sam and Giataganas, Petros and Nehme, Jean and Chow, Andre and Stoyanov, Danail},
	month = aug,
	year = {2017},
	note = {Publisher: Institution of Engineering and Technology (IET)},
	pages = {216--222},
	file = {Full Text:/home/zwerg/Zotero/storage/WD2LJQXL/Zisimopoulos et al. - 2017 - Can surgical simulation be used to train detection.pdf:application/pdf},
}

@article{zisimopoulos_can_2017-1,
	title = {Can surgical simulation be used to train detection and classification of neural networks?},
	volume = {4},
	doi = {10.1049/htl.2017.0064},
	abstract = {Computer-assisted interventions (CAI) aim to increase the effectiveness, precision and repeatability of procedures to improve surgical outcomes. The presence and motion of surgical tools is a key information input for CAI surgical phase recognition algorithms. Vision-based tool detection and recognition approaches are an attractive solution and can be designed to take advantage of the powerful deep learning paradigm that is rapidly advancing image recognition and classification. The challenge for such algorithms is the availability and quality of labelled data used for training. In this Letter, surgical simulation is used to train tool detection and segmentation based on deep convolutional neural networks and generative adversarial networks. The authors experiment with two network architectures for image segmentation in tool classes commonly encountered during cataract surgery. A commercially-available simulator is used to create a simulated cataract dataset for training models prior to performing transfer learning on real surgical data. To the best of authors' knowledge, this is the first attempt to train deep learning models for surgical instrument detection on simulated data while demonstrating promising results to generalise on real data. Results indicate that simulated data does have some potential for training advanced classification methods for CAI systems.},
	number = {5},
	journal = {Healthcare Technology Letters},
	author = {Zisimopoulos, Odysseas and Flouty, Evangello and Stacey, Mark and Muscroft, Sam and Giataganas, Petros and Nehme, Jean and Chow, Andre and Stoyanov, Danail},
	month = aug,
	year = {2017},
	note = {Publisher: Institution of Engineering and Technology (IET)},
	pages = {216--222},
}

@article{guo_accelerating_nodate,
	title = {Accelerating iterative deconvolution and multiview fusion by orders of magnitude},
	url = {http://dx.doi.org/10.1101/647370},
	doi = {10.1101/647370},
	author = {Guo, Min and Li, Yue and Su, Yijun and Lambert, Talley and Dalle Nogare, Damian and Moyle, Mark W and Duncan, Leighton H and Ikegami, Richard and Santella, Anthony and Rey-Suarez, Ivan and Green, Daniel and Chen, Jiji and Vishwasrao, Harshad and Ganesan, Sundar and Waters, Jennifer C and Annunziata, Christina M and Hafner, Markus and Mohler, William A and Chitnis, Ajay B and Upadhyaya, Arpita and Usdin, Ted B and Bao, Zhirong and Colόn-Ramos, Daniel and La Riviere, Patrick and Liu, Huafeng and Wu, Yicong and Shroff, Hari and Rico, Puerto and Juan, San},
}

@article{kornblith_better_2018,
	title = {Do {Better} {ImageNet} {Models} {Transfer} {Better}?},
	url = {http://arxiv.org/abs/1805.08974},
	abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classification networks on 12 image classification datasets. We find that, when networks are used as fixed feature extractors or fine-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (\$r = 0.99\$ and \$0.96\$, respectively). In the former setting, we find that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we find that, on two small fine-grained image classification datasets, pretraining on ImageNet provides minimal benefits, indicating the learned features from ImageNet do not transfer well to fine-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.},
	author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
	month = may,
	year = {2018},
}

@article{zhang_mixup_2017,
	title = {mixup: {Beyond} {Empirical} {Risk} {Minimization}},
	url = {http://arxiv.org/abs/1710.09412},
	abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
	author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
	month = oct,
	year = {2017},
}

@article{pinckaers_training_2018,
	title = {Training convolutional neural networks with megapixel images},
	url = {http://arxiv.org/abs/1804.05712},
	abstract = {To train deep convolutional neural networks, the input data and the intermediate activations need to be kept in memory to calculate the gradient descent step. Given the limited memory available in the current generation accelerator cards, this limits the maximum dimensions of the input data. We demonstrate a method to train convolutional neural networks holding only parts of the image in memory while giving equivalent results. We quantitatively compare this new way of training convolutional neural networks with conventional training. In addition, as a proof of concept, we train a convolutional neural network with 64 megapixel images, which requires 97\% less memory than the conventional approach.},
	author = {Pinckaers, Hans and Litjens, Geert},
	month = apr,
	year = {2018},
}

@article{tellez_neural_2018,
	title = {Neural {Image} {Compression} for {Gigapixel} {Histopathology} {Image} {Analysis}},
	url = {http://arxiv.org/abs/1811.02840},
	abstract = {We present Neural Image Compression (NIC), a method to reduce the size of gigapixel images by mapping them to a compact latent space using neural networks. We show that this compression allows us to train convolutional neural networks on histopathology whole-slide images end-to-end using weak image-level labels.},
	author = {Tellez, David and Litjens, Geert and van der Laak, Jeroen and Ciompi, Francesco},
	month = nov,
	year = {2018},
}

@article{pinkard_deep_2019,
	title = {Deep learning for single-shot autofocus microscopy},
	volume = {6},
	url = {https://www.osapublishing.org/abstract.cfm?URI=optica-6-6-794},
	doi = {10.1364/OPTICA.6.000794},
	abstract = {Maintaining an in-focus image over long time scales is an essential and nontrivial task for a variety of microscopy applications. Here, we describe a fast, robust autofocusing method compatible with a wide range of existing microscopes. It requires only the addition of one or a few off-axis illumination sources (e.g., LEDs), and can predict the focus correction from a single image with this illumination. We designed a neural network architecture, the fully connected Fourier neural network (FCFNN), that exploits an understanding of the physics of the illumination to make accurate predictions with 2\&\#x2013;3 orders of magnitude fewer learned parameters and less memory usage than existing state-of-the-art architectures, allowing it to be trained without any specialized hardware. We provide an open-source implementation of our method, to enable fast, inexpensive autofocus compatible with a variety of microscopes.},
	number = {6},
	journal = {Optica},
	author = {Pinkard, Henry and Phillips, Zachary and Babakhani, Arman and Fletcher, Daniel A. and Waller, Laura},
	month = jun,
	year = {2019},
	note = {Publisher: Optical Society of America},
	keywords = {Image processing, Neural networks, Fourier transforms, Image metrics, Laser interferometry, Phase contrast},
	pages = {794--794},
}

@article{hwang_single-cell_2018,
	title = {Single-cell {RNA} sequencing technologies and bioinformatics pipelines},
	volume = {50},
	doi = {10.1038/s12276-018-0071-8},
	abstract = {Rapid progress in the development of next-generation sequencing (NGS) technologies in recent years has provided many valuable insights into complex biological systems, ranging from cancer genomics to diverse microbial communities. NGS-based technologies for genomics, transcriptomics, and epigenomics are now increasingly focused on the characterization of individual cells. These single-cell analyses will allow researchers to uncover new and potentially unexpected biological discoveries relative to traditional profiling methods that assess bulk populations. Single-cell RNA sequencing (scRNA-seq), for example, can reveal complex and rare cell populations, uncover regulatory relationships between genes, and track the trajectories of distinct cell lineages in development. In this review, we will focus on technical challenges in single-cell isolation and library preparation and on computational analysis pipelines available for analyzing scRNA-seq data. Further technical improvements at the level of molecular and cell biology and in available bioinformatics tools will greatly facilitate both the basic science and medical applications of these sequencing technologies.},
	number = {8},
	journal = {Experimental and Molecular Medicine},
	author = {Hwang, Byungjin and Lee, Ji Hyun and Bang, Duhee},
	month = aug,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
}

@article{al-rfou_character-level_2018,
	title = {Character-{Level} {Language} {Modeling} with {Deeper} {Self}-{Attention}},
	url = {http://arxiv.org/abs/1808.04444},
	abstract = {LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.},
	author = {Al-Rfou, Rami and Choe, Dokook and Constant, Noah and Guo, Mandy and Jones, Llion},
	month = aug,
	year = {2018},
}

@techreport{tellez_quantifying_nodate,
	title = {Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology},
	url = {https://arxiv.org/pdf/1902.06543.pdf},
	abstract = {Stain variation is a phenomenon observed when distinct pathology laboratories stain tissue slides that exhibit similar but not identical color appearance. Due to this color shift between laboratories, convolutional neural networks (CNNs) trained with images from one lab often underperform on unseen images from the other lab. Several techniques have been proposed to reduce the generalization error, mainly grouped into two categories: stain color augmentation and stain color normalization. The former simulates a wide variety of realistic stain variations during training, producing stain-invariant CNNs. The latter aims to match training and test color distributions in order to reduce stain variation. For the first time, we compared some of these techniques and quantified their effect on CNN classification performance using a heterogeneous dataset of hematoxylin and eosin histopathology images from 4 organs and 9 pathology laboratories. Additionally, we propose a novel unsupervised method to perform stain color normalization using a neural network. Based on our experimental results, we provide practical guidelines on how to use stain color augmentation and stain color normalization in future computational pathology applications.},
	author = {Tellez, David and Litjens, Geert and Bándi, Péter and Bulten, Wouter and Bokhorst, John-Melle and Ciompi, Francesco and Van Der Laak, Jeroen},
}

@techreport{simpson_google_2019,
	title = {Google {DeepMind}. 10. {Imaging} {Biomarkers} and {Computer}-aided {Diagnosis} {Lab}, {Radiology} and {Imag}-ing {Sciences}, {National} {Institutes} of {Health} {Clinical} {Center}. 11. {Department} of {Radiology}},
	url = {https://arxiv.org/pdf/1902.09063.pdf},
	abstract = {Semantic segmentation of medical images aims to associate a pixel with a label in a medical image without human initialization. The success of semantic segmentation algorithms is contingent on the availability of high-quality imaging data with corresponding labels provided by experts.},
	author = {Simpson, Amber L and Antonelli, Michela and Bakas, Spyridon and Bilello, Michel and Farahani, Keyvan and Van Ginneken, Bram and Kopp-Schneider, Annette and Landman, Bennett A and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M and Bilic, Patrick and Christ, Patrick F and Do, Richard K G and Gollub, Marc and Golia-Pernicka, Jennifer and Heckers, Stephan H and Jarnagin, William R and Mchugo, Maureen K and Napel, Sandy and Vorontsov, Eugene and Maier-Hein, Lena and Cardoso, M Jorge},
	year = {2019},
	note = {Issue: 9
Volume: 12},
}

@article{nilsson_supplementary_2005,
	title = {Supplementary {Figure} {S1} {Supplementary} {Figure} {S2}},
	doi = {10.1038/nature08119},
	abstract = {주생활은 의생활 식생활과 함께 인간이 생활을 영위하는데 있어서 기본바탕이 되는 것으로 주거내에서 이루어지는 모든 생활행위를 말하는데 이는 사회생활을 형성하는 소단위이다. 그래서 주거공간내에서 일어나는 생활패턴과 주의식은 주체인 인간의 행위와 사회적, 심리적, 미적 여건을 충족하기에 알맞는 가치를 가지고 있어야 하며 이로부터 접근되어야 할 것이다. 그러면 주거공간내에서 이루어지는 주생활과 주의식을 고찰하고저 한다.},
	author = {Nilsson, Avlant and Nielsen, Jens},
	year = {2005},
	pages = {184--186},
}

@article{ouyang_competition_nodate,
	title = {Competition of pattern classification methods to identify protein localizations in microscope images},
	author = {Ouyang, Wei and Winsnes, Casper F and Hjelmare, Martin and Cesnik, Anthony J and Åkesson, Lovisa and Xu, Hao and Sullivan, Devin P and Dai, Shubin and Lan, Jun and Jinmo, Park and Galib, Shaikat Mahmood and Henkel, Christof and Hwang, Kevin and Poplavskiy, Dmytro and Tunguz, Bojan and Wolfinger, Russ},
}

@article{kirillov_panoptic_2019,
	title = {Panoptic {Feature} {Pyramid} {Networks}},
	url = {http://arxiv.org/abs/1901.02446},
	abstract = {The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.},
	author = {Kirillov, Alexander and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = jan,
	year = {2019},
	file = {Full Text:/home/zwerg/Zotero/storage/JCUNZ7FA/Kirillov et al. - 2019 - Panoptic Feature Pyramid Networks.pdf:application/pdf},
}

@article{selvaraju_grad-cam_2016,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
	url = {http://arxiv.org/abs/1610.02391},
	abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multimodal inputs (e.g. VQA) or reinforcement learning, without any architectural changes or re-training. We combine GradCAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes (showing that seemingly unreasonable predictions have reasonable explanations), (b) are robust to adversarial images, (c) outperform previous methods on weakly-supervised localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, our visualizations show that even non-attention based models can localize inputs. Finally, we conduct human studies to measure if GradCAM explanations help users establish trust in predictions from deep networks and show that GradCAM helps untrained users successfully discern a "stronger" deep network from a "weaker" one. Our code is available at https://github.com/ramprs/grad-cam. A demo and a video of the demo can be found at http://gradcam.cloudcv.org and youtu.be/COjUB9Izk6E.},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = oct,
	year = {2016},
}

@article{todorov_automated_2019,
	title = {Automated analysis of whole brain vasculature using machine learning {Automated} analysis of whole brain vasculature using machine learning},
	author = {Todorov, Mihail Ivilinov and Paetzold, Johannes C and Schoppe, Oliver and Tetteh, Giles and Efremov, Velizar and Völgyi, Katalin and Düring, Marco and Dichgans, Martin and Piraud, Marie and Menze, Bjoern and Ertürk, Ali and Todorov, Mihail Ivilinov and Paetzold, Johannes C and Schoppe, Oliver and Tetteh, Giles and Völgyi, Katalin and Düring, Marco and Dichgans, Martin and Piraud, Marie and Menze, Bjoern},
	year = {2019},
	pages = {0--34},
}

@techreport{devries_improved_nodate,
	title = {Improved {Regularization} of {Convolutional} {Neural} {Networks} with {Cutout}},
	url = {https://github.com/},
	abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of con-volutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56\%, 15.20\%, and 1.30\% test error respectively. Code available at https://github.com/ uoguelph-mlrg/Cutout.},
	author = {Devries, Terrance and Taylor, Graham W},
}

@article{hafner_learning_2018,
	title = {Learning {Latent} {Dynamics} for {Planning} from {Pixels}},
	url = {http://arxiv.org/abs/1811.04551},
	abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this problem using a latent dynamics model with both deterministic and stochastic transition components and a multi-step variational inference objective that we call latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
	author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
	month = nov,
	year = {2018},
}

@article{ounkomol_label-free_nodate,
	title = {Label-free prediction of three-dimensional fluorescence images from transmitted light microscopy},
	url = {http://dx.doi.org/10.1101/289504},
	doi = {10.1101/289504},
	abstract = {Understanding cells as integrated systems is a challenge central to modern biology. While different microscopy approaches may be used to probe diverse aspects of biological organization, each method presents limitations which ultimately restrict a view into unified cellular organization. For example, while fluorescence microscopy can resolve subcellular structure in living cells, it is expensive, slow, and can damage cells. Here, we present a label-free method for predicting 3D fluorescence directly from transmitted light images and demonstrate that it can be used to generate multi-structure, integrated images. We then demonstrate that this same method can be used to predict immunofluorescence from electron micrograph inputs, extending the method to a wider range of bioimaging applications. The various imaging methods currently used to capture details of cellular organization all present restrictions with respect to expense, spatio-temporal resolution, and sample perturbation. Fluorescence microscopy permits imaging of specific proteins and structures of interest by labeling them specifically; but it requires advanced instrumentation and time-consuming sample preparation. Critically, samples are subject to significant phototoxicity and photobleaching, creating a tradeoff between data quality and time scales available for live cell imaging 1 , 2. Furthermore, the number of simultaneous fluorescent tags is restricted by both spectrum saturation and cell health, limiting the number of parallel labels that can be imaged together. Transmitted light microscopy, e.g., bright-field, phase, DIC, etc., in contrast, is relatively low-cost and label/dye-free, with greatly reduced phototoxicity 3 and simplified sample preparation. Although valuable information about cellular organization is apparent in transmitted light images; they lack the clear contrast of fluorescence labeling, a limitation also present in other widely used microscopy modalities. Electron micrographs, for example, contain a rich set of biological detail about subcellular structure, but often require tedious expert interpretation. A method that could combine the clarity of fluorescence microscopy with the relative simplicity and modest cost of other imaging techniques would present a groundbreaking tool for biological insight into the integrated organization of subcellular structures. Convolutional neural networks (CNNs) capture non-linear relationships over large spatial areas of images, resulting in vastly improved performance for image recognition tasks as compared to classical machine learning methods. Here, we present a CNN-based tool, employing a U-Net architecture 4 (Supplementary Fig. 1, see Methods) to model. CC-BY-NC 4.0 International license It is made available under a (which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.},
	author = {Ounkomol, Chawin and Seshamani, Sharmishtaa and Maleckar, Mary M and Collman, Forrest and Johnson, Gregory R},
}

@article{li_selective_2019,
	title = {Selective {Kernel} {Networks}},
	url = {http://arxiv.org/abs/1903.06586},
	abstract = {In standard Convolutional Neural Networks (CNNs), the receptive fields of artificial neurons in each layer are designed to share the same size. It is well-known in the neuroscience community that the receptive field size of visual cortical neurons are modulated by the stimulus, which has been rarely considered in constructing CNNs. We propose a dynamic selection mechanism in CNNs that allows each neuron to adaptively adjust its receptive field size based on multiple scales of input information. A building block called Selective Kernel (SK) unit is designed, in which multiple branches with different kernel sizes are fused using softmax attention that is guided by the information in these branches. Different attentions on these branches yield different sizes of the effective receptive fields of neurons in the fusion layer. Multiple SK units are stacked to a deep network termed Selective Kernel Networks (SKNets). On the ImageNet and CIFAR benchmarks, we empirically show that SKNet outperforms the existing state-of-the-art architectures with lower model complexity. Detailed analyses show that the neurons in SKNet can capture target objects with different scales, which verifies the capability of neurons for adaptively adjusting their receptive field sizes according to the input. The code and models are available at https://github.com/implus/SKNet.},
	author = {Li, Xiang and Wang, Wenhai and Hu, Xiaolin and Yang, Jian},
	month = mar,
	year = {2019},
}

@article{schmidhuber_deep_2014-1,
	title = {Deep {Learning} in {Neural} {Networks}: {An} {Overview}},
	url = {http://arxiv.org/abs/1404.7828},
	doi = {10.1016/j.neunet.2014.09.003},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	author = {Schmidhuber, Juergen},
	month = apr,
	year = {2014},
}

@techreport{christensen_image_nodate,
	title = {Image restoration using deep convolutional neural networks applied to high-speed optical microscopy of living samples},
	url = {http://www.focusonmicroscopy.org/2019/PDF/1306_Christensen.pdf},
	author = {Christensen, Charles Nicklas and Scherer, Katharina and Liò, Pietro and Kaminski, Clemens F},
	keywords = {Deep learning, image restoration, convolutional neural network, fluorescence microscopy, live-cell imaging, denoising, high-speed imaging, optical microscopy},
}

@article{you_reducing_2019,
	title = {Reducing {BERT} {Pre}-{Training} {Time} from 3 {Days} to 76 {Minutes}},
	url = {http://arxiv.org/abs/1904.00962},
	abstract = {Large-batch training is key to speeding up deep neural network training in large distributed systems. However, large-batch training is difficult because it produces a generalization gap. Straightforward optimization often leads to accuracy loss on the test set. BERT {\textbackslash}cite\{devlin2018bert\} is a state-of-the-art deep learning model that builds on top of deep bidirectional transformers for language understanding. Previous large-batch training techniques do not perform well for BERT when we scale the batch size (e.g. beyond 8192). BERT pre-training also takes a long time to finish (around three days on 16 TPUv3 chips). To solve this problem, we propose the LAMB optimizer, which helps us to scale the batch size to 65536 without losing accuracy. LAMB is a general optimizer that works for both small and large batch sizes and does not need hyper-parameter tuning besides the learning rate. The baseline BERT-Large model needs 1 million iterations to finish pre-training, while LAMB with batch size 65536/32768 only needs 8599 iterations. We push the batch size to the memory limit of a TPUv3 pod and can finish BERT training in 76 minutes.},
	author = {You, Yang and Li, Jing and Hseu, Jonathan and Song, Xiaodan and Demmel, James and Hsieh, Cho-Jui},
	month = apr,
	year = {2019},
}

@article{choi_gaussian_2019,
	title = {Gaussian {YOLOv3}: {An} {Accurate} and {Fast} {Object} {Detector} {Using} {Localization} {Uncertainty} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/1904.04620},
	abstract = {The use of object detection algorithms is becoming increasingly important in autonomous vehicles, and object detection at high accuracy and a fast inference speed is essential for safe autonomous driving. A false positive (FP) from a false localization during autonomous driving can lead to fatal accidents and hinder safe and efficient driving. Therefore, a detection algorithm that can cope with mislocalizations is required in autonomous driving applications. This paper proposes a method for improving the detection accuracy while supporting a real-time operation by modeling the bounding box (bbox) of YOLOv3, which is the most representative of one-stage detectors, with a Gaussian parameter and redesigning the loss function. In addition, this paper proposes a method for predicting the localization uncertainty that indicates the reliability of bbox. By using the predicted localization uncertainty during the detection process, the proposed schemes can significantly reduce the FP and increase the true positive (TP), thereby improving the accuracy. Compared to a conventional YOLOv3, the proposed algorithm, Gaussian YOLOv3, improves the mean average precision (mAP) by 3.09 and 3.5 on the KITTI and Berkeley deep drive (BDD) datasets, respectively. In addition, on the same datasets, the proposed algorithm can reduce the FP by 41.40\% and 40.62\%, and increase the TP by 7.26\% and 4.3\%, respectively. Nevertheless, the proposed algorithm is capable of real-time detection at faster than 42 frames per second (fps).},
	author = {Choi, Jiwoong and Chun, Dayoung and Kim, Hyun and Lee, Hyuk-Jae},
	month = apr,
	year = {2019},
}

@techreport{learned-miller_entropy_2013,
	title = {Entropy and {Mutual} {Information}},
	url = {https://people.cs.umass.edu/~elm/Teaching/Docs/mutInf.pdf},
	abstract = {This document is an introduction to entropy and mutual information for discrete random variables. It gives their definitions in terms of probabilities , and a few simple examples.},
	author = {Learned-Miller, Erik G},
	year = {2013},
}

@article{hoffer_norm_2018,
	title = {Norm matters: efficient and accurate normalization schemes in deep networks},
	url = {http://arxiv.org/abs/1803.01814},
	abstract = {Over the past few years, Batch-Normalization has been commonly used in deep networks, allowing faster training and high performance for a wide variety of applications. However, the reasons behind its merits remained unanswered, with several shortcomings that hindered its use for certain tasks. In this work, we present a novel view on the purpose and function of normalization methods and weight-decay, as tools to decouple weights' norm from the underlying optimized objective. This property highlights the connection between practices such as normalization, weight decay and learning-rate adjustments. We suggest several alternatives to the widely used \$L{\textasciicircum}2\$ batch-norm, using normalization in \$L{\textasciicircum}1\$ and \$L{\textasciicircum}{\textbackslash}infty\$ spaces that can substantially improve numerical stability in low-precision implementations as well as provide computational and memory benefits. We demonstrate that such methods enable the first batch-norm alternative to work for half-precision implementations. Finally, we suggest a modification to weight-normalization, which improves its performance on large-scale tasks.},
	author = {Hoffer, Elad and Banner, Ron and Golan, Itay and Soudry, Daniel},
	month = mar,
	year = {2018},
}

@article{zhang_three_2018,
	title = {Three {Mechanisms} of {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1810.12281},
	abstract = {Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of \$L\_2\$ regularization. Literal weight decay has been shown to outperform \$L\_2\$ regularization for optimizers for which they differ. We empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. Our results provide insight into how to improve the regularization of neural networks.},
	author = {Zhang, Guodong and Wang, Chaoqi and Xu, Bowen and Grosse, Roger},
	month = oct,
	year = {2018},
}

@article{botev_nesterovs_2016,
	title = {Nesterov's {Accelerated} {Gradient} and {Momentum} as approximations to {Regularised} {Update} {Descent}},
	url = {http://arxiv.org/abs/1607.01981},
	abstract = {We present a unifying framework for adapting the update direction in gradient-based iterative optimization methods. As natural special cases we re-derive classical momentum and Nesterov's accelerated gradient method, lending a new intuitive interpretation to the latter algorithm. We show that a new algorithm, which we term Regularised Gradient Descent, can converge more quickly than either Nesterov's algorithm or the classical momentum algorithm.},
	author = {Botev, Aleksandar and Lever, Guy and Barber, David},
	month = jul,
	year = {2016},
}

@article{kingma_adam_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
}

@article{ling_fast_2019,
	title = {Fast {Interactive} {Object} {Annotation} with {Curve}-{GCN}},
	url = {http://arxiv.org/abs/1903.06874},
	abstract = {Manually labeling objects by tracing their boundaries is a laborious process. In Polygon-RNN++ the authors proposed Polygon-RNN that produces polygonal annotations in a recurrent manner using a CNN-RNN architecture, allowing interactive correction via humans-in-the-loop. We propose a new framework that alleviates the sequential nature of Polygon-RNN, by predicting all vertices simultaneously using a Graph Convolutional Network (GCN). Our model is trained end-to-end. It supports object annotation by either polygons or splines, facilitating labeling efficiency for both line-based and curved objects. We show that Curve-GCN outperforms all existing approaches in automatic mode, including the powerful PSP-DeepLab and is significantly more efficient in interactive mode than Polygon-RNN++. Our model runs at 29.3ms in automatic, and 2.6ms in interactive mode, making it 10x and 100x faster than Polygon-RNN++.},
	author = {Ling, Huan and Gao, Jun and Kar, Amlan and Chen, Wenzheng and Fidler, Sanja},
	month = mar,
	year = {2019},
}

@article{wu_fastfcn_2019,
	title = {{FastFCN}: {Rethinking} {Dilated} {Convolution} in the {Backbone} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1903.11816},
	abstract = {Modern approaches for semantic segmentation usually employ dilated convolutions in the backbone to extract high-resolution feature maps, which brings heavy computation complexity and memory footprint. To replace the time and memory consuming dilated convolutions, we propose a novel joint upsampling module named Joint Pyramid Upsampling (JPU) by formulating the task of extracting high-resolution feature maps into a joint upsampling problem. With the proposed JPU, our method reduces the computation complexity by more than three times without performance loss. Experiments show that JPU is superior to other upsampling modules, which can be plugged into many existing approaches to reduce computation complexity and improve performance. By replacing dilated convolutions with the proposed JPU module, our method achieves the state-of-the-art performance in Pascal Context dataset (mIoU of 53.13\%) and ADE20K dataset (final score of 0.5584) while running 3 times faster.},
	author = {Wu, Huikai and Zhang, Junge and Huang, Kaiqi and Liang, Kongming and Yu, Yizhou},
	month = mar,
	year = {2019},
}

@article{ji_invariant_2018,
	title = {Invariant {Information} {Clustering} for {Unsupervised} {Image} {Classification} and {Segmentation}},
	url = {http://arxiv.org/abs/1807.06653},
	abstract = {We present a novel clustering objective that learns a neural network classifier from scratch, given only unlabelled data samples. The model discovers clusters that accurately match semantic classes, achieving state-of-the-art results in eight unsupervised clustering benchmarks spanning image classification and segmentation. These include STL10, an unsupervised variant of ImageNet, and CIFAR10, where we significantly beat the accuracy of our closest competitors by 8 and 9.5 absolute percentage points respectively. The method is not specialised to computer vision and operates on any paired dataset samples; in our experiments we use random transforms to obtain a pair from each image. The trained network directly outputs semantic labels, rather than high dimensional representations that need external processing to be usable for semantic clustering. The objective is simply to maximise mutual information between the class assignments of each pair. It is easy to implement and rigorously grounded in information theory, meaning we effortlessly avoid degenerate solutions that other clustering methods are susceptible to. In addition to the fully unsupervised mode, we also test two semi-supervised settings. The first achieves 88.8\% accuracy on STL10 classification, setting a new global state-of-the-art over all existing methods (whether supervised, semi supervised or unsupervised). The second shows robustness to 90\% reductions in label coverage, of relevance to applications that wish to make use of small amounts of labels. github.com/xu-ji/IIC},
	author = {Ji, Xu and Henriques, João F. and Vedaldi, Andrea},
	month = jul,
	year = {2018},
}

@techreport{al-shabi_gated-dilated_nodate,
	title = {Gated-{Dilated} {Networks} for {Lung} {Nodule} {Classification} in {CT} scans},
	url = {https://arxiv.org/ftp/arxiv/papers/1901/1901.00120.pdf},
	abstract = {Different types of Convolutional Neural Networks (CNNs) have been applied to detect cancerous lung nodules from computed tomography (CT) scans. However, the size of a nodule is very diverse and can range anywhere between 3 and 30 millimeters. The high variation of nodule sizes makes classifying them a difficult and challenging task. In this study, we propose a novel CNN architecture called Gated-Dilated (GD) Networks to classify nodules as malignant or benign. Unlike previous studies, the GD network uses multiple dilated convolutions instead of max-poolings to capture the scale variations. Moreover, the GD network has a Context-Aware sub-network that analyzes the input features and guides the features to a suitable dilated convolution. We evaluated the proposed network on more than 1,000 CT scans from the LIDC-LDRI dataset. Our proposed network outperforms baseline models including conventional CNNs, Resnet, and Densenet, with an AUC of {\textgreater}0.95. Compared to the baseline models, the GD network improves the classification accuracies of mid-range sized nodules. Furthermore, we observe a relationship between the size of the nodule and the attention signal generated by the Context-Aware sub-network, which validates our new network architecture.},
	author = {Al-Shabi, Mundher and Lee, Hwee Kuan and Tan, Maxine},
	keywords = {Convolutional Neural Network, Attention Network, Computed Tomography, Dilated Convolution, Lung Cancer},
}

@techreport{cortes-ciriano_kekulescope_nodate,
	title = {{KekuleScope}: improved prediction of cancer cell line sensitivity using convolutional neural networks trained on compound images},
	url = {https://github.com/isidroc/kekulescope.},
	abstract = {The application of convolutional neural networks (ConvNets) to harness high-content screening images or 2D compound representations is gaining increasing attention in drug discovery. However, existing applications often require large data sets for training, or sophisticated pretraining schemes for the networks. Here, we show on eight cytotoxicity IC50 data sets from ChEMBL 23 that the in vitro activity of compounds on cancer cell lines can be accurately predicted on a continuous scale from their Kekulé structure representations alone by extending existing architectures (AlexNet, DenseNet-201, ResNet152 and VGG-19), which were pretrained on unrelated image data sets. We show that the predictive power of the generated models, which just require standard 2D compound representations as input, is comparable to that of Random Forest (RF) models trained on circular (Morgan) fingerprints, a combination which is considered to be the state of the art. Notably, including additional fully-connected layers further increases the predictive power of the networks by up to 10\%. Analysis of the predictions generated by RF models and ConvNets shows that by simply averaging the output of the RF models and ConvNets we constantly obtain significantly lower errors in prediction (4-12\% decrease in RMSE on the test set) than those obtained with either model alone, indicating that the features extracted by the convolutional layers of the ConvNets provide complementary predictive signal to Morgan fingerprints. Overall, in this work we present a set of ConvNet architectures for the prediction of compound activity from their Kekulé structure representations with state-of-the-art performance, that require no generation of compound descriptors or use of sophisticated image processing techniques. The data sets and all the code needed to reproduce the results presented in this study are provided at https://github.com/isidroc/kekulescope.},
	author = {Cortés-Ciriano, Isidro and Bender, Andreas},
}

@article{purohit_spatially-adaptive_2019,
	title = {Spatially-{Adaptive} {Residual} {Networks} for {Efficient} {Image} and {Video} {Deblurring}},
	url = {http://arxiv.org/abs/1903.11394},
	abstract = {In this paper, we address the problem of dynamic scene deblurring in the presence of motion blur. Restoration of images affected by severe blur necessitates a network design with a large receptive field, which existing networks attempt to achieve through simple increment in the number of generic convolution layers, kernel-size, or the scales at which the image is processed. However, increasing the network capacity in this manner comes at the expense of increase in model size and inference speed, and ignoring the non-uniform nature of blur. We present a new architecture composed of spatially adaptive residual learning modules that implicitly discover the spatially varying shifts responsible for non-uniform blur in the input image and learn to modulate the filters. This capability is complemented by a self-attentive module which captures non-local relationships among the intermediate features and enhances the receptive field. We then incorporate a spatiotemporal recurrent module in the design to also facilitate efficient video deblurring. Our networks can implicitly model the spatially-varying deblurring process, while dispensing with multi-scale processing and large filters entirely. Extensive qualitative and quantitative comparisons with prior art on benchmark dynamic scene deblurring datasets clearly demonstrate the superiority of the proposed networks via reduction in model-size and significant improvements in accuracy and speed, enabling almost real-time deblurring.},
	author = {Purohit, Kuldeep and Rajagopalan, A. N.},
	month = mar,
	year = {2019},
}

@article{qin_thundernet_2019,
	title = {{ThunderNet}: {Towards} {Real}-time {Generic} {Object} {Detection}},
	url = {http://arxiv.org/abs/1903.11752},
	abstract = {Real-time generic object detection on mobile platforms is a crucial but challenging computer vision task. However, previous CNN-based detectors suffer from enormous computational cost, which hinders them from real-time inference in computation-constrained scenarios. In this paper, we investigate the effectiveness of two-stage detectors in real-time generic detection and propose a lightweight two-stage detector named ThunderNet. In the backbone part, we analyze the drawbacks in previous lightweight backbones and present a lightweight backbone designed for object detection. In the detection part, we exploit an extremely efficient RPN and detection head design. To generate more discriminative feature representation, we design two efficient architecture blocks, Context Enhancement Module and Spatial Attention Module. At last, we investigate the balance between the input resolution, the backbone, and the detection head. Compared with lightweight one-stage detectors, ThunderNet achieves superior performance with only 40\% of the computational cost on PASCAL VOC and COCO benchmarks. Without bells and whistles, our model runs at 24.1 fps on an ARM-based device. To the best of our knowledge, this is the first real-time detector reported on ARM platforms. Code will be released for paper reproduction.},
	author = {Qin, Zheng and Li, Zeming and Zhang, Zhaoning and Bao, Yiping and Yu, Gang and Peng, Yuxing and Sun, Jian},
	month = mar,
	year = {2019},
}

@techreport{roels_domain_nodate,
	title = {{DOMAIN} {ADAPTIVE} {SEGMENTATION} {IN} {VOLUME} {ELECTRON} {MICROSCOPY} {IMAGING}},
	url = {https://github.com/JorisRoels/domain-adaptive-},
	abstract = {In the last years, automated segmentation has become a necessary tool for volume electron microscopy (EM) imaging. So far, the best performing techniques have been largely based on fully supervised encoder-decoder CNNs, requiring a substantial amount of annotated images. Domain Adaptation (DA) aims to alleviate the annotation burden by 'adapting' the networks trained on existing groundtruth data (source domain) to work on a different (target) domain with as little additional annotation as possible. Most DA research is focused on the classification task, whereas volume EM segmentation remains rather unexplored. In this work, we extend recently proposed classification DA techniques to an encoder-decoder layout and propose a novel method that adds a reconstruction decoder to the classical encoder-decoder segmentation in order to align source and target encoder features. The method has been validated on the task of segmenting mitochondria in EM volumes. We have performed DA from brain EM images to HeLa cells and from isotropic FIB/SEM volumes to anisotropic TEM volumes. In all cases, the proposed method has outper-formed the extended classification DA techniques and the finetuning baseline. An implementation of our work can be found on https://github.com/JorisRoels/domain-adaptive-segmentation.},
	author = {Roels, Joris and Hennies, Julian and Saeys, Yvan and Philips, Wilfried and Kreshuk, Anna},
	keywords = {segmentation, do-main adaptation, Index Terms-Electron microscopy},
}

@article{huang_improving_2019,
	title = {Improving {Object} {Detection} with {Inverted} {Attention}},
	url = {http://arxiv.org/abs/1903.12255},
	abstract = {Improving object detectors against occlusion, blur and noise is a critical step to deploy detectors in real applications. Since it is not possible to exhaust all image defects through data collection, many researchers seek to generate hard samples in training. The generated hard samples are either images or feature maps with coarse patches dropped out in the spatial dimensions. Significant overheads are required in training the extra hard samples and/or estimating drop-out patches using extra network branches. In this paper, we improve object detectors using a highly efficient and fine-grain mechanism called Inverted Attention (IA). Different from the original detector network that only focuses on the dominant part of objects, the detector network with IA iteratively inverts attention on feature maps and puts more attention on complementary object parts, feature channels and even context. Our approach (1) operates along both the spatial and channels dimensions of the feature maps; (2) requires no extra training on hard samples, no extra network parameters for attention estimation, and no testing overheads. Experiments show that our approach consistently improved both two-stage and single-stage detectors on benchmark databases.},
	author = {Huang, Zeyi and Ke, Wei and Huang, Dong},
	month = mar,
	year = {2019},
}

@article{santurkar_how_2018,
	title = {How {Does} {Batch} {Normalization} {Help} {Optimization}?},
	url = {http://arxiv.org/abs/1805.11604},
	abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
	author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
	month = may,
	year = {2018},
}

@article{he_identity_2016,
	title = {Identity {Mappings} in {Deep} {Residual} {Networks}},
	url = {http://arxiv.org/abs/1603.05027},
	abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = mar,
	year = {2016},
}

@article{dolz_hyperdense-net_2018,
	title = {{HyperDense}-{Net}: {A} hyper-densely connected {CNN} for multi-modal image segmentation},
	url = {https://arxiv.org/abs/1804.02967},
	author = {Dolz, Jose and Gopinath, Karthik and Yuan, Jing and Lombaert, Herve and Desrosiers, Christian and Ayed, Ismail Ben},
	month = apr,
	year = {2018},
}

@article{rivenson_virtual_2019,
	title = {Virtual histological staining of unlabelled tissue-autofluorescence images via deep learning},
	url = {http://www.nature.com/articles/s41551-019-0362-y},
	doi = {10.1038/s41551-019-0362-y},
	journal = {Nature Biomedical Engineering},
	author = {Rivenson, Yair and Wang, Hongda and Wei, Zhensong and de Haan, Kevin and Zhang, Yibo and Wu, Yichen and Günaydın, Harun and Zuckerman, Jonathan E. and Chong, Thomas and Sisk, Anthony E. and Westbrook, Lindsey M. and Wallace, W. Dean and Ozcan, Aydogan},
	year = {2019},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	file = {He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:/home/zwerg/Zotero/storage/BVITW94W/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@article{fort_stiffness_2019,
	title = {Stiffness: {A} {New} {Perspective} on {Generalization} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1901.09491},
	abstract = {We investigate neural network training and generalization using the concept of stiffness. We measure how stiff a network is by looking at how a small gradient step on one example affects the loss on another example. In particular, we study how stiffness varies with 1) class membership, 2) distance between data points (in the input space as well as in latent spaces), 3) training iteration, and 4) learning rate. We empirically study the evolution of stiffness on MNIST, FASHION MNIST, CIFAR-10 and CIFAR-100 using fully-connected and convolutional neural networks. Our results demonstrate that stiffness is a useful concept for diagnosing and characterizing generalization. We observe that small learning rates lead to initial learning of more specific features that do not translate well to improvements on inputs from all classes, whereas high learning rates initially benefit all classes at once. We measure stiffness as a function of distance between data points and observe that higher learning rates induce positive correlation between changes in loss further apart, pointing towards a regularization effect of learning rate. When training on CIFAR-100, the stiffness matrix exhibits a coarse-grained behavior suggestive of the model's awareness of super-class membership.},
	author = {Fort, Stanislav and Nowak, Paweł Krzysztof and Narayanan, Srini},
	month = jan,
	year = {2019},
}

@article{huang_condensenet_2017,
	title = {{CondenseNet}: {An} {Efficient} {DenseNet} using {Learned} {Group} {Convolutions}},
	url = {http://arxiv.org/abs/1711.09224},
	abstract = {Deep neural networks are increasingly used on mobile devices, where computational resources are limited. In this paper we develop CondenseNet, a novel network architecture with unprecedented efficiency. It combines dense connectivity with a novel module called learned group convolution. The dense connectivity facilitates feature re-use in the network, whereas learned group convolutions remove connections between layers for which this feature re-use is superfluous. At test time, our model can be implemented using standard group convolutions, allowing for efficient computation in practice. Our experiments show that CondenseNets are far more efficient than state-of-the-art compact convolutional networks such as MobileNets and ShuffleNets.},
	author = {Huang, Gao and Liu, Shichen and van der Maaten, Laurens and Weinberger, Kilian Q.},
	month = nov,
	year = {2017},
}

@article{xie_aggregated_2016,
	title = {Aggregated {Residual} {Transformations} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1611.05431},
	abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
	author = {Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},
	month = nov,
	year = {2016},
}

@techreport{glorot_understanding_2010,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	url = {http://www.iro.umontreal.},
	abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random ini-tialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training , with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new ini-tialization scheme that brings substantially faster convergence.},
	author = {Glorot, Xavier and Bengio, Yoshua},
	year = {2010},
}

@article{zhang_fixup_2019,
	title = {Fixup {Initialization}: {Residual} {Learning} {Without} {Normalization}},
	url = {http://arxiv.org/abs/1901.09321},
	abstract = {Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic. In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization. Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization. We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers. Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation.},
	author = {Zhang, Hongyi and Dauphin, Yann N. and Ma, Tengyu},
	month = jan,
	year = {2019},
	file = {Full Text:/home/zwerg/Zotero/storage/ES2TMS7V/Zhang et al. - 2019 - Fixup Initialization Residual Learning Without Normalization.pdf:application/pdf},
}

@article{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	url = {http://arxiv.org/abs/1502.01852},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015},
	file = {Full Text:/home/zwerg/Zotero/storage/DUIYHQE4/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification.pdf:application/pdf},
}

@article{he_bag_2018,
	title = {Bag of {Tricks} for {Image} {Classification} with {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1812.01187},
	abstract = {Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3\% to 79.29\% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.},
	author = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
	month = dec,
	year = {2018},
	file = {Full Text:/home/zwerg/Zotero/storage/3W6J9K36/He et al. - 2018 - Bag of Tricks for Image Classification with Convol.pdf:application/pdf},
}

@article{smith_ultra-fast_2019,
	title = {Ultra-fast fit-free analysis of complex fluorescence lifetime imaging via deep learning},
	author = {Smith, Jason T and Yao, Ruoyang and Sinsuebphon, Nattawut and Rudkouskaya, Alena and Barroso, Margarida and Yan, Pingkun and Intes, Xavier},
	year = {2019},
}

@incollection{d_sculley_summary_2018,
	title = {Summary for {Policymakers}},
	isbn = {0-262-01709-1 978-0-262-01709-1},
	url = {https://www.cambridge.org/core/product/identifier/CBO9781107415324A009/type/book_part},
	abstract = {Machine learning offers a fantastically powerful toolkit for building useful com-plex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.},
	booktitle = {Climate {Change} 2013 - {The} {Physical} {Science} {Basis}},
	author = {D. Sculley, Daniel Golovin, Eugene Davydov, Todd Phillips, Gary Holt},
	year = {2018},
	doi = {10.1017/CBO9781107415324.004},
	pages = {1--30},
}

@article{erhan_scalable_2013,
	title = {Scalable {Object} {Detection} using {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1312.2249},
	abstract = {Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.},
	author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir},
	month = dec,
	year = {2013},
}

@article{trinh_learning_2018,
	title = {Learning {Longer}-term {Dependencies} in {RNNs} with {Auxiliary} {Losses}},
	url = {http://arxiv.org/abs/1803.00144},
	abstract = {Despite recent advances in training recurrent neural networks (RNNs), capturing long-term dependencies in sequences remains a fundamental challenge. Most approaches use backpropagation through time (BPTT), which is difficult to scale to very long sequences. This paper proposes a simple method that improves the ability to capture long term dependencies in RNNs by adding an unsupervised auxiliary loss to the original objective. This auxiliary loss forces RNNs to either reconstruct previous events or predict next events in a sequence, making truncated backpropagation feasible for long sequences and also improving full BPTT. We evaluate our method on a variety of settings, including pixel-by-pixel image classification with sequence lengths up to 16{\textbackslash},000, and a real document classification benchmark. Our results highlight good performance and resource efficiency of this approach over competitive baselines, including other recurrent models and a comparable sized Transformer. Further analyses reveal beneficial effects of the auxiliary loss on optimization and regularization, as well as extreme cases where there is little to no backpropagation.},
	author = {Trinh, Trieu H. and Dai, Andrew M. and Luong, Minh-Thang and Le, Quoc V.},
	month = feb,
	year = {2018},
}

@article{dai_transformer-xl_2019,
	title = {Transformer-{XL}: {Attentive} {Language} {Models} {Beyond} a {Fixed}-{Length} {Context}},
	url = {http://arxiv.org/abs/1901.02860},
	abstract = {Transformer networks have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. As a solution, we propose a novel neural architecture, Transformer-XL, that enables Transformer to learn dependency beyond a fixed length without disrupting temporal coherence. Concretely, it consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the problem of context fragmentation. As a result, Transformer-XL learns dependency that is about 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformer during evaluation. Additionally, we improve the state-of-the-art (SoTA) results of bpc/perplexity from 1.06 to 0.99 on enwiki8, from 1.13 to 1.08 on text8, from 20.5 to 18.3 on WikiText-103, from 23.7 to 21.8 on One Billion Word, and from 55.3 to 54.5 on Penn Treebank (without finetuning). Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
	month = jan,
	year = {2019},
}

@article{haldar_applying_2018,
	title = {Applying {Deep} {Learning} {To} {Airbnb} {Search}},
	url = {http://arxiv.org/abs/1810.09591},
	abstract = {The application to search ranking is one of the biggest machine learning success stories at Airbnb. Much of the initial gains were driven by a gradient boosted decision tree model. The gains, however, plateaued over time. This paper discusses the work done in applying neural networks in an attempt to break out of that plateau. We present our perspective not with the intention of pushing the frontier of new modeling techniques. Instead, ours is a story of the elements we found useful in applying neural networks to a real life product. Deep learning was steep learning for us. To other teams embarking on similar journeys, we hope an account of our struggles and triumphs will provide some useful pointers. Bon voyage!},
	author = {Haldar, Malay and Abdool, Mustafa and Ramanathan, Prashant and Xu, Tao and Yang, Shulin and Duan, Huizhong and Zhang, Qing and Barrow-Williams, Nick and Turnbull, Bradley C. and Collins, Brendan M. and Legrand, Thomas},
	month = oct,
	year = {2018},
}

@article{mengu_analysis_2018,
	title = {Analysis of {Diffractive} {Optical} {Neural} {Networks} and {Their} {Integration} with {Electronic} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1810.01916},
	abstract = {Optical machine learning offers advantages in terms of power efficiency, scalability and computation speed. Recently, an optical machine learning method based on Diffractive Deep Neural Networks (D2NNs) has been introduced to execute a function as the input light diffracts through passive layers, designed by deep learning using a computer. Here we introduce improvements to D2NNs by changing the training loss function and reducing the impact of vanishing gradients in the error back-propagation step. Using five phase-only diffractive layers, we numerically achieved a classification accuracy of 97.18\% and 89.13\% for optical recognition of handwritten digits and fashion products, respectively; using both phase and amplitude modulation (complex-valued) at each layer, our inference performance improved to 97.81\% and 89.32\%, respectively. Furthermore, we report the integration of D2NNs with electronic neural networks to create hybrid classifiers that significantly reduce the number of input pixels into an electronic network using an ultra-compact front-end D2NN with a layer-to-layer distance of a few wavelengths, also reducing the complexity of the successive electronic network. Using a 5-layer phase-only D2NN jointly-optimized with a single fully-connected electronic layer, we achieved a classification accuracy of 98.71\% and 90.04\% for the recognition of handwritten digits and fashion products, respectively. Moreover, the input to the electronic network was compressed by {\textgreater}7.8 times down to 10x10 pixels, reducing the memory and computational power demand of the electronic part of the jointly-optimized hybrid classification system. Beyond creating low-power and high-frame rate ubiquitous machine learning platforms, such D2NN-based hybrid neural networks will find applications in smart optical imager and sensor design.},
	author = {Mengu, Deniz and Luo, Yi and Rivenson, Yair and Ozcan, Aydogan},
	month = oct,
	year = {2018},
}

@article{jayram_learning_2018,
	title = {Learning to {Remember}, {Forget} and {Ignore} using {Attention} {Control} in {Memory}},
	url = {http://arxiv.org/abs/1809.11087},
	abstract = {Typical neural networks with external memory do not effectively separate capacity for episodic and working memory as is required for reasoning in humans. Applying knowledge gained from psychological studies, we designed a new model called Differentiable Working Memory (DWM) in order to specifically emulate human working memory. As it shows the same functional characteristics as working memory, it robustly learns psychology inspired tasks and converges faster than comparable state-of-the-art models. Moreover, the DWM model successfully generalizes to sequences two orders of magnitude longer than the ones used in training. Our in-depth analysis shows that the behavior of DWM is interpretable and that it learns to have fine control over memory, allowing it to retain, ignore or forget information based on its relevance.},
	author = {Jayram, T. S. and Bouhadjar, Younes and McAvoy, Ryan L. and Kornuta, Tomasz and Asseman, Alexis and Rocki, Kamil and Ozcan, Ahmet S.},
	month = sep,
	year = {2018},
}

@article{mengu_response_2018,
	title = {Response to {Comment} on "{All}-optical machine learning using diffractive deep neural networks"},
	url = {http://arxiv.org/abs/1810.04384},
	abstract = {In their Comment, Wei et al. (arXiv:1809.08360v1 [cs.LG]) claim that our original interpretation of Diffractive Deep Neural Networks (D2NN) represent a mischaracterization of the system due to linearity and passivity. In this Response, we detail how this mischaracterization claim is unwarranted and oblivious to several sections detailed in our original manuscript (Science, DOI: 10.1126/science.aat8084) that specifically introduced and discussed optical nonlinearities and reconfigurability of D2NNs, as part of our proposed framework to enhance its performance. To further refute the mischaracterization claim of Wei et al., we, once again, demonstrate the depth feature of optical D2NNs by showing that multiple diffractive layers operating collectively within a D2NN present additional degrees-of-freedom compared to a single diffractive layer to achieve better classification accuracy, as well as improved output signal contrast and diffraction efficiency as the number of diffractive layers increase, showing the deepness of a D2NN, and its inherent depth advantage for improved performance. In summary, the Comment by Wei et al. does not provide an amendment to the original teachings of our original manuscript, and all of our results, core conclusions and methodology of research reported in Science (DOI: 10.1126/science.aat8084) remain entirely valid.},
	author = {Mengu, Deniz and Luo, Yi and Rivenson, Yair and Lin, Xing and Veli, Muhammed and Ozcan, Aydogan},
	month = oct,
	year = {2018},
}

@article{liu_deep_2018,
	title = {Deep learning-based super-resolution in coherent imaging systems},
	url = {http://arxiv.org/abs/1810.06611},
	abstract = {We present a deep learning framework based on a generative adversarial network (GAN) to perform super-resolution in coherent imaging systems. We demonstrate that this framework can enhance the resolution of both pixel size-limited and diffraction-limited coherent imaging systems. We experimentally validated the capabilities of this deep learning-based coherent imaging approach by super-resolving complex images acquired using a lensfree on-chip holographic microscope, the resolution of which was pixel size-limited. Using the same GAN-based approach, we also improved the resolution of a lens-based holographic imaging system that was limited in resolution by the numerical aperture of its objective lens. This deep learning-based super-resolution framework can be broadly applied to enhance the space-bandwidth product of coherent imaging systems using image data and convolutional neural networks, and provides a rapid, non-iterative method for solving inverse image reconstruction or enhancement problems in optics.},
	author = {Liu, Tairan and de Haan, Kevin and Rivenson, Yair and Wei, Zhensong and Zeng, Xin and Zhang, Yibo and Ozcan, Aydogan},
	month = oct,
	year = {2018},
}

@article{de_haan_resolution_2019,
	title = {Resolution enhancement in scanning electron microscopy using deep learning},
	url = {http://arxiv.org/abs/1901.11094},
	abstract = {We report resolution enhancement in scanning electron microscopy (SEM) images using a generative adversarial network. We demonstrate the veracity of this deep learning-based super-resolution technique by inferring unresolved features in low-resolution SEM images and comparing them with the accurately co-registered high-resolution SEM images of the same samples. Through spatial frequency analysis, we also report that our method generates images with frequency spectra matching higher resolution SEM images of the same fields-of-view. By using this technique, higher resolution SEM images can be taken faster, while also reducing both electron charging and damage to the samples.},
	author = {de Haan, Kevin and Ballard, Zachary S. and Rivenson, Yair and Wu, Yichen and Ozcan, Aydogan},
	month = jan,
	year = {2019},
}

@article{wu_three-dimensional_2019,
	title = {Three-dimensional propagation and time-reversal of fluorescence images},
	url = {http://arxiv.org/abs/1901.11252},
	abstract = {Unlike holography, fluorescence microscopy lacks an image propagation and time-reversal framework, which necessitates scanning of fluorescent objects to obtain 3D images. We demonstrate that a neural network can inherently learn the physical laws governing fluorescence wave propagation and time-reversal to enable 3D imaging of fluorescent samples using a single 2D image, without mechanical scanning, additional hardware, or a trade-off of resolution or speed. Using this data-driven framework, we increased the depth-of-field of a microscope by 20-fold, imaged Caenorhabditis elegans neurons in 3D using a single fluorescence image, and digitally propagated fluorescence images onto user-defined 3D surfaces, also correcting various aberrations. Furthermore, this learning-based approach cross-connects different imaging modalities, permitting 3D propagation of a wide-field fluorescence image to match confocal microscopy images acquired at different sample planes.},
	author = {Wu, Yichen and Rivenson, Yair and Wang, Hongda and Luo, Yilin and Ben-David, Eyal and Ozcan, Aydogan},
	month = jan,
	year = {2019},
}

@article{malacrida_multidimensional_nodate,
	title = {A multidimensional phasor approach reveals {LAURDAN} photophysics in {NIH}-{3T3} cell membranes},
	issn = {4159801708564},
	url = {www.nature.com/scientificreports},
	doi = {10.1038/s41598-017-08564-z},
	abstract = {Mammalian cell membranes have different phospholipid composition and cholesterol content, displaying a profile of fluidity that depends on their intracellular location. Among the dyes used in membrane studies, LAURDAN has the advantage to be sensitive to the lipid composition as well as to membrane fluidity. The LAURDAN spectrum is sensitive to the lipid composition and dipolar relaxation arising from water penetration, but disentangling lipid composition from membrane fluidity can be obtained if time resolved spectra could be measured at each cell location. Here we describe a method in which spectral and lifetime information obtained in different measurements at the same plane in a cell are used in the phasor plot providing a solution to analyze multiple lifetime or spectral data through a common visualization approach. We exploit a property of phasor plots based on the reciprocal role of the phasor plot and the image. In the phasor analysis each pixel of the image is associated with a phasor and each phasor maps to pixels and features in the image. In this paper the lifetime and spectral fluorescence data are used simultaneously to determine the contribution of polarity and dipolar relaxations of LAURDAN in each pixel of an image. With the advances in confocal and camera based microscopy in the last two decades, lifetime and spectral information are becoming common tools to study in vivo complex photophysical processes with high spatial and temporal resolution. The complementarity between these two observations, if measured together, could provide a deeper description of molecular interactions, metabolic profiles and membrane organization, among other properties 1, 2. Analysis of these large and complex data sets from FLIM and hyperspectral measurements extensively use global analysis 3 and deconvolution 4. Currently, analyses of data from these complementary techniques are performed independently, which provide fragmented interpretation and could preclude a more basic description of the underlying photophysics. The phasor plot approach is a model-free method to analyze and interpret lifetime and spectral resolved information from microscopy and cuvette experiments. Based on the mathematical derivation originally described by Weber at the early 1980s 5 , Jameson et al. 6 introduced the phasor plot representation. Later Digman et al. further developed the phasor approach for fluorescence-lifetime imaging microscopy (FLIM) data 7. More recently, Fereidouni et al. used the phasor approach for resolving fluorescence spectral components, specifically to unmix fluorescence signals from multiple dye staining and to interpret the data of FRET experiments without previous knowledge about the system under study 8-10. Since these developments, phasor plots were applied in the biological field to analyze complex fluorescence processes 11-16. The combination of lifetime and spectral information in a single analysis (so-called spectral resolved FLIM, sFLIM) was previously proposed to analyze processes like FRET 17 , demixing multiple dye staining 18 or for auto-fluorescence interpretation 19. In these reports complex instrumentation and/or analysis were developed to obtain Published: xx xx xxxx OPEN},
	author = {Malacrida, Leonel and Jameson, David M and Gratton, Enrico},
}

@article{malacrida_multidimensional_2017,
	title = {A multidimensional phasor approach reveals {LAURDAN} photophysics in {NIH}-{3T3} cell membranes.},
	volume = {7},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/28835608},
	doi = {10.1038/s41598-017-08564-z},
	abstract = {Mammalian cell membranes have different phospholipid composition and cholesterol content, displaying a profile of fluidity that depends on their intracellular location. Among the dyes used in membrane studies, LAURDAN has the advantage to be sensitive to the lipid composition as well as to membrane fluidity. The LAURDAN spectrum is sensitive to the lipid composition and dipolar relaxation arising from water penetration, but disentangling lipid composition from membrane fluidity can be obtained if time resolved spectra could be measured at each cell location. Here we describe a method in which spectral and lifetime information obtained in different measurements at the same plane in a cell are used in the phasor plot providing a solution to analyze multiple lifetime or spectral data through a common visualization approach. We exploit a property of phasor plots based on the reciprocal role of the phasor plot and the image. In the phasor analysis each pixel of the image is associated with a phasor and each phasor maps to pixels and features in the image. In this paper the lifetime and spectral fluorescence data are used simultaneously to determine the contribution of polarity and dipolar relaxations of LAURDAN in each pixel of an image.},
	number = {1},
	journal = {Scientific reports},
	author = {Malacrida, Leonel and Jameson, David M and Gratton, Enrico},
	month = aug,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	pages = {9215--9215},
}

@article{wu_deep_2018,
	title = {Deep learning enables high-throughput analysis of particle-aggregation-based bio-sensors imaged using holography},
	url = {http://arxiv.org/abs/1810.10184},
	doi = {10.1021/acsphotonics.8b01479},
	abstract = {Aggregation-based assays, using micro- and nano-particles have been widely accepted as an efficient and cost-effective bio-sensing tool, particularly in microbiology, where particle clustering events are used as a metric to infer the presence of a specific target analyte and quantify its concentration. Here, we present a sensitive and automated readout method for aggregation-based assays using a wide-field lens-free on-chip microscope, with the ability to rapidly analyze and quantify microscopic particle aggregation events in 3D, using deep learning-based holographic image reconstruction. In this method, the computation time for hologram reconstruction and particle autofocusing steps remains constant, regardless of the number of particles/clusters within the 3D sample volume, which provides a major throughput advantage, brought by deep learning-based image reconstruction. As a proof of concept, we demonstrate rapid detection of herpes simplex virus (HSV) by monitoring the clustering of antibody-coated micro-particles, achieving a detection limit of {\textasciitilde}5 viral copies per micro-liter (i.e., {\textasciitilde}25 copies per test).},
	author = {Wu, Yichen and Ray, Aniruddha and Wei, Qingshan and Feizi, Alborz and Tong, Xin and Chen, Eva and Luo, Yi and Ozcan, Aydogan},
	month = oct,
	year = {2018},
}

@article{wu_cross-modality_2018,
	title = {Cross-modality deep learning brings bright-field microscopy contrast to holography},
	url = {http://arxiv.org/abs/1811.07103},
	abstract = {Deep learning brings bright-field microscopy contrast to holographic images of a sample volume, bridging the volumetric imaging capability of holography with the speckle- and artifact-free image contrast of bright-field incoherent microscopy.},
	author = {Wu, Yichen and Luo, Yilin and Chaudhari, Gunvant and Rivenson, Yair and Calis, Ayfer and De Haan, Kevin and Ozcan, Aydogan},
	month = nov,
	year = {2018},
}

@article{rivenson_toward_2018,
	title = {Toward a {Thinking} {Microscope}: {Deep} {Learning} in {Optical} {Microscopy} and {Image} {Reconstruction}},
	url = {http://arxiv.org/abs/1805.08970},
	doi = {10.1364/OPN.29.7.000034},
	abstract = {We discuss recently emerging applications of the state-of-art deep learning methods on optical microscopy and microscopic image reconstruction, which enable new transformations among different modes and modalities of microscopic imaging, driven entirely by image data. We believe that deep learning will fundamentally change both the hardware and image reconstruction methods used in optical microscopy in a holistic manner.},
	author = {Rivenson, Yair and Ozcan, Aydogan},
	month = may,
	year = {2018},
	file = {Full Text:/home/zwerg/Zotero/storage/V2DI3GSS/Rivenson and Ozcan - 2018 - Toward a Thinking Microscope Deep Learning in Opt.pdf:application/pdf},
}

@techreport{centonze_multispectral_2012,
	title = {Multispectral and hyperspectral imaging ;(110.2960) {Image} analysis},
	url = {https://www.osapublishing.org/DirectPDFAccess/D7DBF176-06F8-0298-FD2EADA4CDEB9441_233521/oe-20-12-12729.pdf?da=1&id=233521&seq=0&mobile=no},
	abstract = {A new global analysis algorithm to analyse (hyper-) spectral images is presented. It is based on the phasor representation that has been demonstrated to be very powerful for the analysis of lifetime imaging data. In spectral phasor analysis the fluorescence spectrum of each pixel in the image is Fourier transformed. Next, the real and imaginary components of the first harmonic of the transform are employed as X and Y coordinates in a scatter (spectral phasor) plot. Importantly, the spectral phasor representation allows for rapid (real time) semi-blind spectral unmixing of up to three components in the image. This is demonstrated on slides with fixed cells containing three fluorescent labels. In addition the method is used to analyse autofluorescence of cells in a fresh grass blade. It is shown that the spectral phasor approach is compatible with spectral imaging data recorded with a low number of spectral channels.},
	author = {Centonze, V E and Sun, M and Masuda, A and Gerritsen, H C and Herman, B},
	year = {2012},
	note = {Issue: 2
Volume: 9},
	pages = {542--560},
}

@techreport{guha_roy_recalibrating_2018,
	title = {Recalibrating {Fully} {Convolutional} {Networks} with {Spatial} and {Channel} '{Squeeze} \& {Excitation}' {Blocks}},
	url = {https://arxiv.org/pdf/1808.08127.pdf},
	abstract = {In a wide range of semantic segmentation tasks, fully convolutional neural networks (F-CNNs) have been successfully leveraged to achieve state-of-the-art performance. Architectural innovations of F-CNNs have mainly been on improving spatial encoding or network connectivity to aid gradient flow. In this article, we aim towards an alternate direction of recalibrating the learned feature maps adaptively; boosting meaningful features while suppressing weak ones. The recalibration is achieved by simple computational blocks that can be easily integrated in F-CNNs architectures. We draw our inspiration from the recently proposed 'squeeze \& excitation' (SE) modules for channel recalibration for image classification. Towards this end, we introduce three variants of SE modules for segmentation, (i) squeezing spatially and exciting channel-wise, (ii) squeezing channel-wise and exciting spatially and (iii) joint spatial and channel squeeze \& excitation. We effectively incorporate the proposed SE blocks in three state-of-the-art F-CNNs and demonstrate a consistent improvement of segmentation accuracy on three challenging benchmark datasets. Importantly, SE blocks only lead to a minimal increase in model complexity of about 1.5\%, while the Dice score increases by 4-9\% in the case of U-Net. Hence, we believe that SE blocks can be an integral part of future F-CNN architectures.},
	author = {Guha Roy, Abhijit and Navab, Nassir and Wachinger, Christian},
	year = {2018},
	note = {Volume: 1},
	keywords = {image seg-mentation, Index Terms-Fully convolutional networks, squeeze \& excitation},
}

@techreport{ramesh_spectral_nodate,
	title = {A {Spectral} {Regularizer} for {Unsupervised} {Disentanglement}},
	url = {https://drive.google.com/open?id=},
	abstract = {A generative model with a disentangled representation allows for control over independent aspects of the output. Learning disentangled representations has been a recent topic of great interest, but it remains poorly understood. We show that even for GANs that do not possess disentangled representations , one can find curved trajectories in latent space over which local disentanglement occurs. These trajectories are found by iteratively following the leading right-singular vectors of the Jacobian of the generator with respect to its input. Based on this insight, we describe an efficient regularizer that aligns these vectors with the coordinate axes, and show that it can be used to induce disentangled representations in GANs, in a completely unsupervised manner.},
	author = {Ramesh, Aditya and Choi, Youngduck and Lecun, Yann},
}

@article{shi_convolutional_2015,
	title = {Convolutional {LSTM} {Network}: {A} {Machine} {Learning} {Approach} for {Precipitation} {Nowcasting}},
	issn = {1884918859},
	doi = {10.1074/jbc.M200827200},
	abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
	author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
	year = {2015},
}

@article{acuna_efficient_2018,
	title = {Efficient {Interactive} {Annotation} of {Segmentation} {Datasets} with {Polygon}-{RNN}++},
	url = {http://arxiv.org/abs/1803.09693},
	abstract = {Manually labeling datasets with object masks is extremely time consuming. In this work, we follow the idea of Polygon-RNN to produce polygonal annotations of objects interactively using humans-in-the-loop. We introduce several important improvements to the model: 1) we design a new CNN encoder architecture, 2) show how to effectively train the model with Reinforcement Learning, and 3) significantly increase the output resolution using a Graph Neural Network, allowing the model to accurately annotate high-resolution objects in images. Extensive evaluation on the Cityscapes dataset shows that our model, which we refer to as Polygon-RNN++, significantly outperforms the original model in both automatic (10\% absolute and 16\% relative improvement in mean IoU) and interactive modes (requiring 50\% fewer clicks by annotators). We further analyze the cross-domain scenario in which our model is trained on one dataset, and used out of the box on datasets from varying domains. The results show that Polygon-RNN++ exhibits powerful generalization capabilities, achieving significant improvements over existing pixel-wise methods. Using simple online fine-tuning we further achieve a high reduction in annotation time for new datasets, moving a step closer towards an interactive annotation tool to be used in practice.},
	author = {Acuna, David and Ling, Huan and Kar, Amlan and Fidler, Sanja},
	month = mar,
	year = {2018},
}

@article{acuna_efficient_2018-1,
	title = {Efficient {Interactive} {Annotation} of {Segmentation} {Datasets} with {Polygon}-{RNN}++},
	url = {http://arxiv.org/abs/1803.09693},
	abstract = {Manually labeling datasets with object masks is extremely time consuming. In this work, we follow the idea of Polygon-RNN to produce polygonal annotations of objects interactively using humans-in-the-loop. We introduce several important improvements to the model: 1) we design a new CNN encoder architecture, 2) show how to effectively train the model with Reinforcement Learning, and 3) significantly increase the output resolution using a Graph Neural Network, allowing the model to accurately annotate high-resolution objects in images. Extensive evaluation on the Cityscapes dataset shows that our model, which we refer to as Polygon-RNN++, significantly outperforms the original model in both automatic (10\% absolute and 16\% relative improvement in mean IoU) and interactive modes (requiring 50\% fewer clicks by annotators). We further analyze the cross-domain scenario in which our model is trained on one dataset, and used out of the box on datasets from varying domains. The results show that Polygon-RNN++ exhibits powerful generalization capabilities, achieving significant improvements over existing pixel-wise methods. Using simple online fine-tuning we further achieve a high reduction in annotation time for new datasets, moving a step closer towards an interactive annotation tool to be used in practice.},
	author = {Acuna, David and Ling, Huan and Kar, Amlan and Fidler, Sanja},
	year = {2018},
}

@article{pezzotti_linear_2018,
	title = {Linear {tSNE} optimization for the {Web}},
	url = {http://arxiv.org/abs/1805.10817},
	abstract = {The t-distributed Stochastic Neighbor Embedding (tSNE) algorithm has become in recent years one of the most used and insightful techniques for the exploratory data analysis of high-dimensional data. tSNE reveals clusters of high-dimensional data points at different scales while it requires only minimal tuning of its parameters. Despite these advantages, the computational complexity of the algorithm limits its application to relatively small datasets. To address this problem, several evolutions of tSNE have been developed in recent years, mainly focusing on the scalability of the similarity computations between data points. However, these contributions are insufficient to achieve interactive rates when visualizing the evolution of the tSNE embedding for large datasets. In this work, we present a novel approach to the minimization of the tSNE objective function that heavily relies on modern graphics hardware and has linear computational complexity. Our technique does not only beat the state of the art, but can even be executed on the client side in a browser. We propose to approximate the repulsion forces between data points using adaptive-resolution textures that are drawn at every iteration with WebGL. This approximation allows us to reformulate the tSNE minimization problem as a series of tensor operation that are computed with TensorFlow.js, a JavaScript library for scalable tensor computations.},
	author = {Pezzotti, Nicola and Mordvintsev, Alexander and Hollt, Thomas and Lelieveldt, Boudewijn P. F. and Eisemann, Elmar and Vilanova, Anna},
	month = may,
	year = {2018},
}

@inproceedings{castrejon_annotating_2017,
	title = {Annotating object instances with a polygon-{RNN}},
	volume = {2017-Janua},
	isbn = {978-1-5386-0457-1},
	doi = {10.1109/CVPR.2017.477},
	abstract = {We propose an approach for semi-automatic annotation of object instances. While most current methods treat object segmentation as a pixel-labeling problem, we here cast it as a polygon prediction task, mimicking how most current datasets have been annotated. In particular, our approach takes as input an image crop and sequentially produces vertices of the polygon outlining the object. This allows a human annotator to interfere at any time and correct a vertex if needed, producing as accurate segmentation as desired by the annotator. We show that our approach speeds up the annotation process by a factor of 4.7 across all classes in Cityscapes, while achieving 78.4\% agreement in IoU with original ground-truth, matching the typical agreement between human annotators. For cars, our speed-up factor is 7.3 for an agreement of 82.2\%. We further show generalization capabilities of our approach to unseen datasets.},
	author = {Castrejón, Lluís and Kundu, Kaustav and Urtasun, Raquel and Fidler, Sanja},
	year = {2017},
	pages = {4485--4493},
}

@article{dai_transformer-xl_2019-1,
	title = {Transformer-{XL}: {Attentive} {Language} {Models} {Beyond} a {Fixed}-{Length} {Context}},
	url = {http://arxiv.org/abs/1901.02860},
	abstract = {Transformer networks have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. As a solution, we propose a novel neural architecture, Transformer-XL, that enables Transformer to learn dependency beyond a fixed length without disrupting temporal coherence. Concretely, it consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the problem of context fragmentation. As a result, Transformer-XL learns dependency that is about 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformer during evaluation. Additionally, we improve the state-of-the-art (SoTA) results of bpc/perplexity from 1.06 to 0.99 on enwiki8, from 1.13 to 1.08 on text8, from 20.5 to 18.3 on WikiText-103, from 23.7 to 21.8 on One Billion Word, and from 55.3 to 54.5 on Penn Treebank (without finetuning). Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
	year = {2019},
}

@article{hafner_learning_2018-1,
	title = {Learning {Latent} {Dynamics} for {Planning} from {Pixels}},
	abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from pixels and chooses actions through online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this problem using a latent dynamics model with both deterministic and stochastic transition function and a generalized variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards. PlaNet uses significantly fewer episodes and reaches final performance close to and sometimes higher than top model-free algorithms.},
	author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
	year = {2018},
}

@article{rupp_fast_2012,
	title = {Fast and {Accurate} {Modeling} of {Molecular} {Atomization} {Energies} with {Machine} {Learning}},
	volume = {108},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.108.058301},
	doi = {10.1103/PHYSREVLETT.108.058301},
	number = {5},
	journal = {Physical Review Letters},
	author = {Rupp, Matthias and Tkatchenko, Alexandre and Müller, Klaus-Robert and von Lilienfeld, O. Anatole},
	year = {2012},
	pages = {058301--058301},
}

@article{noauthor_kunstliche_2002,
	title = {Künstliche {Nasen}},
	year = {2002},
}

@article{krull_noise2void_2018-1,
	title = {{Noise2Void} - {Learning} {Denoising} from {Single} {Noisy} {Images}},
	url = {http://arxiv.org/abs/1811.10980},
	abstract = {The field of image denoising is currently dominated by discriminative deep learning methods that are trained on pairs of noisy input and clean target images. Recently it has been shown that such methods can also be trained without clean targets. Instead, independent pairs of noisy images can be used, in an approach known as Noise2Noise (N2N). Here, we introduce Noise2Void (N2V), a training scheme that takes this idea one step further. It does not require noisy image pairs, nor clean target images. Consequently, N2V allows us to train directly on the body of data to be denoised and can therefore be applied when other methods cannot. Especially interesting is the application to biomedical image data, where the acquisition of training targets, clean or noisy, is frequently not possible. We compare the performance of N2V to approaches that have either clean target images and/or noisy image pairs available. Intuitively, N2V cannot be expected to outperform methods that have more information available during training. Still, we observe that the denoising performance of Noise2Void drops in moderation and compares favorably to training-free denoising methods.},
	author = {Krull, Alexander and Buchholz, Tim-Oliver and Jug, Florian},
	year = {2018},
}

@article{brendel_aproximating_2019,
	title = {Aproximating {CNNs} with {Bag}-{Of}-{Local}-{Features} {Models} {Works} {Surprisingly} {Well}},
	author = {Brendel, Wieland and Bethge, Matthias},
	year = {2019},
}

@article{gefen_human-level_2014,
	title = {Human-level protein localization with convolutional neural networks},
	number = {JANUARY},
	author = {Gefen, David},
	year = {2014},
	pages = {600--605},
}

@article{jetley_earn_2018,
	title = {Earn to},
	abstract = {We propose an end-to-end-trainable attention module for convolutional neural net-work (CNN) architectures built for image classification. The module takes as in-put the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D ma-trix of scores for each map. Standard CNN architectures are modified through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parameterised by the score matrices, must alone be used for classification. Incentivised to amplify the rel-evant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classification, demon-strating superior generalisation over 6 unseen benchmark datasets. When bina-rised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.},
	author = {Jetley, Saumya and Lord, Nicholas A and Lee, Namhoon and Torr, Philip H S},
	year = {2018},
	pages = {1--14},
}

@article{batson_noise2self_2019-1,
	title = {{Noise2Self}: {Blind} {Denoising} by {Self}-{Supervision}},
	url = {http://arxiv.org/abs/1901.11365},
	abstract = {We propose a general framework for denoising high-dimensional measurements which requires no prior on the signal, no estimate of the noise, and no clean training data. The only assumption is that the noise exhibits statistical independence across different dimensions of the measurement. Moreover, our framework is not restricted to a particular denoising model. We show how it can be used to calibrate any parameterised denoising algorithm, from the single hyperparameter of a median filter to the millions of weights of a deep neural network. We demonstrate this on natural image and microscopy data, where we exploit noise independence between pixels, and on single-cell gene expression data, where we exploit independence between detections of individual molecules. Finally, we prove a theoretical lower bound on the performance of an optimal denoiser. This framework generalizes recent work on training neural nets from noisy images and on cross-validation for matrix factorization.},
	author = {Batson, Joshua and Royer, Loic},
	year = {2019},
}

@article{batson_noise2self_2019-2,
	title = {{Noise2Self}: {Blind} {Denoising} by {Self}-{Supervision}},
	url = {http://arxiv.org/abs/1901.11365},
	abstract = {We propose a general framework for denoising high-dimensional measurements which requires no prior on the signal, no estimate of the noise, and no clean training data. The only assumption is that the noise exhibits statistical independence across different dimensions of the measurement. Moreover, our framework is not restricted to a particular denoising model. We show how it can be used to calibrate any parameterised denoising algorithm, from the single hyperparameter of a median filter to the millions of weights of a deep neural network. We demonstrate this on natural image and microscopy data, where we exploit noise independence between pixels, and on single-cell gene expression data, where we exploit independence between detections of individual molecules. Finally, we prove a theoretical lower bound on the performance of an optimal denoiser. This framework generalizes recent work on training neural nets from noisy images and on cross-validation for matrix factorization.},
	author = {Batson, Joshua and Royer, Loic},
	year = {2019},
}

@article{cubuk_autoaugment_2018,
	title = {{AutoAugment}: {Learning} {Augmentation} {Policies} from {Data}},
	url = {https://www.semanticscholar.org/paper/AutoAugment%3A-Learning-Augmentation-Policies-from-Cubuk-Zoph/ba483d0f057e42d2a154f711f202a50bb38d938e},
	journal = {undefined},
	author = {Cubuk, Ekin Dogus and Zoph, Barret and Mané, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
	year = {2018},
}

@article{schneider_deep_2018,
	title = {Deep {Learning} {Object} {Detection} {Methods} for {Ecological} {Camera} {Trap} {Data}},
	url = {http://arxiv.org/abs/1803.10842},
	abstract = {Deep learning methods for computer vision tasks show promise for automating the data analysis of camera trap images. Ecological camera traps are a common approach for monitoring an ecosystem's animal population, as they provide continual insight into an environment without being intrusive. However, the analysis of camera trap images is expensive, labour intensive, and time consuming. Recent advances in the field of deep learning for object detection show promise towards automating the analysis of camera trap images. Here, we demonstrate their capabilities by training and comparing two deep learning object detection classifiers, Faster R-CNN and YOLO v2.0, to identify, quantify, and localize animal species within camera trap images using the Reconyx Camera Trap and the self-labeled Gold Standard Snapshot Serengeti data sets. When trained on large labeled datasets, object recognition methods have shown success. We demonstrate their use, in the context of realistically sized ecological data sets, by testing if object detection methods are applicable for ecological research scenarios when utilizing transfer learning. Faster R-CNN outperformed YOLO v2.0 with average accuracies of 93.0{\textbackslash}\% and 76.7{\textbackslash}\% on the two data sets, respectively. Our findings show promising steps towards the automation of the labourious task of labeling camera trap images, which can be used to improve our understanding of the population dynamics of ecosystems across the planet.},
	author = {Schneider, Stefan and Taylor, Graham W. and Kremer, Stefan C.},
	year = {2018},
}

@article{l_yang_attention_2017,
	title = {Attention {Inspiring} {Receptive}-{Fields} {Network} for {Learning} {Invariant} {Representations}},
	volume = {91},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {L. Yang, Y. Wu, M. Hu, Q. Song},
	year = {2017},
	pages = {399--404},
}

@article{brendel_pproximating_2019,
	title = {A {PPROXIMATING} {CNN} {S} {WITH} {B} {AG} - {OF} - {LOCAL} - {F} {EATURES} {MODELS} {WORKS} {SURPRISINGLY} {WELL}},
	author = {Brendel, Wieland and Bethge, Matthias},
	year = {2019},
	pages = {1--15},
}

@article{kornblith_better_2018-1,
	title = {Do {Better} {ImageNet} {Models} {Transfer} {Better}?},
	url = {http://arxiv.org/abs/1805.08974},
	abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classification networks on 12 image classification datasets. We find that, when networks are used as fixed feature extractors or fine-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (\$r = 0.99\$ and \$0.96\$, respectively). In the former setting, we find that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we find that, on two small fine-grained image classification datasets, pretraining on ImageNet provides minimal benefits, indicating the learned features from ImageNet do not transfer well to fine-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.},
	author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
	month = may,
	year = {2018},
}

@article{fonseca_simple_2018-1,
	title = {A {Simple} {Fusion} of {Deep} and {Shallow} {Learning} for {Acoustic} {Scene} {Classification}},
	url = {http://arxiv.org/abs/1806.07506},
	abstract = {In the past, Acoustic Scene Classification systems have been based on hand crafting audio features that are input to a classifier. Nowadays, the common trend is to adopt data driven techniques, e.g., deep learning, where audio representations are learned from data. In this paper, we propose a system that consists of a simple fusion of two methods of the aforementioned types: a deep learning approach where log-scaled mel-spectrograms are input to a convolutional neural network, and a feature engineering approach, where a collection of hand-crafted features is input to a gradient boosting machine. We first show that both methods provide complementary information to some extent. Then, we use a simple late fusion strategy to combine both methods. We report classification accuracy of each method individually and the combined system on the TUT Acoustic Scenes 2017 dataset. The proposed fused system outperforms each of the individual methods and attains a classification accuracy of 72.8\% on the evaluation set, improving the baseline system by 11.8\%.},
	author = {Fonseca, Eduardo and Gong, Rong and Serra, Xavier},
	year = {2018},
}

@article{christin_applications_nodate,
	title = {Applications for {Deep} {Learning} in {Ecology}},
	url = {http://dx.doi.org/10.1101/334854},
	doi = {10.1101/334854},
	abstract = {A lot of hype has recently been generated around deep learning, a group of artificial intelligence approaches able to break accuracy records in pattern recognition. Over the course of just a few years, deep learning revolutionized several research fields such as bioinformatics or medicine. Yet such a surge of tools and knowledge is still in its infancy in ecology despite the ever-growing size and the complexity of ecological datasets. Here we performed a literature review of deep learning implementations in ecology to identify its benefits in most ecological disciplines, even in applied ecology, up to decision makers and conservationists alike. We also provide guidelines on useful resources and recommendations for ecologists to start adding deep learning to their toolkit. At a time when automatic monitoring of populations and ecosystems generates a vast amount of data that cannot be processed by humans anymore, deep learning could become a necessity in ecology.},
	author = {Christin, Sylvain and Hervet, Éric and Lecomte, Nicolas},
	keywords = {Deep Learning, Artificial Intelligence, Automatic Monitoring, Ecology, Neural Network, Pattern Recognition},
}

@article{galtier_substra_2019,
	title = {Substra: a framework for privacy-preserving, traceable and collaborative {Machine} {Learning}},
	shorttitle = {Substra},
	url = {http://arxiv.org/abs/1910.11567},
	abstract = {Machine learning is promising, but it often needs to process vast amounts of sensitive data which raises concerns about privacy. In this white-paper, we introduce Substra, a distributed framework for privacy-preserving, traceable and collaborative Machine Learning. Substra gathers data providers and algorithm designers into a network of nodes that can train models on demand but under advanced permission regimes. To guarantee data privacy, Substra implements distributed learning: the data never leave their nodes; only algorithms, predictive models and non-sensitive metadata are exchanged on the network. The computations are orchestrated by a Distributed Ledger Technology which guarantees traceability and authenticity of information without needing to trust a third party. Although originally developed for Healthcare applications, Substra is not data, algorithm or programming language specific. It supports many types of computation plans including parallel computation plan commonly used in Federated Learning. With appropriate guidelines, it can be deployed for numerous Machine Learning use-cases with data or algorithm providers where trust is limited.},
	urldate = {2021-07-22},
	journal = {arXiv:1910.11567 [cs]},
	author = {Galtier, Mathieu N. and Marini, Camille},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.11567},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/YNRQ35ZE/Galtier and Marini - 2019 - Substra a framework for privacy-preserving, trace.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/I23VDZBZ/1910.html:text/html},
}

@article{verbraeken_survey_2019,
	title = {A {Survey} on {Distributed} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1912.09789},
	abstract = {The demand for artificial intelligence has grown significantly over the last decade and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, in order to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges, first and foremost the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.},
	urldate = {2021-07-22},
	journal = {arXiv:1912.09789 [cs, stat]},
	author = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S.},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.09789
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/RGU6RW4K/Verbraeken et al. - 2019 - A Survey on Distributed Machine Learning.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/DN6QLEQ7/1912.html:text/html},
}

@article{ricci_removing_2021,
	title = {Removing striping artifacts in light-sheet fluorescence microscopy: {A} review},
	issn = {0079-6107},
	shorttitle = {Removing striping artifacts in light-sheet fluorescence microscopy},
	url = {https://www.sciencedirect.com/science/article/pii/S0079610721000821},
	doi = {10.1016/j.pbiomolbio.2021.07.003},
	abstract = {In recent years, light-sheet fluorescence microscopy (LSFM) has found a broad application for imaging of diverse biological samples, ranging from sub-cellular structures to whole animals, both in-vivo and ex-vivo, owing to its many advantages relative to point-scanning methods. By providing the selective illumination of sample single planes, LSFM achieves an intrinsic optical sectioning and direct 2D image acquisition, with low out-of-focus fluorescence background, sample photo-damage and photo-bleaching. On the other hand, such an illumination scheme is prone to light absorption or scattering effects, which lead to uneven illumination and striping artifacts in the images, oriented along the light sheet propagation direction. Several methods have been developed to address this issue, ranging from fully optical solutions to entirely digital post-processing approaches. In this work, we present them, outlining their advantages, performance and limitations.},
	language = {en},
	urldate = {2021-07-22},
	journal = {Progress in Biophysics and Molecular Biology},
	author = {Ricci, Pietro and Gavryusev, Vladislav and Müllenbroich, Caroline and Turrini, Lapo and de Vito, Giuseppe and Silvestri, Ludovico and Sancataldo, Giuseppe and Pavone, Francesco Saverio},
	month = jul,
	year = {2021},
	keywords = {3D microscopy, Light-sheet microscopy, Brain imaging, Striping},
	file = {ScienceDirect Full Text PDF:/home/zwerg/Zotero/storage/FMEX6FNR/Ricci et al. - 2021 - Removing striping artifacts in light-sheet fluores.pdf:application/pdf},
}

@misc{noauthor_how_2021,
	title = {How to {Do} {Multi}-{Task} {Learning} {Intelligently}},
	url = {https://thegradient.pub/how-to-do-multi-task-learning-intelligently/},
	abstract = {On new multi-task learning methods that automatically learn what to learn together},
	language = {en},
	urldate = {2021-07-22},
	journal = {The Gradient},
	month = jun,
	year = {2021},
	file = {Snapshot:/home/zwerg/Zotero/storage/LVTVSR3R/how-to-do-multi-task-learning-intelligently.html:text/html},
}

@article{liu_video_2021,
	title = {Video {Swin} {Transformer}},
	url = {http://arxiv.org/abs/2106.13230},
	abstract = {The vision community is witnessing a modeling shift from CNNs to Transformers, where pure Transformer architectures have attained top accuracy on the major video recognition benchmarks. These video models are all built on Transformer layers that globally connect patches across the spatial and temporal dimensions. In this paper, we instead advocate an inductive bias of locality in video Transformers, which leads to a better speed-accuracy trade-off compared to previous approaches which compute self-attention globally even with spatial-temporal factorization. The locality of the proposed video architecture is realized by adapting the Swin Transformer designed for the image domain, while continuing to leverage the power of pre-trained image models. Our approach achieves state-of-the-art accuracy on a broad range of video recognition benchmarks, including on action recognition (84.9 top-1 accuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with {\textasciitilde}20x less pre-training data and {\textasciitilde}3x smaller model size) and temporal modeling (69.6 top-1 accuracy on Something-Something v2). The code and models will be made publicly available at https://github.com/SwinTransformer/Video-Swin-Transformer.},
	urldate = {2021-07-22},
	journal = {arXiv:2106.13230 [cs]},
	author = {Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.13230
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/FIIJP8NT/Liu et al. - 2021 - Video Swin Transformer.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/XNZGN2IF/2106.html:text/html},
}

@article{yuan_volo_2021,
	title = {{VOLO}: {Vision} {Outlooker} for {Visual} {Recognition}},
	shorttitle = {{VOLO}},
	url = {http://arxiv.org/abs/2106.13112},
	abstract = {Visual recognition has been dominated by convolutional neural networks (CNNs) for years. Though recently the prevailing vision transformers (ViTs) have shown great potential of self-attention based models in ImageNet classification, their performance is still inferior to that of the latest SOTA CNNs if no extra data are provided. In this work, we try to close the performance gap and demonstrate that attention-based models are indeed able to outperform CNNs. We find a major factor limiting the performance of ViTs for ImageNet classification is their low efficacy in encoding fine-level features into the token representations. To resolve this, we introduce a novel outlook attention and present a simple and general architecture, termed Vision Outlooker (VOLO). Unlike self-attention that focuses on global dependency modeling at a coarse level, the outlook attention efficiently encodes finer-level features and contexts into tokens, which is shown to be critically beneficial to recognition performance but largely ignored by the self-attention. Experiments show that our VOLO achieves 87.1\% top-1 accuracy on ImageNet-1K classification, which is the first model exceeding 87\% accuracy on this competitive benchmark, without using any extra training data In addition, the pre-trained VOLO transfers well to downstream tasks, such as semantic segmentation. We achieve 84.3\% mIoU score on the cityscapes validation set and 54.3\% on the ADE20K validation set. Code is available at {\textbackslash}url\{https://github.com/sail-sg/volo\}.},
	urldate = {2021-07-22},
	journal = {arXiv:2106.13112 [cs]},
	author = {Yuan, Li and Hou, Qibin and Jiang, Zihang and Feng, Jiashi and Yan, Shuicheng},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.13112
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: code: https://github.com/sail-sg/volo},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/6QFZ2YGA/Yuan et al. - 2021 - VOLO Vision Outlooker for Visual Recognition.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/YBKAZ2IS/2106.html:text/html},
}

@article{liu_bi-real_2018,
	title = {Bi-{Real} {Net}: {Enhancing} the {Performance} of 1-bit {CNNs} {With} {Improved} {Representational} {Capability} and {Advanced} {Training} {Algorithm}},
	shorttitle = {Bi-{Real} {Net}},
	url = {http://arxiv.org/abs/1808.00278},
	abstract = {In this work, we study the 1-bit convolutional neural networks (CNNs), of which both the weights and activations are binary. While being efficient, the classification accuracy of the current 1-bit CNNs is much worse compared to their counterpart real-valued CNN models on the large-scale dataset, like ImageNet. To minimize the performance gap between the 1-bit and real-valued CNN models, we propose a novel model, dubbed Bi-Real net, which connects the real activations (after the 1-bit convolution and/or BatchNorm layer, before the sign function) to activations of the consecutive block, through an identity shortcut. Consequently, compared to the standard 1-bit CNN, the representational capability of the Bi-Real net is significantly enhanced and the additional cost on computation is negligible. Moreover, we develop a specific training algorithm including three technical novelties for 1- bit CNNs. Firstly, we derive a tight approximation to the derivative of the non-differentiable sign function with respect to activation. Secondly, we propose a magnitude-aware gradient with respect to the weight for updating the weight parameters. Thirdly, we pre-train the real-valued CNN model with a clip function, rather than the ReLU function, to better initialize the Bi-Real net. Experiments on ImageNet show that the Bi-Real net with the proposed training algorithm achieves 56.4\% and 62.2\% top-1 accuracy with 18 layers and 34 layers, respectively. Compared to the state-of-the-arts (e.g., XNOR Net), Bi-Real net achieves up to 10\% higher top-1 accuracy with more memory saving and lower computational cost. Keywords: binary neural network, 1-bit CNNs, 1-layer-per-block},
	urldate = {2021-07-16},
	journal = {arXiv:1808.00278 [cs]},
	author = {Liu, Zechun and Wu, Baoyuan and Luo, Wenhan and Yang, Xin and Liu, Wei and Cheng, Kwang-Ting},
	month = sep,
	year = {2018},
	note = {arXiv: 1808.00278},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to European Conference on Computer Vision (ECCV) 2018. Code is available on: https://github.com/liuzechun/Bi-Real-net},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/QD5IUR6Z/Liu et al. - 2018 - Bi-Real Net Enhancing the Performance of 1-bit CN.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/ITMKNYCD/1808.html:text/html},
}

@article{sun_fast_2018,
	title = {Fast object detection based on binary deep convolution neural networks},
	volume = {3},
	issn = {2468-2322},
	url = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/trit.2018.1026},
	doi = {10.1049/trit.2018.1026},
	abstract = {In this study, a fast object detection algorithm based on binary deep convolution neural networks (CNNs) is proposed. Convolution kernels of different sizes are used to predict classes and bounding boxes of multi-scale objects directly in the last feature map of a deep CNN. In this way, rapid object detection with acceptable precision loss is achieved. In addition, binary quantisation for weight values and input data of each layer is used to squeeze the networks for faster object detection. Compared to full-precision convolution, the proposed binary deep CNNs for object detection results in 62 times faster convolutional operations and 32 times memory saving in theory, what's more, the proposed method is easy to be implemented in embedded computing systems because of the binary operation for convolution and low memory requirement. Experimental results on Pascal VOC2007 validate the effectiveness of the authors’ proposed method.},
	language = {en},
	number = {4},
	urldate = {2021-07-16},
	journal = {CAAI Transactions on Intelligence Technology},
	author = {Sun, Siyang and Yin, Yingjie and Wang, Xingang and Xu, De and Wu, Wenqi and Gu, Qingyi},
	year = {2018},
	note = {\_eprint: https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/trit.2018.1026},
	keywords = {object detection, 62 times faster convolutional operations, B6135 Optical, binary deep CNNs, binary deep convolution neural networks, binary operation, binary quantisation, C5260B Computer vision and image processing techniques, C5290 Neural computing techniques, convolution, convolution kernels, deep CNN, fast object detection algorithm, faster object detection, full-precision convolution, image and video signal processing, multiscale objects, neural nets, object detection results, rapid object detection},
	pages = {191--197},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/RU2DENVB/Sun et al. - 2018 - Fast object detection based on binary deep convolu.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/X8EZ3RGS/trit.2018.html:text/html},
}

@misc{noauthor_fast_nodate,
	title = {Fast object detection based on binary deep convolution neural networks - {Sun} - 2018 - {CAAI} {Transactions} on {Intelligence} {Technology} - {Wiley} {Online} {Library}},
	url = {https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/trit.2018.1026},
	urldate = {2021-07-16},
}

@article{liu_pay_2021,
	title = {Pay {Attention} to {MLPs}},
	url = {http://arxiv.org/abs/2105.08050},
	abstract = {Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple network architecture, gMLP, based on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream NLP tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.},
	urldate = {2021-07-16},
	journal = {arXiv:2105.08050 [cs]},
	author = {Liu, Hanxiao and Dai, Zihang and So, David R. and Le, Quoc V.},
	month = jun,
	year = {2021},
	note = {arXiv: 2105.08050},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/AH2EQDVD/Liu et al. - 2021 - Pay Attention to MLPs.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/BHT4TRWZ/2105.html:text/html},
}

@inproceedings{ge_zero-shot_2020,
	title = {Zero-shot {Synthesis} with {Group}-{Supervised} {Learning}},
	url = {https://openreview.net/forum?id=8wqCDnBmnrT},
	abstract = {Visual cognition of primates is superior to that of artificial neural networks in its ability to “envision” a visual object, even a newly-introduced one, in different attributes including pose...},
	language = {en},
	urldate = {2021-07-16},
	author = {Ge, Yunhao and Abu-El-Haija, Sami and Xin, Gan and Itti, Laurent},
	month = sep,
	year = {2020},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/5I4GF7CN/Ge et al. - 2020 - Zero-shot Synthesis with Group-Supervised Learning.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/7KWQCIB7/forum.html:text/html},
}

@article{wang_eca-net_2020,
	title = {{ECA}-{Net}: {Efficient} {Channel} {Attention} for {Deep} {Convolutional} {Neural} {Networks}},
	shorttitle = {{ECA}-{Net}},
	url = {http://arxiv.org/abs/1910.03151},
	abstract = {Recently, channel attention mechanism has demonstrated to offer great potential in improving the performance of deep convolutional neural networks (CNNs). However, most existing methods dedicate to developing more sophisticated attention modules for achieving better performance, which inevitably increase model complexity. To overcome the paradox of performance and complexity trade-off, this paper proposes an Efficient Channel Attention (ECA) module, which only involves a handful of parameters while bringing clear performance gain. By dissecting the channel attention module in SENet, we empirically show avoiding dimensionality reduction is important for learning channel attention, and appropriate cross-channel interaction can preserve performance while significantly decreasing model complexity. Therefore, we propose a local cross-channel interaction strategy without dimensionality reduction, which can be efficiently implemented via \$1D\$ convolution. Furthermore, we develop a method to adaptively select kernel size of \$1D\$ convolution, determining coverage of local cross-channel interaction. The proposed ECA module is efficient yet effective, e.g., the parameters and computations of our modules against backbone of ResNet50 are 80 vs. 24.37M and 4.7e-4 GFLOPs vs. 3.86 GFLOPs, respectively, and the performance boost is more than 2\% in terms of Top-1 accuracy. We extensively evaluate our ECA module on image classification, object detection and instance segmentation with backbones of ResNets and MobileNetV2. The experimental results show our module is more efficient while performing favorably against its counterparts.},
	urldate = {2021-07-15},
	journal = {arXiv:1910.03151 [cs]},
	author = {Wang, Qilong and Wu, Banggu and Zhu, Pengfei and Li, Peihua and Zuo, Wangmeng and Hu, Qinghua},
	month = apr,
	year = {2020},
	note = {arXiv: 1910.03151},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to CVPR 2020; Project Page: https://github.com/BangguWu/ECANet},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/ZT4JHXLS/Wang et al. - 2020 - ECA-Net Efficient Channel Attention for Deep Conv.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/ZJWAA8Q9/1910.html:text/html},
}

@article{qin_binary_2020,
	title = {Binary {Neural} {Networks}: {A} {Survey}},
	volume = {105},
	issn = {00313203},
	shorttitle = {Binary {Neural} {Networks}},
	url = {http://arxiv.org/abs/2004.03333},
	doi = {10.1016/j.patcog.2020.107281},
	abstract = {The binary neural network, largely saving the storage and computation, serves as a promising technique for deploying deep models on resource-limited devices. However, the binarization inevitably causes severe information loss, and even worse, its discontinuity brings difficulty to the optimization of the deep network. To address these issues, a variety of algorithms have been proposed, and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these algorithms, mainly categorized into the native solutions directly conducting binarization, and the optimized ones using techniques like minimizing the quantization error, improving the network loss function, and reducing the gradient error. We also investigate other practical aspects of binary neural networks such as the hardware-friendly design and the training tricks. Then, we give the evaluation and discussions on different tasks, including image classification, object detection and semantic segmentation. Finally, the challenges that may be faced in future research are prospected.},
	urldate = {2021-07-15},
	journal = {Pattern Recognition},
	author = {Qin, Haotong and Gong, Ruihao and Liu, Xianglong and Bai, Xiao and Song, Jingkuan and Sebe, Nicu},
	month = sep,
	year = {2020},
	note = {arXiv: 2004.03333},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	pages = {107281},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/GAQMDZPE/Qin et al. - 2020 - Binary Neural Networks A Survey.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/4W2RRKRI/2004.html:text/html},
}

@inproceedings{chung_extremely_2020,
	address = {Online},
	title = {Extremely {Low} {Bit} {Transformer} {Quantization} for {On}-{Device} {Neural} {Machine} {Translation}},
	url = {https://aclanthology.org/2020.findings-emnlp.433},
	doi = {10.18653/v1/2020.findings-emnlp.433},
	abstract = {The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quantization is an effective technique to address such challenges. Our analysis shows that for a given number of quantization bits, each block of Transformer contributes to translation quality and inference computations in different manners. Moreover, even inside an embedding block, each word presents vastly different contributions. Correspondingly, we propose a mixed precision quantization strategy to represent Transformer weights by an extremely low number of bits (e.g., under 3 bits). For example, for each word in an embedding block, we assign different quantization bits based on statistical property. Our quantized Transformer model achieves 11.8{\textbackslash}mbox\${\textbackslash}times\$ smaller model size than the baseline model, with less than -0.5 BLEU. We achieve 8.3{\textbackslash}mbox\${\textbackslash}times\$ reduction in run-time memory footprints and 3.5{\textbackslash}mbox\${\textbackslash}times\$ speed up (Galaxy N10+) such that our proposed compression strategy enables efficient implementation for on-device NMT.},
	urldate = {2021-07-15},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Chung, Insoo and Kim, Byeongwook and Choi, Yoonjung and Kwon, Se Jung and Jeon, Yongkweon and Park, Baeseong and Kim, Sangha and Lee, Dongsoo},
	month = nov,
	year = {2020},
	pages = {4812--4826},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/7BRR9726/Chung et al. - 2020 - Extremely Low Bit Transformer Quantization for On-.pdf:application/pdf},
}

@article{singh_learning_2020,
	title = {Learning {Architectures} for {Binary} {Networks}},
	doi = {10.1007/978-3-030-58610-2_34},
	abstract = {Backbone architectures of most binary networks are well-known floating point architectures such as the ResNet family. Questioning that the architectures designed for floating point networks would not be the best for binary networks, we propose to search architectures for binary networks (BNAS) by defining a new search space for binary architectures and a novel search objective. Specifically, based on the cell based search method, we define the new search space of binary layer types, design a new cell template, and rediscover the utility of and propose to use the Zeroise layer instead of using it as a placeholder. The novel search objective diversifies early search to learn better performing binary architectures. We show that our proposed method searches architectures with stable training curves despite the quantization error inherent in binary networks. Quantitative analyses demonstrate that our searched architectures outperform the architectures used in state-of-the-art binary networks and outperform or perform on par with state-of-the-art binary networks that employ various techniques other than architectural changes.},
	journal = {ECCV},
	author = {Singh, Kunal Pratap and Kim, Dahyun and Choi, Jonghyun},
	year = {2020},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/MRMZCNTL/Singh et al. - 2020 - Learning Architectures for Binary Networks.pdf:application/pdf},
}

@article{kim_aresb-net_2021,
	title = {{AresB}-{Net}: accurate residual binarized neural networks using shortcut concatenation and shuffled grouped convolution},
	shorttitle = {{AresB}-{Net}},
	doi = {10.7717/peerj-cs.454},
	abstract = {This article proposes a novel network model to achieve better accurate residual binarized convolutional neural networks (CNNs), denoted as AresB-Net. Even though residual CNNs enhance the classification accuracy of binarized neural networks with increasing feature resolution, the degraded classification accuracy is still the primary concern compared with real-valued residual CNNs. AresB-Net consists of novel basic blocks to amortize the severe error from the binarization, suggesting a well-balanced pyramid structure without downsampling convolution. In each basic block, the shortcut is added to the convolution output and then concatenated, and then the expanded channels are shuffled for the next grouped convolution. In the downsampling when stride {\textgreater}1, our model adopts only the max-pooling layer for generating low-cost shortcut. This structure facilitates the feature reuse from the previous layers, thus alleviating the error from the binarized convolution and increasing the classification accuracy with reduced computational costs and small weight storage requirements. Despite low hardware costs from the binarized computations, the proposed model achieves remarkable classification accuracies on the CIFAR and ImageNet datasets.},
	journal = {PeerJ Comput. Sci.},
	author = {Kim, Hyunjin},
	year = {2021},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/6KSTBK2W/Kim - 2021 - AresB-Net accurate residual binarized neural netw.pdf:application/pdf},
}

@article{rastegari_xnor-net_2016,
	title = {{XNOR}-{Net}: {ImageNet} {Classification} {Using} {Binary} {Convolutional} {Neural} {Networks}},
	shorttitle = {{XNOR}-{Net}},
	url = {http://arxiv.org/abs/1603.05279},
	abstract = {We propose two efﬁcient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-WeightNetworks, the ﬁlters are approximated with binary values resulting in 32× memory saving. In XNOR-Networks, both the ﬁlters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58× faster convolutional operations (in terms of number of the high precision operations) and 32× memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efﬁcient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classiﬁcation task. The classiﬁcation accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than 16\% in top-1 accuracy. Our code is available at: http://allenai.org/plato/xnornet.},
	language = {en},
	urldate = {2021-07-15},
	journal = {arXiv:1603.05279 [cs]},
	author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
	month = aug,
	year = {2016},
	note = {arXiv: 1603.05279},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Rastegari et al. - 2016 - XNOR-Net ImageNet Classification Using Binary Con.pdf:/home/zwerg/Zotero/storage/E7SKEX7A/Rastegari et al. - 2016 - XNOR-Net ImageNet Classification Using Binary Con.pdf:application/pdf},
}

@article{bulat_xnor-net_2019,
	title = {{XNOR}-{Net}++: {Improved} {Binary} {Neural} {Networks}},
	shorttitle = {{XNOR}-{Net}++},
	url = {http://arxiv.org/abs/1909.13863},
	abstract = {This paper proposes an improved training algorithm for binary neural networks in which both weights and activations are binary numbers. A key but fairly overlooked feature of the current state-of-the-art method of XNOR-Net is the use of analytically calculated real-valued scaling factors for re-weighting the output of binary convolutions. We argue that analytic calculation of these factors is sub-optimal. Instead, in this work, we make the following contributions: (a) we propose to fuse the activation and weight scaling factors into a single one that is learned discriminatively via backpropagation. (b) More importantly, we explore several ways of constructing the shape of the scale factors while keeping the computational budget fixed. (c) We empirically measure the accuracy of our approximations and show that they are significantly more accurate than the analytically calculated one. (d) We show that our approach significantly outperforms XNOR-Net within the same computational budget when tested on the challenging task of ImageNet classification, offering up to 6{\textbackslash}\% accuracy gain.},
	urldate = {2021-07-15},
	journal = {arXiv:1909.13863 [cs, eess]},
	author = {Bulat, Adrian and Tzimiropoulos, Georgios},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.13863},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: Accepted to BMVC 2019},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/3SAI87SY/Bulat and Tzimiropoulos - 2019 - XNOR-Net++ Improved Binary Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/RT8MWSKV/1909.html:text/html},
}

@article{tellez_quantifying_2019,
	title = {Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology},
	volume = {58},
	url = {https://arxiv.org/abs/1902.06543v2},
	doi = {10.1016/j.media.2019.101544},
	abstract = {Stain variation is a phenomenon observed when distinct pathology laboratories
stain tissue slides that exhibit similar but not identical color appearance.
Due to this color shift between laboratories, convolutional neural networks
(CNNs) trained with images from one lab often underperform on unseen images
from the other lab. Several techniques have been proposed to reduce the
generalization error, mainly grouped into two categories: stain color
augmentation and stain color normalization. The former simulates a wide variety
of realistic stain variations during training, producing stain-invariant CNNs.
The latter aims to match training and test color distributions in order to
reduce stain variation. For the first time, we compared some of these
techniques and quantified their effect on CNN classification performance using
a heterogeneous dataset of hematoxylin and eosin histopathology images from 4
organs and 9 pathology laboratories. Additionally, we propose a novel
unsupervised method to perform stain color normalization using a neural
network. Based on our experimental results, we provide practical guidelines on
how to use stain color augmentation and stain color normalization in future
computational pathology applications.},
	journal = {Medical Image Analysis},
	author = {Tellez, David and Litjens, Geert and Bandi, Peter and Bulten, Wouter and Bokhorst, John-Melle and Ciompi, Francesco and van der Laak, Jeroen},
	month = feb,
	year = {2019},
	note = {Publisher: Elsevier B.V.},
	keywords = {Deep learning, Computational pathology, Convolutional neural network},
}

@article{microscopy_understandable_2021,
	title = {Understandable {Images}},
	author = {Microscopy, Light},
	year = {2021},
	pages = {18--19},
}

@techreport{hannun_deep_nodate,
	title = {Deep {Speech}: {Scaling} up end-to-end speech recognition},
	abstract = {We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a "phoneme." Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0\% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
	author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y},
}

@techreport{david_tensorflow_nodate,
	title = {{TENSORFLOW} {LITE} {MICRO}: {EMBEDDED} {MACHINE} {LEARNING} {ON} {TINYML} {SYSTEMS}},
	abstract = {Deep learning inference on embedded devices is a burgeoning field with myriad applications because tiny embedded devices are omnipresent. But we must overcome major challenges before we can benefit from this opportunity. Embedded processors are severely resource constrained. Their nearest mobile counterparts exhibit at least a 100-1,000x difference in compute capability, memory availability, and power consumption. As a result, the machine-learning (ML) models and associated ML inference framework must not only execute efficiently but also operate in a few kilobytes of memory. Also, the embedded devices' ecosystem is heavily fragmented. To maximize efficiency, system vendors often omit many features that commonly appear in mainstream systems, including dynamic memory allocation and virtual memory, that allow for cross-platform interoperability. The hardware comes in many flavors (e.g., instruction-set architecture and FPU support, or lack thereof). We introduce TensorFlow Lite Micro (TF Micro), an open-source ML inference framework for running deep-learning models on embedded systems. TF Micro tackles the efficiency requirements imposed by embedded-system resource constraints and the fragmentation challenges that make cross-platform interoperability nearly impossible. The framework adopts a unique interpreter-based approach that provides flexibility while overcoming these challenges. This paper explains the design decisions behind TF Micro and describes its implementation details. Also, we present an evaluation to demonstrate its low resource requirement and minimal run-time performance overhead.},
	author = {David, Robert and Duke, Jared and Jain, Advait and Reddi, Vijay Janapa and Jeffries, Nat and Li, Jian and Kreeger, Nick and Nappier, Ian and Natraj, Meghna and Regev, Shlomi and Rhodes, Rocky and Wang, Tiezhen and Warden, Pete},
}

@article{afifi_what_2019,
	title = {What {Else} {Can} {Fool} {Deep} {Learning}? {Addressing} {Color} {Constancy} {Errors} on {Deep} {Neural} {Network} {Performance}},
	volume = {2019-October},
	url = {https://arxiv.org/abs/1912.06960v1},
	abstract = {There is active research targeting local image manipulations that can fool
deep neural networks (DNNs) into producing incorrect results. This paper
examines a type of global image manipulation that can produce similar adverse
effects. Specifically, we explore how strong color casts caused by incorrectly
applied computational color constancy - referred to as white balance (WB) in
photography - negatively impact the performance of DNNs targeting image
segmentation and classification. In addition, we discuss how existing image
augmentation methods used to improve the robustness of DNNs are not well suited
for modeling WB errors. To address this problem, a novel augmentation method is
proposed that can emulate accurate color constancy degradation. We also explore
pre-processing training and testing images with a recent WB correction
algorithm to reduce the effects of incorrectly white-balanced images. We
examine both augmentation and pre-processing strategies on different datasets
and demonstrate notable improvements on the CIFAR-10, CIFAR-100, and ADE20K
datasets.},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	author = {Afifi, Mahmoud and Brown, Michael S},
	month = dec,
	year = {2019},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	pages = {243--252},
}

@article{baevski_wav2vec_2020,
	title = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
	url = {http://arxiv.org/abs/2006.11477},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
	month = jun,
	year = {2020},
}

@techreport{david_tensorflow_nodate-1,
	title = {{TENSORFLOW} {LITE} {MICRO}: {EMBEDDED} {MACHINE} {LEARNING} {ON} {TINYML} {SYSTEMS}},
	abstract = {Deep learning inference on embedded devices is a burgeoning field with myriad applications because tiny embedded devices are omnipresent. But we must overcome major challenges before we can benefit from this opportunity. Embedded processors are severely resource constrained. Their nearest mobile counterparts exhibit at least a 100-1,000x difference in compute capability, memory availability, and power consumption. As a result, the machine-learning (ML) models and associated ML inference framework must not only execute efficiently but also operate in a few kilobytes of memory. Also, the embedded devices' ecosystem is heavily fragmented. To maximize efficiency, system vendors often omit many features that commonly appear in mainstream systems, including dynamic memory allocation and virtual memory, that allow for cross-platform interoperability. The hardware comes in many flavors (e.g., instruction-set architecture and FPU support, or lack thereof). We introduce TensorFlow Lite Micro (TF Micro), an open-source ML inference framework for running deep-learning models on embedded systems. TF Micro tackles the efficiency requirements imposed by embedded-system resource constraints and the fragmentation challenges that make cross-platform interoperability nearly impossible. The framework adopts a unique interpreter-based approach that provides flexibility while overcoming these challenges. This paper explains the design decisions behind TF Micro and describes its implementation details. Also, we present an evaluation to demonstrate its low resource requirement and minimal run-time performance overhead.},
	author = {David, Robert and Duke, Jared and Jain, Advait and Reddi, Vijay Janapa and Jeffries, Nat and Li, Jian and Kreeger, Nick and Nappier, Ian and Natraj, Meghna and Regev, Shlomi and Rhodes, Rocky and Wang, Tiezhen and Warden, Pete},
}

@techreport{sutton_reinforcement_nodate,
	title = {Reinforcement {Learning}: {An} {Introduction} {Second} edition, in progress},
	author = {Sutton, Richard S and Barto, Andrew G},
}

@article{goffinet_inferring_2019,
	title = {Inferring low-dimensional latent descriptions of animal vocalizations},
	url = {https://doi.org/10.1101/811661},
	doi = {10.1101/811661},
	abstract = {Vocalization is an essential medium for social and sexual signaling in most birds and mammals. Consequently, the analysis of vocal behavior is of great interest to fields such as neuroscience and linguistics. A standard approach to analyzing vocalization involves segmenting the sound stream into discrete vocal elements, calculating a number of handpicked acoustic features, and then using the feature values for subsequent quantitative analysis. While this approach has proven powerful, it suffers from several crucial limitations: First, handpicked acoustic features may miss important dimensions of variability that are important for communicative function. Second, many analyses assume vocalizations fall into discrete vocal categories, often without rigorous justification. Third, a syllable-level analysis requires a consistent definition of syllable boundaries, which is often difficult to maintain in practice and limits the sorts of structure one can find in the data. To address these shortcomings, we apply a data-driven approach based on the variational autoencoder (VAE), an unsupervised learning method, to the task of characterizing vocalizations in two model species: the laboratory mouse (Mus musculus) and the zebra finch (Taeniopygia guttata). We find that the VAE converges on a parsimonious representation of vocal behavior that outperforms handpicked acoustic features on a variety of common analysis tasks, including representing acoustic similarity and recovering a known effect of social context on birdsong. Additionally, we use our learned acoustic features to argue against the widespread view that mouse ultrasonic vocalizations form discrete syllable categories. Lastly, we present a novel “shotgun VAE” that can quantify moment-by-moment variability in vocalizations. In all, we show that data-derived acoustic features confirm and extend existing approaches while offering distinct advantages in several critical applications.},
	journal = {bioRxiv},
	author = {Goffinet, Jack and Mooney, Richard and Pearson, John},
	month = oct,
	year = {2019},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {811661--811661},
}

@article{sharir_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}, {What} is a {Video} {Worth}?},
	url = {https://arxiv.org/abs/2103.13915},
	abstract = {Leading methods in the domain of action recognition try to distill
information from both the spatial and temporal dimensions of an input video.
Methods that reach State of the Art (SotA) accuracy, usually make use of 3D
convolution layers as a way to abstract the temporal information from video
frames. The use of such convolutions requires sampling short clips from the
input video, where each clip is a collection of closely sampled frames. Since
each short clip covers a small fraction of an input video, multiple clips are
sampled at inference in order to cover the whole temporal length of the video.
This leads to increased computational load and is impractical for real-world
applications. We address the computational bottleneck by significantly reducing
the number of frames required for inference. Our approach relies on a temporal
transformer that applies global attention over video frames, and thus better
exploits the salient information in each frame. Therefore our approach is very
input efficient, and can achieve SotA results (on Kinetics dataset) with a
fraction of the data (frames per video), computation and latency. Specifically
on Kinetics-400, we reach \$80.5\$ top-1 accuracy with \${\textbackslash}times 30\$ less frames
per video, and \${\textbackslash}times 40\$ faster inference than the current leading method.
Code is available at: https://github.com/Alibaba-MIIL/STAM},
	author = {Sharir, Gilad and Noy, Asaf and Zelnik-Manor, Lihi},
	month = mar,
	year = {2021},
}

@techreport{canziani_analysis_nodate,
	title = {{AN} {ANALYSIS} {OF} {DEEP} {NEURAL} {NETWORK} {MODELS} {FOR} {PRACTICAL} {APPLICATIONS}},
	abstract = {Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important met-rics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key findings are: (1) power consumption is independent of batch size and architecture; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint is an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efficient DNNs.},
	author = {Canziani, Alfredo and Culurciello, Eugenio and Paszke, Adam},
}

@techreport{gemp_eigengame_nodate,
	title = {{EIGENGAME}: {PCA} {AS} {A} {NASH} {EQUILIBRIUM}},
	abstract = {We present a novel view on principal component analysis (PCA) as a competitive game in which each approximate eigenvector is controlled by a player whose goal is to maximize their own utility function. We analyze the properties of this PCA game and the behavior of its gradient based updates. The resulting algorithm-which combines elements from Oja's rule with a generalized Gram-Schmidt orthogonalization-is naturally decentralized and hence parallelizable through message passing. We demonstrate the scalability of the algorithm with experiments on large image datasets and neural network activations. We discuss how this new view of PCA as a differentiable game can lead to further algorithmic developments and insights.},
	author = {Gemp, Ian and Mcwilliams, Brian and Vernade, Claire and Thore, \& and Deepmind, Graepel},
}

@techreport{warden_speech_2018,
	title = {Speech {Commands}: {A} {Dataset} for {Limited}-{Vocabulary} {Speech} {Recognition}},
	abstract = {1 Abstract Describes an audio dataset[1] of spoken words designed to help train and evaluate keyword spotting systems. Discusses why this task is an interesting challenge, and why it requires a specialized dataset that's different from conventional datasets used for automatic speech recognition of full sentences. Suggests a methodology for reproducible and comparable accuracy metrics for this task. Describes how the data was collected and verified, what it contains, previous versions[2] and properties. Concludes by reporting baseline results of models trained on this dataset.},
	author = {Warden, Pete},
	year = {2018},
}

@techreport{zhong_learning_2018,
	title = {Learning to {Draw} {Vector} {Graphics}: {Applying} {Generative} {Modeling} to {Font} {Glyphs}},
	author = {Zhong, Kimberli},
	year = {2018},
}

@article{purohit_mimii_2019,
	title = {{MIMII} {Dataset}: {Sound} {Dataset} for {Malfunctioning} {Industrial} {Machine} {Investigation} and {Inspection}},
	url = {http://arxiv.org/abs/1909.09347},
	abstract = {Factory machinery is prone to failure or breakdown, resulting in significant expenses for companies. Hence, there is a rising interest in machine monitoring using different sensors including microphones. In the scientific community, the emergence of public datasets has led to advancements in acoustic detection and classification of scenes and events, but there are no public datasets that focus on the sound of industrial machines under normal and anomalous operating conditions in real factory environments. In this paper, we present a new dataset of industrial machine sounds that we call a sound dataset for malfunctioning industrial machine investigation and inspection (MIMII dataset). Normal sounds were recorded for different types of industrial machines (i.e., valves, pumps, fans, and slide rails), and to resemble a real-life scenario, various anomalous sounds were recorded (e.g., contamination, leakage, rotating unbalance, and rail damage). The purpose of releasing the MIMII dataset is to assist the machine-learning and signal-processing community with their development of automated facility maintenance. The MIMII dataset is freely available for download at: https://zenodo.org/record/3384388},
	author = {Purohit, Harsh and Tanabe, Ryo and Ichige, Kenji and Endo, Takashi and Nikaido, Yuki and Suefusa, Kaori and Kawaguchi, Yohei},
	month = sep,
	year = {2019},
	note = {Publisher: New York University},
	keywords = {Acoustic scene classi-fication, Anomaly detection, Index Terms-Machine sound dataset, Unsupervised anomalous sound detec-tion},
	pages = {209--213},
}

@techreport{goncharova_improving_nodate,
	title = {Improving {Blind} {Spot} {Denoising} for {Microscopy}},
	abstract = {Many microscopy applications are limited by the total amount of usable light and are consequently challenged by the resulting levels of noise in the acquired images. This problem is often addressed via (su-pervised) deep learning based denoising. Recently, by making assumptions about the noise statistics, self-supervised methods have emerged. Such methods are trained directly on the images that are to be denoised and do not require additional paired training data. While achieving remarkable results, self-supervised methods can produce high-frequency artifacts and achieve inferior results compared to supervised approaches. Here we present a novel way to improve the quality of self-supervised de-noising. Considering that light microscopy images are usually diffraction-limited, we propose to include this knowledge in the denoising process. We assume the clean image to be the result of a convolution with a point spread function (PSF) and explicitly include this operation at the end of our neural network. As a consequence, we are able to eliminate high-frequency artifacts and achieve self-supervised results that are very close to the ones achieved with traditional supervised methods.},
	author = {Goncharova, Anna S and Honigmann, Alf and Jug, Florian and Krull, Alexander},
	keywords = {deconvolution, CNN, denoising, light microscopy},
}

@article{wagner_continuous_2021,
	title = {continuous validation},
	volume = {18},
	issn = {4159202101},
	url = {http://dx.doi.org/10.1038/s41592-021-01136-0},
	doi = {10.1038/s41592-021-01136-0},
	number = {May},
	journal = {Nature Methods},
	author = {Wagner, Nils and Beuttenmueller, Fynn and Norlin, Nils and Gierten, Jakob and Boffi, Juan Carlos and Wittbrodt, Joachim and Weigert, Martin and Hufnagel, Lars and Prevedel, Robert and Kreshuk, Anna},
	year = {2021},
	note = {Publisher: Springer US},
}

@article{hore_image_2010,
	title = {Image quality metrics: {PSNR} vs. {SSIM}},
	issn = {9780769541099},
	doi = {10.1109/ICPR.2010.579},
	abstract = {In this paper, we analyse two well-known objective image quality metrics, the peak-signal-to-noise ratio (PSNR) as well as the structural similarity index measure (SSIM), and we derive a simple mathematical relationship between them which works for various kinds of image degradations such as Gaussian blur, additive Gaussian white noise, jpeg and jpeg2000 compression. A series of tests realized on images extracted from the Kodak database gives a better understanding of the similarity and difference between the SSIM and the PSNR. © 2010 IEEE.},
	number = {August},
	journal = {Proceedings - International Conference on Pattern Recognition},
	author = {Horé, Alain and Ziou, Djemel},
	year = {2010},
	keywords = {Image quality metrics, PSNR, SSIM},
	pages = {2366--2369},
}

@article{morrill_neural_2020,
	title = {Neural {Rough} {Differential} {Equations} for {Long} {Time} {Series}},
	url = {http://arxiv.org/abs/2009.08295},
	abstract = {Neural controlled differential equations (CDEs) are the continuous-time analogue of recurrent neural networks, as Neural ODEs are to residual networks, and offer a memory-efficient continuous-time way to model functions of potentially irregular time series. Existing methods for computing the forward pass of a Neural CDE involve embedding the incoming time series into path space, often via interpolation, and using evaluations of this path to drive the hidden state. Here, we use rough path theory to extend this formulation. Instead of directly embedding into path space, we instead represent the input signal over small time intervals through its {\textbackslash}textit\{log-signature\}, which are statistics describing how the signal drives a CDE. This is the approach for solving {\textbackslash}textit\{rough differential equations\} (RDEs), and correspondingly we describe our main contribution as the introduction of Neural RDEs. This extension has a purpose: by generalising the Neural CDE approach to a broader class of driving signals, we demonstrate particular advantages for tackling long time series. In this regime, we demonstrate efficacy on problems of length up to 17k observations and observe significant training speed-ups, improvements in model performance, and reduced memory requirements compared to existing approaches.},
	author = {Morrill, James and Salvi, Cristopher and Kidger, Patrick and Foster, James and Lyons, Terry},
	month = sep,
	year = {2020},
}

@article{ren_3d_2021,
	title = {{3D} {Spatial} {Recognition} without {Spatially} {Labeled} {3D}},
	url = {http://arxiv.org/abs/2105.06461},
	abstract = {We introduce WyPR, a Weakly-supervised framework for Point cloud Recognition, requiring only scene-level class tags as supervision. WyPR jointly addresses three core 3D recognition tasks: point-level semantic segmentation, 3D proposal generation, and 3D object detection, coupling their predictions through self and cross-task consistency losses. We show that in conjunction with standard multiple-instance learning objectives, WyPR can detect and segment objects in point cloud data without access to any spatial labels at training time. We demonstrate its efficacy using the ScanNet and S3DIS datasets, outperforming prior state of the art on weakly-supervised segmentation by more than 6\% mIoU. In addition, we set up the first benchmark for weakly-supervised 3D object detection on both datasets, where WyPR outperforms standard approaches and establishes strong baselines for future work.},
	author = {Ren, Zhongzheng and Misra, Ishan and Schwing, Alexander G. and Girdhar, Rohit},
	month = may,
	year = {2021},
}

@article{noauthor_google_nodate,
	title = {Google {Replaces} {BERT} {Self}-{Attention} with {Fourier} {Transform}: 92\% {Accuracy}, 7 {Times} {Faster} on {GPUs} {\textbar} {Synced}},
	url = {https://syncedreview.com/2021/05/14/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-19/},
}

@article{liu_pay_2021-1,
	title = {Pay {Attention} to {MLPs}},
	url = {http://arxiv.org/abs/2105.08050},
	abstract = {Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple attention-free network architecture, gMLP, based solely on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.},
	author = {Liu, Hanxiao and Dai, Zihang and So, David R. and Le, Quoc V.},
	month = may,
	year = {2021},
}

@article{liu_pay_2021-2,
	title = {Pay {Attention} to {MLPs}},
	url = {https://arxiv.org/abs/2105.08050},
	abstract = {Transformers have become one of the most important architectural innovations
in deep learning and have enabled many breakthroughs over the past few years.
Here we propose a simple attention-free network architecture, gMLP, based
solely on MLPs with gating, and show that it can perform as well as
Transformers in key language and vision applications. Our comparisons show that
self-attention is not critical for Vision Transformers, as gMLP can achieve the
same accuracy. For BERT, our model achieves parity with Transformers on
pretraining perplexity and is better on some downstream tasks. On finetuning
tasks where gMLP performs worse, making the gMLP model substantially larger can
close the gap with Transformers. In general, our experiments show that gMLP can
scale as well as Transformers over increased data and compute.},
	author = {Liu, Hanxiao and Dai, Zihang and So, David R. and Le, Quoc V.},
	month = may,
	year = {2021},
}

@article{paul_vision_2021,
	title = {Vision {Transformers} are {Robust} {Learners}},
	url = {https://arxiv.org/abs/2105.07581},
	abstract = {Transformers, composed of multiple self-attention layers, hold strong
promises toward a generic learning primitive applicable to different data
modalities, including the recent breakthroughs in computer vision achieving
state-of-the-art (SOTA) standard accuracy with better parameter efficiency.
Since self-attention helps a model systematically align different components
present inside the input data, it leaves grounds to investigate its performance
under model robustness benchmarks. In this work, we study the robustness of the
Vision Transformer (ViT) against common corruptions and perturbations,
distribution shifts, and natural adversarial examples. We use six different
diverse ImageNet datasets concerning robust classification to conduct a
comprehensive performance comparison of ViT models and SOTA convolutional
neural networks (CNNs), Big-Transfer. Through a series of six systematically
designed experiments, we then present analyses that provide both quantitative
and qualitative indications to explain why ViTs are indeed more robust
learners. For example, with fewer parameters and similar dataset and
pre-training combinations, ViT gives a top-1 accuracy of 28.10\% on ImageNet-A
which is 4.3x higher than a comparable variant of BiT. Our analyses on image
masking, Fourier spectrum sensitivity, and spread on discrete cosine energy
spectrum reveal intriguing properties of ViT attributing to improved
robustness. Code for reproducing our experiments is available here:
https://git.io/J3VO0.},
	author = {Paul, Sayak and Chen, Pin-Yu},
	month = may,
	year = {2021},
	file = {Full Text:/home/zwerg/Zotero/storage/6HSIGBGI/Paul and Chen - 2021 - Vision Transformers are Robust Learners.pdf:application/pdf},
}

@article{warden_speech_2018-1,
	title = {Speech {Commands}: {A} {Dataset} for {Limited}-{Vocabulary} {Speech} {Recognition}},
	url = {http://arxiv.org/abs/1804.03209},
	abstract = {Describes an audio dataset of spoken words designed to help train and evaluate keyword spotting systems. Discusses why this task is an interesting challenge, and why it requires a specialized dataset that is different from conventional datasets used for automatic speech recognition of full sentences. Suggests a methodology for reproducible and comparable accuracy metrics for this task. Describes how the data was collected and verified, what it contains, previous versions and properties. Concludes by reporting baseline results of models trained on this dataset.},
	journal = {arXiv},
	author = {Warden, Pete},
	month = apr,
	year = {2018},
	note = {Publisher: arXiv},
}

@techreport{touvron_training_nodate,
	title = {Training data-efficient image transformers \& distillation through attention},
	abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2\% accuracy) and when transferring to other tasks. We share our code and models.},
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jégou, Hervé and Ai, Facebook},
}

@article{reinhart_unsupervised_2021,
	title = {Unsupervised learning of atomic environments from simple features},
	volume = {196},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0927025621002366},
	doi = {10.1016/j.commatsci.2021.110511},
	journal = {Computational Materials Science},
	author = {Reinhart, Wesley F.},
	month = aug,
	year = {2021},
	note = {Publisher: Elsevier},
	pages = {110511--110511},
}

@article{goldsmith_digital_nodate,
	title = {Digital {Biosensing} by {Foundry}-{Fabricated} {Graphene} sensors},
	url = {www.nature.com/scientificreports},
	doi = {10.1038/s41598-019-38700-w},
	abstract = {the prevailing philosophy in biological testing has been to focus on simple tests with easy to interpret information such as ELISA or lateral flow assays. At the same time, there has been a decades long understanding in device physics and nanotechnology that electrical approaches have the potential to drastically improve the quality, speed, and cost of biological testing provided that computational resources are available to analyze the resulting complex data. this concept can be conceived of as "the internet of biology" in the same way miniaturized electronic sensors have enabled "the internet of things." It is well established in the nanotechnology literature that techniques such as field effect biosensing are capable of rapid and flexible biological testing. Until now, access to this new technology has been limited to academic researchers focused on bioelectronic devices and their collaborators. Here we show that this capability is retained in an industrially manufactured device, opening access to this technology generally. Access to this type of production opens the door for rapid deployment of nanoelectronic sensors outside the research space. the low power and resource usage of these biosensors enables biotech engineers to gain immediate control over precise biological and environmental data. The world is entering an inflection point in medical and biological testing with the simultaneous emergence of improved testing technology, advanced software tools, and increased expectations for quality healthcare worldwide. Organizations like the Qualcomm Tricorder XPRIZE and Gates Foundation have pushed for integrations of varied technologies in clinical tests to demonstrate potential application 1. Traditional healthcare companies market point-of-care tools with limited test libraries 2. In each case, complex, analyte-specific reagents and intricate protocols create a need for multiple platforms and deep biochemical or clinical expertise to replicate the capability of a central lab 3. There is a need for information-dense single assays that break the mold of expensive labs running colorimetric and PCR based assays 4. Label-free measurement tools based on field-effect sensors should remove the need for most liquid reagents, decrease power requirements, and shrink the size of handheld testing devices 5. These tools will be capable of performing a wide variety of chemical and biochemical assays built on top of a single sensor manufacturing chain, leading to lower overall cost for biological measurements. To demonstrate and validate this approach, we have commercially produced and sold a digital biosensor based on graphene-enabled Field Effect Biosensing (FEB) 6. These sensors can be described as a biologically specialized Ion Sensitive Field Effect Transistor (ISFET) 7. We describe here the sensing mechanism, demonstrate a label-free capture assay, and summarize the critical manufacturing and quality control milestones met during recent sensor production. The unique attributes that are required to build effective field effect biosensors are a combination of semiconductor behavior with chemical stability of the sensor surface in air and salt water 8. Materials like silicon require oxide layers between the transistor channel and the environment, limiting the sensitivity of field effect sensors made using those conventional materials 5. Materials, such as graphene, carbon nanotubes, and molybde-num disulfide have the unique combination of chemical stability and electric field sensitivity desirable to create sensitive electronic interfaces to biological molecules 9. This has led to a dense literature covering chemical and biological sensors using these materials 10-22. Several attempts were made to produce carbon nanotube biosensors for biomedical use in the early part of the 21 st century with frustrating results due to manufacturing difficulties 16. Fabrication techniques using molybdenum disulfide have not matured sufficiently for devices to move beyond the proof of concept stage 17 .},
	author = {Goldsmith, Brett R and Locascio, Lauren and Gao, Yingning and Lerner, Mitchell and Walker, Amy and Lerner, Jeremy and Kyaw, Jayla and shue, Angela and Afsahi, savannah and pan, Deng and Nokes, Jolie and Barron, Francie},
}

@article{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {https://arxiv.org/abs/2104.14294},
	abstract = {In this paper, we question if self-supervised learning provides new
properties to Vision Transformer (ViT) that stand out compared to convolutional
networks (convnets). Beyond the fact that adapting self-supervised methods to
this architecture works particularly well, we make the following observations:
first, self-supervised ViT features contain explicit information about the
semantic segmentation of an image, which does not emerge as clearly with
supervised ViTs, nor with convnets. Second, these features are also excellent
k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study
also underlines the importance of momentum encoder, multi-crop training, and
the use of small patches with ViTs. We implement our findings into a simple
self-supervised method, called DINO, which we interpret as a form of
self-distillation with no labels. We show the synergy between DINO and ViTs by
achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = apr,
	year = {2021},
	file = {Full Text:/home/zwerg/Zotero/storage/VYAW8KNK/Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf:application/pdf},
}

@article{assran_semi-supervised_2021,
	title = {Semi-{Supervised} {Learning} of {Visual} {Features} by {Non}-{Parametrically} {Predicting} {View} {Assignments} with {Support} {Samples}},
	url = {http://arxiv.org/abs/2104.13963},
	abstract = {This paper proposes a novel method of learning by predicting view assignments with support samples (PAWS). The method trains a model to minimize a consistency loss, which ensures that different views of the same unlabeled instance are assigned similar pseudo-labels. The pseudo-labels are generated non-parametrically, by comparing the representations of the image views to those of a set of randomly sampled labeled images. The distance between the view representations and labeled representations is used to provide a weighting over class labels, which we interpret as a soft pseudo-label. By non-parametrically incorporating labeled samples in this way, PAWS extends the distance-metric loss used in self-supervised methods such as BYOL and SwAV to the semi-supervised setting. Despite the simplicity of the approach, PAWS outperforms other semi-supervised methods across architectures, setting a new state-of-the-art for a ResNet-50 on ImageNet trained with either 10\% or 1\% of the labels, reaching 75.5\% and 66.5\% top-1 respectively. PAWS requires 4x to 12x less training than the previous best methods.},
	author = {Assran, Mahmoud and Caron, Mathilde and Misra, Ishan and Bojanowski, Piotr and Joulin, Armand and Ballas, Nicolas and Rabbat, Michael},
	month = apr,
	year = {2021},
}

@techreport{tan_efficientnetv2_nodate,
	title = {{EfficientNetV2}: {Smaller} {Models} and {Faster} {Training}},
	url = {https://github.com/},
	abstract = {This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller. Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose an improved method of progressive learning, which adaptively adjusts regularization (e.g., dropout and data augmentation) along with image size. With progressive learning, our EfficientNetV2 significantly outperforms previous models on Im-ageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our Effi-cientNetV2 achieves 87.3\% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0\% accuracy while training 5x-11x faster using the same computing resources. Code will be available at https://github.com/ google/automl/efficientnetv2.},
	author = {Tan, Mingxing and Le, Quoc V},
}

@article{jiang_transgan_2021,
	title = {{TransGAN}: {Two} {Transformers} {Can} {Make} {One} {Strong} {GAN}},
	url = {http://arxiv.org/abs/2102.07074},
	abstract = {The recent explosive interest on transformers has suggested their potential to become powerful "universal" models for computer vision tasks, such as classification, detection, and segmentation. However, how further transformers can go - are they ready to take some more notoriously difficult vision tasks, e.g., generative adversarial networks (GANs)? Driven by that curiosity, we conduct the first pilot study in building a GAN {\textbackslash}textbf\{completely free of convolutions\}, using only pure transformer-based architectures. Our vanilla GAN architecture, dubbed {\textbackslash}textbf\{TransGAN\}, consists of a memory-friendly transformer-based generator that progressively increases feature resolution while decreasing embedding dimension, and a patch-level discriminator that is also transformer-based. We then demonstrate TransGAN to notably benefit from data augmentations (more than standard GANs), a multi-task co-training strategy for the generator, and a locally initialized self-attention that emphasizes the neighborhood smoothness of natural images. Equipped with those findings, TransGAN can effectively scale up with bigger models and high-resolution image datasets. Specifically, our best architecture achieves highly competitive performance compared to current state-of-the-art GANs based on convolutional backbones. Specifically, TransGAN sets {\textbackslash}textbf\{new state-of-the-art\} IS score of 10.10 and FID score of 25.32 on STL-10. It also reaches competitive 8.64 IS score and 11.89 FID score on Cifar-10, and 12.23 FID score on CelebA \$64{\textbackslash}times64\$, respectively. We also conclude with a discussion of the current limitations and future potential of TransGAN. The code is available at {\textbackslash}url\{https://github.com/VITA-Group/TransGAN\}.},
	author = {Jiang, Yifan and Chang, Shiyu and Wang, Zhangyang},
	month = feb,
	year = {2021},
}

@techreport{oshea_radio_nodate,
	title = {Radio {Transformer} {Networks}: {Attention} {Models} for {Learning} to {Synchronize} in {Wireless} {Systems}},
	abstract = {We introduce learned attention models into the radio machine learning domain for the task of modulation recognition by leveraging spatial transformer networks and introducing new radio domain appropriate transformations. This attention model allows the network to learn a localization network capable of synchronizing and normalizing a radio signal blindly with zero knowledge of the signal's structure based on optimization of the network for classification accuracy, sparse representation, and regularization. Using this architecture we are able to outperform our prior results in accuracy vs signal to noise ratio against an identical system without attention, however we believe such an attention model has implication far beyond the task of modulation recognition.},
	number = {1605.00716v1},
	author = {O'shea, Timothy J and Pemula, Latha and Batra, Dhruv and Clancy, T Charles},
	keywords = {Deep Learning, Machine Learning, Attention Models, Cognitive Radio, Con-volutional Autoencoders, Neural Networks, Radio communica-tions, Radio Transformer Networks, RadioML, Signal Processing, Software Radio, Spatial Transformer Networks, Synchronization},
}

@article{cates_highly_2017,
	title = {Highly multiplexed signal readout for a time-of-flight positron emission tomography detector based on silicon photomultipliers},
	volume = {4},
	doi = {10.1117/1.JMI.4.1.011012},
	abstract = {, "Highly multiplexed signal readout for a time-of-flight positron emission tomography detector based on silicon photomultipliers," Abstract. Maintaining excellent timing resolution in the generation of silicon photomultiplier (SiPM)-based time-of-flight positron emission tomography (TOF-PET) systems requires a large number of high-speed, high-band-width electronic channels and components. To minimize the cost and complexity of a system's back-end architecture and data acquisition, many analog signals are often multiplexed to fewer channels using techniques that encode timing, energy, and position information. With progress in the development SiPMs having lower dark noise, after pulsing, and cross talk along with higher photodetection efficiency, a coincidence timing resolution (CTR) well below 200 ps FWHM is now easily achievable in single pixel, bench-top setups using 20-mm length, lutetium-based inorganic scintillators. However, multiplexing the output of many SiPMs to a single channel will significantly degrade CTR without appropriate signal processing. We test the performance of a PET detector readout concept that multiplexes 16 SiPMs to two channels. One channel provides timing information with fast comparators, and the second channel encodes both position and energy information in a time-over-threshold-based pulse sequence. This multiplexing readout concept was constructed with discrete components to process signals from a 4 × 4 array of SensL MicroFC-30035 SiPMs coupled to 2.9 × 2.9 × 20 mm 3 Lu 1.8 Gd 0.2 SiO 5 (LGSO):Ce (0.025 mol. \%) scintillators. This readout method yielded a calibrated, global energy resolution of 15.3\% FWHM at 511 keV with a CTR of 198 AE 2 ps FWHM between the 16-pixel multiplexed detector array and a 2.9 × 2.9 × 20 mm 3 LGSO-SiPM reference detector. In summary, results indicate this multiplexing scheme is a scalable readout technique that provides excellent coincidence timing performance.},
	number = {1},
	journal = {J. Med. Imag},
	author = {Cates, Joshua W and Bieniosek, Matthew F and Levin, Craig S},
	year = {2017},
	note = {Publisher: SPIE},
	keywords = {coincidence time resolution, fast timing Paper 16162SSRR, multiplexing, silicon photomultipliers, time-of-flight positron emission tomography},
	pages = {11012--11012},
}

@article{bieniosek_compact_nodate,
	title = {Compact {Pulse} {Width} {Modulation} {Circuitry} for {Silicon} {Photomultiplier} {Readout}},
	doi = {10.1088/0031-9155/58/15/5049},
	abstract = {The adoption of solid state photo-detectors for positron emission tomography (PET) system design and the interest in 3D interaction information from PET detectors has lead to an increasing number of readout channels in PET systems. To handle these additional readout channels, PET readout electronics should be simplified to reduce the power consumption, cost, and size of the electronics for a single channel. Pulse width modulation (PWM), where detector pulses are converted to digital pulses with width proportional to the detected photon energy, promises to simplify PET readout by converting the signals to digital form at the beginning of the processing chain, and allowing a single time-to-digital converter to perform the data acquisition for many channels rather than routing many analog channels and digitizing in the back end. Integrator based PWM systems, also known as charge-to-time converters (QTC), are especially compact, reducing the front-end electronics to an op-amp integrator with a resistor discharge, and a comparator. QTCs, however, have a long dead-time during which dark count noise is integrated, reducing the output signal to noise ratio. This work presents a QTC based PWM circuit with a gated integrator that shows performance improvements over existing QTC based PWM. By opening and closing an analog switch on the input of the integrator, the circuit can be controlled to integrate only the portions of the signal with a high signal-to-noise ratio. It also allows for multiplexing different detectors into the same PWM circuit while avoiding uncorrelated noise propagation between photodetector channels. Four gated integrator PWM circuits were built to readout the spatial channels of two position sensitive solid state photomultiplier (PS-SSPM). Results show a 4×4 array 0.9mm×0.9mm×15mm of LYSO crystals being identified on the 5mm×5mm PS-SSPM at room temperature with no degradation for 2-fold multiplexing. In principle, much larger multiplexing ratios are possible, limited only by count rate issues.},
	author = {Bieniosek, M F and Olcott, P D and Levin, C S},
}

@techreport{noauthor_arlington_nodate,
	title = {in {Arlington}, {VA}},
}

@article{noauthor_application_nodate,
	title = {Application {Note}: {TauSense}: the potential of {STELLARIS}},
	url = {https://www.nature.com/articles/d42473-020-00364-w},
}

@article{kettunen_e-lpips_2019,
	title = {E-{LPIPS}: {Robust} {Perceptual} {Image} {Similarity} via {Random} {Transformation} {Ensembles}},
	url = {http://arxiv.org/abs/1906.03973},
	abstract = {It has been recently shown that the hidden variables of convolutional neural networks make for an efficient perceptual similarity metric that accurately predicts human judgment on relative image similarity assessment. First, we show that such learned perceptual similarity metrics (LPIPS) are susceptible to adversarial attacks that dramatically contradict human visual similarity judgment. While this is not surprising in light of neural networks' well-known weakness to adversarial perturbations, we proceed to show that self-ensembling with an infinite family of random transformations of the input --- a technique known not to render classification networks robust --- is enough to turn the metric robust against attack, while retaining predictive power on human judgments. Finally, we study the geometry imposed by our our novel self-ensembled metric (E-LPIPS) on the space of natural images. We find evidence of "perceptual convexity" by showing that convex combinations of similar-looking images retain appearance, and that discrete geodesics yield meaningful frame interpolation and texture morphing, all without explicit correspondences.},
	journal = {arXiv},
	author = {Kettunen, Markus and Härkönen, Erik and Lehtinen, Jaakko},
	month = jun,
	year = {2019},
	note = {Publisher: arXiv},
}

@article{noauthor_richzhangperceptualsimilarity_nodate,
	title = {richzhang/{PerceptualSimilarity}: {LPIPS} metric. pip install lpips},
	url = {https://github.com/richzhang/PerceptualSimilarity},
}

@article{noauthor_messengers_nodate,
	title = {Messengers of hope},
	url = {https://doi.org/10.1038/s41587-020-00807-1},
	doi = {10.1038/s41587-020-00807-1},
}

@techreport{caron_unsupervised_nodate,
	title = {Unsupervised {Learning} of {Visual} {Features} by {Contrasting} {Cluster} {Assignments}},
	url = {https://github.com/facebookresearch/swav},
	abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of con-trastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or "views") of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a "swapped" prediction mechanism where we predict the code of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
	author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
}

@article{hoffman_promise_2021,
	title = {The promise and peril of deep learning in microscopy},
	volume = {18},
	url = {https://www.nature.com/articles/s41592-020-01035-w},
	doi = {10.1038/s41592-020-01035-w},
	number = {2},
	journal = {Nature Methods},
	author = {Hoffman, David P. and Slavitt, Isaac and Fitzpatrick, Casey A.},
	month = feb,
	year = {2021},
	note = {Publisher: Nature Research},
	keywords = {resolution microscopy, Super, Research data},
	pages = {131--132},
}

@inproceedings{jiang_super_2018,
	title = {Super {SloMo}: {High} {Quality} {Estimation} of {Multiple} {Intermediate} {Frames} for {Video} {Interpolation}},
	isbn = {978-1-5386-6420-9},
	url = {http://arxiv.org/abs/1712.00080},
	doi = {10.1109/CVPR.2018.00938},
	abstract = {Given two consecutive frames, video interpolation aims at generating intermediate frame(s) to form both spatially and temporally coherent video sequences. While most existing methods focus on single-frame interpolation, we propose an end-to-end convolutional neural network for variable-length multi-frame video interpolation, where the motion interpretation and occlusion reasoning are jointly modeled. We start by computing bi-directional optical flow between the input images using a U-Net architecture. These flows are then linearly combined at each time step to approximate the intermediate bi-directional optical flows. These approximate flows, however, only work well in locally smooth regions and produce artifacts around motion boundaries. To address this shortcoming, we employ another U-Net to refine the approximated flow and also predict soft visibility maps. Finally, the two input images are warped and linearly fused to form each intermediate frame. By applying the visibility maps to the warped images before fusion, we exclude the contribution of occluded pixels to the interpolated intermediate frame to avoid artifacts. Since none of our learned network parameters are time-dependent, our approach is able to produce as many intermediate frames as needed. To train our network, we use 1,132 240-fps video clips, containing 300K individual video frames. Experimental results on several datasets, predicting different numbers of interpolated frames, demonstrate that our approach performs consistently better than existing methods.},
	publisher = {IEEE Computer Society},
	author = {Jiang, Huaizu and Sun, Deqing and Jampani, Varan and Yang, Ming Hsuan and Learned-Miller, Erik and Kautz, Jan},
	month = dec,
	year = {2018},
	pages = {9000--9008},
}

@article{srinivas_bottleneck_2021,
	title = {Bottleneck {Transformers} for {Visual} {Recognition}},
	url = {http://arxiv.org/abs/2101.11605},
	abstract = {We present BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classification, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet and no other changes, our approach improves upon the baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency. Through the design of BoTNet, we also point out how ResNet bottleneck blocks with self-attention can be viewed as Transformer blocks. Without any bells and whistles, BoTNet achieves 44.4\% Mask AP and 49.7\% Box AP on the COCO Instance Segmentation benchmark using the Mask R-CNN framework; surpassing the previous best published single model and single scale results of ResNeSt evaluated on the COCO validation set. Finally, we present a simple adaptation of the BoTNet design for image classification, resulting in models that achieve a strong performance of 84.7\% top-1 accuracy on the ImageNet benchmark while being up to 2.33x faster in compute time than the popular EfficientNet models on TPU-v3 hardware. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision.},
	author = {Srinivas, Aravind and Lin, Tsung-Yi and Parmar, Niki and Shlens, Jonathon and Abbeel, Pieter and Vaswani, Ashish},
	month = jan,
	year = {2021},
}

@techreport{fehrenbach_variational_nodate,
	title = {Variational algorithms to remove stationary noise. {Applications} to microscopy imaging},
	url = {www.math.univ-toulouse.fr/},
	abstract = {A framework and an algorithm are presented in order to remove stationary noise from images. This algorithm is called VSNR (Variational Stationary Noise Remover). It can be interpreted both as a restoration method in a Bayesian framework and as a cartoon+texture decomposition method. In numerous denoising applications the white noise assumption fails: structured patterns (e.g. stripes) appear in the images. The model described here addresses these cases. Applications are presented with images acquired using different modalities: scanning electron microscope, FIB-nanotomography, and an emerging fluorescence microscopy technique called SPIM (Selective Plane Illumination Microscope).},
	author = {Fehrenbach, Jérôme and Weiss, Pierre and Lorenzo, Corinne},
	keywords = {Atomic Force Microscope, convex optimization, Index Terms-Stripe removal, Light Sheet Fluorescence Microscope, nanotomography, non linear filtering, primal-dual scheme, Scanning Electron Micro-scope, stationary noise, texture-geometry decomposition, total variation},
}

@article{jonsson_essential_2017,
	title = {Essential chemistry for biochemists},
	volume = {61},
	url = {/pmc/articles/PMC5869253/},
	doi = {10.1042/EBC20160094},
	abstract = {Within every living organism, countless reactions occur every second. These reactions typically occur more rapidly and with greater efficiency than would be possible under the same conditions in the chemical laboratory, and while using only the subset of elements that are readily available in nature. Despite these apparent differences between life and the laboratory, biological reactions are governed by the same rules as any other chemical reaction. Thus, a firm understanding of the fundamentals of chemistry is invaluable in biochemistry. There are entire textbooks devoted to the application of chemical principles in biological systems and so it is not possible to cover all of the relevant topics in depth in this short article. The aim is instead to provide a brief overview of those areas in chemistry that are most relevant to biochemistry. We summarize the basic principles, give examples of how these principles are applied in biological systems and suggest further reading on individual topics.},
	number = {4},
	journal = {Essays in Biochemistry},
	author = {Jonsson, Amanda L. and Roberts, Mark A.J. and Kiappes, J. L. and Scott, Kathryn A.},
	month = oct,
	year = {2017},
	note = {Publisher: Portland Press Ltd},
	pages = {401--427},
}

@article{shifat-e-rabbi_cell_2020,
	title = {Cell {Image} {Classification}: {A} {Comparative} {Overview}},
	volume = {97},
	doi = {10.1002/cyto.a.23984},
	abstract = {Cell image classification methods are currently being used in numerous applications in cell biology and medicine. Applications include understanding the effects of genes and drugs in screening experiments, understanding the role and subcellular localization of different proteins, as well as diagnosis and prognosis of cancer from images acquired using cytological and histological techniques. The article also reviews three main approaches for cell image classification most often used: numerical feature extraction, end-to-end classification with neural networks (NNs), and transport-based morphometry (TBM). In addition, we provide comparisons on four different cell imaging datasets to highlight the relative strength of each method. The results computed using four publicly available datasets show that numerical features tend to carry the best discriminative information for most of the classification tasks. Results also show that NN-based methods produce state-of-the-art results in the dataset that contains a relatively large number of training samples. Data augmentation or the choice of a more recently reported architecture does not necessarily improve the classification performance of NNs in the datasets with limited number of training samples. If understanding and visualization are desired aspects, TBM methods can offer the ability to invert classification functions, and thus can aid in the interpretation of results. These and other comparison outcomes are discussed with the aim of clarifying the advantages and disadvantages of each method. © 2020 International Society for Advancement of Cytometry.},
	number = {4},
	journal = {Cytometry Part A},
	author = {Shifat-E-Rabbi, Mohammad and Yin, Xuwang and Fitzgerald, Cailey E. and Rohde, Gustavo K.},
	year = {2020},
	keywords = {cell biology, computational biology, digital pathology, image informatics},
	pages = {347--362},
}

@article{vijayalakshmi_novel_2011,
	title = {A {Novel} {Approach} to {Texture} classification using statistical feature},
	url = {http://arxiv.org/abs/1111.2391},
	abstract = {Texture is an important spatial feature which plays a vital role in content based image retrieval. The enormous growth of the internet and the wide use of digital data have increased the need for both efficient image database creation and retrieval procedure. This paper describes a new approach for texture classification by combining statistical texture features of Local Binary Pattern and Texture spectrum. Since most significant information of a texture often appears in the high frequency channels, the features are extracted by the computation of LBP and Texture Spectrum and Legendre Moments. Euclidean distance is used for similarity measurement. The experimental result shows that 97.77\% classification accuracy is obtained by the proposed method.},
	author = {Vijayalakshmi, B. and Bharathi, V. Subbiah},
	year = {2011},
	keywords = {legendre moment, local binary pattern, similarity measure, texture features, texture spectrum},
}

@book{ali_ismail_awad_image_2016,
	series = {Studies in {Computational} {Intelligence}},
	title = {Image {Feature} {Detectors} and {Descriptors}},
	volume = {630},
	isbn = {978-3-319-28854-3},
	abstract = {This book provides readers with a selection of high-quality chapters that cover both theoretical concepts and practical applications of image feature detectors and descriptors. It serves as a reference for researchers and practitioners by featuring survey chapters and research contributions on image feature detectors and descriptors. Additionally, it emphasizes several keywords in both theoretical and practical aspects of image feature extraction. The keywords include acceleration of feature detection and extraction, hardware implantations, image segmentation, evolutionary algorithm, ordinal measures, as well as visual speech recognition.},
	author = {{Ali Ismail Awad} and {Mahmoud Hassaballah}},
	year = {2016},
	doi = {10.1007/978-3-319-28854-3},
	note = {Issue: February
Pages: 45},
}

@article{lacombe_modal_2020,
	title = {Modal features for image texture classification},
	volume = {135},
	doi = {10.1016/j.patrec.2020.04.036},
	abstract = {Feature extraction is a key step in image processing for pattern recognition and machine learning processes. Its purpose lies in reducing the dimensionality of the input data through the computing of features which accurately describe the original information. In this article, a new feature extraction method based on Discrete Modal Decomposition (DMD) is introduced, to extend the group of space and frequency based features. These new features are called modal features. Initially aiming to decompose a signal into a modal basis built from a vibration mechanics problem, the DMD projection is applied to images in order to extract modal features with two approaches. The first one, called full scale DMD, consists in exploiting directly the decomposition resulting coordinates as features. The second one, called filtering DMD, consists in using the DMD modes as filters to obtain features through a local transformation process. Experiments are performed on image texture classification tasks including several widely used data bases, compared to several classic feature extraction methods. We show that the DMD approach achieves good classification performances, comparable to the state of the art techniques, with a lower extraction time.},
	journal = {Pattern Recognition Letters},
	author = {Lacombe, Thomas and Favreliere, Hugues and Pillet, Maurice},
	year = {2020},
	keywords = {Feature extraction, Discrete modal decomposition, Texture classification},
	pages = {249--255},
}

@article{tamura_textural_1978,
	title = {Textural {Features} {Corresponding} to {Visual} {Perception}},
	volume = {8},
	doi = {10.1109/TSMC.1978.4309999},
	abstract = {Textural features corresponding to human visual perception are very useful for optimum feature selection and texture analyzer design. We approximated in computational form six basic textural features, namely, coarseness, contrast, directionality, line-likeness, regularity, and roughness. In comparison with psychological measurements for human subjects, the computational measures gave good correspondences in rank correlation of 16 typical texture patterns. Similarity measurements using these features were attempted. The discrepancies between human vision and computerized techniques that we encountered in this study indicate fundamental problems in digital analysis of textures. Some of them could be overcome by analyzing their causes and using more sophisticated techniques. Copyright © 1978 by The Institute of Electrical and Electronics Engineers, Inc.},
	number = {6},
	journal = {IEEE Transactions on Systems, Man and Cybernetics},
	author = {Tamura, Hideyuki and Mori, Shunji and Yamawaki, Takashi},
	year = {1978},
	pages = {460--473},
}

@article{chen_transunet_2021,
	title = {{TransUNet}: {Transformers} {Make} {Strong} {Encoders} for {Medical} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/2102.04306},
	abstract = {Medical image segmentation is an essential prerequisite for developing healthcare systems, especially for disease diagnosis and treatment planning. On various medical image segmentation tasks, the u-shaped architecture, also known as U-Net, has become the de-facto standard and achieved tremendous success. However, due to the intrinsic locality of convolution operations, U-Net generally demonstrates limitations in explicitly modeling long-range dependency. Transformers, designed for sequence-to-sequence prediction, have emerged as alternative architectures with innate global self-attention mechanisms, but can result in limited localization abilities due to insufficient low-level details. In this paper, we propose TransUNet, which merits both Transformers and U-Net, as a strong alternative for medical image segmentation. On one hand, the Transformer encodes tokenized image patches from a convolution neural network (CNN) feature map as the input sequence for extracting global contexts. On the other hand, the decoder upsamples the encoded features which are then combined with the high-resolution CNN feature maps to enable precise localization. We argue that Transformers can serve as strong encoders for medical image segmentation tasks, with the combination of U-Net to enhance finer details by recovering localized spatial information. TransUNet achieves superior performances to various competing methods on different medical applications including multi-organ segmentation and cardiac segmentation. Code and models are available at https://github.com/Beckschen/TransUNet.},
	author = {Chen, Jieneng and Lu, Yongyi and Yu, Qihang and Luo, Xiangde and Adeli, Ehsan and Wang, Yan and Lu, Le and Yuille, Alan L. and Zhou, Yuyin},
	month = feb,
	year = {2021},
}

@article{noauthor_texturalfeaturespdf_1973,
	title = {{TexturalFeatures}.pdf},
	year = {1973},
	note = {Publisher: IEEE},
	keywords = {Haralick},
}

@article{omahony_deep_2020,
	title = {Deep {Learning} vs. {Traditional} {Computer} {Vision}},
	volume = {943},
	issn = {9783030177942},
	doi = {10.1007/978-3-030-17795-9_10},
	abstract = {Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will also explore how the two sides of computer vision can be combined. Several recent hybrid methodologies are reviewed which have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learning. For example, combining traditional computer vision techniques with Deep Learning has been popular in emerging domains such as Panoramic Vision and 3D vision for which Deep Learning models have not yet been fully optimised.},
	number = {Cv},
	journal = {Advances in Intelligent Systems and Computing},
	author = {O’Mahony, Niall and Campbell, Sean and Carvalho, Anderson and Harapanahalli, Suman and Hernandez, Gustavo Velasco and Krpalkova, Lenka and Riordan, Daniel and Walsh, Joseph},
	year = {2020},
	keywords = {Deep learning, Computer vision, Hybrid techniques},
	pages = {128--144},
}

@article{weigert_biobeammultiplexed_2018-1,
	title = {Biobeam—{Multiplexed} wave-optical simulations of light-sheet microscopy},
	volume = {14},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1006079},
	doi = {10.1371/journal.pcbi.1006079},
	abstract = {Sample-induced image-degradation remains an intricate wave-optical problem in light-sheet microscopy. Here we present biobeam, an open-source software package that enables simulation of operational light-sheet microscopes by combining data from 105–106multiplexed and GPU-accelerated point-spread-function calculations. The wave-optical nature of these simulations leads to the faithful reproduction of spatially varying aberrations, diffraction artifacts, geometric image distortions, adaptive optics, and emergent wave-optical phenomena, and renders image-formation in light-sheet microscopy computationally tractable.},
	number = {4},
	journal = {PLOS Computational Biology},
	author = {Weigert, Martin and Subramanian, Kaushikaram and Bundschuh, Sebastian T. and Myers, Eugene W. and Kreysing, Moritz},
	editor = {Jbabdi, Saad},
	month = apr,
	year = {2018},
	note = {Publisher: Public Library of Science},
	keywords = {Light, Fluorescence imaging, Adaptive optics, Computer software, Diffraction, Digital video imaging microscopy, Refractive index, Simulation and modeling},
	pages = {e1006079--e1006079},
}

@article{niva_solemates_2003,
	title = {{SOLEMATES} {PROGRAMME} {Shallow} raceway design {Prepared} by {Akvaplan} {Niva} {October} 2003},
	number = {October},
	author = {Niva, Akvaplan},
	year = {2003},
}

@article{oiestad_shallow_1999,
	title = {Shallow raceways as a compact, resource-maximizing farming procedure for marine fish species},
	volume = {30},
	doi = {10.1046/j.1365-2109.1999.00408.x},
	abstract = {To develop a compact farming system, a shallow raceway has been tested on five flatfish, three bottom-dwelling species and three open-water species. The main effort has been on turbot Scophthalmus maximus L., Atlantic halibut Hippoglossus hippoglossus L. and spotted wolffish Anarhichas minor (Olafsen). The system is almost a standard raceway, but with a very low water level (7 mm for fish at about 100 mg; 25 cm for fish above 2 kg). The system is a package with some crucial preconditions. These include high fish density (often 100500 kg m-3), no countercurrent in the levelled raceways (no jet currents), adjustment of water intake with the most remote fish in mind and feeding with floating pellets. Self-cleaning is easily obtained in the system. The system has been tested for a wide size range of raceways (0.07-80 m2) and fish sizes (2 mg to 10 kg), normally with growth and survival rates as good as with traditional rearing systems. There seems to be a 'learning window'; the earlier the fish are introduced to shallow raceways, the better they perform. The results indicate that a variety of fish species can be produced in shallow raceways; these can be stacked in racks, which facilitate re-use of water from level to level. A very compact and cost-effective farming system will be the outcome.},
	number = {11-12},
	journal = {Aquaculture Research},
	author = {Øiestad, V.},
	year = {1999},
	pages = {831--840},
}

@article{a_goodall_predicting_nodate,
	title = {Predicting materials properties without crystal structure: deep representation learning from stoichiometry},
	url = {https://doi.org/10.1038/s41467-020-19964-7},
	doi = {10.1038/s41467-020-19964-7},
	abstract = {Machine learning has the potential to accelerate materials discovery by accurately predicting materials properties at a low computational cost. However, the model inputs remain a key stumbling block. Current methods typically use descriptors constructed from knowledge of either the full crystal structure-therefore only applicable to materials with already char-acterised structures-or structure-agnostic fixed-length representations hand-engineered from the stoichiometry. We develop a machine learning approach that takes only the stoi-chiometry as input and automatically learns appropriate and systematically improvable descriptors from data. Our key insight is to treat the stoichiometric formula as a dense weighted graph between elements. Compared to the state of the art for structure-agnostic methods, our approach achieves lower errors with less data.},
	author = {A Goodall, Rhys E and Lee, Alpha A},
}

@article{shen_machine_2020,
	title = {From machine learning to deep learning: {Advances} in scoring functions for protein–ligand docking},
	volume = {10},
	doi = {10.1002/wcms.1429},
	abstract = {Molecule docking has been regarded as a routine tool for drug discovery, but its accuracy highly depends on the reliability of scoring functions (SFs). With the rapid development of machine learning (ML) techniques, ML-based SFs have gradually emerged as a promising alternative for protein–ligand binding affinity prediction and virtual screening, and most of them have shown significantly better performance than a wide range of classical SFs. Emergence of more data-hungry deep learning (DL) approaches in recent years further fascinates the exploitation of more accurate SFs. Here, we summarize the progress of traditional ML-based SFs in the last few years and provide insights into recently developed DL-based SFs. We believe that the continuous improvement in ML-based SFs can surely guide the early-stage drug design and accelerate the discovery of new drugs. This article is categorized under: Computer and Information Science {\textgreater} Chemoinformatics.},
	number = {1},
	journal = {Wiley Interdisciplinary Reviews: Computational Molecular Science},
	author = {Shen, Chao and Ding, Junjie and Wang, Zhe and Cao, Dongsheng and Ding, Xiaoqin and Hou, Tingjun},
	month = jan,
	year = {2020},
	note = {Publisher: Blackwell Publishing Inc.},
	keywords = {machine learning, deep learning, molecular docking, scoring function, structure-based drug design},
}

@techreport{ragoza_protein-ligand_2016,
	title = {Protein-{Ligand} {Scoring} with {Convolutional} {Neural} {Networks}},
	abstract = {Computational approaches to drug discovery can reduce the time and cost associated with experimental assays and enable the screening of novel chemotypes. Structure-based drug design methods rely on scoring functions to rank and predict binding affinities and poses. The ever-expanding amount of protein-ligand binding and structural data enables the use of deep machine learning techniques for protein-ligand scoring. We describe convolutional neural network (CNN) scoring functions that take as input a comprehensive 3D representation of a protein-ligand interaction. A CNN scoring function automatically learns the key features of protein-ligand interactions that correlate with binding. We train and optimize our CNN scoring functions to discriminate between correct and incorrect binding poses and known binders and non-binders. We find that our CNN scoring function outperforms the AutoDock Vina scoring function when ranking poses both for pose prediction and virtual screening.},
	author = {Ragoza, Matthew and Hochuli, Joshua and Idrobo, Elisa and Sunseri, Jocelyn and Koes, David Ryan},
	year = {2016},
}

@article{verma_ssnet_2019,
	title = {{SSnet} - {Secondary} structure based end-to-end learning model for protein-ligand interaction prediction},
	doi = {10.1101/2019.12.20.884841},
	abstract = {Computational prediction of bioactivity has become a critical aspect of modern drug discovery as it mitigates the cost, time, and resources required to find and screen new compounds. Deep Neural Networks (DNN) have recently shown excellent performance in modeling Protein-Ligand Interaction (PLI). However, DNNs are only effective when physically sound descriptions of ligands and proteins are fed into the network for further processing. Furthermore, previous research has not incorporated the secondary structure of the protein in a meaningful manner. In this work, we utilize secondary structure information of the protein which is extracted as the curvature and torsion of the backbone of protein to predict PLI. We demonstrate how our model outperforms previous machine and non-machine learning models on three major datasets: humans, C.elegans, and DUD-E. Visualization of the intermediate layers of our model shows a potential latent space for proteins which extracts important information about the activity of the protein. We further investigate the inner workings of our model by visualizing heatmaps through Grad-CAM. This analysis is adapted to visualize the most important aspects of the protein that the algorithm has learned. We observed that the important residues highlighted by Grad-CAM are the ones responsible for non-covalent interactions with a ligand and is not just confined to the binding site as it also includes allosteric sites and other locations where a ligand interacts. Our new model opens the door in exploration of DNN based on the secondary structure which is not just confined to protein ligand interactions.},
	journal = {bioRxiv},
	author = {Verma, Niraj and Qu, Xingming and Trozzi, Francesco and Elsaied, Mohamed and Tao, Yunwen and Larson, Eric and Kraka, Elfi},
	month = dec,
	year = {2019},
	note = {Publisher: bioRxiv},
}

@article{li_machine-learning_2020,
	title = {Machine-learning scoring functions for structure-based drug lead optimization},
	volume = {10},
	doi = {10.1002/wcms.1465},
	abstract = {Molecular docking can be used to predict how strongly small-molecule binders and their chemical derivatives bind to a macromolecular target using its available three-dimensional structures. Scoring functions (SFs) are employed to rank these molecules by their predicted binding affinity (potency). A classical SF assumes a predetermined theory-inspired functional form for the relationship between the features characterizing the structure of the protein–ligand complex and its predicted binding affinity (this relationship is almost always assumed to be linear). Recent years have seen the prosperity of machine-learning SFs, which are fast regression models built instead with contemporary supervised learning algorithms. In this review, we analyzed machine-learning SFs for drug lead optimization in the 2015–2019 period. The performance gap between classical and machine-learning SFs was large and has now broadened owing to methodological improvements and the availability of more training data. Against the expectations of many experts, SFs employing deep learning techniques were not always more predictive than those based on more established machine learning techniques and, when they were, the performance gain was small. More codes and webservers are available and ready to be applied to prospective structure-based drug lead optimization studies. These have exhibited excellent predictive accuracy in compelling retrospective tests, outperforming in some cases much more computationally demanding molecular simulation-based methods. A discussion of future work completes this review. This article is categorized under: Computer and Information Science {\textgreater} Chemoinformatics.},
	number = {5},
	journal = {Wiley Interdisciplinary Reviews: Computational Molecular Science},
	author = {Li, Hongjian and Sze, Kam Heung and Lu, Gang and Ballester, Pedro J.},
	month = sep,
	year = {2020},
	note = {Publisher: Blackwell Publishing Inc.},
	keywords = {machine learning, binding affinity prediction, lead optimization, molecular docking, scoring function, Structural bioinformatics},
}

@article{zheng_intracytoplasmic_2019,
	title = {Intracytoplasmic sperm injection ({ICSI}) versus conventional in vitro fertilisation ({IVF}) in couples with non-severe male infertility ({NSMI}-{ICSI}): {Protocol} for a multicentre randomised controlled trial},
	volume = {9},
	url = {/pmc/articles/PMC6773417/},
	doi = {10.1136/bmjopen-2019-030366},
	abstract = {Introduction Intracytoplasmic sperm injection (ICSI), originally introduced as add-on to in vitro fertilisation (IVF) for couples with severe male infertility, is in current clinical practice also used in couples with mild male or even unexplained infertility. However, ICSI has involved unresolved concerns regarding the selection and damage to gametes and the health conditions of the offspring, and it is also labour intensive and therefore more expensive than conventional IVF. High-quality well-powered randomised clinical trials (RCTs) comparing ICSI and IVF are lacking. Methods and analysis We propose a multicentre, open-label RCT in 10 reproductive medical centres across China. We will study couples with non-severe male infertility (defined as a semen concentrate 5-15×10 6/mL or sperm with a progressive motility 10\%-32\%) scheduled for their first or second ICSI or IVF cycle, as low fertility rate after fertilisation are more frequent in this population, which could lead to controversy about ICSI or conventional IVF for fertilisation. On the day of oocyte retrieval, eligible participants are after informed consent be randomised to undergo either ICSI or conventional IVF in a 1:1 treatment ratio. Other standard assisted reproductive treatments are similar and parallel between two groups. Our primary outcome is ongoing pregnancy leading to live birth after the first cycle with embryo transfer. To demonstrate or refute a difference of 7\% between ICSI and conventional IVF, we need to include 2346 women (1173 in each intervention arm). In addition, we will follow-up neonatal outcomes after delivery to identify the influence of ICSI on offspring. Ethics and dissemination Ethical approval was obtained from Peking University Third Hospital medical science research ethics committee. The findings will be disseminated to the public through conference presentations and peer-reviewed scientific journals.},
	number = {9},
	journal = {BMJ Open},
	author = {Zheng, Danni and Zeng, Lin and Yang, Rui and Lian, Ying and Zhu, Yi Min and Liang, Xiaoyan and Tang, Li and Wang, Huichun and Cao, Yunxia and Hao, Guimin and Liu, Jianqiao and Zhao, Junli and Wang, Rui and Mol, Ben Willem and Li, Rong and Huang, He Feng and Qiao, Jie},
	month = sep,
	year = {2019},
	note = {Publisher: BMJ Publishing Group},
	keywords = {assisted reproductive technology, in vitro fertilisation, intracytoplasmic sperm injection, non-severe male infertility},
}

@article{mccallum_deep_2019,
	title = {Deep learning-based selection of human sperm with high {DNA} integrity},
	volume = {2},
	doi = {10.1038/s42003-019-0491-6},
	abstract = {Despite the importance of sperm DNA to human reproduction, currently no method exists to assess individual sperm DNA quality prior to clinical selection. Traditionally, skilled clinicians select sperm based on a variety of morphological and motility criteria, but without direct knowledge of their DNA cargo. Here, we show how a deep convolutional neural network can be trained on a collection of {\textasciitilde}1000 sperm cells of known DNA quality, to predict DNA quality from brightfield images alone. Our results demonstrate moderate correlation (bivariate correlation {\textasciitilde}0.43) between a sperm cell image and DNA quality and the ability to identify higher DNA integrity cells relative to the median. This deep learning selection process is directly compatible with current, manual microscopy-based sperm selection and could assist clinicians, by providing rapid DNA quality predictions (under 10 ms per cell) and sperm selection within the 86th percentile from a given sample.},
	number = {1},
	journal = {Communications Biology},
	author = {McCallum, Christopher and Riordon, Jason and Wang, Yihe and Kong, Tian and You, Jae Bem and Sanner, Scott and Lagunov, Alexander and Hannam, Thomas G. and Jarvi, Keith and Sinton, David},
	month = dec,
	year = {2019},
	note = {Publisher: Nature Research},
}

@article{khosravi_deep_2019,
	title = {Deep learning enables robust assessment and selection of human blastocysts after in vitro fertilization},
	volume = {2},
	url = {https://doi.org/10.1038/s41746-019-0096-y},
	doi = {10.1038/s41746-019-0096-y},
	abstract = {Visual morphology assessment is routinely used for evaluating of embryo quality and selecting human blastocysts for transfer after in vitro fertilization (IVF). However, the assessment produces different results between embryologists and as a result, the success rate of IVF remains low. To overcome uncertainties in embryo quality, multiple embryos are often implanted resulting in undesired multiple pregnancies and complications. Unlike in other imaging fields, human embryology and IVF have not yet leveraged artificial intelligence (AI) for unbiased, automated embryo assessment. We postulated that an AI approach trained on thousands of embryos can reliably predict embryo quality without human intervention. We implemented an AI approach based on deep neural networks (DNNs) to select highest quality embryos using a large collection of human embryo time-lapse images (about 50,000 images) from a high-volume fertility center in the United States. We developed a framework (STORK) based on Google's Inception model. STORK predicts blastocyst quality with an AUC of {\textgreater}0.98 and generalizes well to images from other clinics outside the US and outperforms individual embryologists. Using clinical data for 2182 embryos, we created a decision tree to integrate embryo quality and patient age to identify scenarios associated with pregnancy likelihood. Our analysis shows that the chance of pregnancy based on individual embryos varies from 13.8\% (age ≥41 and poor-quality) to 66.3\% (age {\textless}37 and good-quality) depending on automated blastocyst quality assessment and patient age. In conclusion, our AI-driven approach provides a reproducible way to assess embryo quality and uncovers new, potentially personalized strategies to select embryos.},
	number = {1},
	journal = {npj Digital Medicine},
	author = {Khosravi, Pegah and Kazemi, Ehsan and Zhan, Qiansheng and Malmsten, Jonas E. and Toschi, Marco and Zisimopoulos, Pantelis and Sigaras, Alexandros and Lavery, Stuart and Cooper, Lee A. D. and Hickman, Cristina and Meseguer, Marcos and Rosenwaks, Zev and Elemento, Olivier and Zaninovic, Nikica and Hajirasouliha, Iman},
	month = dec,
	year = {2019},
	note = {Publisher: Springer Science and Business Media LLC},
	keywords = {Image processing, Machine learning},
	pages = {1--9},
}

@article{wang_artificial_2019,
	title = {Artificial intelligence in reproductive medicine},
	volume = {158},
	url = {/pmc/articles/PMC6733338/},
	doi = {10.1530/REP-18-0523},
	abstract = {Artificial intelligence (AI) has experienced rapid growth over the past few years, moving from the experimental to the implementation phase in various fields, including medicine. Advances in learning algorithms and theories, the availability of large datasets and improvements in computing power have contributed to breakthroughs in current AI applications. Machine learning (ML), a subset of AI, allows computers to detect patterns from large complex datasets automatically and uses these patterns to make predictions. AI is proving to be increasingly applicable to healthcare, and multiple machine learning techniques have been used to improve the performance of assisted reproductive technology (ART). Despite various challenges, the integration of AI and reproductive medicine is bound to give an essential direction to medical development in the future. In this review, we discuss the basic aspects of AI and machine learning, and we address the applications, potential limitations and challenges of AI. We also highlight the prospects and future directions in the context of reproductive medicine.},
	number = {4},
	journal = {Reproduction},
	author = {Wang, Renjie and Pan, Wei and Jin, Lei and Li, Yuehan and Geng, Yudi and Gao, Chun and Chen, Gang and Wang, Hui and Ma, Ding and Liao, Shujie},
	year = {2019},
	note = {Publisher: BioScientifica Ltd.},
	pages = {R139--R154},
}

@techreport{damour_underspecification_2020,
	title = {Underspecification in {Machine} {Learning} {Underspecification} {Presents} {Challenges} for {Credibility} in {Modern} {Machine} {Learning}},
	abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
	author = {D'amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and Mclean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D and D'amour, A and Heller, K and Moldovan, D},
	year = {2020},
	keywords = {medical imaging, computer vision, distribution shift, electronic health records, fairness, genomics, identifiability, natural language processing, spurious correlation},
}

@article{qiao_evaluation_2021,
	title = {Evaluation and development of deep neural networks for image super-resolution in optical microscopy},
	doi = {10.1038/s41592-020-01048-5},
	abstract = {Deep neural networks have enabled astonishing transformations from low-resolution (LR) to super-resolved images. However, whether, and under what imaging conditions, such deep-learning models outperform super-resolution (SR) microscopy is poorly explored. Here, using multimodality structured illumination microscopy (SIM), we first provide an extensive dataset of LR–SR image pairs and evaluate the deep-learning SR models in terms of structural complexity, signal-to-noise ratio and upscaling factor. Second, we devise the deep Fourier channel attention network (DFCAN), which leverages the frequency content difference across distinct features to learn precise hierarchical representations of high-frequency information about diverse biological structures. Third, we show that DFCAN’s Fourier domain focalization enables robust reconstruction of SIM images under low signal-to-noise ratio conditions. We demonstrate that DFCAN achieves comparable image quality to SIM over a tenfold longer duration in multicolor live-cell imaging experiments, which reveal the detailed structures of mitochondrial cristae and nucleoids and the interaction dynamics of organelles and cytoskeleton.},
	journal = {Nature Methods},
	author = {Qiao, Chang and Li, Di and Guo, Yuting and Liu, Chong and Jiang, Tao and Dai, Qionghai and Li, Dong},
	year = {2021},
}

@techreport{ramachandran_stand-alone_nodate-1,
	title = {Stand-{Alone} {Self}-{Attention} in {Vision} {Models}},
	url = {https://github.com/google-research/google-research/tree/master/standalone_self_attention_in_vision_models},
	abstract = {Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12\% fewer FLOPS and 29\% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39\% fewer FLOPS and 34\% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox. Code for this project is made available. 1},
	author = {Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jonathon},
}

@article{van_gansbeke_unsupervised_2021,
	title = {Unsupervised {Semantic} {Segmentation} by {Contrasting} {Object} {Mask} {Proposals}},
	url = {http://arxiv.org/abs/2102.06191},
	abstract = {Being able to learn dense semantic representations of images without supervision is an important problem in computer vision. However, despite its significance, this problem remains rather unexplored, with a few exceptions that considered unsupervised semantic segmentation on small-scale datasets with a narrow visual domain. In this paper, we make a first attempt to tackle the problem on datasets that have been traditionally utilized for the supervised case. To achieve this, we introduce a novel two-step framework that adopts a predetermined prior in a contrastive optimization objective to learn pixel embeddings. This marks a large deviation from existing works that relied on proxy tasks or end-to-end clustering. Additionally, we argue about the importance of having a prior that contains information about objects, or their parts, and discuss several possibilities to obtain such a prior in an unsupervised manner. Extensive experimental evaluation shows that the proposed method comes with key advantages over existing works. First, the learned pixel embeddings can be directly clustered in semantic groups using K-Means. Second, the method can serve as an effective unsupervised pre-training for the semantic segmentation task. In particular, when fine-tuning the learned representations using just 1\% of labeled examples on PASCAL, we outperform supervised ImageNet pre-training by 7.1\% mIoU. The code is available at https://github.com/wvangansbeke/Unsupervised-Semantic-Segmentation.},
	author = {Van Gansbeke, Wouter and Vandenhende, Simon and Georgoulis, Stamatios and Van Gool, Luc},
	month = feb,
	year = {2021},
}

@article{hollandi_nucleaizer_2020,
	title = {{nucleAIzer}: {A} {Parameter}-free {Deep} {Learning} {Framework} for {Nucleus} {Segmentation} {Using} {Image} {Style} {Transfer}},
	volume = {10},
	url = {http://dx.doi.org/10.1016/j.cels.2020.04.003},
	doi = {10.1016/j.cels.2020.04.003},
	abstract = {Single-cell segmentation is typically a crucial task of image-based cellular analysis. We present nucleAIzer, a deep-learning approach aiming toward a truly general method for localizing 2D cell nuclei across a diverse range of assays and light microscopy modalities. We outperform the 739 methods submitted to the 2018 Data Science Bowl on images representing a variety of realistic conditions, some of which were not represented in the training data. The key to our approach is that during training nucleAIzer automatically adapts its nucleus-style model to unseen and unlabeled data using image style transfer to automatically generate augmented training samples. This allows the model to recognize nuclei in new and different experiments efficiently without requiring expert annotations, making deep learning for nucleus segmentation fairly simple and labor free for most biological light microscopy experiments. It can also be used online, integrated into CellProfiler and freely downloaded at www.nucleaizer.org. A record of this paper's transparent peer review process is included in the Supplemental Information. Microscopy image analysis of single cells can be challenging but also eased and improved. We developed a deep learning method to segment cell nuclei. Our strategy is adapting to unexpected circumstances automatically by synthesizing artificial microscopy images in such a domain as training samples.},
	number = {5},
	journal = {Cell Systems},
	author = {Hollandi, Reka and Szkalisity, Abel and Toth, Timea and Tasnadi, Ervin and Molnar, Csaba and Mathe, Botond and Grexa, Istvan and Molnar, Jozsef and Balind, Arpad and Gorbe, Mate and Kovacs, Maria and Migh, Ede and Goodman, Allen and Balassa, Tamas and Koos, Krisztian and Wang, Wenyu and Caicedo, Juan Carlos and Bara, Norbert and Kovacs, Ferenc and Paavolainen, Lassi and Danka, Tivadar and Kriston, Andras and Carpenter, Anne Elizabeth and Smith, Kevin and Horvath, Peter},
	year = {2020},
	note = {Publisher: Elsevier},
	keywords = {deep learning, segmentation, high-content screening, cellular analysis, microscopy image analysis},
	pages = {453--458.e6},
}

@article{hollandi_deep_2019,
	title = {A deep learning framework for nucleus segmentation using image style transfer},
	doi = {10.1101/580605},
	abstract = {Single cell segmentation is typically one of the first and most crucial tasks of image-based cellular analysis. We present a deep learning approach aiming towards a truly general method for localizing nuclei across a diverse range of assays and light microscopy modalities. We outperform the 739 methods submitted to the 2018 Data Science Bowl on images representing a variety of realistic conditions, some of which were not represented in the training data. The key to our approach is to adapt our model to unseen and unlabeled data using image style transfer to generate augmented training samples. This allows the model to recognize nuclei in new and different experiments without requiring expert annotations.},
	number = {March},
	journal = {bioRxiv},
	author = {Hollandi, Reka and Szkalisity, Abel and Toth, Timea and Tasnadi, Ervin and Molnar, Csaba and Mathe, Botond and Grexa, Istvan and Molnar, Jozsef and Balind, Arpad and Gorbe, Mate and Kovacs, Maria and Migh, Ede and Goodman, Allen and Balassa, Tamas and Koos, Krisztian and Wang, Wenyu and Bara, Norbert and Kovacs, Ferenc and Paavolainen, Lassi and Danka, Tivadar and Kriston, Andras and Carpenter, Anne E. and Smith, Kevin and Horvath, Peter},
	year = {2019},
}

@article{pfaff_learning_2020,
	title = {Learning {Mesh}-{Based} {Simulation} with {Graph} {Networks}},
	url = {http://arxiv.org/abs/2010.03409},
	abstract = {Mesh-based simulations are central to modeling complex physical systems in many disciplines across science and engineering. Mesh representations support powerful numerical integration methods and their resolution can be adapted to strike favorable trade-offs between accuracy and efficiency. However, high-dimensional scientific simulations are very expensive to run, and solvers and parameters must often be tuned individually to each system studied. Here we introduce MeshGraphNets, a framework for learning mesh-based simulations using graph neural networks. Our model can be trained to pass messages on a mesh graph and to adapt the mesh discretization during forward simulation. Our results show it can accurately predict the dynamics of a wide range of physical systems, including aerodynamics, structural mechanics, and cloth. The model's adaptivity supports learning resolution-independent dynamics and can scale to more complex state spaces at test time. Our method is also highly efficient, running 1-2 orders of magnitude faster than the simulation on which it is trained. Our approach broadens the range of problems on which neural network simulators can operate and promises to improve the efficiency of complex, scientific modeling tasks.},
	journal = {arXiv},
	author = {Pfaff, Tobias and Fortunato, Meire and Sanchez-Gonzalez, Alvaro and Battaglia, Peter W.},
	month = oct,
	year = {2020},
	note = {Publisher: arXiv},
}

@techreport{mnih_playing_nodate,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
}

@article{mund_ai-driven_nodate,
	title = {{AI}-driven {Deep} {Visual} {Proteomics} defines cell identity and heterogeneity {Proteomics} {Program}, 2 {Protein} {Signaling} {Program}, and 3 {Protein} {Imaging} {Platform}},
	issn = {10.1101/2021.01.2},
	url = {https://doi.org/10.1101/2021.01.25.427969},
	doi = {10.1101/2021.01.25.427969},
	author = {Mund, Andreas and Coscia, Fabian and Hollandi, Réka and Kovács, Ferenc and Kriston, András and Brunner, Andreas-David and Bzorek, Michael and Naimy, Soraya and Mette Rahbek Gjerdrum, Lise and Dyring-Andersen, Beatrice and Bulkescher, Jutta and Lukas, Claudia and Gnann, Christian and Lundberg, Emma and Horvath, Peter and Mann, Matthias and Zuckerberg Biohub, Chan and Francisco, San},
}

@techreport{allen-zhu_towards_2020,
	title = {Towards {Understanding} {Ensemble}, {Knowledge} {Distillation} and {Self}-{Distillation} in {Deep} {Learning}},
	url = {https://www.microsoft.com/en-us/research/publication/towards-understanding-ensemble-knowledge-distillation-and-self-distillation-in-deep-learning/},
	abstract = {We formally study how ensemble of deep learning models can improve test accuracy, and how the superior performance of ensemble can be distilled into a single model using knowledge distillation. We consider the challenging case where the ensemble is simply an average of the outputs of a few independently trained neural networks with the same architecture, trained using the same algorithm on the same data set, and they only differ by the random seeds used in the initialization. We empirically show that ensemble/knowledge distillation in deep learning works very differently from traditional learning theory, especially differently from ensemble of random feature mappings or the neural-tangent-kernel feature mappings, and is potentially out of the scope of existing theorems. Thus, to properly understand ensemble and knowledge distillation in deep learning, we develop a theory showing that when data has a structure we refer to as "multi-view", then ensemble of independently trained neural networks can provably improve test accuracy, and such superior test accuracy can also be provably distilled into a single model by training a single model to match the output of the ensemble instead of the true label. Our result sheds light on how ensemble works in deep learning in a way that is completely different from traditional theorems, and how the "dark knowledge" is hidden in the outputs of the ensemble-that can be used in knowledge distillation-comparing to the true data labels. In the end, we prove that self-distillation can also be viewed as implicitly combining ensemble and knowledge distillation to improve test accuracy. * V1.5 appears on December 18, 2020 and V1.6 fixes minor typos. Future editions can be found at https: //arxiv.org/abs/2012.09816.},
	author = {Allen-Zhu, Zeyuan and Research, Microsoft and Li, Redmond Yuanzhi},
	month = jan,
	year = {2020},
}

@techreport{choi_channel_nodate,
	title = {Channel {Attention} {Is} {All} {You} {Need} for {Video} {Frame} {Interpolation}},
	url = {https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjBvqeaxKPwAhWMhv0HHWW6Ce0QFjAAegQIBxAD&url=https%3A%2F%2Fojs.aaai.org%2Findex.php%2FAAAI%2Farticle%2Fview%2F6693%2F6547&usg=AOvVaw2ejNjVcn60iRlP7vuiSqyh},
	abstract = {Prevailing video frame interpolation techniques rely heavily on optical flow estimation and require additional model complexity and computational cost; it is also susceptible to error propagation in challenging scenarios with large motion and heavy occlusion. To alleviate the limitation, we propose a simple but effective deep neural network for video frame interpolation , which is end-to-end trainable and is free from a motion estimation network component. Our algorithm employs a special feature reshaping operation, referred to as PixelShuf-fle, with a channel attention, which replaces the optical flow computation module. The main idea behind the design is to distribute the information in a feature map into multiple channels and extract motion information by attending the channels for pixel-level frame synthesis. The model given by this principle turns out to be effective in the presence of challenging motion and occlusion. We construct a comprehensive evaluation benchmark and demonstrate that the proposed approach achieves outstanding performance compared to the existing models with a component for optical flow computation.},
	author = {Choi, Myungsub and Kim, Heewon and Han, Bohyung and Xu, Ning and Lee, Kyoung Mu},
}

@techreport{li_megadepth_nodate,
	title = {{MegaDepth}: {Learning} {Single}-{View} {Depth} {Prediction} from {Internet} {Photos}},
	url = {http://www.cs.cornell.edu/projects/},
	abstract = {Single-view depth prediction is a fundamental problem in computer vision. Recently, deep learning methods have led to significant progress, but such methods are limited by the available training data. Current datasets based on 3D sensors have key limitations, including indoor-only images (NYU), small numbers of training examples (Make3D), and sparse sampling (KITTI). We propose to use multi-view In-ternet photo collections, a virtually unlimited data source, to generate training data via modern structure-from-motion and multi-view stereo (MVS) methods, and present a large depth dataset called MegaDepth based on this idea. Data derived from MVS comes with its own challenges, including noise and unreconstructable objects. We address these challenges with new data cleaning methods, as well as automatically augmenting our data with ordinal depth relations generated using semantic segmentation. We validate the use of large amounts of Internet data by showing that models trained on MegaDepth exhibit strong generalization-not only to novel scenes, but also to other diverse datasets including Make3D, KITTI, and DIW, even when no images from those datasets are seen during training. 1},
	author = {Li, Zhengqi and Snavely, Noah},
}

@article{viazovetskyi_stylegan2_2020,
	title = {{StyleGAN2} {Distillation} for {Feed}-forward {Image} {Manipulation}},
	url = {http://arxiv.org/abs/2003.03581},
	abstract = {StyleGAN2 is a state-of-the-art network in generating realistic images. Besides, it was explicitly trained to have disentangled directions in latent space, which allows efficient image manipulation by varying latent factors. Editing existing images requires embedding a given image into the latent space of StyleGAN2. Latent code optimization via backpropagation is commonly used for qualitative embedding of real world images, although it is prohibitively slow for many applications. We propose a way to distill a particular image manipulation of StyleGAN2 into image-to-image network trained in paired way. The resulting pipeline is an alternative to existing GANs, trained on unpaired data. We provide results of human faces' transformation: gender swap, aging/rejuvenation, style transfer and image morphing. We show that the quality of generation using our method is comparable to StyleGAN2 backpropagation and current state-of-the-art methods in these particular tasks.},
	author = {Viazovetskyi, Yuri and Ivashkin, Vladimir and Kashin, Evgeny},
	month = mar,
	year = {2020},
}

@article{lu_learning_2019,
	title = {Learning unsupervised feature representations for single cell microscopy images with paired cell inpainting},
	volume = {15},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1007348},
	doi = {10.1371/journal.pcbi.1007348},
	abstract = {Cellular microscopy images contain rich insights about biology. To extract this information, researchers use features, or measurements of the patterns of interest in the images. Here, we introduce a convolutional neural network (CNN) to automatically design features for fluorescence microscopy. We use a self-supervised method to learn feature representations of single cells in microscopy images without labelled training data. We train CNNs on a simple task that leverages the inherent structure of microscopy images and controls for variation in cell morphology and imaging: Given one cell from an image, the CNN is asked to predict the fluorescence pattern in a second different cell from the same image. We show that our method learns high-quality features that describe protein expression patterns in single cells both yeast and human microscopy datasets. Moreover, we demonstrate that our features are useful for exploratory biological analysis, by capturing high-resolution cellular components in a proteome-wide cluster analysis of human proteins, and by quantifying multi-localized proteins and single-cell variability. We believe paired cell inpainting is a generalizable method to obtain feature representations of single cells in multichannel microscopy images.},
	number = {9},
	journal = {PLOS Computational Biology},
	author = {Lu, Alex X. and Kraus, Oren Z. and Cooper, Sam and Moses, Alan M.},
	editor = {Caicedo, Juan},
	month = sep,
	year = {2019},
	note = {Publisher: Public Library of Science},
	pages = {e1007348--e1007348},
}

@article{sublett_effect_2018,
	title = {The effect of environment and nutrients on {Hydroponic} {Lettuce} yield, quality, and {Phytonutrients}},
	volume = {4},
	doi = {10.3390/horticulturae4040048},
	abstract = {A study was conducted with green and red-leaf lettuce cultivars grown in a deep-water culture production system. Plants were seeded in rockwool and germinated under greenhouse conditions at 25/20 °C (day/night) for 21 days before transplanting. The experimental design was a randomized complete block with a 2 × 3 factorial arrangement of cultivar and nutrient treatments that consisted of six replications. Treatments consisted of two lettuce genotypes, (1) green (Winter Density) and (2) red (Rhazes), and three nutrient treatments containing electroconductivity (EC) levels of (1) 1.0; (2) 2.0; and (3) 4.0 mScm−1. After 50 days, plants were harvested, processed, and analyzed to determine marketable yield, biomass, plant height, stem diameter, phenolics, and elemental nutrient concentrations. An interaction between growing season and lettuce cultivar was the predominant factor influencing yield, biomass, and quality. Nutrient solution EC treatment significantly affected biomass and water content. EC treatments significantly impacted concentrations of 3-O-glucoside and uptake of phosphorous, potassium, iron, boron, zinc, and molybdenum. Effects of growing season and cultivar on leafy lettuce yield and quality were more pronounced than the effect of nutrient solution EC treatment. Thus, greenhouse production of green and red-leaf lettuce cultivars in the south-eastern United States should be conducted in the spring and fall growing seasons with elevated nutrient solution EC of ≈4.0 mScm−1 to maximize yield and quality.},
	number = {4},
	journal = {Horticulturae},
	author = {Sublett, William L. and Barickman, T. Casey and Sams, Carl E.},
	month = dec,
	year = {2018},
	note = {Publisher: MDPI Multidisciplinary Digital Publishing Institute},
	keywords = {Electro-conductivity, Flavonoids, Phenolics, Polyphenols},
}

@techreport{shi_real-time_nodate-1,
	title = {Real-{Time} {Single} {Image} and {Video} {Super}-{Resolution} {Using} an {Efficient} {Sub}-{Pixel} {Convolutional} {Neural} {Network}},
	abstract = {Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.},
	author = {Shi, Wenzhe and Caballero, Jose and Huszár, Ferenc and Totz, Johannes and Aitken, Andrew P and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
}

@techreport{bao_depth-aware_nodate,
	title = {Depth-{Aware} {Video} {Frame} {Interpolation}},
	url = {http://arxiv.org/abs/1904.00830},
	abstract = {Overlayed inputs Estimated optical flow Estimated depth map Interpolated frame Ground-truth frame Figure 1. Example of video frame interpolation. We propose a depth-aware video frame interpolation approach to exploit the depth cue for detecting occlusion. Our method estimates optical flow with clear motion boundaries and thus generates high-quality frames. Abstract Video frame interpolation aims to synthesize non-existent frames in-between the original frames. While significant advances have been made from the recent deep convolutional neural networks, the quality of interpolation is often reduced due to large object motion or occlu-sion. In this work, we propose a video frame interpolation method which explicitly detects the occlusion by exploring the depth information. Specifically, we develop a depth-aware flow projection layer to synthesize intermediate flows that preferably sample closer objects than farther ones. In addition, we learn hierarchical features to gather contextual information from neighboring pixels. The proposed model then warps the input frames, depth maps, and contextual features based on the optical flow and local interpolation kernels for synthesizing the output frame. Our model is compact, efficient, and fully differentiable. Quantitative and qualitative results demonstrate that the proposed model performs favorably against state-of-the-art frame interpolation methods on a wide variety of datasets. The source code and pre-trained model are available at https://github.com/baowenbo/DAIN .},
	author = {Bao, Wenbo and Lai, Wei-Sheng and Ma, Chao and Zhang, Xiaoyun and Gao, Zhiyong and Yang, Ming-Hsuan},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/HA8EYBT9/Bao et al. - 2019 - Depth-Aware Video Frame Interpolation.pdf:application/pdf},
}

@article{stringer_cellpose_nodate,
	title = {Cellpose: a generalist algorithm for cellular segmentation},
	url = {https://doi.org/10.1101/2020.02.02.931238},
	doi = {10.1101/2020.02.02.931238},
	abstract = {Many biological applications require the segmentation of cell bodies, membranes and nuclei from mi-croscopy images. Deep learning has enabled great progress on this problem, but current methods are specialized for images that have large training datasets. Here we introduce a generalist, deep learning-based segmentation algorithm called Cellpose, which can very precisely segment a wide range of image types out-of-the-box and does not require model retraining or parameter adjustments. We trained Cellpose on a new dataset of highly-varied images of cells, containing over 70,000 segmented objects. To support community contributions to the training data, we developed software for manual labelling and for curation of the automated results, with optional direct upload to our data repository. Periodically retraining the model on the community-contributed data will ensure that Cellpose improves constantly.},
	author = {Stringer, Carsen and Michaelos, Michalis and Pachitariu, Marius},
	file = {Full Text:/home/zwerg/Zotero/storage/7TR6Q654/Stringer et al. - Cellpose a generalist algorithm for cellular segm.pdf:application/pdf},
}

@techreport{zoph_learning_nodate,
	title = {Learning {Data} {Augmentation} {Strategies} for {Object} {Detection}},
	abstract = {Data augmentation is a critical component of training deep learning models. Although data augmentation has been shown to significantly improve image classification, its potential has not been thoroughly investigated for object detection. Given the additional cost for annotating images for object detection, data augmentation may be of even greater importance for this computer vision task. In this work, we study the impact of data augmentation on object detection. We first demonstrate that data augmentation operations borrowed from image classification may be helpful for training detection models, but the improvement is limited. Thus, we investigate how learned, specialized data augmentation policies improve generalization performance for detection models. Importantly, these augmentation policies only affect training and leave a trained model unchanged during evaluation. Experiments on the COCO dataset indicate that an optimized data augmentation policy improves detection accuracy by more than +2.3 mAP, and allow a single inference model to achieve a state-of-the-art accuracy of 50.7 mAP. Importantly, the best policy found on COCO may be transferred unchanged to other detection datasets and models to improve predictive accuracy. For example , the best augmentation policy identified with COCO improves a strong baseline on PASCAL-VOC by +2.7 mAP. Our results also reveal that a learned augmentation policy is superior to state-of-the-art architecture regulariza-tion methods for object detection, even when considering strong baselines. Code for training with the learned policy is available online. 1},
	author = {Zoph, Barret and Cubuk, Ekin D and Ghiasi, Golnaz and Lin, Tsung-Yi and Shlens, Jonathon and Le, Quoc V},
}

@article{justus_predicting_2019,
	title = {Predicting the {Computational} {Cost} of {Deep} {Learning} {Models}},
	issn = {9781538650356},
	doi = {10.1109/BigData.2018.8622396},
	abstract = {Deep learning is rapidly becoming a go-to tool for many artificial intelligence problems due to its ability to outperform other approaches and even humans at many problems. Despite its popularity we are still unable to accurately predict the time it will take to train a deep learning network to solve a given problem. This training time can be seen as the product of the training time per epoch and the number of epochs which need to be performed to reach the desired level of accuracy. Some work has been carried out to predict the training time for an epoch - most have been based around the assumption that the training time is linearly related to the number of floating point operations required. However, this relationship is not true and becomes exacerbated in cases where other activities start to dominate the execution time. Such as the time to load data from memory or loss of performance due to non-optimal parallel execution. In this work we propose an alternative approach in which we train a deep learning network to predict the execution time for parts of a deep learning network. Timings for these individual parts can then be combined to provide a prediction for the whole execution time. This has advantages over linear approaches as it can model more complex scenarios. But, also, it has the ability to predict execution times for scenarios unseen in the training data. Therefore, our approach can be used not only to infer the execution time for a batch, or entire epoch, but it can also support making a well-informed choice for the appropriate hardware and model.},
	number = {April},
	journal = {Proceedings - 2018 IEEE International Conference on Big Data, Big Data 2018},
	author = {Justus, Daniel and Brennan, John and Bonner, Stephen and McGough, Andrew Stephen},
	year = {2019},
	keywords = {Benchmark, Machine Learning, Performance, Prediction},
	pages = {3873--3882},
}

@techreport{sandler_mobilenetv2_nodate,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the in-put/output domains from the expressiveness of the transformation , which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
}

@inproceedings{sandler_mobilenetv2_2018,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	isbn = {978-1-5386-6420-9},
	url = {http://arxiv.org/abs/1801.04381},
	doi = {10.1109/CVPR.2018.00474},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.},
	publisher = {IEEE Computer Society},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang Chieh},
	month = dec,
	year = {2018},
	pages = {4510--4520},
}

@techreport{dhariwal_jukebox_nodate,
	title = {Jukebox: {A} {Generative} {Model} for {Music}},
	url = {https://github.com/openai/jukebox.},
	abstract = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code.},
	author = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
}

@article{cornish_hpasubc_2015,
	title = {{HPASubC}: {A} suite of tools for user subclassification of human protein atlas tissue images},
	volume = {6},
	doi = {10.4103/2153-3539.159213},
	abstract = {BACKGROUND: The human protein atlas (HPA) is a powerful proteomic tool for visualizing the distribution of protein expression across most human tissues and many common malignancies. The HPA includes immunohistochemically-stained images from tissue microarrays (TMAs) that cover 48 tissue types and 20 common malignancies. The TMA data are used to provide expression information at the tissue, cellular, and occasionally, subcellular level. The HPA also provides subcellular data from confocal immunofluorescence data on three cell lines. Despite the availability of localization data, many unique patterns of cellular and subcellular expression are not documented. MATERIALS AND METHODS: To get at this more granular data, we have developed a suite of Python scripts, HPASubC, to aid in subcellular, and cell-type specific classification of HPA images. This method allows the user to download and optimize specific HPA TMA images for review. Then, using a playstation-style video game controller, a trained observer can rapidly step through 10's of 1000's of images to identify patterns of interest. RESULTS: We have successfully used this method to identify 703 endothelial cell (EC) and/or smooth muscle cell (SMCs) specific proteins discovered within 49,200 heart TMA images. This list will assist us in subdividing cardiac gene or protein array data into expression by one of the predominant cell types of the myocardium: Myocytes, SMCs or ECs. CONCLUSIONS: The opportunity to further characterize unique staining patterns across a range of human tissues and malignancies will accelerate our understanding of disease processes and point to novel markers for tissue evaluation in surgical pathology.},
	number = {1},
	journal = {Journal of Pathology Informatics},
	author = {Cornish, TobyC and Chakravarti, Aravinda and Kapoor, Ashish and Halushka, MarcK},
	year = {2015},
	note = {Publisher: Medknow},
	pages = {36--36},
}

@article{mildenhall_nerf_2020,
	title = {{NeRF}: {Representing} {Scenes} as {Neural} {Radiance} {Fields} for {View} {Synthesis}},
	url = {http://arxiv.org/abs/2003.08934},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	month = mar,
	year = {2020},
}

@article{noauthor_nerf_nodate,
	title = {{NeRF}: {Neural} {Radiance} {Fields}},
	url = {http://www.matthewtancik.com/nerf},
}

@techreport{dieleman_challenge_2018,
	title = {The challenge of realistic music generation: modelling raw audio at scale},
	abstract = {Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.},
	author = {Dieleman, Sander and Van Den Oord, Aäron and Simonyan, Karen},
	year = {2018},
	pages = {7989--7999},
}

@inproceedings{xian_latent_2016,
	title = {Latent embeddings for zero-shot classification},
	volume = {2016-December},
	isbn = {978-1-4673-8850-4},
	url = {http://arxiv.org/abs/1603.08895},
	doi = {10.1109/CVPR.2016.15},
	abstract = {We present a novel latent embedding model for learning a compatibility function between image and class embeddings, in the context of zero-shot classification. The proposed method augments the state-of-the-art bilinear compatibility model by incorporating latent variables. Instead of learning a single bilinear map, it learns a collection of maps with the selection, of which map to use, being a latent variable for the current image-class pair. We train the model with a ranking based objective function which penalizes incorrect rankings of the true class for a given image. We empirically demonstrate that our model improves the state-of-the-art for various class embeddings consistently on three challenging publicly available datasets for the zero-shot setting. Moreover, our method leads to visually highly interpretable results with clear clusters of different fine-grained object properties that correspond to different latent variable maps.},
	publisher = {IEEE Computer Society},
	author = {Xian, Yongqin and Akata, Zeynep and Sharma, Gaurav and Nguyen, Quynh and Hein, Matthias and Schiele, Bernt},
	month = dec,
	year = {2016},
	pages = {69--77},
}

@article{woringer_versatile_2017,
	title = {A versatile compressed sensing scheme for faster and less phototoxic {3D} fluorescence microscopy},
	doi = {10.1101/125815},
	abstract = {{\textless}p{\textgreater}Three-dimensional fluorescence microscopy based on Nyquist sampling of focal planes faces harsh trade-offs between acquisition time, light exposure, and signal-to-noise. We propose a 3D compressed sensing approach that uses temporal modulation of the excitation intensity during axial stage sweeping and can be adapted to fluorescence microscopes without hardware modification. We describe implementations on a lattice light sheet microscope and an epifluorescence microscope, and show that images of beads and biological samples can be reconstructed with a 5-10 fold reduction of light exposure and acquisition time. Our scheme opens a new door towards faster and less damaging 3D fluorescence microscopy.{\textless}/p{\textgreater}},
	journal = {Faster and less phototoxic 3D fluorescence microscopy using a versatile compressed sensing scheme},
	author = {Woringer, Maxime and Darzacq, Xavier and Zimmer, Christophe and Mir, Mustafa},
	month = apr,
	year = {2017},
	note = {Publisher: Cold Spring Harbor Laboratory},
	keywords = {Bioinformatics, Fluorescence, Analytical chemistry, Biology, Compressed sensing, Fluorescence microscope, Light sheet fluorescence microscopy, Microscope, Nyquist–Shannon sampling theorem},
	pages = {125815--125815},
}

@techreport{coates_importance_2011,
	title = {The {Importance} of {Encoding} {Versus} {Training} with {Sparse} {Coding} and {Vector} {Quantization}},
	abstract = {While vector quantization (VQ) has been applied widely to generate features for visual recognition problems, much recent work has focused on more powerful methods. In particular , sparse coding has emerged as a strong alternative to traditional VQ approaches and has been shown to achieve consistently higher performance on benchmark datasets. Both approaches can be split into a training phase, where the system learns a dictionary of basis functions, and an encoding phase, where the dictionary is used to extract features from new inputs. In this work, we investigate the reasons for the success of sparse coding over VQ by decoupling these phases, allowing us to separate out the contributions of training and encoding in a controlled way. Through extensive experiments on CIFAR, NORB and Caltech 101 datasets, we compare several training and encoding schemes, including sparse coding and a form of VQ with a soft threshold activation function. Our results show not only that we can use fast VQ algorithms for training, but that we can just as well use randomly chosen exemplars from the training set. Rather than spend resources on training, we find it is more important to choose a good encoder-which can often be a simple feed forward non-linearity. Our results include state-of-the-art performance on both CIFAR and NORB.},
	author = {Coates, Adam and Ng, Andrew Y},
	year = {2011},
}

@article{horstmeyer_architectures_nodate,
	title = {Architectures for {Gigapixel}-scale {High}- speed {Imaging} of {Freely} {Moving} {Organisms}},
	volume = {50},
	number = {or 100},
	author = {Horstmeyer, Roarke and Harfouche, Mark and Park, Jaehee and Konda, Pavan and Cooke, Colin and Naumann, Eva},
	pages = {1--3},
}

@article{eismann_automated_2020,
	title = {Automated {3D} light-sheet screening with high spatiotemporal resolution reveals mitotic phenotypes},
	number = {April},
	author = {Eismann, Björn and Krieger, Teresa G and Beneke, Jürgen and Bulkescher, Ruben},
	year = {2020},
}

@article{socher_zero-shot_2013,
	title = {Zero-{Shot} {Learning} {Through} {Cross}-{Modal} {Transfer}},
	volume = {6},
	url = {http://arxiv.org/abs/1301.3666},
	abstract = {This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.},
	number = {1},
	author = {Socher, Richard and Ganjoo, Milind and Sridhar, Hamsa and Bastani, Osbert and Manning, Christopher D. and Ng, Andrew Y.},
	month = jan,
	year = {2013},
	pages = {106--107},
}

@article{haase_rethinking_2020,
	title = {Rethinking {Depthwise} {Separable} {Convolutions}: {How} {Intra}-{Kernel} {Correlations} {Lead} to {Improved} {MobileNets}},
	url = {http://arxiv.org/abs/2003.13549},
	abstract = {We introduce blueprint separable convolutions (BSConv) as highly efficient building blocks for CNNs. They are motivated by quantitative analyses of kernel properties from trained models, which show the dominance of correlations along the depth axis. Based on our findings, we formulate a theoretical foundation from which we derive efficient implementations using only standard layers. Moreover, our approach provides a thorough theoretical derivation, interpretation, and justification for the application of depthwise separable convolutions (DSCs) in general, which have become the basis of many modern network architectures. Ultimately, we reveal that DSC-based architectures such as MobileNets implicitly rely on cross-kernel correlations, while our BSConv formulation is based on intra-kernel correlations and thus allows for a more efficient separation of regular convolutions. Extensive experiments on large-scale and fine-grained classification datasets show that BSConvs clearly and consistently improve MobileNets and other DSC-based architectures without introducing any further complexity. For fine-grained datasets, we achieve an improvement of up to 13.7 percentage points. In addition, if used as drop-in replacement for standard architectures such as ResNets, BSConv variants also outperform their vanilla counterparts by up to 9.5 percentage points on ImageNet.},
	author = {Haase, Daniel and Amthor, Manuel},
	month = mar,
	year = {2020},
}

@techreport{xu_learning_nodate,
	title = {Learning in the {Frequency} {Domain}},
	abstract = {Deep neural networks have achieved remarkable success in computer vision tasks. Existing neural networks mainly operate in the spatial domain with fixed input sizes. For practical applications, images are usually large and have to be downsampled to the predetermined input size of neural networks. Even though the downsampling operations reduce computation and the required communication bandwidth, it removes both redundant and salient information obliviously, which results in accuracy degradation. Inspired by digital signal processing theories, we analyze the spectral bias from the frequency perspective and propose a learning-based frequency selection method to identify the trivial frequency components which can be removed without accuracy loss. The proposed method of learning in the frequency domain leverages identical structures of the well-known neural networks, such as ResNet-50, MobileNetV2, and Mask R-CNN, while accepting the frequency-domain information as the input. Experiment results show that learning in the frequency domain with static channel selection can achieve higher accuracy than the conventional spatial downsampling approach and meanwhile further reduce the input data size. Specifically for ImageNet classification with the same input size, the proposed method achieves 1.60\% and 0.63\% top-1 accuracy improvements on ResNet-50 and MobileNetV2, respectively. Even with half input size, the proposed method still improves the top-1 accuracy on ResNet-50 by 1.42\%. In addition, we observe a 0.8\% average precision improvement on Mask R-CNN for instance segmentation on the COCO dataset.},
	author = {Xu, Kai and Qin, Minghai and Sun, Fei and Wang, Yuhao and Chen, Yen-Kuang and Ren, Fengbo},
}

@article{pavillon_compressed_2006,
	title = {Compressed sensing based cone-beam computed tomography reconstruction with a first-order method},
	volume = {52},
	issn = {1304013049},
	url = {http://dx.doi.org/10.1364/OE.24.030038},
	doi = {10.1364/OE.24.030038},
	abstract = {We present a measurement and reconstruction method for laser-scanning microscopy based on compressed sensing, which enables significantly higher frame rates and reduced photobleaching. The image reconstruction accuracy is ensured by including a model of the physical imaging process into the compressed sensing reconstruction procedure. We demonstrate its applicability to unmodified commercial confocal fluorescence microscopy systems and for Raman imaging, showing a potential data reduction of 10-15 times, which directly leads to improvements in acquisition speed, or reduction of photobleaching, without significant loss of spatial resolution. Furthermore, the reconstruction model is also robust to noise, and effective for low-light applications. This method has promising applications for all imaging modalities based on laser-scanning acquisition, including fluorescence, Raman, and nonlinear microscopy.},
	number = {4},
	journal = {Proc. Natl. Acad. Sci. U.S.A},
	author = {Pavillon, N and Smith, N I and Studer, V and Bobin, J and Chahid, M and Mousavi, H S and Candes, E and Dahan, M and Brady, D J and Willett, R M and Schulz, T J and Davis, B M and Hemphill, A J and Maltaş, D C and Zipper, M A and Wang, P and Ben-Amotz, D and hyperspectral Raman, Multivariate and Wilcox, D S and Buzzard, G T and Lucier, B J and Rehrauer, O G and Ben, D and Wu, Y and Ye, P and Mirza, I O and Arce, G R and Prather, D W},
	year = {2006},
	keywords = {(1003190) Inverse problems, (1100180) Microscopy, (1103010) Image reconstruction techniques, (1801790) Confocal microscopy, (3006450) Spectroscopy, Raman, OCIS codes: (1805810) Scanning microscopy},
	pages = {4982--4990},
}

@article{studera_compressive_2012,
	title = {Compressive fluorescence microscopy for biological and hyperspectral imaging},
	volume = {109},
	doi = {10.1073/pnas.1119511109},
	abstract = {The mathematical theory of compressed sensing (CS) asserts that one can acquire signals from measurements whose rate is much lower than the total bandwidth. Whereas the CS theory is now well developed, challenges concerning hardware implementations of CS-based acquisition devices - especially in optics - have only started being addressed. This paper presents an implementation of compressive sensing in fluorescence microscopy and its applications to biomedical imaging. Our CS microscope combines a dynamic structured wide-field illumination and a fast and sensitive single-point fluorescence detection to enable reconstructions of images of fluorescent beads, cells, and tissues with undersampling ratios (between the number of pixels and number of measurements) up to 32. We further demonstrate a hyperspectral mode and record images with 128 spectral channels and undersampling ratios up to 64, illustrating the potential benefits of CS acquisition for higher-dimensional signals, which typically exhibits extreme redundancy. Altogether, our results emphasize the interest of CS schemes for acquisition at a significantly reduced rate and point to some remaining challenges for CS fluorescence microscopy.},
	number = {26},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Studera, Vincent and Bobin, Jérome and Chahida, Makhlad and Mousavia, Hamed Shams and Candes, Emmanuel and Dahane, Maxime},
	month = jun,
	year = {2012},
	note = {Publisher: National Academy of Sciences},
	keywords = {Compressed sensing, Biological imaging, Computational imaging, Sparse signals},
	pages = {E1679--E1687},
}

@article{tan_efficientnet_2019-1,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	volume = {2019-June},
	url = {http://arxiv.org/abs/1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	journal = {36th International Conference on Machine Learning, ICML 2019},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = may,
	year = {2019},
	note = {Publisher: International Machine Learning Society (IMLS)},
	pages = {10691--10700},
}

@article{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {http://arxiv.org/abs/1806.01261},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = jun,
	year = {2018},
}

@article{belthangady_applications_2019-1,
	title = {Applications, {Promises}, and {Pitfalls} of {Deep} {Learning} for {Fluorescence} {Image} {Reconstruction}},
	url = {www.preprints.org},
	doi = {10.20944/preprints201812.0137.v2},
	abstract = {Deep Learning is a recent and important addition to the computational toolbox available for image reconstruction in fluorescence microscopy. We review state-of-the-art applications such as image restoration and super-resolution, and discuss how the latest Deep Learning research can be applied to other image reconstruction tasks such as structured illumination, spectral deconvolution, and sample stabilisa-tion. Despite its successes, Deep Learning also poses significant challenges, has often misunderstood capabilities, and overlooked limits. We will address key questions, such as: What are the challenges in obtaining training data? Can we discover structures not present in the training data? And, what is the danger of inferring unsubstantiated image de-tails?},
	author = {Belthangady, Chinmay and Royer, Loic A and Biohub, Chan Zuckerberg and Francisco, San},
	year = {2019},
	keywords = {machine learning, deep learning, deconvolution, fluorescence, image reconstruction, image restoration, inverse problems, microscopy, spectral unmixing, super-resolution},
	file = {Full Text:/home/zwerg/Zotero/storage/EQRPKFUR/Belthangady et al. - 2019 - Applications, Promises, and Pitfalls of Deep Learn.pdf:application/pdf},
}

@article{ramirez-balas_thermal_2018,
	title = {Thermal {3D} {CFD} simulation with active transparent fa ade in buildings},
	volume = {11},
	doi = {10.3390/en11092265},
	abstract = {In recent years active fa ades have acquired greater importance given their capacity to improve the energy efficiency of buildings. One such type is the so-called Active Transparent Fa ade (ATF). A 3D numerical model based on computational fluid dynamics (CFD) and the Finite Element Method (FEM) has been generated to simulate the thermal performance of buildings equipped with this type of fa ade. This model is introduced for general application and allows the design parameters to be adapted for this system. The case study of Le Corbusier's proposal for the City of Refuge in Paris, the clearest example of previous use of an ATF is examined. In addition, a proposal is presented for the energy improvement of Le Corbusier's original solution. In order to do so, the conditions for the supply of air into the ATF cavity and in the mechanical ventilation system are assessed to guarantee comfort conditions.},
	number = {9},
	journal = {Energies},
	author = {Ramírez-Balas, Cristina and Fernández-Nieto, Enrique and Narbona-Reina, Gladys and Sendra, Juan José and Suárez, Rafael},
	year = {2018},
	keywords = {3D numerical modelling, Active transparent façade (ATF), City of refuge by le corbusier, Double-skin façade (DSF), Murneutralisant, Thermal computational fluid dynamics (CFD) simulat},
	pages = {1--19},
}

@article{rojas-fernandez_exploring_2018,
	title = {Exploring the interplay between {CAD} and {FreeFem}++ as an energy decision-making tool for architectural design},
	volume = {11},
	doi = {10.3390/en11102665},
	abstract = {The energy modelling software tools commonly used for architectural purposes do not allow a straightforward real-time implementation within the architectural design programs. In addition, the surrounding exterior spaces of the building, including the inner courtyards, hardly present a specific treatment distinguishing these spaces from the general external temperature in the thermal simulations. This is a clear disadvantage when it comes to streamlining the design process in relation to the whole-building energy optimization. In this context, the present study aims to demonstrate the advantages of the FreeFem++ open source program for performing simulations in architectural environments. These simulations include microclimate tests that describe the interactions between a building architecture and its local exterior. The great potential of this mathematical tool can be realized through its complete system integration within CAD (Computer-Aided Design) software such as SketchUp or AutoCAD. In order to establish the suitability of FreeFem++ for the performance of simulations, the most widely employed energy simulation tools able to consider a proposed architectural geometry in a specific environment are compared. On the basis of this analysis, it can be concluded that FreeFem++ is the only program displaying the best features for the thermal performance simulation of these specific outdoor spaces, excluding the currently unavailable easy interaction with architectural drawing programs. The main contribution of this research is, in fact, the enhancement of FreeFem++ usability by proposing a simple intuitive method for the creation of building geometries and their respective meshing (pre-processing). FreeFem++ is also considered a tool for data analysis (post-processing) able to help engineers and architects with building energy-efficiency-related tasks.},
	number = {10},
	journal = {Energies},
	author = {Rojas-Fernández, Juan and Galán-Marín, Carmen and Rivera-Gómez, Carlos and Fernández-Nieto, Enrique D.},
	year = {2018},
	keywords = {Architectural design tool, Computer-Aided Design (CAD), Courtyard, Energy efficiency, Energy modelling software, FreeFem++, Outdoor space},
}

@techreport{zhang_self-attention_nodate,
	title = {Self-{Attention} {Generative} {Adversarial} {Networks}},
	url = {https://github.com/},
	abstract = {In this paper, we propose the Self-Attention Gen-erative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore , recent work has shown that generator conditioning affects GAN performance. Leverag-ing this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN performs better than prior work 1 , boosting the best published Inception score from 36.8 to 52.52 and reducing Fréchet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visu-alization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.},
	author = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus},
}

@article{sandler_mobilenetv2_2018-1,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	url = {http://arxiv.org/abs/1801.04381},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	year = {2018},
}

@article{chollet_xception_2016,
	title = {Xception: {Deep} {Learning} with {Depthwise} {Separable} {Convolutions}},
	volume = {2017-January},
	url = {http://arxiv.org/abs/1610.02357},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
	journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	author = {Chollet, François},
	month = oct,
	year = {2016},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	pages = {1800--1807},
}

@article{zhang_deep_2020,
	title = {Deep {Learning} on {Graphs}: {A} {Survey}},
	doi = {10.1109/tkde.2020.2981333},
	abstract = {Deep learning has been shown to be successful in a number of domains, ranging from acoustics, images, to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, substantial research efforts have been devoted to applying deep learning methods to graphs, resulting in beneficial advances in graph analysis techniques. In this survey, we comprehensively review the different types of deep learning methods on graphs. We divide the existing methods into five categories based on their model architectures and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods. We then provide a comprehensive overview of these methods in a systematic manner mainly by following their development history. We also analyze the differences and compositions of different methods. Finally, we briefly outline the applications in which they have been used and discuss potential future research directions.},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Ziwei and Cui, Peng and Zhu, Wenwu},
	month = mar,
	year = {2020},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {1--1},
}

@techreport{zhao_approximate_2019,
	title = {Approximate inference with {Graph} {Neural} {Networks}},
	abstract = {Probabilistic graphical models are useful for discovering and analyzing structures of many real-world applications. Belief propagation and other message-passing algorithms have traditionally been used to disseminate evidence among the nodes in the graph. However, these algorithms may be inefficient for large or loopy graphs. Here, we expand the work of Yoon et al. (2018), which uses graph neural network (GNN) as an inference machine for small-scale networks. We reproduce their results and explore how to generalize GNN to larger graphs with approximate labeling schemes and custom training procedures.},
	author = {Zhao, Lingxiao and Korovina, Ksenia and Si, Wenwen and Cheung, Mark},
	year = {2019},
}

@article{noauthor_researchgate_nodate,
	title = {{ResearchGate}},
	url = {https://www.researchgate.net/publication/322861637_INTEGRATION_OF_HYBRID_STRIPED_BASS_MORONE_SAXATILIS_x_M_CHRYSOPS_NOBLE_CRAYFISH_ASTACUS_ASTACUS_WATERCRESS_NASTURTIUM_OFFICINALE_AND_MICROALGAE_NANNOCHLOROPSIS_LIMNETICA_IN_AN_EXPERIMENTAL_AQUAPONIC_S/link/5a73283baca2720bc0dac9ac/download},
}

@techreport{goddek_aquaponics_nodate,
	title = {Aquaponics {Food} {Production} {Systems} {Combined} {Aquaculture} and {Hydroponic} {Production} {Technologies} for the {Future}},
	author = {Goddek, Simon and Joyce, Alyssa and Kotzen, Benz and Burnell Editors, Gavin M},
}

@article{fierro-sanudo_production_2018,
	title = {Production and management of shrimp ({Penaeus} vannamei) in co-culture with basil (ocimum basilicum) using two sources of low-salinity water},
	volume = {46},
	doi = {10.3856/vol46-issue1-fulltext-8},
	abstract = {The aim of this study was to evaluate the production of an aquaponic culture of white shrimp (Penaeus vannamei) and basil (Ocimum basilicum) using two sources of low-salinity water (1.7 g L-1): diluted seawater (DSW) and groundwater (GW) with zero water exchange at a stocking ratio of 4.9 shrimp per basil plant. Six aquaponic treatment systems were constructed: three individual aquaponic systems for DSW-basil, three for GW-basil, and a control (per triplicate) of basil with a hydroponic solution. Stock densities for shrimp were 75 post-larvae m-2 and 16 plants m-2 for basil. With the exception of the yield in the shrimp culture (kg m - 2 or ton ha-1), no significant differences (P {\textgreater} 0.05) were found for the final individual weight, survival, growth rate and feed conversion ratio between DSW and GW, whereas for basil, lower yields were found with DSW. No significant differences in the basil production between the control and the treatment GW were found. Feed consumption per kg of total harvested basil was significantly lower (P {\textless} 0.05) in GW treatment, while for feed intake in the shrimp farming, where no significant differences were found (P {\textgreater} 0.05). The aquaponic culture of shrimp and basil using these two types of low-salinity water sources showed promising results. The estimates of both crop yields were compared with those recorded in the literature and for commercial field crops from northwest Mexico.},
	number = {1},
	journal = {Latin American Journal of Aquatic Research},
	author = {Fierro-Sañudo, Juan F. and De Oca, Gustavo A.Rodríguez Montes and León-Cañedo, Jesús A. and Alarcón-Silvas, Suammy G. and Mariscal-Lagarda, M. Martin and Díaz-Valdés, Tomás and Páez-Osuna, Federico},
	month = mar,
	year = {2018},
	note = {Publisher: Escuela de Ciencias del Mar},
	keywords = {Aquaponic culture, Groundwater, Management, Ocimum basilicum, Penaeus vannamei, Production},
	pages = {63--71},
}

@article{mariscal-lagarda_integrated_2012,
	title = {Integrated culture of white shrimp ({Litopenaeus} vannamei) and tomato ({Lycopersicon} esculentum {Mill}) with low salinity groundwater: {Management} and production},
	volume = {366-367},
	doi = {10.1016/j.aquaculture.2012.09.003},
	abstract = {The optimal utilization of water in arid and semi-arid regions is pivotal for resource sustainability. The integration of aquaculture with traditional agriculture may be a solution to achieve more efficient water use, maximizing farm production without increasing water consumption, avoiding disposition of aquaculture effluents and supplementing additional fertilizer to the agricultural crop. The objective of this study was to test the feasibility of an integrated shrimp (Litopenaeus vannamei) -tomato (Lycopersicon esculentum Mill) culture and evaluating the effects of the irrigation with shrimp farm effluent on tomato yield and to describe shrimp production. A field experiment was carried out in a randomized complete block design to evaluate the effects of three different water types on the growth of tomatoes: effluent water from shrimp culture tanks, nutritive solution prepared for tomatoes, and water directly from the well. Groundwater (0.65gL -1 of salinity) supplemented with KCl and MgNO 3 was used with a shrimp stocking density of 50 postlarvae shrimp per m 2. Evaluations for shrimp (mean weight, growth rate, survival and yield) and for tomato plants (fruit number, mean fruit weight and yield) were performed. Daily monitoring included temperature, conductivity, pH and dissolved oxygen in shrimp tank waters. Chemical analysis in a weekly monitoring included major ions and nutrients. The shrimp yield was 11.1±0.2kg per tank (3.9±2.0tha -1) with a mean survival of 56.3±1.1\%, a mean weight of 13.9±0.4g, and a feed conversion rate of 1.60±0.03. The yield of tomatoes irrigated with shrimp effluent (33.3±2.1kg per 45 plants) was comparable to those irrigated with nutritive solution (35.7±1.7kg), and significantly (P{\textless}0.05) higher than those irrigated with groundwater (25.5±2.4kg). The budget evidenced that most of the N (43.6\%) and P (99.4\%) entered to the shrimp-tomato system as shrimp food. Within the system, 13.2\% and 2.1\% of the input N were converted to harvested shrimp and tomato plants; similarly, 8.9\% and 4.3\% of the input P, were converted to harvested shrimp and tomato plants. From this work, it is demonstrated that the shrimp-tomato culture system is feasible, with a water consumption rate of 2.1m 3 per kg of harvested products. However, more research is needed to adjust the shrimp-tomato culture system in regards to the precise integration of the number of tomato plants per shrimp culture area and to optimize the composition of water used in terms of the major ions (concentration and ratio) and salinity. © 2012 Elsevier B.V.},
	journal = {Aquaculture},
	author = {Mariscal-Lagarda, M. Martin and Páez-Osuna, Federico and Esquer-Méndez, José Luis and Guerrero-Monroy, Ildelfonso and del Vivar, Alonso Romo and Félix-Gastelum, Rubén},
	month = nov,
	year = {2012},
	keywords = {Groundwater, Shrimp culture, Shrimp-tomato integrated culture, Tomato culture, Water consumption},
	pages = {76--84},
}

@article{ende_growth_2018,
	title = {Growth performance of hybrid striped bass ({Morone} chrysops × {M}. saxatilis) fed with commercial pike perch and trout diets},
	volume = {10},
	doi = {10.1007/s40071-018-0188-3},
	abstract = {Two commercial trout diets (Oncorhynchus mykiss) and one commercial pike perch diet (Sander lucioperca) were fed to hybrid striped bass (Morone chrysops × M. saxatilis) (mean initial weight ± SD of 60.7 g ± 12.1; mean initial length SD of 17.2 cm ± 1.1) for 69 days at rations of approximately 1\% average body weight. While final body weight (FBW), final length (FBL) and condition factor (Cf) were not significantly influenced by diets, specific growth rate (SGR) in hybrid striped bass fed with the pike perch diet (1.15) was significantly higher than those fed with either of the two trout diets (1.04 and 1.07). The feed conversion ratio (FCR) in hybrid striped bass fed with the pike perch diet (1.0) was significantly lower than the FCR in hybrid striped bass fed with either of the two trout diets (1.1 and 1.2). When hybrid striped bass (mean initial body weight: 65.7 ± 4.5 and 127.7 ± 2.9 g) were fed with the pike perch diet twice per day until satiation for 52 days, the SGR was 1.7 and 1.15\% d−1 in fishes with an average body weight of 116 and 183 g, respectively. Present results demonstrate that growth performance in hybrid striped bass can be improved when fishes are fed with commercial pike perch diets rather than using commercial trout diets as is the current practice.},
	number = {1},
	journal = {International Aquatic Research},
	author = {Ende, Stephan S.W. and Fuchs, Vanessa and Schuhn, Annabel and von der Marwitz, Christiane and Wirtz, Andrea and Henjes, Joachim and Slater, Matthew},
	month = mar,
	year = {2018},
	note = {Publisher: Springer Verlag},
	keywords = {Commercial diets, Growth, Hybrid striped bass, Nutrition, Pike perch, Trout},
	pages = {57--63},
}

@article{anaya-rosas_effects_2019,
	title = {Effects of a co-culture of marine algae and shrimp ({Litopenaeus} vannamei) on the growth, survival and immune response of shrimp infected with {Vibrio} parahaemolyticus and white spot virus ({WSSV})},
	volume = {87},
	doi = {10.1016/j.fsi.2018.12.071},
	abstract = {In aquaculture, fighting infectious diseases is a necessity. This study measured the immuno-stimulating effect of live macroalgae consumption on Litopenaeus vannamei against Vibrio parahaemolyticus and WSSV infection in two independent bioassays. Shrimps and macroalgae were cultivated in a co-culture with two species of macroalgae separately (Gracilaria vermiculophylla and Dictyota dichotoma), and later, shrimp were infected with V. parahaemolyticus. In another bioassay, shrimp and macroalgae (G. vermiculophylla, D. dichotoma and Ulva lactuca) were grown and subsequently infected with WSSV. For both bioassays, survival after 120 h was determined, the total hemocyte count (TCH) was measured and the activity of superoxide dismutase (SOD) and catalase (CAT) in tissue were measured. The results indicate that the use of macroalgae in co-culture with L. vannamei provides a nutritional benefit that achieves higher growth than the control organisms, as well as improvements of the ammonium concentration and immune response after infection with V. parahaemolyticus and WSSV. A better immune response was obtained in organisms cultured with macroalgae in both bioassays at a ratio of 1.6–1.9 for organisms infected with bacteria and 1.4 to 1.6 times for organisms infected with the virus. In turn, the enzymatic activity of SOD and CAT were higher in the treated organisms relative to the controls in both experiments.},
	journal = {Fish and Shellfish Immunology},
	author = {Anaya-Rosas, Ricardo Ernesto and Rivas-Vega, Martha Elisa and Miranda-Baeza, Anselmo and Piña-Valdez, Pablo and Nieves-Soto, Mario},
	month = apr,
	year = {2019},
	note = {Publisher: Academic Press},
	keywords = {Macroalgae, Shrimp, Vibrio parahaemolyticus, White spot virus},
	pages = {136--143},
}

@article{pinheiro_production_2017,
	title = {Production of the halophyte {Sarcocornia} ambigua and {Pacific} white shrimp in an aquaponic system with biofloc technology},
	volume = {100},
	doi = {10.1016/j.ecoleng.2016.12.024},
	abstract = {The aim of this study was to evaluate the integrated culture of Sarcocornia ambigua and Pacific white shrimp in an aquaponic system with biofloc. The experiment was performed for 73 days and two treatments were evaluated: plants and control (without plants), with four replicates. Each experimental unit consisted of an 800 L tank, a 40 L conical bottom settling chamber, and a hydroponic bench with 0.4 m2 of planting area and a capacity for 40 plants. The water from the shrimp tank was pumped continuously to the settling chamber and the overflow was distributed to the channels to irrigate the plants, then returned to the tank by gravity. Tanks were stocked with 250 shrimp m−3 (1.4 ± 0.0 g). Shrimp were fed a commercial diet containing 35\% crude protein, four times per day. At the end of the experiment, total nitrogen was determined in the shrimp, the plants and the ration, as well as antioxidant activity and total phenolic compounds in Sarcocornia. The water quality remained within the acceptable limits for the culture of marine shrimp. No significant differences were observed in the performance of L. vannamei. The final biomass of shrimp was 2.1 ± 0.1 kg m−3, with a survival rate of 73.5 ± 1.9\%, a final average weight of 11.7 ± 0.4 g, and a feed conversion ratio of 1.7 ± 0.1. The production of plants was 8.2 ± 0.3 kg m−2. The antioxidant activity in S. ambigua was 38.3 ± 1.3 μmol TEAC 100 g−1 FM, which characterizes this species as a functional food. The recovery of nitrogen supplied to the system through the ration was higher in the plants treatment (39.3\%) than in the control (31.4\%). In the proposed aquaponic system it was possible to produce 2 kg of plants for each kilogram of shrimp, integrating the production of L. vannamei and S. ambigua, and thus improve the use of nutrients in the culture.},
	journal = {Ecological Engineering},
	author = {Pinheiro, Isabela and Arantes, Rafael and do Espírito Santo, Carlos Manoel and do Nascimento Vieira, Felipe and Lapa, Katt Regina and Gonzaga, Luciano Valdemiro and Fett, Roseane and Barcelos-Oliveira, Jorge Luiz and Seiffert, Walter Quadros},
	month = mar,
	year = {2017},
	note = {Publisher: Elsevier B.V.},
	keywords = {Antioxidant, Bft, Litopenaeus vannamei, Nitrogen},
	pages = {261--267},
}

@article{ghaye_simulated_2012,
	title = {Simulated biological cells for receptor counting in fluorescence imaging},
	volume = {2},
	issn = {1266801200},
	doi = {10.1007/s12668-012-0041-x},
	abstract = {Digital image processing and epifluorescence microscopy provide one of the main and basic tools for living biological cell analysis and studying. Developing, testing, and comparing those image processing methods properly is eased by the use of a controlled environment. Taking advantage of an existing database of verified and trustworthy images and meta-data helps controlling the validity of the processing results. Manually generating that golden database is a long process involving specialists being able to apprehend and extract useful data out of fluorescent images. Having enough cases in the database to challenge the processing methods and gain trust in them can only be achieved manually through time-consuming, prone to human-error processes. More and more we need to automate this process. This paper presents a framework implementing a novel approach to generate synthetic fluorescent images of fluorescently stained cell populations by simulating the imaging process of fluorescent molecules. Ultimately, the proposed simulator allows us to generate images and golden data to populate the database, thus providing tools for the development, evaluation, and testing of processing algorithms meant to be used in automated systems.},
	number = {2},
	journal = {BioNanoScience},
	author = {Ghaye, Julien and De Micheli, Giovanni and Carrara, Sandro},
	year = {2012},
	keywords = {Fluorescence imaging, Synthetic image, Biological image processing, Cell simulation, Deconvolution, Simulator},
	pages = {94--103},
}

@article{ruusuvuori_benchmark_2008,
	title = {{BENCHMARK} {SET} {OF} {SYNTHETIC} {IMAGES} {FOR} {VALIDATING} {CELL} {IMAGE} {ANALYSIS} {ALGORITHMS} {Department} of {Signal} {Processing} , {Tampere} {University} of {Technology} {P} . {O} . {Box} 553 , 33101 {Tampere} , {Finland}},
	number = {Eusipco},
	journal = {Image (Rochester, N.Y.)},
	author = {Ruusuvuori, Pekka and Lehmussola, Antti and Selinummi, Jyrki and Rajala, Tiina and Huttunen, Heikki and Yli-harja, Olli},
	year = {2008},
}

@article{noauthor_introducing_nodate,
	title = {Introducing {OCTAVE} {Allegro}: {Improving} the {Information} {Security} {Risk} {Assessment} {Process}},
	url = {https://resources.sei.cmu.edu/library/asset-view.cfm?assetID=8419},
}

@techreport{liu_cbnet_nodate,
	title = {{CBNet}: {A} {Novel} {Composite} {Backbone} {Network} {Architecture} for {Object} {Detection}},
	url = {https://github.com/PKUbahuangliuhe/CBNet.},
	abstract = {In existing CNN based detectors, the backbone network is a very important component for basic feature 1 extraction, and the performance of the detectors highly depends on it. In this paper, we aim to achieve better detection performance by building a more powerful backbone from existing backbones like ResNet and ResNeXt. Specifically, we propose a novel strategy for assembling multiple identical backbones by composite connections between the adjacent backbones, to form a more powerful backbone named Composite Backbone Network (CBNet). In this way, CBNet iteratively feeds the output features of the previous backbone, namely high-level features, as part of input features to the succeeding backbone, in a stage-by-stage fashion, and finally the feature maps of the last backbone (named Lead Backbone) are used for object detection. We show that CBNet can be very easily integrated into most state-of-the-art detectors and significantly improve their performances. For example, it boosts the mAP of FPN, Mask R-CNN and Cascade R-CNN on the COCO dataset by about 1.5 to 3.0 percent. Meanwhile, experimental results show that the instance segmentation results can also be improved. Specially, by simply integrating the proposed CBNet into the baseline detector Cascade Mask R-CNN, we achieve a new state-of-the-art result on COCO dataset (mAP of 53.3) with single model, which demonstrates great effectiveness of the proposed CBNet architecture. Code will be made available on https://github.com/PKUbahuangliuhe/CBNet.},
	author = {Liu, Yudong and Wang, Yongtao and Wang, Siwei and Liang, Tingting and Zhao, Qijie and Tang, Zhi and Ling, Haibin},
}

@article{song_revisiting_2020,
	title = {Revisiting the {Sibling} {Head} in {Object} {Detector}},
	url = {http://arxiv.org/abs/2003.07540},
	abstract = {The ``shared head for classification and localization'' (sibling head), firstly denominated in Fast RCNN{\textasciitilde}{\textbackslash}cite\{girshick2015fast\}, has been leading the fashion of the object detection community in the past five years. This paper provides the observation that the spatial misalignment between the two object functions in the sibling head can considerably hurt the training process, but this misalignment can be resolved by a very simple operator called task-aware spatial disentanglement (TSD). Considering the classification and regression, TSD decouples them from the spatial dimension by generating two disentangled proposals for them, which are estimated by the shared proposal. This is inspired by the natural insight that for one instance, the features in some salient area may have rich information for classification while these around the boundary may be good at bounding box regression. Surprisingly, this simple design can boost all backbones and models on both MS COCO and Google OpenImage consistently by {\textasciitilde}3\% mAP. Further, we propose a progressive constraint to enlarge the performance margin between the disentangled and the shared proposals, and gain {\textasciitilde}1\% more mAP. We show the {\textbackslash}algname\{\} breaks through the upper bound of nowadays single-model detector by a large margin (mAP 49.4 with ResNet-101, 51.2 with SENet154), and is the core model of our 1st place solution on the Google OpenImage Challenge 2019.},
	author = {Song, Guanglu and Liu, Yu and Wang, Xiaogang},
	month = mar,
	year = {2020},
}

@techreport{huisman_minimum_nodate,
	title = {Minimum {Information} guidelines for fluorescence microscopy: increasing the value, quality, and fidelity of image data},
	abstract = {These two authors share senior-authorship. ​ 1-​ ABSTRACT High-resolution digital microscopy provides ever more powerful tools for probing the real-time dynamics of subcellular structures, and adequate record-keeping is necessary to evaluate results, share data, and allow experiments to be repeated. In addition to advances in microscopic techniques, post-acquisition procedures such as image-data processing and analysis (i.e., feature counting, distance measurements, intensity comparison and colocalization studies) are often required for the reproducible and quantitative interpretation of images. While these techniques increase the usefulness of microscopy data, the limits to which quantitative results may be interpreted are often poorly quantified and documented. Keeping notes on microscopy experiments and calibration procedures should be relatively unchallenging, as the microscope is a machine whose performance should be easy to assess. Nevertheless, to this date, no widely adopted 'data provenance' and quality control metadata guidelines to be recorded or published with imaging data exist. Metadata automatically recorded by microscopes from different companies vary widely and pose a substantial challenge for microscope users to create a good faith record of their work. Similarly, the complexity and aim of experiments using microscopes vary, leading to different reporting and quality control requirements from the simple description of a sample to the need to document the complexities of sub-diffraction resolution imaging in living cells and beyond. To solve this problem, the 4DN Imaging Standards Working Group has put forth a tiered system of microscopy calibration and metadata standards for images obtained through fluorescence microscopy. The proposal is an extension of the OME data model and aims at increasing data fidelity, ease future analysis, and facilitate objective comparison of different datasets, experimental setups, and essays.},
	author = {Huisman, Maximiliaan and Hammer, Mathias and Rigano, Alex and Farzam, Farzin and Gopinathan, Renu and Carlas and 1, Smith and Grunwald, David and Strambio-De-Castillia, Caterina},
	keywords = {fluorescence, microscopy, super-resolution, Imaging, calibration, data-formats, image-processing, instrumentation, metadata, standards},
}

@article{liu_1st_2020,
	title = {1st {Place} {Solutions} for {OpenImage2019} -- {Object} {Detection} and {Instance} {Segmentation}},
	url = {http://arxiv.org/abs/2003.07557},
	abstract = {This article introduces the solutions of the two champion teams, `MMfruit' for the detection track and `MMfruitSeg' for the segmentation track, in OpenImage Challenge 2019. It is commonly known that for an object detector, the shared feature at the end of the backbone is not appropriate for both classification and regression, which greatly limits the performance of both single stage detector and Faster RCNN {\textbackslash}cite\{ren2015faster\} based detector. In this competition, we observe that even with a shared feature, different locations in one object has completely inconsistent performances for the two tasks. {\textbackslash}textit\{E.g. the features of salient locations are usually good for classification, while those around the object edge are good for regression.\} Inspired by this, we propose the Decoupling Head (DH) to disentangle the object classification and regression via the self-learned optimal feature extraction, which leads to a great improvement. Furthermore, we adjust the soft-NMS algorithm to adj-NMS to obtain stable performance improvement. Finally, a well-designed ensemble strategy via voting the bounding box location and confidence is proposed. We will also introduce several training/inferencing strategies and a bag of tricks that give minor improvement. Given those masses of details, we train and aggregate 28 global models with various backbones, heads and 3+2 expert models, and achieves the 1st place on the OpenImage 2019 Object Detection Challenge on the both public and private leadboards. Given such good instance bounding box, we further design a simple instance-level semantic segmentation pipeline and achieve the 1st place on the segmentation challenge.},
	author = {Liu, Yu and Song, Guanglu and Zang, Yuhang and Gao, Yan and Xie, Enze and Yan, Junjie and Loy, Chen Change and Wang, Xiaogang},
	month = mar,
	year = {2020},
}

@article{lin_feature_2016,
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	volume = {2019-Novem},
	url = {http://arxiv.org/abs/1612.03144},
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	journal = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
	author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	month = dec,
	year = {2016},
	note = {Publisher: IEEE Computer Society},
	keywords = {Object Detection FPN TFPN Twin-Pyramid Structure},
	pages = {1702--1707},
	file = {Full Text:/home/zwerg/Zotero/storage/WF5WX7KC/Lin et al. - 2016 - Feature Pyramid Networks for Object Detection.pdf:application/pdf},
}

@article{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8903269},
}

@techreport{howard_measuring_nodate,
	title = {Measuring {Relative} {Attack} {Surfaces}},
	abstract = {We propose a metric for determining whether one version of a system is more secure than another with respect to a fixed set of dimensions. Rather than count bugs at the code level or count vulnerability reports at the system level, we count a system's attack opportunities. We use this count as an indication of the system's "attackability," likelihood that it will be successfully attacked. We describe a system's attack surface along three abstract dimensions: targets and enablers, channels and protocols, and access rights. Intuitively, the more exposed the system's surface, the more attack opportunities, and hence the more likely it will be a target of attack. Thus, one way to improve system security is to reduce its attack surface. To validate our ideas, we recast Microsoft Security Bulletin MS02-005 using our terminology, and we show how Howard's Relative Attack Surface Quotient for Windows is an instance of our general metric.},
	author = {Howard, Michael and Pincus, Jon and Wing, Jeannette M},
	keywords = {at-tack surface, attacks, Security metrics, threat modeling, vulnerabilities},
}

@techreport{noauthor_efficientdet_nodate,
	title = {{EfficientDet}: {Scalable} and {Efficient} {Object} {Detection}},
	url = {https://github.com/},
	abstract = {Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neu-ral network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and EfficientNet backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular , with single-model and single-scale, our EfficientDet-D7 achieves state-of-the-art 52.2 AP on COCO test-dev with 52M parameters and 325B FLOPs 1 , being 4x-9x smaller and using 13x-42x fewer FLOPs than previous detectors. Code is available at https://github.com/ google/automl/tree/master/efficientdet.},
	number = {1911.09070v4},
}

@inproceedings{dai_deformable_2017-1,
	title = {Deformable {Convolutional} {Networks}},
	volume = {2017-October},
	isbn = {978-1-5386-1032-9},
	doi = {10.1109/ICCV.2017.89},
	abstract = {Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the first time, we show that learning dense spatial transformation in deep CNNs is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/msracver/Deformable-ConvNets.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
	month = dec,
	year = {2017},
	pages = {764--773},
}

@techreport{periago_finite_2013,
	title = {The {Finite} {Element} {Method} with {FreeFem}++ for beginners {Control} of {Quantum} {Wave} {Equations} {View} project {Roberto} {Font} {Biometric} {Vox} {The} {Finite} {Element} {Method} with {FreeFem}++ for beginners},
	url = {https://www.researchgate.net/publication/260002309},
	abstract = {The user has requested enhancement of the downloaded file. All in-text references underlined in blue are added to the original document and are linked to publications on ResearchGate, letting you access and read them immediately. Abstract These notes are concerned with the numerical resolution of Partial Differential Equations (PDE) by the Finite Element Method (FEM). Emphasis is placed in the practical numerical resolution of this type of problems by using the free software FreeFem++. Theoretical background is briefly reviewed (without entering in technical details) and a number of examples are given, along with the code. The goal is to introduce in a simple way this difficult (but very interesting in real-world applications) topic to undergraduate students who do not have a solid background both in variational methods and in numerical analysis for PDE.},
	author = {Periago, Francisco and Font, Roberto},
	year = {2013},
	note = {Issue: 4
Volume: 7},
}

@article{noauthor_no_nodate-3,
	title = {({No} {Title})},
	doi = {10.18154/RWTH-2019-03648},
}

@article{noauthor_pdf_nodate,
	title = {[{PDF}] {A} comprehensive analysis on attention models {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/A-comprehensive-analysis-on-attention-models-Zeyer-Ney/495364aefa4dfef25e534582c4310ed4785c7dfd},
}

@article{tepperman_customizable_nodate,
	title = {Customizable {Live}-{Cell} {Imaging} {Chambers} for {Fluorescence} and {Super}-{Resolution} {Microscopy}},
	url = {https://doi.org/10.1101/2020.02.19.955971},
	doi = {10.1101/2020.02.19.955971},
	abstract = {While new high-resolution microscopy techniques are continually developed, adoption of these methods is often difficult due to an inability to meet the experimental conditions required for an experiment in a format which also meets the demanding optical requirements of these microscopy techniques. Although specialized imaging chambers can meet these challenges, the difficulty of manufacturing customized chambers in-house and the relatively high cost and design inflexibility of commercial chambers has limited the incorporation of imaging chambers into fluorescence and super-resolution microscopy experiments. Herein, we demonstrate the use of fused deposition modeling (3D printing) for producing inexpensive, customized imaging chambers that are compatible with long-duration live-cell imaging using fluorescence and super-resolution microscopy techniques. In this approach, biocompatible 3D printing plastics are used to generate imaging chambers designed to meet the specific needs of an experiment, followed by adhesion of the printed chamber to a glass coverslip suitable for fluorescence and super-resolution imaging. This technique produces a chamber that is impermeant to liquids that can support the growth and imaging of cells over multiple days. The utility of these chambers is then demonstrated using designs for multiplex microscopy, imaging under shear, chemotaxis, and general cellular imaging. Together, this approach represents an inexpensive yet highly customizable approach to produce imaging chambers that are compatible with many modern microscopy techniques.},
	author = {Tepperman, Adam and Zheng, David Jiao and Abou Taka, Maria and Vrieze, Angela and Heit, Bryan},
}

@techreport{jones_end--end_2020,
	title = {End-to-end {Learning}, with or without {Labels}},
	abstract = {We present an approach for end-to-end learning that allows one to jointly learn a feature representation from unlabeled data (with or without labeled data) and predict labels for unlabeled data. The feature representation is assumed to be specified in a differentiable programming framework, that is, as a parameterized mapping amenable to automatic differentiation. The proposed approach can be used with any amount of labeled and unlabeled data, gracefully adjusting to the amount of supervision. We provide experimental results illustrating the effectiveness of the approach.},
	author = {Jones, Corinne and Roulet, Vincent and Harchaoui, Zaid},
	year = {2020},
}

@article{chamier_zerocostdl4mic_2020,
	title = {{ZeroCostDL4Mic}: an open platform to simplify access and use of {Deep}-{Learning} in {Microscopy}},
	issn = {0000000282031},
	url = {https://www.biorxiv.org/content/10.1101/2020.03.20.000133v1.article-metrics},
	doi = {10.1101/2020.03.20.000133},
	abstract = {Deep Learning (DL) methods are increasingly recognised as powerful analytical tools for microscopy. Their potential to outperform conventional image processing pipelines is now well established. Despite the enthusiasm and innovations fuelled by DL technology, the need to access powerful and compatible resources, install multiple computational tools and modify code instructions to train neural networks all lead to an accessibility barrier that novice users often find difficult to cross. Here, we present ZeroCostDL4Mic, an entry-level teaching and deployment DL platform which considerably simplifies access and use of DL for microscopy. It is based on Google Colab which provides the free, cloud-based computational resources needed. ZeroCostDL4Mic allows researchers with little or no coding expertise to quickly test, train and use popular DL networks. In parallel, it guides researchers to acquire more knowledge, to experiment with optimising DL parameters and network architectures. We also highlight the limitations and requirements to use Google Colab. Altogether, ZeroCostDL4Mic accelerates the uptake of DL for new users and promotes their capacity to use increasingly complex DL networks.},
	journal = {bioRxiv},
	author = {Chamier, Lucas von and Jukkala, Johanna and Spahn, Christoph and Lerche, Martina and Hernández-pérez, Sara and Mattila, Pieta and Karinou, Eleni and Holden, Seamus and Solak, Ahmet Can and Krull, Alexander and Buchholz, Tim-Oliver and Jug, Florian and Royer, Loic Alain and Heilemann, Mike and Laine, Romain F. and Jacquemet, Guillaume and Henriques, Ricardo},
	month = mar,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {2020.03.20.000133--2020.03.20.000133},
}

@article{golle_effectiveness_2018,
	title = {Effectiveness of a “{Grass} {Roots}” {Statewide} {Enrichment} {Program} for {Gifted} {Elementary} {School} {Children}},
	volume = {11},
	url = {https://www.tandfonline.com/doi/full/10.1080/19345747.2017.1402396},
	doi = {10.1080/19345747.2017.1402396},
	abstract = {ABSTRACTEnrichment programs provide learning opportunities for a broader or deeper examination of curricular or extracurricular topics and are popular in gifted education. Herein, we investigated t...},
	number = {3},
	journal = {https://doi.org/10.1080/19345747.2017.1402396},
	author = {Golle, Jessika and Zettler, Ingo and Rose, Norman and Trautwein, Ulrich and Hasselhorn, Marcus and Nagengast, Benjamin},
	year = {2018},
	pages = {375--408},
}

@techreport{bachman_learning_2019,
	title = {Learning {Representations} by {Maximizing} {Mutual} {Information} {Across} {Views}},
	url = {https://github.com/Philip-Bachman/amdim-public.},
	abstract = {We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views-e.g., presence of certain objects or occurrence of certain events. Following our proposed approach, we develop a model which learns image representations that significantly outper-form prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1\% accuracy on Im-ageNet using standard linear evaluation. This beats prior results by over 12\% and concurrent results by 7\%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: https://github.com/Philip-Bachman/amdim-public.},
	author = {Bachman, Philip and Hjelm, Devon and Buchwalter, William},
	year = {2019},
}

@article{chamier_artificial_2019,
	title = {Artificial {Intelligence} for {Microscopy}: {What} {You} {Should} {Know}},
	doi = {10.20944/PREPRINTS201902.0004.V1},
	author = {Chamier, Lucas von and Laine, Romain F. and Henriques, Ricardo},
	month = feb,
	year = {2019},
	note = {Publisher: Preprints},
	keywords = {machine learning, segmentation, classification, artificial intelligence, live-cell imaging, super-resolution microscopy},
}

@article{shin_self-attention_2019,
	title = {Self-{Attention} {Based} {Molecule} {Representation} for {Predicting} {Drug}-{Target} {Interaction}},
	url = {http://arxiv.org/abs/1908.06760},
	abstract = {Predicting drug-target interactions (DTI) is an essential part of the drug discovery process, which is an expensive process in terms of time and cost. Therefore, reducing DTI cost could lead to reduced healthcare costs for a patient. In addition, a precisely learned molecule representation in a DTI model could contribute to developing personalized medicine, which will help many patient cohorts. In this paper, we propose a new molecule representation based on the self-attention mechanism, and a new DTI model using our molecule representation. The experiments show that our DTI model outperforms the state of the art by up to 4.9\% points in terms of area under the precision-recall curve. Moreover, a study using the DrugBank database proves that our model effectively lists all known drugs targeting a specific cancer biomarker in the top-30 candidate list.},
	author = {Shin, Bonggun and Park, Sungsoo and Kang, Keunsoo and Ho, Joyce C.},
	month = aug,
	year = {2019},
}

@techreport{wu_correspondence_nodate,
	title = {Correspondence to},
	author = {Wu, Wei},
}

@article{iliopoulou_dynamic_2018,
	title = {A dynamic three-step mechanism drives the {HIV}-1 pre-fusion reaction},
	volume = {25},
	doi = {10.1038/s41594-018-0113-x},
	abstract = {Little is known about the intermolecular dynamics and stoichiometry of the interactions of the human immunodeficiency virus type 1 (HIV-1) envelope (Env) protein with its receptors and co-receptors on the host cell surface. Here we analyze time-resolved HIV-1 Env interactions with T-cell surface glycoprotein CD4 (CD4) and C-C chemokine receptor type 5 (CCR5) or C-X-C chemokine receptor type 4 (CXCR4) on the surface of cells, by combining multicolor super-resolution localization microscopy (direct stochastic optical reconstruction microscopy) with fluorescence fluctuation spectroscopy imaging. Utilizing the primary isolate JR-FL and laboratory HXB2 strains, we reveal the time-resolved stoichiometry of CD4 and CCR5 or CXCR4 in the pre-fusion complex with HIV-1 Env. The HIV-1 Env pre-fusion dynamics for both R5- and X4-tropic strains consists of a three-step mechanism, which seems to differ in stoichiometry. Analyses with the monoclonal HIV-1-neutralizing antibody b12 indicate that the mechanism of inhibition differs between JR-FL and HXB2 Env. The molecular insights obtained here identify assemblies of HIV-1 Env with receptors and co-receptors as potential novel targets for inhibitor design.},
	number = {9},
	journal = {Nature Structural and Molecular Biology},
	author = {Iliopoulou, Maro and Nolan, Rory and Alvarez, Luis and Watanabe, Yasunori and Coomer, Charles A. and Jakobsdottir, G. Maria and Bowden, Thomas A. and Padilla-Parra, Sergi},
	month = sep,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	pages = {814--822},
}

@article{gao_single-shot_2014,
	title = {Single-shot compressed ultrafast photography at one hundred billion frames per second},
	volume = {516},
	doi = {10.1038/nature14005},
	abstract = {The capture of transient scenes at high imaging speed has been long sought by photographers, with early examples being the well known recording in 1878 of a horse in motion and the 1887 photograph of a supersonic bullet. However, not until the late twentieth century were breakthroughs achieved in demonstrating ultrahigh-speed imaging (more than 10 5 frames per second). In particular, the introduction of electronic imaging sensors based on the charge-coupled device (CCD) or complementary metal-oxide-semiconductor (CMOS) technology revolutionized high-speed photography, enabling acquisition rates of up to 10 7 frames per second. Despite these sensors widespread impact, further increasing frame rates using CCD or CMOS technology is fundamentally limited by their on-chip storage and electronic readout speed. Here we demonstrate a two-dimensional dynamic imaging technique, compressed ultrafast photography (CUP), which can capture non-repetitive time-evolving events at up to 10 11 frames per second. Compared with existing ultrafast imaging techniques, CUP has the prominent advantage of measuring an x-y-t (x, y, spatial coordinates; t, time) scene with a single camera snapshot, thereby allowing observation of transient events with temporal resolution as tens of picoseconds. Furthermore, akin to traditional photography, CUP is receive-only, and so does not need the specialized active illumination required by other single-shot ultrafast imagers. As a result, CUP can image a variety of luminescent-such as fluorescent or bioluminescent-objects. Using CUP, we visualize four fundamental physical phenomena with single laser shots only: laser pulse reflection and refraction, photon racing in two media, and faster-than-light propagation of non-information (that is, motion that appears faster than the speed of light but cannot convey information). Given CUP's capability, we expect it to find widespread applications in both fundamental and applied sciences, including biomedical research.},
	number = {729},
	journal = {Nature},
	author = {Gao, Liang and Liang, Jinyang and Li, Chiye and Wang, Lihong V.},
	month = dec,
	year = {2014},
	note = {Publisher: Nature Publishing Group},
	pages = {74--77},
}

@article{dabney_distributional_2020,
	title = {A distributional code for value in dopamine-based reinforcement learning},
	volume = {577},
	doi = {10.1038/s41586-019-1924-6},
	abstract = {Since its introduction, the reward prediction error theory of dopamine has explained a wealth of empirical phenomena, providing a unifying framework for understanding the representation of reward and value in the brain1–3. According to the now canonical theory, reward predictions are represented as a single scalar quantity, which supports learning about the expectation, or mean, of stochastic outcomes. Here we propose an account of dopamine-based reinforcement learning inspired by recent artificial intelligence research on distributional reinforcement learning4–6. We hypothesized that the brain represents possible future rewards not as a single mean, but instead as a probability distribution, effectively representing multiple future outcomes simultaneously and in parallel. This idea implies a set of empirical predictions, which we tested using single-unit recordings from mouse ventral tegmental area. Our findings provide strong evidence for a neural realization of distributional reinforcement learning.},
	number = {7792},
	journal = {Nature},
	author = {Dabney, Will and Kurth-Nelson, Zeb and Uchida, Naoshige and Starkweather, Clara Kwon and Hassabis, Demis and Munos, Rémi and Botvinick, Matthew},
	year = {2020},
	pages = {671--675},
}

@article{sverchkov_review_2017,
	title = {A review of active learning approaches to experimental design for uncovering biological networks},
	volume = {13},
	doi = {10.1371/journal.pcbi.1005466},
	abstract = {Various types of biological knowledge describe networks of interactions among elementary entities. For example, transcriptional regulatory networks consist of interactions among proteins and genes. Current knowledge about the exact structure of such networks is highly incomplete, and laboratory experiments that manipulate the entities involved are conducted to test hypotheses about these networks. In recent years, various automated approaches to experiment selection have been proposed. Many of these approaches can be characterized as active machine learning algorithms. Active learning is an iterative process in which a model is learned from data, hypotheses are generated from the model to propose informative experiments, and the experiments yield new data that is used to update the model. This review describes the various models, experiment selection strategies, validation techniques, and successful applications described in the literature; highlights common themes and notable distinctions among methods; and identifies likely directions of future research and open problems in the area.},
	number = {6},
	journal = {PLoS Computational Biology},
	author = {Sverchkov, Yuriy and Craven, Mark},
	month = jun,
	year = {2017},
	note = {Publisher: Public Library of Science},
}

@incollection{cromey_digital_2012,
	title = {Digital {Images} {Are} {Data}: {And} {Should} {Be} {Treated} as {Such}},
	volume = {931},
	abstract = {The scientific community has become very concerned about inappropriate image manipulation. In journals that check figures after acceptance, 20-25\% of the papers contained at least one figure that did not comply with the journal's instructions to authors. The scientific press continues to report a small, but steady stream of cases of fraudulent image manipulation. Inappropriate image manipulation taints the scientific record, damages trust within science, and degrades science's reputation with the general public. Scientists can learn from historians and photojournalists, who have provided a number of examples of attempts to alter or misrepresent the historical record. Scientists must remember that digital images are numerically sampled data that represent the state of a specific sample when examined with a specific instrument. These data should be carefully managed. Changes made to the original data need to be tracked like the protocols used for other experimental procedures. To avoid pitfalls, unexpected artifacts, and unintentional misrepresentation of the image data, a number of image processing guidelines are offered.},
	booktitle = {Methods in molecular biology ({Clifton}, {N}.{J}.)},
	publisher = {NIH Public Access},
	author = {Cromey, Douglas W.},
	year = {2012},
	doi = {10.1007/978-1-62703-056-4_1},
	pages = {1--27},
}

@article{sanchez-gonzalez_learning_2020,
	title = {Learning to {Simulate} {Complex} {Physics} with {Graph} {Networks}},
	url = {http://arxiv.org/abs/2002.09405},
	abstract = {Here we present a general framework for learning simulation, and provide a single model implementation that yields state-of-the-art performance across a variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework---which we term "Graph Network-based Simulators" (GNS)---represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework is the most accurate general-purpose learned physics simulator to date, and holds promise for solving a wide range of complex forward and inverse problems.},
	author = {Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter W.},
	month = feb,
	year = {2020},
}

@article{chen_slide_2019,
	title = {{SLIDE} : {In} {Defense} of {Smart} {Algorithms} over {Hardware} {Acceleration} for {Large}-{Scale} {Deep} {Learning} {Systems}},
	url = {http://arxiv.org/abs/1903.03129},
	abstract = {Deep Learning (DL) algorithms are the central focus of modern machine learning systems. As data volumes keep growing, it has become customary to train large neural networks with hundreds of millions of parameters to maintain enough capacity to memorize these volumes and obtain state-of-the-art accuracy. To get around the costly computations associated with large models and data, the community is increasingly investing in specialized hardware for model training. However, specialized hardware is expensive and hard to generalize to a multitude of tasks. The progress on the algorithmic front has failed to demonstrate a direct advantage over powerful hardware such as NVIDIA-V100 GPUs. This paper provides an exception. We propose SLIDE (Sub-LInear Deep learning Engine) that uniquely blends smart randomized algorithms, with multi-core parallelism and workload optimization. Using just a CPU, SLIDE drastically reduces the computations during both training and inference outperforming an optimized implementation of Tensorflow (TF) on the best available GPU. Our evaluations on industry-scale recommendation datasets, with large fully connected architectures, show that training with SLIDE on a 44 core CPU is more than 3.5 times (1 hour vs. 3.5 hours) faster than the same network trained using TF on Tesla V100 at any given accuracy level. On the same CPU hardware, SLIDE is over 10x faster than TF. We provide codes and scripts for reproducibility.},
	author = {Chen, Beidi and Medini, Tharun and Farwell, James and Gobriel, Sameh and Tai, Charlie and Shrivastava, Anshumali},
	month = mar,
	year = {2019},
}

@article{noauthor_5_nodate,
	title = {(5) ({PDF}) {A} rotated ellipse from three points},
	url = {https://www.researchgate.net/publication/327977026_A_rotated_ellipse_from_three_points},
}

@techreport{yang_deep_nodate,
	title = {Deep {Learning} for {Single} {Image} {Super}-{Resolution}: {A} {Brief} {Review}},
	abstract = {Single image super-resolution (SISR) is a notoriously challenging ill-posed problem that aims to obtain a high-resolution (HR) output from one of its low-resolution (LR) versions. Recently, powerful deep learning algorithms have been applied to SISR and have achieved state-of-the-art performance. In this survey, we review representative deep learning-based SISR methods and group them into two categories according to their contributions to two essential aspects of SISR: the exploration of efficient neural network architectures for SISR and the development of effective optimization objectives for deep SISR learning. For each category, a baseline is first established, and several critical limitations of the baseline are summarized. Then, representative works on overcoming these limitations are presented based on their original content, as well as our critical exposition and analyses, and relevant comparisons are conducted from a variety of perspectives. Finally, we conclude this review with some current challenges and future trends in SISR that leverage deep learning algorithms.},
	author = {Yang, Wenming and Zhang, Xuechen and Tian, Yapeng and Wang, Wei and Xue, Jing-Hao and Liao, Qingmin},
	keywords = {deep learning, neural networks, Index Terms-Single image super-resolution, objective function},
}

@article{han_unsupervised_2019,
	title = {Unsupervised {Image} {Super}-{Resolution} with an {Indirect} {Supervised} {Path}},
	url = {http://arxiv.org/abs/1910.02593},
	abstract = {The task of single image super-resolution (SISR) aims at reconstructing a high-resolution (HR) image from a low-resolution (LR) image. Although significant progress has been made by deep learning models, they are trained on synthetic paired data in a supervised way and do not perform well on real data. There are several attempts that directly apply unsupervised image translation models to address such a problem. However, unsupervised low-level vision problem poses more challenge on the accuracy of translation. In this work,we propose a novel framework which is composed of two stages: 1) unsupervised image translation between real LR images and synthetic LR images; 2) supervised super-resolution from approximated real LR images to HR images. It takes the synthetic LR images as a bridge and creates an indirect supervised path from real LR images to HR images. Any existed deep learning based image super-resolution model can be integrated into the second stage of the proposed framework for further improvement. In addition it shows great flexibility in balancing between distortion and perceptual quality under unsupervised setting. The proposed method is evaluated on both NTIRE 2017 and 2018 challenge datasets and achieves favorable performance against supervised methods.},
	author = {Han, Zhen and Dai, Enyan and Jia, Xu and Ren, Xiaoying and Chen, Shuaijun and Xu, Chunjing and Liu, Jianzhuang and Tian, Qi},
	month = oct,
	year = {2019},
}

@article{xie_self-training_2019,
	title = {Self-training with {Noisy} {Student} improves {ImageNet} classification},
	url = {http://arxiv.org/abs/1911.04252},
	abstract = {We present a simple self-training method that achieves 88.4\% top-1 accuracy on ImageNet, which is 2.0\% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0\% to 83.7\%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher.},
	author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
	year = {2019},
}

@article{nakkiran_deep_2019,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	url = {http://arxiv.org/abs/1912.02292},
	abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	month = dec,
	year = {2019},
}

@article{locatello_challenging_2018,
	title = {Challenging {Common} {Assumptions} in the {Unsupervised} {Learning} of {Disentangled} {Representations}},
	url = {https://arxiv.org/abs/1811.12359},
	author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Rätsch, Gunnar and Gelly, Sylvain and Schölkopf, Bernhard and Bachem, Olivier},
	month = nov,
	year = {2018},
}

@article{frankle_stabilizing_2019,
	title = {Stabilizing the {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/1903.01611},
	abstract = {Pruning is a well-established technique for removing unnecessary structure from neural networks after training to improve the performance of inference. Several recent results have explored the possibility of pruning at initialization time to provide similar benefits during training. In particular, the "lottery ticket hypothesis" conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. The evidence for this claim is that a procedure based on iterative magnitude pruning (IMP) reliably finds such subnetworks retroactively on small vision tasks. However, IMP fails on deeper networks, and proposed methods to prune before training or train pruned networks encounter similar scaling limitations. In this paper, we argue that these efforts have struggled on deeper networks because they have focused on pruning precisely at initialization. We modify IMP to search for subnetworks that could have been obtained by pruning early in training (0.1\% to 7\% through) rather than at iteration 0. With this change, it finds small subnetworks of deeper networks (e.g., 80\% sparsity on Resnet-50) that can complete the training process to match the accuracy of the original network on more challenging tasks (e.g., ImageNet). In situations where IMP fails at iteration 0, the accuracy benefits of delaying pruning accrue rapidly over the earliest iterations of training. To explain these behaviors, we study subnetwork "stability," finding that - as accuracy improves in this fashion - IMP subnetworks train to parameters closer to those of the full network and do so with improved consistency in the face of gradient noise. These results offer new insights into the opportunity to prune large-scale networks early in training and the behaviors underlying the lottery ticket hypothesis.},
	author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
	month = mar,
	year = {2019},
}

@article{frankle_lottery_2018,
	title = {The {Lottery} {Ticket} {Hypothesis}: {Finding} {Sparse}, {Trainable} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1803.03635},
	abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
	author = {Frankle, Jonathan and Carbin, Michael},
	month = mar,
	year = {2018},
}

@article{nakkiran_deep_2019-1,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	url = {https://arxiv.org/abs/1912.02292},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	month = dec,
	year = {2019},
}

@article{ribeiro_why_2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	url = {http://arxiv.org/abs/1602.04938},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year = {2016},
}

@article{schmidhuber_deep_2014-2,
	title = {Deep {Learning} in {Neural} {Networks}: {An} {Overview}},
	url = {http://arxiv.org/abs/1404.7828},
	doi = {10.1016/j.neunet.2014.09.003},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	author = {Schmidhuber, Juergen},
	month = apr,
	year = {2014},
}

@inproceedings{zhang_poisson-gaussian_2019,
	title = {A poisson-gaussian denoising dataset with real fluorescence microscopy images},
	volume = {2019-June},
	isbn = {978-1-72813-293-8},
	url = {http://tinyurl.com/y6mwqcjs},
	doi = {10.1109/CVPR.2019.01198},
	abstract = {Fluorescence microscopy has enabled a dramatic development in modern biology. Due to its inherently weak signal, fluorescence microscopy is not only much noisier than photography, but also presented with Poisson-Gaussian noise where Poisson noise, or shot noise, is the dominating noise source. To get clean fluorescence microscopy images, it is highly desirable to have effective denoising algorithms and datasets that are specifically designed to denoise fluorescence microscopy images. While such algorithms exist, no such datasets are available. In this paper, we fill this gap by constructing a dataset-the Fluorescence Microscopy Denoising (FMD) dataset-that is dedicated to Poisson-Gaussian denoising. The dataset consists of 12,000 real fluorescence microscopy images obtained with commercial confocal, two-photon, and wide-field microscopes and representative biological samples such as cells, zebrafish, and mouse brain tissues. We use image averaging to effectively obtain ground truth images and 60,000 noisy images with different noise levels. We use this dataset to benchmark 10 representative denoising algorithms and find that deep learning methods have the best performance. To our knowledge, this is the first real microscopy image dataset for Poisson-Gaussian denoising purposes and it could be an important tool for high-quality, real-time denoising applications in biomedical research.},
	author = {Zhang, Yide and Zhu, Yinhao and Nichols, Evan and Wang, Qingfei and Zhang, Siyuan and Smith, Cody and Howard, Scott},
	year = {2019},
	keywords = {Biological and Cell Microscopy, Datasets and Evaluation, Medical},
	pages = {11702--11710},
}

@techreport{marcus_next_nodate,
	title = {The {Next} {Decade} in {AI}: {Four} {Steps} {Towards} {Robust} {Artificial} {Intelligence}},
	abstract = {Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.},
	author = {Marcus, Gary},
}

@techreport{lawrence_data_2017,
	title = {Data {Readiness} {Levels}},
	abstract = {Application of models to data is fraught. Data-generating collaborators often only have a very basic understanding of the complications of collating, processing and curating data. Challenges include: poor data collection practices, missing values, inconvenient storage mechanisms, intellectual property, security and privacy. All these aspects obstruct the sharing and interconnection of data, and the eventual interpretation of data through machine learning or other approaches. In project reporting, a major challenge is in encapsulating these problems and enabling goals to be built around the processing of data. Project overruns can occur due to failure to account for the amount of time required to curate and collate. But to understand these failures we need to have a common language for assessing the readiness of a particular data set. This position paper proposes the use of data readiness levels: it gives a rough outline of three stages of data preparedness and speculates on how formalisation of these levels into a common language for data readiness could facilitate project management.},
	author = {Lawrence, Neil D},
	year = {2017},
	keywords = {()},
}

@article{mandracchia_fast_2020,
	title = {Fast and accurate {sCMOS} noise correction for fluorescence microscopy},
	volume = {11},
	doi = {10.1038/s41467-019-13841-8},
	abstract = {The rapid development of scientific CMOS (sCMOS) technology has greatly advanced optical microscopy for biomedical research with superior sensitivity, resolution, field-of-view, and frame rates. However, for sCMOS sensors, the parallel charge-voltage conversion and different responsivity at each pixel induces extra readout and pattern noise compared to charge-coupled devices (CCD) and electron-multiplying CCD (EM-CCD) sensors. This can produce artifacts, deteriorate imaging capability, and hinder quantification of fluorescent signals, thereby compromising strategies to reduce photo-damage to live samples. Here, we propose a content-adaptive algorithm for the automatic correction of sCMOS-related noise (ACsN) for fluorescence microscopy. ACsN combines camera physics and layered sparse filtering to significantly reduce the most relevant noise sources in a sCMOS sensor while preserving the fine details of the signal. The method improves the camera performance, enabling fast, low-light and quantitative optical microscopy with video-rate denoising for a broad range of imaging conditions and modalities.},
	number = {1},
	journal = {Nature Communications},
	author = {Mandracchia, Biagio and Hua, Xuanwen and Guo, Changliang and Son, Jeonghwan and Urner, Tara and Jia, Shu},
	month = dec,
	year = {2020},
	note = {Publisher: Nature Research},
	keywords = {Microscopy, Fluorescence imaging},
	pages = {1--12},
}

@techreport{caron_deep_nodate,
	title = {Deep {Clustering} for {Unsupervised} {Learning} of {Visual} {Features}},
	abstract = {Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks.},
	author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
	keywords = {clustering, unsupervised learning},
}

@article{noauthor_system_2019,
	title = {System and {Method} for {Detection} and {Classification} of {Objects} of {Interest} in {Microscope} {Images} by {Supervised} {Machine} {Learning} {BACKGROUND} {Supervised} {Convolutional} machine learning methods , particularly {Deep} {Learning} ( {DL} ) using {Neural} {Networks} ( {CNN} ) ar},
	volume = {17},
	number = {51},
	year = {2019},
}

@article{adiwardana_towards_2020,
	title = {Towards a {Human}-like {Open}-{Domain} {Chatbot}},
	url = {http://arxiv.org/abs/2001.09977},
	abstract = {We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is trained to minimize perplexity, an automatic metric that we compare against human judgement of multi-turn conversation quality. To capture this judgement, we propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of good conversation. Interestingly, our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72\% on multi-turn evaluation) suggests that a human-level SSA of 86\% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79\% SSA, 23\% higher than the next highest scoring chatbot that we evaluated.},
	author = {Adiwardana, Daniel and Luong, Minh-Thang and So, David R. and Hall, Jamie and Fiedel, Noah and Thoppilan, Romal and Yang, Zi and Kulshreshtha, Apoorv and Nemade, Gaurav and Lu, Yifeng and Le, Quoc V.},
	month = jan,
	year = {2020},
}

@article{sohn_fixmatch_2020,
	title = {{FixMatch}: {Simplifying} {Semi}-{Supervised} {Learning} with {Consistency} and {Confidence}},
	url = {http://arxiv.org/abs/2001.07685},
	abstract = {Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93\% accuracy on CIFAR-10 with 250 labels and 88.61\% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at https://github.com/google-research/fixmatch.},
	author = {Sohn, Kihyuk and Berthelot, David and Li, Chun-Liang and Zhang, Zizhao and Carlini, Nicholas and Cubuk, Ekin D. and Kurakin, Alex and Zhang, Han and Raffel, Colin},
	year = {2020},
}

@techreport{summerfield_how_nodate,
	title = {How to build a brain from scratch},
	abstract = {This advanced option course discusses the search for a general theory of learning and inference in biological brains. It draws upon diverse themes in the fields of psychology, neuroscience, machine learning and artificial intelligence research. We begin by posing broad questions. What are brains for, and what does it mean to ask how they "work"? Then, over a series of lectures, we discuss parallel computational approaches in machine learning/AI and psychology/neuroscience, including reinforcement learning, deep learning, and Bayesian methods. We contrast computational and representational approaches to understanding neuroscience data. We ask whether current approaches in machine learning are feasible and scaleable, and which methods-if any-resemble the computations observed in biological brains. We review how high-level cognitive functions-attention, episodic memory, concept formation, reasoning and executive control-are being instantiated in artificial agents, and how their implementation draws upon what we know about the mammalian brain. Finally, we contemplate the outlook for the future, and whether AI will be "solved" in the near future. November 2018 2 Contents},
	author = {Summerfield, Christopher},
}

@article{howard_fastai_2020,
	title = {fastai: {A} {Layered} {API} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2002.04688},
	abstract = {fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4-5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We have used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching. NB: This paper covers fastai v2, which is currently in pre-release at http://dev.fast.ai/},
	author = {Howard, Jeremy and Gugger, Sylvain},
	month = feb,
	year = {2020},
	file = {Full Text:/home/zwerg/Zotero/storage/XJNQWPD2/Howard and Gugger - 2020 - fastai A Layered API for Deep Learning.pdf:application/pdf},
}

@article{howard_fastai_2020-1,
	title = {fastai: {A} {Layered} {API} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2002.04688},
	abstract = {fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4-5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We have used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching. NB: This paper covers fastai v2, which is currently in pre-release at http://dev.fast.ai/},
	author = {Howard, Jeremy and Gugger, Sylvain},
	year = {2020},
}

@article{wong_fast_2020,
	title = {Fast is better than free: {Revisiting} adversarial training},
	url = {http://arxiv.org/abs/2001.03994},
	abstract = {Adversarial training, a method for learning robust deep networks, is typically assumed to be more expensive than traditional training due to the necessity of constructing adversarial examples via a first-order method like projected gradient decent (PGD). In this paper, we make the surprising discovery that it is possible to train empirically robust models using a much weaker and cheaper adversary, an approach that was previously believed to be ineffective, rendering the method no more costly than standard training in practice. Specifically, we show that adversarial training with the fast gradient sign method (FGSM), when combined with random initialization, is as effective as PGD-based training but has significantly lower cost. Furthermore we show that FGSM adversarial training can be further accelerated by using standard techniques for efficient training of deep networks, allowing us to learn a robust CIFAR10 classifier with 45\% robust accuracy to PGD attacks with \${\textbackslash}epsilon=8/255\$ in 6 minutes, and a robust ImageNet classifier with 43\% robust accuracy at \${\textbackslash}epsilon=2/255\$ in 12 hours, in comparison to past work based on "free" adversarial training which took 10 and 50 hours to reach the same respective thresholds. Finally, we identify a failure mode referred to as "catastrophic overfitting" which may have caused previous attempts to use FGSM adversarial training to fail. All code for reproducing the experiments in this paper as well as pretrained model weights are at https://github.com/locuslab/fast\_adversarial.},
	author = {Wong, Eric and Rice, Leslie and Kolter, J. Zico},
	month = jan,
	year = {2020},
}

@article{ji_mipre_2020,
	title = {{MIP}*={RE}},
	url = {http://arxiv.org/abs/2001.04383},
	abstract = {We show that the class MIP* of languages that can be decided by a classical verifier interacting with multiple all-powerful quantum provers sharing entanglement is equal to the class RE of recursively enumerable languages. Our proof builds upon the quantum low-degree test of (Natarajan and Vidick, FOCS 2018) by integrating recent developments from (Natarajan and Wright, FOCS 2019) and combining them with the recursive compression framework of (Fitzsimons et al., STOC 2019). An immediate byproduct of our result is that there is an efficient reduction from the Halting Problem to the problem of deciding whether a two-player nonlocal game has entangled value \$1\$ or at most \${\textbackslash}frac\{1\}\{2\}\$. Using a known connection, undecidability of the entangled value implies a negative answer to Tsirelson's problem: we show, by providing an explicit example, that the closure \$C\_\{qa\}\$ of the set of quantum tensor product correlations is strictly included in the set \$C\_\{qc\}\$ of quantum commuting correlations. Following work of (Fritz, Rev. Math. Phys. 2012) and (Junge et al., J. Math. Phys. 2011) our results provide a refutation of Connes' embedding conjecture from the theory of von Neumann algebras.},
	author = {Ji, Zhengfeng and Natarajan, Anand and Vidick, Thomas and Wright, John and Yuen, Henry},
	year = {2020},
}

@article{luo_single-shot_2020,
	title = {Single-shot autofocusing of microscopy images using deep learning},
	url = {http://arxiv.org/abs/2003.09585},
	doi = {10.1021/acsphotonics.0c01774},
	abstract = {We demonstrate a deep learning-based offline autofocusing method, termed Deep-R, that is trained to rapidly and blindly autofocus a single-shot microscopy image of a specimen that is acquired at an arbitrary out-of-focus plane. We illustrate the efficacy of Deep-R using various tissue sections that were imaged using fluorescence and brightfield microscopy modalities and demonstrate snapshot autofocusing under different scenarios, such as a uniform axial defocus as well as a sample tilt within the field-of-view. Our results reveal that Deep-R is significantly faster when compared with standard online algorithmic autofocusing methods. This deep learning-based blind autofocusing framework opens up new opportunities for rapid microscopic imaging of large sample areas, also reducing the photon dose on the sample.},
	journal = {ACS Photonics},
	author = {Luo, Yilin and Huang, Luzhe and Rivenson, Yair and Ozcan, Aydogan},
	month = mar,
	year = {2020},
	keywords = {deep learning, brightfield microscopy, deep learning microscopy 2, fluorescence microscopy, single-shot autofocusing},
	pages = {acsphotonics.0c01774--acsphotonics.0c01774},
}

@article{sharakshane_easy_2018,
	title = {An easy estimate of the {PFDD} for a plant illuminated with white {LEDs}: 1000 lx = 15 μmol/s/m2},
	doi = {10.1101/289280},
	abstract = {Dependencies have been shown and conversion factors have been determined, which allow to estimate PPFD, YPFD and radiometric power density of white LED light according to the known illumination in lux. A technique for estimating photosynthetically active radiation, which has an adequate accuracy for the task of illuminating plants, has been determined.},
	journal = {bioRxiv},
	author = {Sharakshane, Anton},
	month = mar,
	year = {2018},
	note = {Publisher: bioRxiv},
	keywords = {Light, LED, CCT, CRI, PAR, Plant, PPF, PPFD, Ra, YPF, YPFD},
}

@article{sharakshane_white_2017,
	title = {White {LED} {Lighting} for {Plants}},
	doi = {10.1101/215095},
	abstract = {The highest intensity of photosynthesis is obtained under red light, but plants die or their growth gets disrupted if only red light is used. For example, Korean researchers [1] have shown that under pure red light the amount of the grown lettuce is greater than under a combination of red and blue light, but the leaves have a significantly smaller amount of chlorophyll, polyphenols and antioxidants. And the researchers at the Faculty of Biology of the Moscow State University [2] have found that the synthesis of sugars is reduced, growth is inhibited and no blossoming occurs in the leaves of Chinese cabbage under narrow-band red and blue light (as compared to a sodium lamp).What kind of lighting is needed to get a fully developed, large, fragrant and tasty plant with moderate energy consumption?},
	journal = {bioRxiv},
	author = {Sharakshane, Anton},
	month = nov,
	year = {2017},
	note = {Publisher: bioRxiv},
	keywords = {Light, LED, CCT, CRI, PAR, Plant, PPF, YPF, PBAR},
}

@article{bannon_dynamic_2018,
	title = {Dynamic allocation of computational resources for deep learning-enabled cellular image analysis with {Kubernetes}},
	url = {https://doi.org/10.1101/505032},
	doi = {10.1101/505032},
	abstract = {Deep learning is transforming the ability of life scientists to extract information from images. These techniques have better accuracy than conventional approaches and enable previously impossible analyses. As the capability of deep learning methods expands, they are increasingly being applied to large imaging datasets. The computational demands of deep learning present a significant barrier to large-scale image analysis. To meet this challenge, we have developed DeepCell 2.0, a platform for deploying deep learning models on large imaging datasets ({\textgreater}105-megapixel images) in the cloud. This software enables the turnkey deployment of a Kubernetes cluster on all commonly used operating systems. By using a microservice architecture, our platform matches computational operations with their hardware requirements to reduce operating costs. Further, it scales computational resources to meet demand, drastically reducing the time necessary for analysis of large datasets. A thorough analysis of costs demonstrates that cloud computing is economically competitive for this application. By treating hardware infrastructure as software, this work foreshadows a new generation of software packages for biology in which computational resources are a dynamically allocated resource.},
	journal = {bioRxiv},
	author = {Bannon, Dylan and Moen, Erick and Schwartz, Morgan and Borba, Enrico and Cui, Sunny and Huang, Kevin and Camplisson, Isabella and Koe, Nora and Kyme, Daniel and Kudo, Takamasa and Chang, Brian and Pao, Edward and Osterman, Erik and Graf, William and Van Valen, David},
	month = dec,
	year = {2018},
	note = {Publisher: bioRxiv},
	pages = {505032--505032},
}

@techreport{duchovskis_optimization_nodate,
	title = {Optimization of lighting spectrum for photosynthetic system and productivity of lettuce by using light-emitting diodes},
	url = {https://www.researchgate.net/publication/259973431},
	abstract = {CITATIONS 28 READS 949 12 authors, including: Some of the authors of this publication are also working on these related projects: Long-term program "Horticulture: agro-biological basics and technologies" implemented by Lithuanian Research Centre for Agriculture and Forestry View project Fast Advanced Scintillator Timing (FAST) View project Aušra Brazaitytė Sodininkystės ir daržininkystės institutas/Lietuvos agrarinių ir miškų mokslų cen… 192 PUBLICATIONS 1,993 CITATIONS SEE PROFILE},
	author = {Duchovskis, Pavelas and Samuoliene, Giedre},
}

@article{killoran_continuous-variable_2018,
	title = {Continuous-variable quantum neural networks},
	volume = {033063},
	doi = {10.1103/physrevresearch.1.033063},
	abstract = {We introduce a general method for building neural networks on quantum computers. The quantum neural network is a variational quantum circuit built in the continuous-variable (CV) architecture, which encodes quantum information in continuous degrees of freedom such as the amplitudes of the electromagnetic field. This circuit contains a layered structure of continuously parameterized gates which is universal for CV quantum computation. Affine transformations and nonlinear activation functions, two key elements in neural networks, are enacted in the quantum network using Gaussian and non-Gaussian gates, respectively. The non-Gaussian gates provide both the nonlinearity and the universality of the model. Due to the structure of the CV model, the CV quantum neural network can encode highly nonlinear transformations while remaining completely unitary. We show how a classical network can be embedded into the quantum formalism and propose quantum versions of various specialized model such as convolutional, recurrent, and residual networks. Finally, we present numerous modeling experiments built with the Strawberry Fields software library. These experiments, including a classifier for fraud detection, a network which generates Tetris images, and a hybrid classical-quantum autoencoder, demonstrate the capability and adaptability of CV quantum neural networks.},
	journal = {arXiv},
	author = {Killoran, Nathan and Bromley, Thomas R. and Arrazola, Juan Miguel and Schuld, Maria and Quesada, Nicolás and Lloyd, Seth},
	year = {2018},
	note = {Publisher: American Physical Society},
	keywords = {doi:10.1103/PhysRevResearch.1.033063 url:https://d},
	pages = {1--22},
}

@article{muhonen_storing_2014,
	title = {Storing quantum information for 30 seconds in a nanoelectronic device},
	volume = {9},
	doi = {10.1038/nnano.2014.211},
	abstract = {The spin of an electron or a nucleus in a semiconductor1 naturally implements the unit of quantum information-the qubit. In addition, because semiconductors are currently used in the electronics industry, developing qubits in semiconductors would be a promising route to realize scalable quantum information devices2. The solid-state environment, however, may provide deleterious interactions between the qubit and the nuclear spins of surrounding atoms3, or charge and spin fluctuations arising from defects in oxides and interfaces4. For materials such as silicon, enrichment of the spin-zero 28Si isotope drastically reduces spin-bath decoherence5. Experiments on bulk spin ensembles in 28Si crystals have indeed demonstrated extraordinary coherence times6-8. However, it remained unclear whether these would persist at the single-spin level, in gated nanostructures near amorphous interfaces. Here, we present the coherent operation of individual 31P electron and nuclear spin qubits in a top-gated nanostructure, fabricated on an isotopically engineered 28Si substrate. The 31P nuclear spin sets the new benchmark coherence time ({\textgreater}30 s with Carr-Purcell-Meiboom-Gill (CPMG) sequence) of any single qubit in the solid state and reaches {\textgreater}99.99\% control fidelity. The electron spin CPMG coherence time exceeds 0.5 s, and detailed noise spectroscopy9 indicates that-contrary to widespread belief-it is not limited by the proximity to an interface. Instead, decoherence is probably dominated by thermal and magnetic noise external to the device, and is thus amenable to further improvement.},
	number = {12},
	journal = {Nature Nanotechnology},
	author = {Muhonen, Juha T. and Dehollain, Juan P. and Laucht, Arne and Hudson, Fay E. and Kalra, Rachpon and Sekiguchi, Takeharu and Itoh, Kohei M. and Jamieson, David N. and McCallum, Jeffrey C. and Dzurak, Andrew S. and Morello, Andrea},
	year = {2014},
	pages = {986--991},
}

@article{noauthor_visual_nodate,
	title = {A {Visual} {History} of {Interpretation} for {Image} {Recognition}},
	url = {https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/?utm_source=Deep+Learning+Weekly&utm_campaign=71a1499eea-EMAIL_CAMPAIGN_2019_04_24_03_18_COPY_01&utm_medium=email&utm_term=0_384567b42d-71a1499eea-72956657},
}

@article{bravyi_quantum_2018,
	title = {Quantum advantage with shallow circuits},
	volume = {362},
	doi = {10.1126/science.aar3106},
	abstract = {Quantum effects can enhance information-processing capabilities and speed up the solution of certain computational problems. Whether a quantum advantage can be rigorously proven in some setting or demonstrated experimentally using near-term devices is the subject of active debate. We show that parallel quantum algorithms running in a constant time period are strictly more powerful than their classical counterparts; they are provably better at solving certain linear algebra problems associated with binary quadratic forms. Our work gives an unconditional proof of a computational quantum advantage and simultaneously pinpoints its origin: It is a consequence of quantum nonlocality. The proposed quantum algorithm is a suitable candidate for near-future experimental realizations, as it requires only constant-depth quantum circuits with nearest-neighbor gates on a two-dimensional grid of qubits (quantum bits).},
	number = {6412},
	journal = {Science},
	author = {Bravyi, Sergey and Gosset, David and König, Robert},
	year = {2018},
	pages = {308--311},
}

@article{hassantabar_coviddeep_2020,
	title = {{CovidDeep}: {SARS}-{CoV}-2/{COVID}-19 {Test} {Based} on {Wearable} {Medical} {Sensors} and {Efficient} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2007.10497},
	abstract = {The novel coronavirus (SARS-CoV-2) has led to a pandemic. The current testing regime based on Reverse Transcription-Polymerase Chain Reaction for SARS-CoV-2 has been unable to keep up with testing demands, and also suffers from a relatively low positive detection rate in the early stages of the resultant COVID-19 disease. Hence, there is a need for an alternative approach for repeated large-scale testing of SARS-CoV-2/COVID-19. We propose a framework called CovidDeep that combines efficient DNNs with commercially available WMSs for pervasive testing of the virus. We collected data from 87 individuals, spanning three cohorts including healthy, asymptomatic, and symptomatic patients. We trained DNNs on various subsets of the features automatically extracted from six WMS and questionnaire categories to perform ablation studies to determine which subsets are most efficacious in terms of test accuracy for a three-way classification. The highest test accuracy obtained was 98.1\%. We also augmented the real training dataset with a synthetic training dataset drawn from the same probability distribution to impose a prior on DNN weights and leveraged a grow-and-prune synthesis paradigm to learn both DNN architecture and weights. This boosted the accuracy of the various DNNs further and simultaneously reduced their size and floating-point operations.},
	journal = {arXiv},
	author = {Hassantabar, Shayan and Stefano, Novati and Ghanakota, Vishweshwar and Ferrari, Alessandra and Nicola, Gregory N. and Bruno, Raffaele and Marino, Ignazio R. and Hamidouche, Kenza and Jha, Niraj K.},
	month = jul,
	year = {2020},
	note = {Publisher: arXiv},
	keywords = {Neural networks, COVID-19 test, Grow-and-prune synthesis, SARS-CoV-2, Synthetic data generation, Wearable medical sensors},
}

@article{li_fourier_2020,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and the Navier-Stokes equation (including the turbulent regime). Our Fourier neural operator shows state-of-the-art performance compared to existing neural network methodologies and it is up to three orders of magnitude faster compared to traditional PDE solvers.},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	month = oct,
	year = {2020},
}

@techreport{afifi_deep_nodate,
	title = {Deep {White}-{Balance} {Editing}},
	abstract = {We introduce a deep learning approach to realistically edit an sRGB image's white balance. Cameras capture sensor images that are rendered by their integrated signal processor (ISP) to a standard RGB (sRGB) color space encoding. The ISP rendering begins with a white-balance procedure that is used to remove the color cast of the scene's illumination. The ISP then applies a series of nonlinear color manipulations to enhance the visual quality of the final sRGB image. Recent work by [3] showed that sRGB images that were rendered with the incorrect white balance cannot be easily corrected due to the ISP's nonlinear rendering. The work in [3] proposed a k-nearest neighbor (KNN) solution based on tens of thousands of image pairs. We propose to solve this problem with a deep neural network (DNN) architecture trained in an end-to-end manner to learn the correct white balance. Our DNN maps an input image to two additional white-balance settings corresponding to indoor and outdoor illuminations. Our solution not only is more accurate than the KNN approach in terms of correcting a wrong white-balance setting but also provides the user the freedom to edit the white balance in the sRGB image to other illumination settings.},
	author = {Afifi, Mahmoud and Brown, Michael S},
}

@article{rasouli_deep_2020,
	title = {Deep {Learning} for {Vision}-based {Prediction}: {A} {Survey}},
	url = {http://arxiv.org/abs/2007.00095},
	abstract = {Vision-based prediction algorithms have a wide range of applications including autonomous driving, surveillance, human-robot interaction, weather prediction. The objective of this paper is to provide an overview of the field in the past five years with a particular focus on deep learning approaches. For this purpose, we categorize these algorithms into video prediction, action prediction, trajectory prediction, body motion prediction, and other prediction applications. For each category, we highlight the common architectures, training methods and types of data used. In addition, we discuss the common evaluation metrics and datasets used for vision-based prediction tasks. A database of all the information presented in this survey including, cross-referenced according to papers, datasets and metrics, can be found online at https://github.com/aras62/vision-based-prediction.},
	author = {Rasouli, Amir},
	year = {2020},
}

@techreport{burlin_deep_nodate,
	title = {Deep {Image} {Inpainting}},
	abstract = {We present a new take on several image inpaiting techniques on small, simple images from CIFAR10. We improved context encoders by implementing several major training tricks on GAN as well as adapting the network to WGAN. We also compare encoders and discriminators based on existing state-of-the-art models against basic CNN architectures. In parallel, we worked on density-based methods and implemented Pixel CNN, Diagonal BiLSTM and Row LSTM. We propose a variation on the first, and propose a simpler model Flattened Row LSTM. We show that we can get good results on CIFAR10 and reconcile L 2 loss and visual quality.},
	author = {Burlin, Charles and Le Calonnec, Yoann and Duperier, Louis},
}

@article{eecs_semantic_nodate,
	title = {Semantic {White} {Balance}: {Semantic} {Color} {Constancy} {Using} {Convolutional} {Neural} {Network}},
	doi = {10.1145/nnnnnnn.nnnnnnn},
	abstract = {(a) Input image (b) Gray world (c) Adobe Photoshop (d) Semantic mask (e) Ours Figure 1. Color constancy using the proposed method. In (a), an input image in sRGB color space is corrected using: (b) Adobe Photoshop (auto-color correction) and (b) the proposed semantic-based color constancy method. Abstract e goal of computational color constancy is to preserve the perceptive colors of objects under diierent lighting conditions by removing the eeect of color casts caused by the scene's illumination. With the rapid development of deep learning based techniques, sig-niicant progress has been made in image semantic segmentation. In this work, we exploit the semantic information together with the color and spatial information of the input image in order to remove color casts. We train a convolutional neural network (CNN) model that learns to estimate the illuminant color and gamma correction parameters based on the semantic information of the given image. Experimental results show that feeding the CNN with the semantic information leads to a signiicant improvement in the results by reducing the error by more than 40\%.},
	author = {Eecs, Mahmoud Aaa},
	keywords = {CNN, Color constancy, Semantic seg-mentation, White balance},
}

@article{afifi_deep_2020,
	title = {Deep {White}-{Balance} {Editing}},
	url = {http://arxiv.org/abs/2004.01354},
	abstract = {We introduce a deep learning approach to realistically edit an sRGB image's white balance. Cameras capture sensor images that are rendered by their integrated signal processor (ISP) to a standard RGB (sRGB) color space encoding. The ISP rendering begins with a white-balance procedure that is used to remove the color cast of the scene's illumination. The ISP then applies a series of nonlinear color manipulations to enhance the visual quality of the final sRGB image. Recent work by [3] showed that sRGB images that were rendered with the incorrect white balance cannot be easily corrected due to the ISP's nonlinear rendering. The work in [3] proposed a k-nearest neighbor (KNN) solution based on tens of thousands of image pairs. We propose to solve this problem with a deep neural network (DNN) architecture trained in an end-to-end manner to learn the correct white balance. Our DNN maps an input image to two additional white-balance settings corresponding to indoor and outdoor illuminations. Our solution not only is more accurate than the KNN approach in terms of correcting a wrong white-balance setting but also provides the user the freedom to edit the white balance in the sRGB image to other illumination settings.},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Afifi, Mahmoud and Brown, Michael S.},
	month = apr,
	year = {2020},
	note = {Publisher: IEEE Computer Society},
	pages = {1394--1403},
}

@techreport{glau_deep_2020,
	title = {The {Deep} {Parametric} {PDE} {Method}: {Application} to {Option} {Pricing} *},
	url = {https://github.com/LWunderlich/DeepPDE/blob/main/TwoAssetsExample/},
	abstract = {We propose the deep parametric PDE method to solve high-dimensional parametric partial differential equations. A single neural network approximates the solution of a whole family of PDEs after being trained without the need of sample solutions. As a practical application, we compute option prices in the multivariate Black-Scholes model. After a single training phase, the prices for different time, state and model parameters are available in milliseconds. We evaluate the accuracy in the price and a gen-eralisation of the implied volatility with examples of up to 25 dimensions. A comparison with alternative machine learning approaches, confirms the effectiveness of the approach.},
	author = {Glau, Kathrin and Wunderlich, Linus},
	year = {2020},
	keywords = {deep neural networks, basket options, high-dimensional prob-lems, parametric option pricing, parametric partial differential equations, un-certainty quantification},
}

@techreport{shin_submitted_nodate,
	title = {{SUBMITTED} {TO} {IEEE} {TRANSACTIONS} {ON} {NEURAL} {NETWORKS} {AND} {LEARNING} {SYSTEM} 1 {PEPSI}++: {Fast} and {Lightweight} {Network} for {Image} {Inpainting}},
	abstract = {Among the various generative adversarial network (GAN)-based image inpainting methods, coarse-to-fine network with a contextual attention module (CAM) has shown remarkable performance. However, owing to two stacked generative networks, the coarse-to-fine network needs numerous computational resources such as convolution operations and network parameters, which result in low speed. To address this problem, we propose a novel network architecture called PEPSI: parallel extended-decoder path for semantic inpainting network, which aims at reducing the hardware costs and improving the inpainting performance. PEPSI consists of a single shared encoding network and parallel decoding networks called coarse and inpainting paths. The coarse path produces a preliminary inpainting result to train the encoding network for the prediction of features for the CAM. Simultaneously, the inpainting path generates higher inpainting quality using the refined features reconstructed via the CAM. In addition, we propose Diet-PEPSI that significantly reduces the network parameters while maintaining the performance. In Diet-PEPSI, to capture the global contextual information with low hardware costs, we propose novel rate-adaptive dilated convolutional layers, which employ the common weights but produce dynamic features depending on the given dilation rates. Extensive experiments comparing the performance with state-of-the-art image inpainting methods demonstrate that both PEPSI and Diet-PEPSI improve the qualitative scores, i.e. the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM), as well as significantly reduce hardware costs such as computational time and the number of network parameters.},
	author = {Shin, Yong-Goo and Sagong, Min-Cheol and Yeo, Yoon-Jae and Kim, Seung-Wook and Ko, Sung-Jea},
	keywords = {generative adversarial network, image inpainting, Index Terms-Deep learning},
}

@techreport{tamada_review_nodate,
	title = {Review: {Noise} and artifact reduction for {MRI} using deep learning},
	author = {Tamada, Daiki},
}

@techreport{maleki_blockcnn_nodate,
	title = {{BlockCNN}: {A} {Deep} {Network} for {Artifact} {Removal} and {Image} {Compression}},
	url = {http://r0k.us/graphics/kodak/},
	abstract = {We present a general technique that performs both arti-fact removal and image compression. For artifact removal, we input a JPEG image and try to remove its compression artifacts. For compression, we input an image and process its 8 × 8 blocks in a sequence. For each block, we first try to predict its intensities based on previous blocks; then, we store a residual with respect to the input image. Our technique reuses JPEG's legacy compression and decom-pression routines. Both our artifact removal and our image compression techniques use the same deep network, but with different training weights. Our technique is simple and fast and it significantly improves the performance of artifact removal and image compression.},
	author = {Maleki, Danial and Nadalian, Soheila and Derakhshani, Mohammad Mahdi and Sadeghi, Mohammad Amin},
}

@techreport{zhao_hyperspectral_2019,
	title = {Hyperspectral {Unmixing} via {Deep} {Autoencoder} {Networks} for a {Generalized} {Linear}-{Mixture}/{Nonlinear}-{Fluctuation} {Model}},
	abstract = {Spectral unmixing is an important task in hyper-spectral image processing for separating the mixed spectral data pertaining to various materials observed individual pixels. Recently, nonlinear spectral unmixing has received particular attention because a linear mixture is not appropriate under many conditions. However, existing nonlinear unmixing approaches are often based on specific assumptions regarding the inherent nonlinearity, and they can be ineffective when applied to conditions deviating from the original assumptions. Therefore, these approaches are not well suited to scenes with unknown nonlinearity characteristics. This paper presents an unsupervised nonlinear spectral unmixing method based on a deep autoencoder network that applies to a generalized linear-mixture/nonlinear fluctuation model, consisting of a linear mixture component and an additive nonlinear mixture component that depends on both endmembers and abundances. The proposed approach benefits from the universal modeling ability of deep neural networks to learn the inherent nonlinearity of the nonlinear mixture component from the data itself via the autoencoder network, rather than relying on an assumed form. Extensive experiments with numerically synthetic, labeled laboratory-created data and real airborne data, illustrate the generality and effectiveness of this approach compared with state-of-the-art methods. Index Terms-Hyperspectral imaging, nonlinear spectral un-mixing, deep learning, autoencoder network.},
	author = {Zhao, Min and Member, Student and Wang, Mou and Chen, Jie and Member, Senior and Rahardja, Susanto},
	year = {2019},
	note = {Volume: XX},
	pages = {1--1},
}

@article{noauthor_pdf_nodate-1,
	title = {({PDF}) {Hyperspectral} {Unmixing} {Using} {A} {Neural} {Network} {Autoencoder}},
	url = {https://www.researchgate.net/publication/323950012_Hyperspectral_Unmixing_Using_A_Neural_Network_Autoencoder},
}

@techreport{frechin_plos_nodate,
	title = {{PLOS} {Biology} {Question} {Response} {Powered} by {Editorial} {Manager}® and {ProduXion} {Manager}® from {Aries} {Systems} {Corporation}},
	url = {https://biorxiv.altmetric.com/details/47638569},
	abstract = {Holo-tomographic microscopy (HTM) is a label-free microscopy method reporting the fine changes of a cell's refractive indexes (RI) in 3D at high spatial and temporal resolution. By combining HTM with epifluorescence, we demonstrate that mammalian cellular organelles such as Lipid droplets and mitochondria show specific RI 3D patterns. To go further, we developed a computer vision strategy using FIJI, CellProfiler3 and custom code that allows to use the fine images obtained by HTM in quantitative approaches. We could observe shape and dry mass dynamics of lipid droplets, of endocytic structures, or of entire cells division that were so far and to the best of our knowledge out of reach. We finally took advantage of the capacity of HTM to capture the motion of many organelles at the same time to report a multi-organelle spinning phenomenon and study its dynamic properties using pattern-matching and homography analysis. This work demonstrates that HTM gives access to an uncharted field of biological dynamics and describes a unique set of simple computer vision strategies that can be broadly used to quantify HTM images.},
	author = {Frechin, Mathieu and Eccublens, Nanolive SA and First Author, Switzerland and Sandoz, Patrick A and Sandoz Christopher Tremblay F Gisou van der Goot Mathieu Frechin, Patrick A},
}

@article{cotte_marker-free_2013,
	title = {Marker-free phase nanoscopy},
	volume = {7},
	doi = {10.1038/nphoton.2012.329},
	abstract = {We introduce a microscopic method that determines quantitative optical properties beyond the optical diffraction limit and allows direct imaging of unstained living biological specimens. In established holographic microscopy, complex fields are measured using interferometric detection, allowing diffraction-limited phase measurements. Here, we show that non-invasive optical nanoscopy can achieve a lateral resolution of 90 nm by using a quasi-2π-holographic detection scheme and complex deconvolution. We record holograms from different illumination directions on the sample plane and observe subwavelength tomographic variations of the specimen. Nanoscale apertures serve to calibrate the tomographic reconstruction and to characterize the imaging system by means of the coherent transfer function. This gives rise to realistic inverse filtering and guarantees true complex field reconstruction. The observations are shown for nanoscopic porous cell frustule (diatoms), for the direct study of bacteria (Escherichia coli), and for a time-lapse approach to explore the dynamics of living dendritic spines (neurones). © 2013 Macmillan Publishers Limited. All rights reserved.},
	number = {2},
	journal = {Nature Photonics},
	author = {Cotte, Yann and Toy, Fatih and Jourdain, Pascal and Pavillon, Nicolas and Boss, Daniel and Magistretti, Pierre and Marquet, Pierre and Depeursinge, Christian},
	year = {2013},
	pages = {113--117},
}

@article{cotte_marker-free_2013-1,
	title = {Marker-free phase nanoscopy},
	volume = {7},
	url = {http://dx.doi.org/10.1038/nphoton.2012.329},
	doi = {10.1038/nphoton.2012.329},
	abstract = {We introduce a microscopic method that determines quantitative optical properties beyond the optical diffraction limit and allows direct imaging of unstained living biological specimens. In established holographic microscopy, complex fields are measured using interferometric detection, allowing diffraction-limited phase measurements. Here, we show that non-invasive optical nanoscopy can achieve a lateral resolution of 90 nm by using a quasi-2π-holographic detection scheme and complex deconvolution. We record holograms from different illumination directions on the sample plane and observe subwavelength tomographic variations of the specimen. Nanoscale apertures serve to calibrate the tomographic reconstruction and to characterize the imaging system by means of the coherent transfer function. This gives rise to realistic inverse filtering and guarantees true complex field reconstruction. The observations are shown for nanoscopic porous cell frustule (diatoms), for the direct study of bacteria (Escherichia coli), and for a time-lapse approach to explore the dynamics of living dendritic spines (neurones). © 2013 Macmillan Publishers Limited. All rights reserved.},
	number = {2},
	journal = {Nature Photonics},
	author = {Cotte, Yann and Toy, Fatih and Jourdain, Pascal and Pavillon, Nicolas and Boss, Daniel and Magistretti, Pierre and Marquet, Pierre and Depeursinge, Christian},
	year = {2013},
	note = {Publisher: Nature Publishing Group},
	pages = {113--117},
}

@techreport{beyer_image_nodate,
	title = {{AN} {IMAGE} {IS} {WORTH} {16X16} {WORDS}: {TRANSFORMERS} {FOR} {IMAGE} {RECOGNITION} {AT} {SCALE}},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1},
	author = {Beyer, Lucas},
}

@article{palsson_hyperspectral_2018,
	title = {Hyperspectral {Unmixing} {Using} a {Neural} {Network} {Autoencoder}},
	volume = {6},
	doi = {10.1109/ACCESS.2018.2818280},
	abstract = {In this paper, we present a deep learning based method for blind hyperspectral unmixing in the form of a neural network autoencoder. We show that the linear mixture model implicitly puts certain architectural constraints on the network, and it effectively performs blind hyperspectral unmixing. Several different architectural configurations of both shallow and deep encoders are evaluated. Also, deep encoders are tested using different activation functions. Furthermore, we investigate the performance of the method using three different objective functions. The proposed method is compared to other benchmark methods using real data and previously established ground truths of several common data sets. Experiments show that the proposed method compares favorably to other commonly used hyperspectral unmixing methods and exhibits robustness to noise. This is especially true when using spectral angle distance as the network's objective function. Finally, results indicate that a deeper and a more sophisticated encoder does not necessarily give better results.},
	journal = {IEEE Access},
	author = {Palsson, Burkni and Sigurdsson, Jakob and Sveinsson, Johannes R. and Ulfarsson, Magnus O.},
	month = mar,
	year = {2018},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {neural network, deep learning, autoencoder, endmember extraction, Hyperspectral unmixing, spectral angle distance},
	pages = {25646--25656},
}

@inproceedings{ioffe_batch_2015,
	title = {Batch normalization: {Accelerating} deep network training by reducing internal covariate shift},
	volume = {1},
	isbn = {978-1-5108-1058-7},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
	publisher = {International Machine Learning Society (IMLS)},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
	pages = {448--456},
}

@article{tan_vokenization_2020,
	title = {Vokenization: {Improving} {Language} {Understanding} with {Contextualized}, {Visual}-{Grounded} {Supervision}},
	url = {http://arxiv.org/abs/2010.06775},
	abstract = {Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper. We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora. Therefore, we develop a technique named "vokenization" that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call "vokens"). The "vokenizer" is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora. Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG. Code and pre-trained models publicly available at https://github.com/airsplay/vokenization},
	author = {Tan, Hao and Bansal, Mohit},
	month = oct,
	year = {2020},
}

@article{wang_ultralow_2020,
	title = {Ultralow power demand in fluorescence nanoscopy with digitally enhanced stimulated emission depletion},
	volume = {9},
	url = {https://doi.org/10.1515/nanoph-2019-0475},
	doi = {10.1515/nanoph-2019-0475},
	abstract = {Stimulated emission depletion (STED) microscopy breaks the optical diffraction barrier and has become a powerful tool for biological study. However, its application for in vivo study is limited because of its high demand for depletion power. Here, we propose digitally enhanced STED (DE-STED) as a method for reducing the depletion power that is required for STED superresolution imaging. A donut image is the key in this approach, which is composed of the depleted photons by STED laser and represents the intensity difference between confocal and STED images from the same imaging position. The depletion efficiency is digitally enhanced by multiplying the intensity of the donut image with a factor greater than 1, and then the photons from the periphery of the diffraction-limited spot are fully depleted by subtracting the enhanced donut image from the original confocal image. Finally, DE-STED achieves a resolution of {\textasciitilde}λ/8 in biological samples with a depletion power that is an order of magnitude lower than that in traditional STED imaging. Furthermore, the proposed method helps to relax the restrictions on the fluorophore because of its low phototoxicity and photobleaching.},
	number = {4},
	journal = {Nanophotonics},
	author = {Wang, Luwei and Chen, Yue and Peng, Xiao and Zhang, Jia and Wang, Jialin and Liu, Liwei and Yang, Zhigang and Yan, Wei and Qu, Junle},
	month = apr,
	year = {2020},
	note = {Publisher: De Gruyter},
	keywords = {fluorescence microscopy, digital enhancement, stimulated emission depletion, superresolution imaging},
	pages = {831--839},
}

@article{khattak_linking_2019,
	title = {Linking plasma formation in grapes to microwave resonances of aqueous dimers},
	volume = {116},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/30782800},
	doi = {10.1073/PNAS.1818350116},
	abstract = {In a popular parlor trick, plasma is created by irradiating grape hemispheres in a household microwave oven. This work ties the source of the plasma to microwave photonic hotspots at the junction of aqueous dielectric spherical dimers. We use a combination of thermal-imaging techniques and computer simulations to show that grape-sized fruit and hydrogel beads form resonant cavities that concentrate electromagnetic fields to extreme subwavelength regions. This is enabled by the large dielectric susceptibility of water at microwave frequencies. Furthermore, the absorptive properties of water are key to washing out complex internal modes and for allowing the evanescent hotspot build-up. Our approach to microwave resonances in high-dielectric materials opens a sandbox for nanocluster photonics research.},
	number = {10},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Khattak, Hamza K. and Bianucci, Pablo and Slepkov, Aaron D.},
	year = {2019},
	pages = {4000--4005},
}

@article{carion_end--end_2020,
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	url = {http://arxiv.org/abs/2005.12872},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	month = may,
	year = {2020},
}

@article{li_self-supervised_2020,
	title = {Self-supervised {Single}-view {3D} {Reconstruction} via {Semantic} {Consistency}},
	url = {http://arxiv.org/abs/2003.06473},
	abstract = {We learn a self-supervised, single-view 3D reconstruction model that predicts the 3D mesh shape, texture and camera pose of a target object with a collection of 2D images and silhouettes. The proposed method does not necessitate 3D supervision, manually annotated keypoints, multi-view images of an object or a prior 3D template. The key insight of our work is that objects can be represented as a collection of deformable parts, and each part is semantically coherent across different instances of the same category (e.g., wings on birds and wheels on cars). Therefore, by leveraging self-supervisedly learned part segmentation of a large collection of category-specific images, we can effectively enforce semantic consistency between the reconstructed meshes and the original images. This significantly reduces ambiguities during joint prediction of shape and camera pose of an object, along with texture. To the best of our knowledge, we are the first to try and solve the single-view reconstruction problem without a category-specific template mesh or semantic keypoints. Thus our model can easily generalize to various object categories without such labels, e.g., horses, penguins, etc. Through a variety of experiments on several categories of deformable and rigid objects, we demonstrate that our unsupervised method performs comparably if not better than existing category-specific reconstruction methods learned with supervision.},
	author = {Li, Xueting and Liu, Sifei and Kim, Kihwan and De Mello, Shalini and Jampani, Varun and Yang, Ming-Hsuan and Kautz, Jan},
	month = mar,
	year = {2020},
}

@article{tay_efficient_2020,
	title = {Efficient {Transformers}: {A} {Survey}},
	url = {http://arxiv.org/abs/2009.06732},
	abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
	month = sep,
	year = {2020},
}

@article{zhang_unreasonable_2018,
	title = {The {Unreasonable} {Effectiveness} of {Deep} {Features} as a {Perceptual} {Metric}},
	url = {http://arxiv.org/abs/1801.03924},
	abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
	month = jan,
	year = {2018},
	note = {Publisher: IEEE Computer Society},
	pages = {586--595},
}

@article{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	year = {2014},
}

@article{parmar_image_2018,
	title = {Image {Transformer}},
	url = {http://arxiv.org/abs/1802.05751},
	abstract = {Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.},
	author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Łukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
	month = feb,
	year = {2018},
}

@article{chi_relationnet_2020,
	title = {{RelationNet}++: {Bridging} {Visual} {Representations} for {Object} {Detection} via {Transformer} {Decoder}},
	url = {http://arxiv.org/abs/2010.15831},
	abstract = {Existing object detection frameworks are usually built on a single format of object/part representation, i.e., anchor/proposal rectangle boxes in RetinaNet and Faster R-CNN, center points in FCOS and RepPoints, and corner points in CornerNet. While these different representations usually drive the frameworks to perform well in different aspects, e.g., better classification or finer localization, it is in general difficult to combine these representations in a single framework to make good use of each strength, due to the heterogeneous or non-grid feature extraction by different representations. This paper presents an attention-based decoder module similar as that in Transformer{\textasciitilde}{\textbackslash}cite\{vaswani2017attention\} to bridge other representations into a typical object detector built on a single representation format, in an end-to-end fashion. The other representations act as a set of {\textbackslash}emph\{key\} instances to strengthen the main {\textbackslash}emph\{query\} representation features in the vanilla detectors. Novel techniques are proposed towards efficient computation of the decoder module, including a {\textbackslash}emph\{key sampling\} approach and a {\textbackslash}emph\{shared location embedding\} approach. The proposed module is named {\textbackslash}emph\{bridging visual representations\} (BVR). It can perform in-place and we demonstrate its broad effectiveness in bridging other representations into prevalent object detection frameworks, including RetinaNet, Faster R-CNN, FCOS and ATSS, where about \$1.5{\textbackslash}sim3.0\$ AP improvements are achieved. In particular, we improve a state-of-the-art framework with a strong backbone by about \$2.0\$ AP, reaching \$52.7\$ AP on COCO test-dev. The resulting network is named RelationNet++. The code will be available at https://github.com/microsoft/RelationNet2.},
	author = {Chi, Cheng and Wei, Fangyun and Hu, Han},
	month = oct,
	year = {2020},
}

@inproceedings{vincent_extracting_2008,
	title = {Extracting and composing robust features with denoising autoencoders},
	isbn = {978-1-60558-205-4},
	doi = {10.1145/1390156.1390294},
	abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite. Copyright 2008 by the author(s)/owner(s).},
	publisher = {Association for Computing Machinery (ACM)},
	author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre Antoine},
	year = {2008},
	pages = {1096--1103},
}

@article{eisenstein_smart_nodate,
	title = {Smart solutions for automated imaging},
	doi = {10.1038/s41592-020-00988-2},
	author = {Eisenstein, Michael},
	note = {Publisher: Springer US},
}

@article{bau_understanding_2020,
	title = {Understanding the {Role} of {Individual} {Units} in a {Deep} {Neural} {Network}},
	url = {http://arxiv.org/abs/2009.05041},
	doi = {10.1073/pnas.1907375117},
	abstract = {Deep neural networks excel at finding hierarchical representations that solve complex tasks over large data sets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Lapedriza, Agata and Zhou, Bolei and Torralba, Antonio},
	month = sep,
	year = {2020},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {201907375--201907375},
}

@article{sucholutsky_less_2020,
	title = {'{Less} {Than} {One}'-{Shot} {Learning}: {Learning} {N} {Classes} {From} {M}},
	url = {http://arxiv.org/abs/2009.08449},
	abstract = {Deep neural networks require large training sets but suffer from high computational cost and long training times. Training on much smaller training sets while maintaining nearly the same accuracy would be very beneficial. In the few-shot learning setting, a model must learn a new class given only a small number of samples from that class. One-shot learning is an extreme form of few-shot learning where the model must learn a new class from a single example. We propose the `less than one'-shot learning task where models must learn \$N\$ new classes given only \$M{\textless}N\$ examples and we show that this is achievable with the help of soft labels. We use a soft-label generalization of the k-Nearest Neighbors classifier to explore the intricate decision landscapes that can be created in the `less than one'-shot learning setting. We analyze these decision landscapes to derive theoretical lower bounds for separating \$N\$ classes using \$M{\textless}N\$ soft-label samples and investigate the robustness of the resulting systems.},
	author = {Sucholutsky, Ilia and Schonlau, Matthias},
	month = sep,
	year = {2020},
}

@article{zhou_unet_2018,
	title = {{UNet}++: {A} {Nested} {U}-{Net} {Architecture} for {Medical} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1807.10165},
	abstract = {In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
	author = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
	month = jul,
	year = {2018},
}

@article{bochkovskiy_yolov4_2020,
	title = {{YOLOv4}: {Optimal} {Speed} and {Accuracy} of {Object} {Detection}},
	url = {http://arxiv.org/abs/2004.10934},
	abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of {\textasciitilde}65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet},
	author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
	month = apr,
	year = {2020},
}

@article{zhou_learning_2015,
	title = {Learning {Deep} {Features} for {Discriminative} {Localization}},
	volume = {2016-December},
	url = {http://arxiv.org/abs/1512.04150},
	abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2\% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
	month = dec,
	year = {2015},
	note = {Publisher: IEEE Computer Society},
	pages = {2921--2929},
}

@article{sandler_mobilenetv2_2018-2,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	url = {http://arxiv.org/abs/1801.04381},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	month = jan,
	year = {2018},
	note = {Publisher: IEEE Computer Society},
	pages = {4510--4520},
}

@techreport{sandler_mobilenetv2_nodate-1,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the in-put/output domains from the expressiveness of the transformation , which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
}

@article{zoph_learning_2017-2,
	title = {Learning {Transferable} {Architectures} for {Scalable} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1707.07012},
	abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4\% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on ImageNet. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0\% achieving 43.1\% mAP on the COCO dataset.},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
	month = jul,
	year = {2017},
	note = {Publisher: IEEE Computer Society},
	pages = {8697--8710},
}

@techreport{tan_mnasnet_nodate,
	title = {{MnasNet}: {Platform}-{Aware} {Neural} {Architecture} {Search} for {Mobile}},
	url = {https://github.com/tensorflow/tpu/},
	abstract = {Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2\% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8× faster than MobileNetV2 [29] with 0.5\% higher accuracy and 2.3× faster than NASNet [36] with 1.2\% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/ tree/master/models/official/mnasnet.},
	number = {1807.11626v3},
	author = {Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V},
}

@article{weigert_biobeammultiplexed_2018-2,
	title = {Biobeam—{Multiplexed} wave-optical simulations of light-sheet microscopy},
	volume = {14},
	doi = {10.1371/journal.pcbi.1006079},
	abstract = {Sample-induced image-degradation remains an intricate wave-optical problem in light-sheet microscopy. Here we present biobeam, an open-source software package that enables simulation of operational light-sheet microscopes by combining data from 105–106multiplexed and GPU-accelerated point-spread-function calculations. The wave-optical nature of these simulations leads to the faithful reproduction of spatially varying aberrations, diffraction artifacts, geometric image distortions, adaptive optics, and emergent wave-optical phenomena, and renders image-formation in light-sheet microscopy computationally tractable.},
	number = {4},
	journal = {PLoS Computational Biology},
	author = {Weigert, Martin and Subramanian, Kaushikaram and Bundschuh, Sebastian T. and Myers, Eugene W. and Kreysing, Moritz},
	month = apr,
	year = {2018},
	note = {Publisher: Public Library of Science},
}

@article{zhang_lookahead_2019,
	title = {Lookahead {Optimizer}: k steps forward, 1 step back},
	url = {http://arxiv.org/abs/1907.08610},
	abstract = {The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of fast weights generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank.},
	author = {Zhang, Michael R. and Lucas, James and Hinton, Geoffrey and Ba, Jimmy},
	month = jul,
	year = {2019},
}

@article{younis_real-time_2020,
	title = {Real-time object detection using pre-trained deep learning models mobilenet- {SSD}},
	issn = {9781450376730},
	doi = {10.1145/3379247.3379264},
	abstract = {Mobile networks and binary neural networks are the most commonly used techniques for modern deep learning models to perform a variety of tasks on embedded systems. In this paper, we develop a technique to identify an object considering the deep learning pre-trained model MobileNet for Single Shot Multi-Box Detector (SSD). This algorithm is used for real-time detection, and for webcam feed to detect the purpose webcam which detects the object in a video stream. Therefore, we use an object detection module that can detect what is in the video stream. In order to implement the module, we combine the MobileNet and the SSD framework for a fast and efficient deep learning-based method of object detection. The main purpose of our research is to elaborate the accuracy of an object detection method SSD and the importance of pre-trained deep learning model MobileNet. The experimental results show that the Average Precision (AP) of the algorithm to detect different classes as car, person and chair is 99.76\%, 97.76\% and 71.07\%, respectively. This improves the accuracy of behavior detection at a processing speed which is required for the real-time detection and the requirements of daily monitoring indoor and outdoor.},
	number = {March},
	journal = {ACM International Conference Proceeding Series},
	author = {Younis, Ayesha and Shixin, Li and Shelembi, J. N. and Hai, Zhang},
	year = {2020},
	keywords = {Computer vision, Deep Learning Neural Network, Real-time Object Detection, Single Shot Detector},
	pages = {44--48},
}

@article{dai_transformer-xl_2019-2,
	title = {Transformer-{XL}: {Attentive} {Language} {Models} {Beyond} a {Fixed}-{Length} {Context}},
	url = {http://arxiv.org/abs/1901.02860},
	abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
	journal = {ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
	month = jan,
	year = {2019},
	note = {Publisher: Association for Computational Linguistics (ACL)},
	pages = {2978--2988},
}

@article{schick_exploiting_2020,
	title = {Exploiting {Cloze} {Questions} for {Few} {Shot} {Text} {Classification} and {Natural} {Language} {Inference}},
	url = {http://arxiv.org/abs/2001.07676},
	abstract = {Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with "task descriptions" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, regular supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms both supervised training and unsupervised approaches in low-resource settings by a large margin.},
	author = {Schick, Timo and Schütze, Hinrich},
	month = jan,
	year = {2020},
}

@techreport{chen_generative_nodate,
	title = {Generative {Pretraining} from {Pixels}},
	abstract = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3\% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0\% accuracy with full fine-tuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0\% top-1 accuracy on a linear probe of our features.},
	author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeff and Jun, Heewoo and Dhariwal, Prafulla and Luan, David and Sutskever, Ilya},
}

@article{cardoen_specht_nodate,
	title = {{SPECHT} : {Self}-tuning {Plausibility} {Based} {Object} {Detection} {Enables} {Quantification} of {Conflict} in {Heterogeneous} {Multi}-scale {Microscopy}},
	volume = {1},
	author = {Cardoen, Ben and Member, Student and Wong, Timothy and Alan, Parsa and Lee, Sieun and Matsubara, Joanne Aiko and Robert, Ivan},
	pages = {1--12},
}

@article{udrescu_ai_2020,
	title = {{AI} {Feynman}: {A} physics-inspired method for symbolic regression},
	volume = {6},
	url = {http://advances.sciencemag.org/},
	doi = {10.1126/sciadv.aay2631},
	abstract = {A core challenge for both physics and artificial intelligence (AI) is symbolic regression: Finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality, and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-based test set, we improve the state-of-the-art success rate from 15 to 90\%.},
	number = {16},
	journal = {Science Advances},
	author = {Udrescu, Silviu Marian and Tegmark, Max},
	month = apr,
	year = {2020},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eaay2631--eaay2631},
}

@techreport{yong_gradient_nodate,
	title = {Gradient {Centralization}: {A} {New} {Optimization} {Technique} for {Deep} {Neural} {Networks}},
	url = {https://github.com/Yonghongwei/Gradient-Centralization.},
	abstract = {Optimization techniques are of great importance to effectively and efficiently train a deep neural network (DNN). It has been shown that using the first and second order statistics (e.g., mean and variance) to perform Z-score standardization on network activations or weight vectors, such as batch normalization (BN) and weight standardization (WS), can improve the training performance. Different from these existing methods that mostly operate on activations or weights, we present a new optimization technique, namely gradient centralization (GC), which operates directly on gradients by centralizing the gradient vectors to have zero mean. GC can be viewed as a projected gradient descent method with a constrained loss function. We show that GC can reg-ularize both the weight space and output feature space so that it can boost the generalization performance of DNNs. Moreover, GC improves the Lipschitzness of the loss function and its gradient so that the training process becomes more efficient and stable. GC is very simple to implement and can be easily embedded into existing gradient based DNN optimizers with only one line of code. It can also be directly used to fine-tune the pre-trained DNNs. Our experiments on various applications, including general image classification, fine-grained image classification, detection and segmentation, demonstrate that GC can consistently improve the performance of DNN learning. The code of GC can be found at https://github.com/Yonghongwei/Gradient-Centralization.},
	author = {Yong, Hongwei and Huang, Jianqiang and Hua, Xiansheng and Zhang, Lei},
	keywords = {Deep network optimization, gradient descent},
}

@article{liu_variance_2019,
	title = {On the {Variance} of the {Adaptive} {Learning} {Rate} and {Beyond}},
	url = {http://arxiv.org/abs/1908.03265},
	abstract = {The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: https://github.com/LiyuanLucasLiu/RAdam.},
	author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
	month = aug,
	year = {2019},
}

@article{yong_gradient_2020,
	title = {Gradient {Centralization}: {A} {New} {Optimization} {Technique} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2004.01461},
	abstract = {Optimization techniques are of great importance to effectively and efficiently train a deep neural network (DNN). It has been shown that using the first and second order statistics (e.g., mean and variance) to perform Z-score standardization on network activations or weight vectors, such as batch normalization (BN) and weight standardization (WS), can improve the training performance. Different from these existing methods that mostly operate on activations or weights, we present a new optimization technique, namely gradient centralization (GC), which operates directly on gradients by centralizing the gradient vectors to have zero mean. GC can be viewed as a projected gradient descent method with a constrained loss function. We show that GC can regularize both the weight space and output feature space so that it can boost the generalization performance of DNNs. Moreover, GC improves the Lipschitzness of the loss function and its gradient so that the training process becomes more efficient and stable. GC is very simple to implement and can be easily embedded into existing gradient based DNN optimizers with only one line of code. It can also be directly used to fine-tune the pre-trained DNNs. Our experiments on various applications, including general image classification, fine-grained image classification, detection and segmentation, demonstrate that GC can consistently improve the performance of DNN learning. The code of GC can be found at https://github.com/Yonghongwei/Gradient-Centralization.},
	author = {Yong, Hongwei and Huang, Jianqiang and Hua, Xiansheng and Zhang, Lei},
	month = apr,
	year = {2020},
}

@techreport{kirillov_panoptic_nodate,
	title = {Panoptic {Segmentation}},
	abstract = {We propose and study a task we name panoptic segmen-tation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete , an important step toward real-world vision systems. While early work in computer vision addressed related im-age/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation.},
	author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollár, Piotr},
}

@techreport{prakash_divnoising_2020,
	title = {{DIVNOISING}: {Diversity} {Denoising} with {Fully} {Convolutional} {Variational} {Autoencoders}},
	abstract = {Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. But there are limitations to what can be restored in corrupted images, and any given method needs to make a sensible compromise between many possible clean signals when predicting a restored image. Here, we propose DIVNOISING-a denoising approach based on fully-convolutional variational autoencoders, overcoming this problem by predicting a whole distribution of denoised images. Our method is unsupervised, requiring only noisy images and a description of the imaging noise, which can be measured or bootstrapped from noisy data. If desired, consensus predictions can be inferred from a set of DIVNOISING predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DIVNOISING samples from the posterior enable a plethora of useful applications. We are (i) discussing how optical character recognition (OCR) applications could benefit from diverse predictions on ambiguous data, and (ii) show in detail how instance cell segmentation gains performance when using diverse DIVNOISING predictions.},
	author = {Prakash, Mangal and Krull, Alexander and Jug, Florian},
	year = {2020},
}

@article{buchholz_denoiseg_2020,
	title = {{DenoiSeg}: {Joint} {Denoising} and {Segmentation}},
	url = {http://arxiv.org/abs/2005.02987},
	abstract = {Microscopy image analysis often requires the segmentation of objects, but training data for this task is typically scarce and hard to obtain. Here we propose DenoiSeg, a new method that can be trained end-to-end on only a few annotated ground truth segmentations. We achieve this by extending Noise2Void, a self-supervised denoising scheme that can be trained on noisy images alone, to also predict dense 3-class segmentations. The reason for the success of our method is that segmentation can profit from denoising, especially when performed jointly within the same network. The network becomes a denoising expert by seeing all available raw data, while co-learning to segment, even if only a few segmentation labels are available. This hypothesis is additionally fueled by our observation that the best segmentation results on high quality (very low noise) raw data are obtained when moderate amounts of synthetic noise are added. This renders the denoising-task non-trivial and unleashes the desired co-learning effect. We believe that DenoiSeg offers a viable way to circumvent the tremendous hunger for high quality training data and effectively enables few-shot learning of dense segmentations.},
	author = {Buchholz, Tim-Oliver and Prakash, Mangal and Krull, Alexander and Jug, Florian},
	month = may,
	year = {2020},
}

@article{xiao_neural_2020,
	title = {Neural {Supersampling} for {Real}-time {Rendering}},
	volume = {39},
	url = {https://doi.org/10.1145/3386569.3392376%0Ahttps://research.fb.com/publications/neural-supersampling-for-real-time-rendering/%0Ahttps://research.fb.com/blog/2020/07/introducing-neural-supersampling-for-real-time-rendering/},
	doi = {10.1145/3386569.3392376},
	abstract = {Input Ours Reference Fig. 1. Results of our real-time, learned 4 × 4 supersampling are shown for four sample scenes. From top to bottom: the rendered low-resolution color input, our reconstruction, and the rendered reference images. Our supersampling method takes the color, depth, and motion vectors of multiple low-resolution frames, and produces high-fidelity reconstructions by reducing aliasing and recovering scene details. Due to higher resolutions and refresh rates, as well as more photorealistic effects, real-time rendering has become increasingly challenging for video games and emerging virtual reality headsets. To meet this demand, modern graphics hardware and game engines often reduce the computational cost by rendering at a lower resolution and then upsampling to the native resolution. Following the recent advances in image and video superreso-lution in computer vision, we propose a machine learning approach that is specifically tailored for high-quality upsampling of rendered content in real-time applications. The main insight of our work is that in rendered content, the image pixels are point-sampled, but precise temporal dynamics are available. Our method combines this specific information that is typically available in modern renderers (i.e., depth and dense motion vectors) with a novel temporal network design that takes into account such specifics and is aimed at maximizing video quality while delivering real-time performance. By training on a large synthetic dataset rendered from multiple 3D scenes with recorded camera motion, we demonstrate high fidelity and temporally stable results in real-time, even in the highly challenging 4 × 4 upsampling},
	number = {4},
	journal = {Siggraph},
	author = {Xiao, L E I and Nouri, Salah and Chapman, Matt and Fix, Alexander and Lanman, Douglas},
	year = {2020},
	pages = {1--12},
}

@article{mannam_machine_2020,
	title = {Machine learning for faster \& smarter fluorescence lifetime imaging microscopy},
	author = {Mannam, Varun and Zhang, Yide and Yuan, Xiaotong and Howard, Scott S},
	year = {2020},
	keywords = {machine learning, deep learning, segmentation, microscopy, classification, convolutional neural network, fluorescence lifetime imaging microscopy},
	pages = {1--24},
}

@article{eismann_automated_2020-1,
	title = {Automated {3D} light-sheet screening with high spatiotemporal resolution reveals mitotic phenotypes},
	volume = {133},
	url = {https://jcs.biologists.org/content/133/11/jcs245043},
	doi = {10.1242/jcs.245043},
	abstract = {3D cell cultures enable the in vitro study of dynamic biological processes such as the cell cycle, but their use in high-Throughput screens remains impractical with conventional fluorescent microscopy. Here, we present a screening workflow for the automated evaluation of mitotic phenotypes in 3D cell cultures by light-sheet microscopy. After sample preparation by a liquid handling robot, cell spheroids are imaged for 24 h in toto with a dual-view inverted selective plane illumination microscope (diSPIM) with a much improved signal-Tonoise ratio, higher imaging speed, isotropic resolution and reduced light exposure compared to a spinning disc confocal microscope. A dedicated high-content image processing pipeline implements convolutional neural network-based phenotype classification. We illustrate the potential of our approach using siRNA knockdown and epigenetic modification of 28 mitotic target genes for assessing their phenotypic role in mitosis. By rendering light-sheet microscopy operational for high-Throughput screening applications, this workflow enables target gene characterization or drug candidate evaluation in tissue-like 3D cell culture models.},
	number = {11},
	journal = {Journal of Cell Science},
	author = {Eismann, Bjorn and Krieger, Teresa G. and Beneke, Jurgen and Bulkescher, Ruben and Adam, Lukas and Erfle, Holger and Herrmann, Carl and Eils, Roland and Conrad, Christian},
	month = jun,
	year = {2020},
	note = {Publisher: Company of Biologists Ltd},
	keywords = {Cell cycle, High-content screening, Light-sheet microscopy},
}

@article{intarapanich_fast_2016,
	title = {Fast processing of microscopic images using object-based extended depth of field},
	volume = {17},
	url = {http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1373-2},
	doi = {10.1186/s12859-016-1373-2},
	abstract = {Background: Microscopic analysis requires that foreground objects of interest, e.g. cells, are in focus. In a typical microscopic specimen, the foreground objects may lie on different depths of field necessitating capture of multiple images taken at different focal planes. The extended depth of field (EDoF) technique is a computational method for merging images from different depths of field into a composite image with all foreground objects in focus. Composite images generated by EDoF can be applied in automated image processing and pattern recognition systems. However, current algorithms for EDoF are computationally intensive and impractical, especially for applications such as medical diagnosis where rapid sample turnaround is important. Since foreground objects typically constitute a minor part of an image, the EDoF technique could be made to work much faster if only foreground regions are processed to make the composite image. We propose a novel algorithm called object-based extended depths of field (OEDoF) to address this issue. Methods: The OEDoF algorithm consists of four major modules: 1) color conversion, 2) object region identification, 3) good contrast pixel identification and 4) detail merging. First, the algorithm employs color conversion to enhance contrast followed by identification of foreground pixels. A composite image is constructed using only these foreground pixels, which dramatically reduces the computational time. Results: We used 250 images obtained from 45 specimens of confirmed malaria infections to test our proposed algorithm. The resulting composite images with all in-focus objects were produced using the proposed OEDoF algorithm. We measured the performance of OEDoF in terms of image clarity (quality) and processing time. The features of interest selected by the OEDoF algorithm are comparable in quality with equivalent regions in images processed by the state-of-the-art complex wavelet EDoF algorithm; however, OEDoF required four times less processing time. Conclusions: This work presents a modification of the extended depth of field approach for efficiently enhancing microscopic images. This selective object processing scheme used in OEDoF can significantly reduce the overall processing time while maintaining the clarity of important image features. The empirical results from parasite-infected red cell images revealed that our proposed method efficiently and effectively produced in-focus composite images. With the speed improvement of OEDoF, this proposed algorithm is suitable for processing large numbers of microscope images, e.g., as required for medical diagnosis.},
	number = {S19},
	journal = {BMC Bioinformatics},
	author = {Intarapanich, Apichart and Kaewkamnerd, Saowaluck and Pannarut, Montri and Shaw, Philip J. and Tongsima, Sissades},
	month = dec,
	year = {2016},
	note = {Publisher: BioMed Central Ltd.},
	keywords = {Algorithms, Bioinformatics, Computational Biology/Bioinformatics, Computer Appl. in Life Sciences, Microarrays},
	pages = {516--516},
}

@article{caicedo_nucleus_2019,
	title = {Nucleus segmentation across imaging experiments: the 2018 {Data} {Science} {Bowl}},
	volume = {16},
	doi = {10.1038/s41592-019-0612-7},
	abstract = {Segmenting the nuclei of cells in microscopy images is often the first step in the quantitative analysis of imaging data for biological and biomedical applications. Many bioimage analysis tools can segment nuclei in images but need to be selected and configured for every experiment. The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Top participants in the challenge succeeded in this task, developing deep-learning-based models that identified cell nuclei across many image types and experimental conditions without the need to manually adjust segmentation parameters. This represents an important step toward configuration-free bioimage analysis software tools.},
	number = {12},
	journal = {Nature Methods},
	author = {Caicedo, Juan C. and Goodman, Allen and Karhohs, Kyle W. and Cimini, Beth A. and Ackerman, Jeanelle and Haghighi, Marzieh and Heng, Cher Keng and Becker, Tim and Doan, Minh and McQuin, Claire and Rohban, Mohammad and Singh, Shantanu and Carpenter, Anne E.},
	month = dec,
	year = {2019},
	note = {Publisher: Nature Research},
	pages = {1247--1253},
}

@article{long_microscopy_2020,
	title = {Microscopy cell nuclei segmentation with enhanced {U}-{Net}},
	volume = {21},
	url = {/pmc/articles/PMC6950983/?report=abstract},
	doi = {10.1186/s12859-019-3332-1},
	abstract = {Background: Cell nuclei segmentation is a fundamental task in microscopy image analysis, based on which multiple biological related analysis can be performed. Although deep learning (DL) based techniques have achieved state-of-the-art performances in image segmentation tasks, these methods are usually complex and require support of powerful computing resources. In addition, it is impractical to allocate advanced computing resources to each dark- or bright-field microscopy, which is widely employed in vast clinical institutions, considering the cost of medical exams. Thus, it is essential to develop accurate DL based segmentation algorithms working with resources-constraint computing. Results: An enhanced, light-weighted U-Net (called U-Net+) with modified encoded branch is proposed to potentially work with low-resources computing. Through strictly controlled experiments, the average IOU and precision of U-Net+ predictions are confirmed to outperform other prevalent competing methods with 1.0\% to 3.0\% gain on the first stage test set of 2018 Kaggle Data Science Bowl cell nuclei segmentation contest with shorter inference time. Conclusions: Our results preliminarily demonstrate the potential of proposed U-Net+ in correctly spotting microscopy cell nuclei with resources-constraint computing.},
	number = {1},
	journal = {BMC Bioinformatics},
	author = {Long, Feixiao},
	month = jan,
	year = {2020},
	note = {Publisher: BioMed Central Ltd.},
	keywords = {Deep learning, Cell and cell nuclei segmentation, Enhanced U-Net},
}

@techreport{ganin_domain-adversarial_2016,
	title = {Domain-{Adversarial} {Training} of {Neural} {Networks}},
	abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and Marchand, Mario and Lempitsky, Victor and Dogan, Urun and Kloft, Marius and Orabona, Francesco and Tommasi, Tatiana and Ganin, al},
	year = {2016},
	note = {Volume: 17},
	keywords = {neural network, deep learning, representation learning, domain adaptation, image classification, person re-identification, sentiment analysis, synthetic data},
	pages = {1--35},
}

@inproceedings{wulfmeier_addressing_2017,
	title = {Addressing appearance change in outdoor robotics with adversarial domain adaptation},
	volume = {2017-September},
	isbn = {978-1-5386-2682-5},
	url = {http://arxiv.org/abs/1703.01461},
	doi = {10.1109/IROS.2017.8205961},
	abstract = {Appearance changes due to weather and seasonal conditions represent a strong impediment to the robust implementation of machine learning systems in outdoor robotics. While supervised learning optimises a model for the training domain, it will deliver degraded performance in application domains that underlie distributional shifts caused by these changes. Traditionally, this problem has been addressed via the collection of labelled data in multiple domains or by imposing priors on the type of shift between both domains. We frame the problem in the context of unsupervised domain adaptation and develop a framework for applying adversarial techniques to adapt popular, state-of-the-art network architectures with the additional objective to align features across domains. Moreover, as adversarial training is notoriously unstable, we first perform an extensive ablation study, adapting many techniques known to stabilise generative adversarial networks, and evaluate on a surrogate classification task with the same appearance change. The distilled insights are applied to the problem of free-space segmentation for motion planning in autonomous driving.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Wulfmeier, Markus and Bewley, Alex and Posner, Ingmar},
	month = dec,
	year = {2017},
	pages = {1551--1558},
}

@techreport{chopra_dlid_nodate,
	title = {{DLID}: {Deep} {Learning} for {Domain} {Adaptation} by {Interpolating} between {Domains}},
	abstract = {In many real world applications of machine learning, the distribution of the training data (on which the machine learning model is trained) is different from the distribution of the test data (where the learnt model is actually deployed). This is known as the problem of Domain Adaptation. We propose a novel deep learning model for domain adaptation which attempts to learn a predictively useful representation of the data by taking into account information from the distribution shift between the training and test data. Our key proposal is to successively learn multiple intermediate representations along an "inter-polating path" between the train and test domains. Our experiments on a standard object recognition dataset show a significant performance improvement over the state-of-the-art.},
	author = {Chopra, Sumit and Balakrishnan, Suhrid and Gopalan, Raghuraman},
}

@article{abe_neuroscience_2020,
	title = {Neuroscience {Cloud} {Analysis} {As} a {Service}},
	url = {https://www.biorxiv.org/content/early/2020/06/12/2020.06.11.146746},
	doi = {10.1101/2020.06.11.146746},
	abstract = {A major goal of computational neuroscience is to develop powerful analysis tools that operate on large datasets. These methods provide an essential toolset to unlock scientific insights from new experiments. Unfortunately, a major obstacle currently impedes progress: while existing analysis methods are frequently shared as open source software, the infrastructure needed to deploy these methods \{{\textbackslash}textendash\} at scale, reproducibly, cheaply, and quickly \{{\textbackslash}textendash\} remains totally inaccessible to all but a minority of expert users. As a result, many users can not fully exploit these tools, due to constrained computational resources (limited or costly compute hardware) and/or mismatches in expertise (experimentalists vs. large-scale computing experts). In this work we develop Neuroscience Cloud Analysis As a Service (NeuroCAAS): a fully-managed infrastructure platform, based on modern large-scale computing advances, that makes state-of-the-art data analysis tools accessible to the neuroscience community. We offer NeuroCAAS as an open source service with a drag-and-drop interface, entirely removing the burden of infrastructure expertise, purchasing, maintenance, and deployment. NeuroCAAS is enabled by three key contributions. First, NeuroCAAS cleanly separates tool implementation from usage, allowing cutting-edge methods to be served directly to the end user with no need to read or install any analysis software. Second, NeuroCAAS automatically scales as needed, providing reliable, highly elastic computational resources that are more efficient than personal or lab-supported hardware, without management overhead. Finally, we show that many popular data analysis tools offered through NeuroCAAS outperform typical analysis solutions (in terms of speed and cost) while improving ease of use and maintenance, dispelling the myth that cloud compute is prohibitively expensive and technically inaccessible. By removing barriers to fast, efficient cloud computation, NeuroCAAS can dramatically accelerate both the dissemination and the effective use of cutting-edge analysis tools for neuroscientific discovery.Competing Interest StatementThe authors have declared no competing interest.},
	journal = {bioRxiv},
	author = {Abe, Taiga and Kinsella, Ian and Saxena, Shreya and Paninski, Liam and Cunningham, John P},
	year = {2020},
	pages = {1--32},
}

@techreport{liang_distant_nodate,
	title = {Distant {Supervised} {Centroid} {Shift}: {A} {Simple} and {Efficient} {Approach} to {Visual} {Domain} {Adaptation} *},
	abstract = {Conventional domain adaptation methods usually resort to deep neural networks or subspace learning to find invariant representations across domains. However, most deep learning methods highly rely on large-size source domains and are computationally expensive to train, while subspace learning methods always have a quadratic time complexity that suffers from the large domain size. This paper provides a simple and efficient solution, which could be regarded as a well-performing baseline for domain adaptation tasks. Our method is built upon the nearest centroid classifier, seeking a subspace where the centroids in the target domain are moderately shifted from those in the source domain. Specifically, we design a unified objective without accessing the source domain data and adopt an alternating minimization scheme to iteratively discover the pseudo target labels, invariant subspace, and target centroids. Besides its privacy-preserving property (distant supervision), the algorithm is provably convergent and has a promising linear time complexity. In addition, the proposed method can be readily extended to multi-source setting and domain generalization, and it remarkably enhances popular deep adaptation methods by borrowing the learned transferable features. Extensive experiments on several benchmarks including object, digit, and face recognition datasets validate that our methods yield state-of-the-art results in various domain adaptation tasks.},
	author = {Liang, Jian and He, Ran and Sun, Zhenan and Tan, Tieniu},
}

@techreport{ganin_unsupervised_2015,
	title = {Unsupervised {Domain} {Adaptation} by {Backpropagation}},
	url = {http://proceedings.mlr.press/v37/ganin15.html},
	abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard back-propagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.},
	author = {Ganin, Yaroslav and Lempitsky, Victor and Ru, Lempitsky@skoltech},
	month = jun,
	year = {2015},
	pages = {1180--1189},
}

@techreport{mayer_adversarial_nodate,
	title = {Adversarial {Feature} {Distribution} {Alignment} for {Semi}-{Supervised} {Learning}},
	abstract = {Training deep neural networks with only a few labeled samples can lead to overfitting. This is problematic in Semi-Supervised Learning (SSL) where only a few labeled samples are available. In this paper, we show that a consequence of overfitting in SSL is feature distribution mis-alignment between labeled and unlabeled samples. Hence, we propose two new feature distribution alignment methods to reduce overfitting. Our methods are particularly effective when using only a small amount of labeled samples. Furthermore, we add consistency regularization to our adversarial alignment method and demonstrate that we always outperform pure consistency regularization and achieve particularly high improvements when using only a small amount of labeled samples. We test our method on CIFAR-10 and SVHN. On SVHN we achieve a test error of 3.88\% (250 labeled samples) and 3.39\% (1000 labeled samples) which is close to the fully supervised model 2.89\% (73k labeled samples). In comparison, the current state-of-the-art achieves only 4.29\% and 3.74\%.},
	author = {Mayer, Christoph and Paul, Matthieu and Timofte, Radu},
}

@techreport{tejankar_simple_nodate,
	title = {A simple baseline for domain adaptation using rotation prediction},
	abstract = {Recently, domain adaptation has become a hot research area with lots of applications. The goal is to adapt a model trained in one domain to another domain with scarce annotated data. We propose a simple yet effective method based on self-supervised learning that outperforms or is on par with most state-of-the-art algorithms, e.g. adversar-ial domain adaptation. Our method involves two phases: predicting random rotations (self-supervised) on the target domain along with correct labels for the source domain (supervised), and then using self-distillation on the target domain. Our simple method achieves state-of-the-art results on semi-supervised domain adaptation on DomainNet dataset. Further, we observe that the unlabeled target datasets of popular domain adaptation benchmarks do not contain any categories apart from testing categories. We believe this introduces a bias that does not exist in many real applications. We show that removing this bias from the unlabeled data results in a large drop in performance of state-of-the-art methods, while our simple method is relatively robust.},
	author = {Tejankar, Ajinkya and Pirsiavash, Hamed},
}

@techreport{luo_unsupervised_nodate,
	title = {Unsupervised {Domain} {Adaptation} via {Discriminative} {Manifold} {Embedding} and {Alignment}},
	url = {www.aaai.org},
	abstract = {Unsupervised domain adaptation is effective in leveraging the rich information from the source domain to the unsuper-vised target domain. Though deep learning and adversarial strategy make an important breakthrough in the adaptability of features, there are two issues to be further explored. First, the hard-assigned pseudo labels on the target domain are risky to the intrinsic data structure. Second, the batch-wise training manner in deep learning limits the description of the global structure. In this paper, a Riemannian mani-fold learning framework is proposed to achieve transferabil-ity and discriminability consistently. As to the first problem, this method establishes a probabilistic discriminant criterion on the target domain via soft labels. Further, this criterion is extended to a global approximation scheme for the second issue; such approximation is also memory-saving. The manifold metric alignment is exploited to be compatible with the embedding space. A theoretical error bound is derived to facilitate the alignment. Extensive experiments have been conducted to investigate the proposal and results of the comparison study manifest the superiority of consistent manifold learning framework.},
	author = {Luo, You-Wei and Ren, Chuan-Xian and Ge, Pengfei and Huang, Ke-Kun and Yu, Yu-Feng},
}

@techreport{yoo_learning_nodate,
	title = {Learning {Condensed} and {Aligned} {Features} for {Unsupervised} {Domain} {Adaptation} {Using} {Label} {Propagation}},
	abstract = {Unsupervised domain adaptation aiming to learn a specific task for one domain using another domain data, has emerged to address the labeling issue in supervised learning, especially because it is difficult to obtain massive amounts of labeled data in practice. The existing methods have succeeded by reducing the difference between the embedded features of both domains, but the performance is still unsatisfactory compared to the supervised learning scheme. This is attributable to the embedded features that lay around each other but do not align perfectly and establish clearly separable clusters. We propose a novel domain adaptation method based on label propagation and cycle consistency to let the clusters of the features from the two domains overlap exactly and become clear for high accuracy. Specifically, we introduce cycle consistency to enforce the relationship between each cluster and exploit label propagation to achieve the association between the data from the perspective of the manifold structure instead of a one-to-one relation. Hence, we successfully formed aligned and discriminative clusters. We present the empirical results of our method for various domain adaptation scenarios and visualize the embedded features to prove that our method is critical for better domain adaptation.},
	author = {Yoo, Jaeyoon and Park, Changhwa and Hong, Yongjun and Yoon, Sungroh},
}

@article{gadosey_sd-unet_2020,
	title = {{SD}-{UNet}: {Stripping} down {U}-{Net} for {Segmentation} of {Biomedical} {Images} on {Platforms} with {Low} {Computational} {Budgets}},
	volume = {10},
	url = {https://www.mdpi.com/2075-4418/10/2/110},
	doi = {10.3390/diagnostics10020110},
	abstract = {During image segmentation tasks in computer vision, achieving high accuracy performance while requiring fewer computations and faster inference is a big challenge. This is especially important in medical imaging tasks but one metric is usually compromised for the other. To address this problem, this paper presents an extremely fast, small and computationally effective deep neural network called Stripped-Down UNet (SD-UNet), designed for the segmentation of biomedical data on devices with limited computational resources. By making use of depthwise separable convolutions in the entire network, we design a lightweight deep convolutional neural network architecture inspired by the widely adapted U-Net model. In order to recover the expected performance degradation in the process, we introduce a weight standardization algorithm with the group normalization method. We demonstrate that SD-UNet has three major advantages including: (i) smaller model size (23x smaller than U-Net); (ii) 8x fewer parameters; and (iii) faster inference time with a computational complexity lower than 8M floating point operations (FLOPs). Experiments on the benchmark dataset of the Internatioanl Symposium on Biomedical Imaging (ISBI) challenge for segmentation of neuronal structures in electron microscopic (EM) stacks and the Medical Segmentation Decathlon (MSD) challenge brain tumor segmentation (BRATs) dataset show that the proposed model achieves comparable and sometimes better results compared to the current state-of-the-art.},
	number = {2},
	journal = {Diagnostics},
	author = {Gadosey, Pius Kwao and Li, Yujian and Agyekum, Enock Adjei and Zhang, Ting and Liu, Zhaoying and Yamak, Peter T. and Essaf, Firdaous},
	month = feb,
	year = {2020},
	keywords = {Computer vision, Biomedical image segmentation, Depthwise separable convolutions, Group normalization, Weight standardization},
	pages = {110--110},
}

@article{bailer_resource-efficient_2019,
	title = {Resource-{Efficient} {Object} {Detection} by {Sharing} {Backbone} {CNNs}},
	issn = {9781728156064},
	doi = {10.1109/ISM46123.2019.00042},
	abstract = {The detection of objects in image and video has made huge progress in recent years due to the use of deep convolutional neural networks (DNNs), with some network architectures becoming de-facto standards. This paper addresses the problem of sharing a backbone CNN for different tasks, for example, to enable detection of additional classes when an already trained network is available. When using multiple such neural networks, sharing a backbone can save inference time and memory consumption. We study sharing a common backbone between neural networks trained for different tasks (logoness and text block detection) based on Yolo v3. We provide results on the impact of different lengths of the shared backbone on performance and resource efficiency.},
	journal = {Proceedings - 2019 IEEE International Symposium on Multimedia, ISM 2019},
	author = {Bailer, Werner and Fassold, Hannes},
	year = {2019},
	keywords = {deep learning, CNN, backbone},
	pages = {196--199},
}

@article{long_terahertz_2019,
	title = {Terahertz image super-resolution based on a deep convolutional neural network},
	volume = {58},
	url = {https://www.osapublishing.org/viewmedia.cfm?uri=ao-58-10-2731&seq=0&html=true},
	doi = {10.1364/ao.58.002731},
	abstract = {We propose an effective and robust method for terahertz (THz) image super-resolution based on a deep convolutional neural network (CNN). A deep CNN model is designed. It learns an end-to-end mapping between the low- and high-resolution images. Blur kernels with multiple width and noise with multiple levels are taken into the training set so that the network can handle THz images very well. Quantitative comparison of the proposed method and other super-resolution methods on the synthetic THz images indicates that the proposed method performs better than other methods in accuracy and visual improvements. Experimental results on real THz images show that the proposed method significantly improves the quality of THz images with increased resolution and decreased noise, which proves the practicability and exactitude of the proposed method.},
	number = {10},
	journal = {Applied Optics},
	author = {Long, Zhenyu and Wang, Tianyi and You, ChengWu and Yang, Zhengang and Wang, Kejia and Liu, Jinsong},
	month = apr,
	year = {2019},
	note = {Publisher: The Optical Society},
	keywords = {Image processing, Neural networks, Medical imaging, Imaging systems, Spatial resolution, Terahertz imaging},
	pages = {2731--2731},
}

@article{long_terahertz_2019-1,
	title = {Terahertz image super-resolution based on a deep convolutional neural network},
	volume = {58},
	url = {https://www.osapublishing.org/viewmedia.cfm?uri=ao-58-10-2731&seq=0&html=true},
	doi = {10.1364/ao.58.002731},
	abstract = {We propose an effective and robust method for terahertz (THz) image super-resolution based on a deep convolutional neural network (CNN). A deep CNN model is designed. It learns an end-to-end mapping between the low- and high-resolution images. Blur kernels with multiple width and noise with multiple levels are taken into the training set so that the network can handle THz images very well. Quantitative comparison of the proposed method and other super-resolution methods on the synthetic THz images indicates that the proposed method performs better than other methods in accuracy and visual improvements. Experimental results on real THz images show that the proposed method significantly improves the quality of THz images with increased resolution and decreased noise, which proves the practicability and exactitude of the proposed method.},
	number = {10},
	journal = {Applied Optics},
	author = {Long, Zhenyu and Wang, Tianyi and You, ChengWu and Yang, Zhengang and Wang, Kejia and Liu, Jinsong},
	month = apr,
	year = {2019},
	note = {Publisher: The Optical Society},
	keywords = {Image processing, Neural networks, Medical imaging, Imaging systems, Spatial resolution, Terahertz imaging},
	pages = {2731--2731},
}

@techreport{fortiz_title_nodate,
	title = {Title: {Deep} {In}-{Situ} {Learning} for {Object} {Recognition}},
	url = {http://research-information.bristol.ac.uk},
	author = {Fortiz, Lagunes},
}

@book{viehmann_eli_nodate,
	title = {Eli {Stevens} {Luca} {Antiga}},
	isbn = {978-1-61729-526-3},
	url = {https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf},
	publisher = {Manning},
	author = {Viehmann, Luca; Stevens, Eli, Thomas; Antiga},
}

@article{guo_rapid_2020,
	title = {Rapid image deconvolution and multiview fusion for optical microscopy},
	doi = {10.1038/s41587-020-0560-x},
	abstract = {The contrast and resolution of images obtained with optical microscopes can be improved by deconvolution and computational fusion of multiple views of the same sample, but these methods are computationally expensive for large datasets. Here we describe theoretical and practical advances in algorithm and software design that result in image processing times that are tenfold to several thousand fold faster than with previous methods. First, we show that an ‘unmatched back projector’ accelerates deconvolution relative to the classic Richardson–Lucy algorithm by at least tenfold. Second, three-dimensional image-based registration with a graphics processing unit enhances processing speed 10- to 100-fold over CPU processing. Third, deep learning can provide further acceleration, particularly for deconvolution with spatially varying point spread functions. We illustrate our methods from the subcellular to millimeter spatial scale on diverse samples, including single cells, embryos and cleared tissue. Finally, we show performance enhancement on recently developed microscopes that have improved spatial resolution, including dual-view cleared-tissue light-sheet microscopes and reflective lattice light-sheet microscopes.},
	journal = {Nature Biotechnology},
	author = {Guo, Min and Li, Yue and Su, Yijun and Lambert, Talley and Nogare, Damian Dalle and Moyle, Mark W. and Duncan, Leighton H. and Ikegami, Richard and Santella, Anthony and Rey-Suarez, Ivan and Green, Daniel and Beiriger, Anastasia and Chen, Jiji and Vishwasrao, Harshad and Ganesan, Sundar and Prince, Victoria and Waters, Jennifer C. and Annunziata, Christina M. and Hafner, Markus and Mohler, William A. and Chitnis, Ajay B. and Upadhyaya, Arpita and Usdin, Ted B. and Bao, Zhirong and Colón-Ramos, Daniel and La Riviere, Patrick and Liu, Huafeng and Wu, Yicong and Shroff, Hari},
	year = {2020},
}

@article{fortiz_deep_2019,
	title = {Deep in-situ learning for object recognition},
	journal = {undefined},
	author = {Fortiz, Lagunes and Miguel, A},
	year = {2019},
}

@inproceedings{pinheiro_unsupervised_2018,
	title = {Unsupervised {Domain} {Adaptation} with {Similarity} {Learning}},
	isbn = {978-1-5386-6420-9},
	url = {http://arxiv.org/abs/1711.08995},
	doi = {10.1109/CVPR.2018.00835},
	abstract = {The objective of unsupervised domain adaptation is to leverage features from a labeled source domain and learn a classifier for an unlabeled target domain, with a similar but different data distribution. Most deep learning approaches to domain adaptation consist of two steps: (i) learn features that preserve a low risk on labeled samples (source domain) and (ii) make the features from both domains to be as indistinguishable as possible, so that a classifier trained on the source can also be applied on the target domain. In general, the classifiers in step (i) consist of fully-connected layers applied directly on the indistinguishable features learned in (ii). In this paper, we propose a different way to do the classification, using similarity learning. The proposed method learns a pairwise similarity function in which classification can be performed by computing similarity between prototype representations of each category. The domain-invariant features and the categorical prototype representations are learned jointly and in an end-to-end fashion. At inference time, images from the target domain are compared to the prototypes and the label associated with the one that best matches the image is outputed. The approach is simple, scalable and effective. We show that our model achieves state-of-the-art performance in different unsupervised domain adaptation scenarios.},
	publisher = {IEEE Computer Society},
	author = {Pinheiro, Pedro O.},
	month = dec,
	year = {2018},
	pages = {8004--8013},
}

@techreport{wang_deep_nodate,
	title = {Deep {Visual} {Domain} {Adaptation}: {A} {Survey}},
	abstract = {Deep domain adaptation has emerged as a new learning technique to address the lack of massive amounts of labeled data. Compared to conventional methods, which learn shared feature subspaces or reuse important source instances with shallow representations, deep domain adaptation methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning. There have been comprehensive surveys for shallow domain adaptation, but few timely reviews the emerging deep learning based methods. In this paper, we provide a comprehensive survey of deep domain adaptation methods for computer vision applications with four major contributions. First, we present a taxonomy of different deep domain adaptation scenarios according to the properties of data that define how two domains are diverged. Second, we summarize deep domain adaptation approaches into several categories based on training loss, and analyze and compare briefly the state-of-the-art methods under these categories. Third, we overview the computer vision applications that go beyond image classification, such as face recognition, semantic segmentation and object detection. Fourth, some potential deficiencies of current methods and several future directions are highlighted.},
	author = {Wang, Mei and Deng, Weihong},
}

@article{fei_learning_1997,
	title = {Learning {Parameter} {Optimization} of {Stochastic} {Gradient} {Descent} with {Momentum} for a {Stochastic} {Quadratic}},
	volume = {14},
	url = {https://doi.org/10.1364/BOE.8.005675},
	doi = {10.1364/BOE.8.005675},
	abstract = {The adaptive optics (AO) can be used to compensate for ocular aberrations to achieve near diffraction limited high-resolution retinal images. However, many factors such as the limited aberration measurement and correction accuracy with AO, intraocular scatter, imaging noise and so on will degrade the quality of retinal images. Image post processing is an indispensable and economical method to make up for the limitation of AO retinal imaging procedure. In this paper, we proposed a deep learning method to restore the degraded retinal images for the first time. The method directly learned an end-to-end mapping between the blurred and restored retinal images. The mapping was represented as a deep convolutional neural network that was trained to output high-quality images directly from blurry inputs without any preprocessing. This network was validated on synthetically generated retinal images as well as real AO retinal images. The assessment of the restored retinal images demonstrated that the image quality had been significantly improved.},
	number = {11},
	journal = {BIOMEDICAL OPTICS EXPRESS 5675 Ophthalmic Physiol. Opt},
	author = {Fei, Xiao and Zhao, Junlei and Zhao, Haoxin and Yun, Dai and Zhang, Yudong and Bao, H and Rao, C and Zhang, Y and Dai, Y and Rao, X and Loy, C C and Tang, X and Wang, X and Zeng, X and Qiu, S and Luo, P and Tian, Y and Li, H and Yang, S and Arathorn, D W and Yang, Q and Vogel, C R and Tiruveedhula, P and Roorda, A},
	year = {1997},
	keywords = {(1003010) Image reconstruction techniques, (0101080) Active or adaptive optics, (1704470) Ophthalmology, OCIS codes: (1001455) Blind deconvolution},
	pages = {23--23},
}

@article{luo_pixel_2019,
	title = {Pixel super-resolution for lens-free holographic microscopy using deep learning neural networks},
	volume = {27},
	url = {https://doi.org/10.1364/OE.27.013581},
	doi = {10.1364/oe.27.013581},
	abstract = {Lens-free holographic microscopy (LFHM) provides a cost-effective tool for large field-of-view imaging in various biomedical applications. However, due to the unit optical magnification, its spatial resolution is limited by the pixel size of the imager. Pixel super-resolution (PSR) technique tackles this problem by using a series of sub-pixel shifted low-resolution (LR) lens-free holograms to form the high-resolution (HR) hologram. Conventional iterative PSR methods require a large number of measurements and a time-consuming reconstruction process, limiting the throughput of LFHM in practice. Here we report a deep learning-based PSR approach to enhance the resolution of LFHM. Compared with the existing PSR methods, our neural network-based approach outputs the HR hologram in an end-to-end fashion and maintains consistency in resolution improvement with a reduced number of LR holograms. Moreover, by exploiting the resolution degradation model in the imaging process, the network can be trained with a data set synthesized from the LR hologram itself without resorting to the HR ground truth. We validated the effectiveness and the robustness of our method by imaging various types of samples using a single network trained on an entirely different data set. This deep learning-based PSR approach can significantly accelerate both the data acquisition and the HR hologram reconstruction processes, therefore providing a practical solution to fast, lens-free, super-resolution imaging.},
	number = {10},
	journal = {Optics Express},
	author = {Luo, Zhenxiang and Yurt, Abdulkadir and Stahl, Richard and Lambrechts, Andy and Reumers, Veerle and Braeken, Dries and Lagae, Liesbet},
	month = may,
	year = {2019},
	note = {Publisher: The Optical Society},
	keywords = {Image processing, Spatial resolution, High numerical aperture optics, Holographic microscopy, Image registration, Synthetic aperture imaging},
	pages = {13581--13581},
}

@article{nehme_deep-storm_2018-1,
	title = {Deep-{STORM}: super-resolution single-molecule microscopy by deep learning},
	volume = {5},
	url = {https://doi.org/10.1364/OPTICA.5.000458},
	doi = {10.1364/optica.5.000458},
	abstract = {We present an ultra-fast, precise, parameter-free method, which we term Deep-STORM, for obtaining super-resolution images from stochastically-blinking emitters, such as fluorescent molecules used for localization microscopy. Deep-STORM uses a deep convolutional neural network that can be trained on simulated data or experimental measurements, both of which are demonstrated. The method achieves state-of-the-art resolution under challenging signal-to-noise conditions and high emitter densities, and is significantly faster than existing approaches. Additionally, no prior information on the shape of the underlying structure is required, making the method applicable to any blinking data-set. We validate our approach by super-resolution image reconstruction of simulated and experimentally obtained data.},
	number = {4},
	journal = {Optica},
	author = {Nehme, Elias and Weiss, Lucien E. and Michaeli, Tomer and Shechtman, Yoav},
	month = apr,
	year = {2018},
	note = {Publisher: The Optical Society},
	keywords = {Image processing, Neural networks, High numerical aperture optics, Diffraction limit, Image enhancement, Image reconstruction},
	pages = {458--458},
}

@article{qiao_detectors_2020,
	title = {{DetectoRS}: {Detecting} {Objects} with {Recursive} {Feature} {Pyramid} and {Switchable} {Atrous} {Convolution}},
	url = {http://arxiv.org/abs/2006.02334},
	abstract = {Many modern object detectors demonstrate outstanding performances by using the mechanism of looking and thinking twice. In this paper, we explore this mechanism in the backbone design for object detection. At the macro level, we propose Recursive Feature Pyramid, which incorporates extra feedback connections from Feature Pyramid Networks into the bottom-up backbone layers. At the micro level, we propose Switchable Atrous Convolution, which convolves the features with different atrous rates and gathers the results using switch functions. Combining them results in DetectoRS, which significantly improves the performances of object detection. On COCO test-dev, DetectoRS achieves state-of-the-art 54.7\% box AP for object detection, 47.1\% mask AP for instance segmentation, and 49.6\% PQ for panoptic segmentation. The code is made publicly available.},
	author = {Qiao, Siyuan and Chen, Liang-Chieh and Yuille, Alan},
	month = jun,
	year = {2020},
}

@inproceedings{cohen_bidirectional_2019,
	title = {Bidirectional one-shot unsupervised domain mapping},
	volume = {2019-Octob},
	isbn = {978-1-72814-803-8},
	url = {https://github.com/tomercohen11/BiOST.},
	doi = {10.1109/ICCV.2019.00187},
	abstract = {We study the problem of mapping between a domain A, in which there is a single training sample and a domain B, for which we have a richer training set. The method we present is able to perform this mapping in both directions. For example, we can transfer all MNIST images to the visual domain captured by a single SVHN image and transform the SVHN image to the domain of the MNIST images. Our method is based on employing one encoder and one decoder for each domain, without utilizing weight sharing. The autoencoder of the single sample domain is trained to match both this sample and the latent space of domain B. Our results demonstrate convincing mapping between domains, where either the source or the target domain are defined by a single sample, far surpassing existing solutions. Our code is made publicly available at https://github.com/tomercohen11/BiOST.},
	author = {Cohen, Tomer and Wolf, Lior},
	year = {2019},
	pages = {1784--1792},
}

@article{cohen_bidirectional_2019-1,
	title = {Bidirectional {One}-{Shot} {Unsupervised} {Domain} {Mapping}},
	volume = {2019-October},
	url = {http://arxiv.org/abs/1909.01595},
	abstract = {We study the problem of mapping between a domain \$A\$, in which there is a single training sample and a domain \$B\$, for which we have a richer training set. The method we present is able to perform this mapping in both directions. For example, we can transfer all MNIST images to the visual domain captured by a single SVHN image and transform the SVHN image to the domain of the MNIST images. Our method is based on employing one encoder and one decoder for each domain, without utilizing weight sharing. The autoencoder of the single sample domain is trained to match both this sample and the latent space of domain \$B\$. Our results demonstrate convincing mapping between domains, where either the source or the target domain are defined by a single sample, far surpassing existing solutions. Our code is made publicly available at https://github.com/tomercohen11/BiOST},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	author = {Cohen, Tomer and Wolf, Lior},
	month = sep,
	year = {2019},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	pages = {1784--1792},
}

@article{zhang_high-throughput_2019,
	title = {High-throughput, high-resolution deep learning microscopy based on registration-free generative adversarial network},
	volume = {10},
	url = {https://doi.org/10.1364/BOE.10.001044},
	doi = {10.1364/boe.10.001044},
	abstract = {We combine a generative adversarial network (GAN) with light microscopy to achieve deep learning super-resolution under a large field of view (FOV). By appropriately adopting prior microscopy data in an adversarial training, the neural network can recover a high-resolution, accurate image of new specimen from its single low-resolution measurement. Its capacity has been broadly demonstrated via imaging various types of samples, such as USAF resolution target, human pathological slides, fluorescence-labelled fibroblast cells, and deep tissues in transgenic mouse brain, by both wide-field and light-sheet microscopes. The gigapixel, multi-color reconstruction of these samples verifies a successful GAN-based single image super-resolution procedure. We also propose an image degrading model to generate low resolution images for training, making our approach free from the complex image registration during training data set preparation. After a well-trained network has been created, this deep learning-based imaging approach is capable of recovering a large FOV ({\textasciitilde}95 mm2) enhanced resolution of {\textasciitilde}1.7 μm at high speed (within 1 second), while not necessarily introducing any changes to the setup of existing microscopes.},
	number = {3},
	journal = {Biomedical Optics Express},
	author = {Zhang, Hao and Fang, Chunyu and Xie, Xinlin and Yang, Yicong and Mei, Wei and Jin, Di and Fei, Peng},
	month = mar,
	year = {2019},
	note = {Publisher: The Optical Society},
	keywords = {Image registration, Image quality, Optical microscopy, Scanning microscopy, Synthetic aperture microscopy, Three dimensional microscopy},
	pages = {1044--1044},
}

@techreport{yuan_segfix_nodate,
	title = {{SegFix}: {Model}-{Agnostic} {Boundary} {Refinement} for {Segmentation}},
	url = {https://github.com/openseg-group/openseg.pytorch.},
	abstract = {We present a model-agnostic post-processing scheme to improve the boundary quality for the segmentation result that is generated by any existing segmentation model. Motivated by the empirical observation that the label predictions of interior pixels are more reliable, we propose to replace the originally unreliable predictions of boundary pixels by the predictions of interior pixels. Our approach processes only the input image through two steps: (i) localize the boundary pix-els and (ii) identify the corresponding interior pixel for each boundary pixel. We build the correspondence by learning a direction away from the boundary pixel to an interior pixel. Our method requires no prior information of the segmentation models and achieves nearly real-time speed. We empirically verify that our SegFix consistently reduces the boundary errors for segmentation results generated from various state-of-the-art models on Cityscapes, ADE20K and GTA5. Code is available at: https://github.com/openseg-group/openseg.pytorch.},
	author = {Yuan, Yuhui and Xie, Jingyi and Chen, Xilin and Wang, Jingdong},
	keywords = {Boundary Refinement, Instance Segmentation, Model Agnostic, Semantic Segmentation},
}

@article{prakash_divnoising_2020-1,
	title = {{DivNoising}: {Diversity} {Denoising} with {Fully} {Convolutional} {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/2006.06072},
	abstract = {Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. But there are limitations to what can be restored in corrupted images, and any given method needs to make a sensible compromise between many possible clean signals when predicting a restored image. Here, we propose DivNoising -- a denoising approach based on fully-convolutional variational autoencoders, overcoming this problem by predicting a whole distribution of denoised images. Our method is unsupervised, requiring only noisy images and a description of the imaging noise, which can be measured or bootstrapped from noisy data. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) discussing how optical character recognition (OCR) applications could benefit from diverse predictions on ambiguous data, and (ii) show in detail how instance cell segmentation gains performance when using diverse DivNoising predictions.},
	author = {Prakash, Mangal and Krull, Alexander and Jug, Florian},
	month = jun,
	year = {2020},
}

@article{saha_w-cell-net_2020,
	title = {W-{Cell}-{Net}: {Multi}-frame {Interpolation} of {Cellular} {Microscopy} {Videos}},
	url = {http://arxiv.org/abs/2005.06684},
	abstract = {Deep Neural Networks are increasingly used in video frame interpolation tasks such as frame rate changes as well as generating fake face videos. Our project aims to apply recent advances in Deep video interpolation to increase the temporal resolution of fluorescent microscopy time-lapse movies. To our knowledge, there is no previous work that uses Convolutional Neural Networks (CNN) to generate frames between two consecutive microscopy images. We propose a fully convolutional autoencoder network that takes as input two images and generates upto seven intermediate images. Our architecture has two encoders each with a skip connection to a single decoder. We evaluate the performance of several variants of our model that differ in network architecture and loss function. Our best model out-performs state of the art video frame interpolation algorithms. We also show qualitative and quantitative comparisons with state-of-the-art video frame interpolation algorithms. We believe deep video interpolation represents a new approach to improve the time-resolution of fluorescent microscopy.},
	author = {Saha, Rohit and Teklemariam, Abenezer and Hsu, Ian and Moses, Alan M.},
	month = may,
	year = {2020},
}

@article{zhang_image_2018,
	title = {Image {Super}-{Resolution} {Using} {Very} {Deep} {Residual} {Channel} {Attention} {Networks}},
	url = {http://arxiv.org/abs/1807.02758},
	abstract = {Convolutional neural network (CNN) depth is of crucial importance for image super-resolution (SR). However, we observe that deeper networks for image SR are more difficult to train. The low-resolution inputs and features contain abundant low-frequency information, which is treated equally across channels, hence hindering the representational ability of CNNs. To solve these problems, we propose the very deep residual channel attention networks (RCAN). Specifically, we propose a residual in residual (RIR) structure to form very deep network, which consists of several residual groups with long skip connections. Each residual group contains some residual blocks with short skip connections. Meanwhile, RIR allows abundant low-frequency information to be bypassed through multiple skip connections, making the main network focus on learning high-frequency information. Furthermore, we propose a channel attention mechanism to adaptively rescale channel-wise features by considering interdependencies among channels. Extensive experiments show that our RCAN achieves better accuracy and visual improvements against state-of-the-art methods.},
	journal = {Eccv},
	author = {Zhang, Yulun and Li, Kunpeng and Li, Kai and Wang, Lichen and Zhong, Bineng and Fu, Yun},
	month = jul,
	year = {2018},
	keywords = {super-resolution, channel attention, residual in residual},
}

@misc{noauthor_open_nodate,
	title = {Open {Graph} {Benchmark}},
	url = {https://snap-stanford.github.io/ogb-web/},
	abstract = {A collection of benchmark datasets, data-loaders and evaluators for graph machine learning in PyTorch.},
	language = {en-US},
	urldate = {2021-07-23},
	journal = {Open Graph Benchmark},
	file = {Snapshot:/home/zwerg/Zotero/storage/PZRQGDP9/ogb.stanford.edu.html:text/html},
}

@article{hu_open_2021,
	title = {Open {Graph} {Benchmark}: {Datasets} for {Machine} {Learning} on {Graphs}},
	shorttitle = {Open {Graph} {Benchmark}},
	url = {http://arxiv.org/abs/2005.00687},
	abstract = {We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu .},
	urldate = {2021-07-23},
	journal = {arXiv:2005.00687 [cs, stat]},
	author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
	month = feb,
	year = {2021},
	note = {arXiv: 2005.00687},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	annote = {Comment: Fix dataset bug in ogbg-code},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/5MDJN8VQ/Hu et al. - 2021 - Open Graph Benchmark Datasets for Machine Learnin.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/FUEZAMUA/2005.html:text/html},
}

@misc{href_wilds_nodate,
	title = {{WILDS}: {A} {Benchmark} of in-the-{Wild} {Distribution} {Shifts}},
	shorttitle = {{WILDS}},
	url = {http://ai.stanford.edu/blog/wilds/},
	abstract = {One of the most common assumptions in machine learning (ML) is that the training and test data are independently and identically distributed (i.i.d.). For example, we might collect some number of data points and then randomly split them, assigning half to the training set and half to the test set. However, this assumption is often broken in ML systems deployed in the wild. In real-world applications, distribution shifts— instances where a model is trained on data from one distribution but then deployed on data from a different distribution— are ubiquitous. For example, in medical applications, we might train a diagnosis model on patients from a few hospitals, and then deploy it more broadly to hospitals outside the training set 1; and in wildlife monitoring, we might train an animal recognition model on images from one set of camera traps and then deploy it to new camera traps 2. A large body of prior work has shown that these distribution shifts can significantly degrade model performance in a variety of real-world ML applications: models can perform poorly out-of-distribution, despite achieving high in-distribution performance 3. To be able to reliably deploy ML models in the wild, we urgently need to develop methods for training models that are robust to real-world distribution shifts. The WILDS benchmark To facilitate the development of ML models that are robust to real-world distribution shifts, our ICML 2021 paper presents WILDS, a curated benchmark of 10 datasets that reflect natural distribution shifts arising from different cameras, hospitals, molecular scaffolds, experiments, demographics, countries, time periods, users, and codebases. The WILDS datasets cover two common types of distribution shifts: domain generalization and subpopulation shift. In domain generalization, the training and test distributions comprise data from related but distinct domains. The figure shows an example from the OGB-MolPCBA dataset 4 in WILDS, where the task is to predict the biochemical properties of molecules, and the goal is to generalize to molecules with different molecular scaffolds that have not been seen in the training set. In subpopulation shift, we consider test distributions that are subpopulations of the training distribution, and seek to perform well even on the worst-case subpopulation. As an example, consider the CivilComments-WILDS dataset 5, where the task is toxicity classification on online text comments. Standard models perform well on average but poorly on comments that mention certain minority demographic groups (e.g., they might be likely to erroneously flag innocuous comments mentioning Black people as toxic), and we seek to train models that can perform equally well on comments that correspond to different demographic subpopulations. Finally, some datasets exhibit both types of distribution shifts. For example, the second example in the figure above is from the FMoW-WILDS dataset 6, where there is both a domain generalization problem over time (the training set consists of satellite images taken before 2013, while the test images were taken after 2016) as well as a subpopulation shift problem over different geographical regions (we seek to do well over all regions). Selection criteria for WILDS datasets WILDS builds on extensive data collection efforts by domain experts working on applying ML methods in their application areas, and who are often forced to grapple with distribution shifts to make progress in their applications. To design WILDS, we worked with these experts to identify, select, and adapt datasets that fulfilled the following criteria: Real-world relevance. The training/test splits and evaluation metrics are motivated by real-world scenarios and chosen in conjunction with domain experts. By focusing on realistic distribution shifts, WILDS complements existing distribution shift benchmarks, which have largely studied shifts that are cleanly characterized but are not likely to arise in real-world deployments. For example, many recent papers have studied datasets with shifts induced by synthetic transformations, such as changing the color of MNIST digits 7. Though these are important testbeds for systematic studies, model robustness need not transfer across shifts—e.g., a method that improves robustness on a standard vision dataset can consistently harm robustness on real-world satellite imagery datasets 8. So, in order to evaluate and develop methods for real-world distribution shifts, benchmarks like WILDS that capture shifts in the wild serve as an important complement to more synthetic benchmarks. Distribution shifts with large performance gaps. The train/test splits reflect shifts that substantially degrade model performance, i.e., with a large gap between in-distribution and out-of-distribution performance. Measuring the in-distribution versus out-of-distribution gap is an important but subtle problem, as it relies on carefully constructing an appropriate in-distribution setting. We discuss its complexities and our approach in more detail in the paper. Apart from the 10 datasets in WILDS, we also survey distribution shifts that occur in other application areas—algorithmic fairness and policing, medicine and healthcare, genomics, natural language and speech processing, education, and robotics—and discuss examples of datasets from these areas that we considered but did not include in WILDS. We investigated datasets in autonomous driving, fairness in policing, and computational biology, but either did not observe substantial performance drops or found that performance disparities arose from factors beyond distribution shifts. Using WILDS To make it easy to work with WILDS and to enable systematic comparisons between approaches, we developed an open-source Python package that fully automates data loading and evaluation. This package also contains default models and hyperparameters that can easily reproduce all of the baseline numbers we have in our paper. The package is simple to install—just run pip install wilds—and straightforward to use with any PyTorch-based algorithms and models: We are also hosting a public leaderboard at https://wilds.stanford.edu/leaderboard/ to track the state of the art in algorithms for learning robust models. In our paper, we benchmarked several existing algorithms for learning robust models, but found that they did not consistently improve upon standard models trained with empirical risk minimization (i.e., minimizing the average loss). We thus believe that there is substantial room for developing algorithms and model architectures that can close the gaps between in-distribution and out-of-distribution performance on the WILDS datasets. Just in the past few months, WILDS has been used to develop methods for domain generalization—such as Fish, which introduces an inter-domain gradient matching objective and is currently state-of-the-art on our leaderboard for several datasets 9, and a Model-Based Domain Generalization (MBDG) approach that uses generative modeling 10—as well as for subpopulation shift settings through environment inference 11 or a variant of distributionally robust optimization 12. WILDS has also been used to develop methods for out-of-distribution calibration 13, uncertainty measurement 14, gradual domain adaptation 15, and self-training 16. Finally, it has also been used to study out-of-distribution selective classification 17, and to investigate the relationship between in-distribution and out-of-distribution generalization 18. However, we have only just begun to scratch the surface of how we can train models that are robust to the distribution shifts that are unavoidable in real-world applications, and we’re excited to see what the ML research community will come up with. If you’re interested in trying WILDS out, please check out https://wilds.stanford.edu, and let us know if you have any questions or feedback. We’ll be presenting WILDS at ICML at 6pm Pacific Time on Thursday, July 22, 2021, with the poster session from 9pm to 11pm Pacific Time on the same day. If you’d like to find out more, please drop by https://icml.cc/virtual/2021/poster/10117! (The link requires ICML registration.) Acknowledgements WILDS is a large collaborative effort by researchers from Stanford, UC Berkeley, Cornell, INRAE, the University of Saskatchewan, the University of Tokyo, Recursion, Caltech, and Microsoft Research. This blog post is based on the WILDS paper: WILDS: A Benchmark of in-the-Wild Distribution Shifts. Pang Wei Koh*, Shiori Sagawa*, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. ICML 2021. We are grateful to the many people who generously volunteered their time and expertise to advise us on WILDS. J. R. Zech, M. A. Badgeley, M. Liu, A. B. Costa, J. J. Titano, and E. K. Oermann. Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study. In PLOS Medicine, 2018. ↩ S. Beery, G. V. Horn, and P. Perona. Recognition in terra incognita. In European Conference on Computer Vision (ECCV), pages 456–473, 2018. ↩ J. Quiñonero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset shift in machine learning. The MIT Press, 2009. ↩ W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open Graph Benchmark: Datasets for machine learning on graphs. In Advances in Neural Information Processing Systems (NeurIPS), 2020. ↩ D. Borkan, L. Dixon, J. Sorensen, N. Thain, and L. Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification. In WWW, pages 491–500, 2019. ↩ G. Christie, N. Fendley, J. Wilson, and R. Mukherjee. Functional map of the world. In Computer Vision and Pattern Recognition (CVPR), 2018. ↩ B. Kim, H. Kim, K. Kim, S. Kim, and J. Kim, 2019. Learning not to learn: Training deep neural networks with biased data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 9012-9020). ↩ S. M. Xie, A. Kumar, R. Jones, F. Khani, T. Ma, and P. Liang. In-N-Out: Pre-training and self-training using auxiliary information for out-of-distribution robustness. In International Conference on Learning Representations (ICLR), 2021. ↩ Y. Shi, J. Seely, P. H. Torr, N. Siddharth, A. Hannun, N. Usunier, and G. Synnaeve. Gradient Matching for Domain Generalization. arXiv preprint arXiv:2104.09937, 2021. ↩ A Robey, H. Hassani, and G. J. Pappas. Model-Based Robust Deep Learning. arXiv preprint arXiv:2005.10247, 2020. ↩ E. Creager, J. H. Jacobsen, and R. Zemel. Environment inference for invariant learning. In International Conference on Machine Learning, 2021. ↩ E. Liu, B. Haghgoo, A. Chen, A. Raghunathan, P. W. Koh, S. Sagawa, P. Liang, and C. Finn. Just Train Twice: Improving group robustness without training group information. In International Conference on Machine Learning (ICML), 2021. ↩ Y. Wald, A. Feder, D. Greenfeld, and U. Shalit. On calibration and out-of-domain generalization. arXiv preprint arXiv:2102.10395, 2021. ↩ E. Daxberger, A., Kristiadi, A., Immer, R., Eschenhagen, M., Bauer, and P. Hennig. Laplace Redux–Effortless Bayesian Deep Learning. arXiv preprint arXiv:2106.14806, 2021. ↩ S. Abnar, R. V. D. Berg, G. Ghiasi, M. Dehghani, N., Kalchbrenner, and H. Sedghi. Gradual Domain Adaptation in the Wild: When Intermediate Distributions are Absent. arXiv preprint arXiv:2106.06080, 2021. ↩ J. Chen, F. Liu, B. Avci, X. Wu, Y. Liang, and S. Jha. Detecting Errors and Estimating Accuracy on Unlabeled Data with Self-training Ensembles. arXiv preprint arXiv:2106.15728, 2021. ↩ E. Jones, S. Sagawa, P. W. Koh, A. Kumar, and P. Liang. Selective classification can magnify disparities across groups. In International Conference on Learning Representations (ICLR), 2021. ↩ J. Miller, R. Taori, A. Raghunathan, S. Sagawa, P. W. Koh, V. Shankar, P. Liang, Y. Carmon, and L. Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In International Conference on Machine Learning (ICML), 2021. ↩, A curated benchmark of 10 datasets with real-world distribution shifts.},
	urldate = {2021-07-23},
	author = {href=, {\textless}a},
	file = {Snapshot:/home/zwerg/Zotero/storage/ARGTXR22/wilds.html:text/html},
}

@misc{jean_frechet_2018,
	title = {Fréchet {Inception} {Distance}},
	url = {https://nealjean.com/ml/frechet-inception-distance/nealjean.com/ml/frechet-inception-distance/},
	abstract = {TL;DR: FID measures the distance between the Inception-v3 activation distributions for generated and real samples},
	language = {en},
	urldate = {2021-07-23},
	journal = {Neal Jean},
	author = {Jean, Neal},
	month = jul,
	year = {2018},
}

@article{chen_wavegrad_2020,
	title = {{WaveGrad}: {Estimating} {Gradients} for {Waveform} {Generation}},
	shorttitle = {{WaveGrad}},
	url = {http://arxiv.org/abs/2009.00713},
	abstract = {This paper introduces WaveGrad, a conditional model for waveform generation which estimates gradients of the data density. The model is built on prior work on score matching and diffusion probabilistic models. It starts from a Gaussian white noise signal and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad offers a natural way to trade inference speed for sample quality by adjusting the number of refinement steps, and bridges the gap between non-autoregressive and autoregressive models in terms of audio quality. We find that it can generate high fidelity audio samples using as few as six iterations. Experiments reveal WaveGrad to generate high fidelity audio, outperforming adversarial non-autoregressive baselines and matching a strong likelihood-based autoregressive baseline using fewer sequential operations. Audio samples are available at https://wavegrad.github.io/.},
	urldate = {2021-07-23},
	journal = {arXiv:2009.00713 [cs, eess, stat]},
	author = {Chen, Nanxin and Zhang, Yu and Zen, Heiga and Weiss, Ron J. and Norouzi, Mohammad and Chan, William},
	month = oct,
	year = {2020},
	note = {arXiv: 2009.00713},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/D43RK2HH/Chen et al. - 2020 - WaveGrad Estimating Gradients for Waveform Genera.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/AWUGRVW3/2009.html:text/html},
}

@article{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2021-07-23},
	journal = {arXiv:2006.11239 [cs, stat]},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv: 2006.11239},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/WZJFVJYZ/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/K6Y54ZIC/2006.html:text/html},
}

@article{song_generative_2020,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution}},
	url = {http://arxiv.org/abs/1907.05600},
	abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
	urldate = {2021-07-23},
	journal = {arXiv:1907.05600 [cs, stat]},
	author = {Song, Yang and Ermon, Stefano},
	month = oct,
	year = {2020},
	note = {arXiv: 1907.05600},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2019 (Oral)},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/VW4VQJXA/Song and Ermon - 2020 - Generative Modeling by Estimating Gradients of the.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/R88TQRQD/1907.html:text/html},
}

@article{nichol_improved_2021,
	title = {Improved {Denoising} {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2102.09672},
	abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion},
	urldate = {2021-07-23},
	journal = {arXiv:2102.09672 [cs, stat]},
	author = {Nichol, Alex and Dhariwal, Prafulla},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.09672},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/XPG9EMRE/Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/44RS992P/2102.html:text/html},
}

@article{ho_cascaded_nodate,
	title = {Cascaded {Diﬀusion} {Models} for {High} {Fidelity} {Image} {Generation}},
	abstract = {We show that cascaded diﬀusion models are capable of generating high ﬁdelity images on the class-conditional ImageNet generation challenge, without any assistance from auxiliary image classiﬁers to boost sample quality. A cascaded diﬀusion model comprises a pipeline of multiple diﬀusion models that generate images of increasing resolution, beginning with a standard diﬀusion model at the lowest resolution, followed by one or more super-resolution diﬀusion models that successively upsample the image and add higher resolution details. We ﬁnd that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64×64, 3.52 at 128×128 and 4.88 at 256×256 resolutions, outperforming BigGAN-deep, and classiﬁcation accuracy scores of 63.02\% (top-1) and 84.06\% (top-5) at 256×256, outperforming VQ-VAE-2.},
	language = {en},
	author = {Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J and Norouzi, Mohammad and Salimans, Tim},
	pages = {28},
	file = {Ho et al. - Cascaded Diﬀusion Models for High Fidelity Image G.pdf:/home/zwerg/Zotero/storage/58492VQP/Ho et al. - Cascaded Diﬀusion Models for High Fidelity Image G.pdf:application/pdf},
}

@article{saharia_image_2021,
	title = {Image {Super}-{Resolution} via {Iterative} {Refinement}},
	url = {http://arxiv.org/abs/2104.07636},
	abstract = {We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50\%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34\%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet.},
	urldate = {2021-07-23},
	journal = {arXiv:2104.07636 [cs, eess]},
	author = {Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
	month = jun,
	year = {2021},
	note = {arXiv: 2104.07636},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/JM5DA7MF/Saharia et al. - 2021 - Image Super-Resolution via Iterative Refinement.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/SRL47P3Y/2104.html:text/html},
}

@article{tunyasuvunakool_highly_2021,
	title = {Highly accurate protein structure prediction for the human proteome},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03828-1},
	doi = {10.1038/s41586-021-03828-1},
	abstract = {Protein structures can provide invaluable information, both for reasoning about biological processes and for enabling interventions such as structure-based drug development or targeted mutagenesis. After decades of effort, 17\% of the total residues in human protein sequences are covered by an experimentally-determined structure1. Here we dramatically expand structural coverage by applying the state-of-the-art machine learning method, AlphaFold2, at scale to almost the entire human proteome (98.5\% of human proteins). The resulting dataset covers 58\% of residues with a confident prediction, of which a subset (36\% of all residues) have very high confidence. We introduce several metrics developed by building on the AlphaFold model, and use them to interpret the dataset, identifying strong multi-domain predictions as well as regions likely to be disordered. Finally, we provide some case studies illustrating how high-quality predictions may be used to generate biological hypotheses. Importantly, we are making our predictions freely available to the community via a public database (hosted by the European Bioinformatics Institute at https://alphafold.ebi.ac.uk/). We anticipate that routine large-scale and high-accuracy structure prediction will become an important tool, allowing new questions to be addressed from a structural perspective.},
	language = {en},
	urldate = {2021-07-23},
	journal = {Nature},
	author = {Tunyasuvunakool, Kathryn and Adler, Jonas and Wu, Zachary and Green, Tim and Zielinski, Michal and Žídek, Augustin and Bridgland, Alex and Cowie, Andrew and Meyer, Clemens and Laydon, Agata and Velankar, Sameer and Kleywegt, Gerard J. and Bateman, Alex and Evans, Richard and Pritzel, Alexander and Figurnov, Michael and Ronneberger, Olaf and Bates, Russ and Kohl, Simon A. A. and Potapenko, Anna and Ballard, Andrew J. and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Clancy, Ellen and Reiman, David and Petersen, Stig and Senior, Andrew W. and Kavukcuoglu, Koray and Birney, Ewan and Kohli, Pushmeet and Jumper, John and Hassabis, Demis},
	month = jul,
	year = {2021},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Machine learning;Protein structure predictions;Proteomic analysis;Structural biology
Subject\_term\_id: machine-learning;protein-structure-predictions;proteomic-analysis;structural-biology},
	pages = {1--9},
	file = {Snapshot:/home/zwerg/Zotero/storage/GGB2JEI8/s41586-021-03828-1.html:text/html},
}

@article{mildenhall_nerf_2020-1,
	title = {{NeRF}: {Representing} {Scenes} as {Neural} {Radiance} {Fields} for {View} {Synthesis}},
	shorttitle = {{NeRF}},
	url = {http://arxiv.org/abs/2003.08934},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	urldate = {2021-08-01},
	journal = {arXiv:2003.08934 [cs]},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	month = aug,
	year = {2020},
	note = {arXiv: 2003.08934},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {arXiv Fulltext PDF.pdf:/home/zwerg/Zotero/storage/FRCK83IH/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Fields for View Synthesis.pdf:application/pdf},
}

@misc{cicero_7_2018,
	title = {The 7 {Key} {Principles} of {Platform} {Design}},
	url = {https://stories.platformdesigntoolkit.com/7-key-platform-design-principles-d84cc78b9218},
	abstract = {To design Strategies that mobilize, in the XXIst Century},
	language = {en},
	urldate = {2021-07-28},
	journal = {Medium},
	author = {Cicero, Simone},
	month = sep,
	year = {2018},
}

@misc{cicero_12_2018,
	title = {12 {Patterns} of {Platform} {Design} to kickstart {Innovation} {Strategies}},
	url = {https://stories.platformdesigntoolkit.com/12-patterns-of-platform-design-to-kickstart-innovation-strategies-500c6dec9c3b},
	abstract = {DIY Pattern Cards that can help you frame the Platform Design opportunity available in your context},
	language = {en},
	urldate = {2021-07-28},
	journal = {Medium},
	author = {Cicero, Simone},
	month = sep,
	year = {2018},
	file = {Snapshot:/home/zwerg/Zotero/storage/87KBDHH9/12-patterns-of-platform-design-to-kickstart-innovation-strategies-500c6dec9c3b.html:text/html},
}

@misc{noauthor_top_nodate,
	title = {Top {C}/{C}++ {Machine} {Learning} {Libraries} {For} {Data} {Science} {\textbar} {Hacker} {Noon}},
	url = {https://hackernoon.com/top-cc-machine-learning-libraries-for-data-science-nl183wo1},
	abstract = {Importance of C++ in Data Science and Big Data},
	language = {en},
	urldate = {2021-07-28},
}

@article{liu_reactnet_2020,
	title = {{ReActNet}: {Towards} {Precise} {Binary} {Neural} {Network} with {Generalized} {Activation} {Functions}},
	shorttitle = {{ReActNet}},
	url = {http://arxiv.org/abs/2003.03488},
	abstract = {In this paper, we propose several ideas for enhancing a binary network to close its accuracy gap from real-valued networks without incurring any additional computational cost. We first construct a baseline network by modifying and binarizing a compact real-valued network with parameter-free shortcuts, bypassing all the intermediate convolutional layers including the downsampling layers. This baseline network strikes a good trade-off between accuracy and efficiency, achieving superior performance than most of existing binary networks at approximately half of the computational cost. Through extensive experiments and analysis, we observed that the performance of binary networks is sensitive to activation distribution variations. Based on this important observation, we propose to generalize the traditional Sign and PReLU functions, denoted as RSign and RPReLU for the respective generalized functions, to enable explicit learning of the distribution reshape and shift at near-zero extra cost. Lastly, we adopt a distributional loss to further enforce the binary network to learn similar output distributions as those of a real-valued network. We show that after incorporating all these ideas, the proposed ReActNet outperforms all the state-of-the-arts by a large margin. Specifically, it outperforms Real-to-Binary Net and MeliusNet29 by 4.0\% and 3.6\% respectively for the top-1 accuracy and also reduces the gap to its real-valued counterpart to within 3.0\% top-1 accuracy on ImageNet dataset. Code and models are available at: https://github.com/liuzechun/ReActNet.},
	urldate = {2021-07-28},
	journal = {arXiv:2003.03488 [cs, eess]},
	author = {Liu, Zechun and Shen, Zhiqiang and Savvides, Marios and Cheng, Kwang-Ting},
	month = jul,
	year = {2020},
	note = {arXiv: 2003.03488},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: Accepted to ECCV 2020. Code is available at: https://github.com/liuzechun/ReActNet},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/98T65TWI/Liu et al. - 2020 - ReActNet Towards Precise Binary Neural Network wi.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/PTZQ47UE/2003.html:text/html},
}

@misc{noauthor_juliacon2020_2020,
	title = {{JuliaCon2020}: {Julia} is production ready!},
	shorttitle = {{JuliaCon2020}},
	url = {https://bkamins.github.io/julialang/2020/08/07/production-ready.html},
	abstract = {Introduction},
	language = {en},
	urldate = {2021-07-28},
	journal = {Blog by Bogumił Kamiński},
	month = aug,
	year = {2020},
	file = {Snapshot:/home/zwerg/Zotero/storage/8NKMCPSK/production-ready.html:text/html},
}

@misc{julia_model_nodate,
	title = {Model {Productization}: {Crafting} a {Web} {Application} for {Iris} {Classifier} {\textbar} juliabloggers.com},
	shorttitle = {Model {Productization}},
	url = {https://www.juliabloggers.com/model-productization-crafting-a-web-application-for-iris-classifier/},
	abstract = {Any successful data science project must end with productization. This is the stage where trained models are deployed as application that can be easily accessed by the end users. The application can either be part of already existing system, or it coul...},
	language = {en-US},
	urldate = {2021-07-28},
	author = {Julia, Estadistika--},
	note = {Section: Julia},
}

@misc{noauthor_julia_2021,
	title = {Julia {Libraries} {\textbar} {Top} {Julia} {Machine} {Learning} {Libraries}},
	url = {https://www.analyticsvidhya.com/blog/2021/05/top-julia-machine-learning-libraries/},
	abstract = {Julia libraries are very useful for ML and deep learning. In this article we are going to discuss top julia machine learning libraries},
	urldate = {2021-07-28},
	journal = {Analytics Vidhya},
	month = may,
	year = {2021},
	file = {Snapshot:/home/zwerg/Zotero/storage/4395KDLS/top-julia-machine-learning-libraries.html:text/html},
}

@article{ryffel_generic_2018,
	title = {A generic framework for privacy preserving deep learning},
	url = {http://arxiv.org/abs/1811.04017},
	abstract = {We detail a new framework for privacy preserving deep learning and discuss its assets. The framework puts a premium on ownership and secure processing of data and introduces a valuable representation based on chains of commands and tensors. This abstraction allows one to implement complex privacy preserving constructs such as Federated Learning, Secure Multiparty Computation, and Differential Privacy while still exposing a familiar deep learning API to the end-user. We report early results on the Boston Housing and Pima Indian Diabetes datasets. While the privacy features apart from Differential Privacy do not impact the prediction accuracy, the current implementation of the framework introduces a significant overhead in performance, which will be addressed at a later stage of the development. We believe this work is an important milestone introducing the first reliable, general framework for privacy preserving deep learning.},
	urldate = {2021-07-27},
	journal = {arXiv:1811.04017 [cs, stat]},
	author = {Ryffel, Theo and Trask, Andrew and Dahl, Morten and Wagner, Bobby and Mancuso, Jason and Rueckert, Daniel and Passerat-Palmbach, Jonathan},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.04017},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: PPML 2018, 5 pages},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/HJ9I9WVH/Ryffel et al. - 2018 - A generic framework for privacy preserving deep le.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/3ZDDG9G2/1811.html:text/html},
}

@techreport{yang_high-resolution_2020,
	type = {preprint},
	title = {High-{Resolution}, {Large} {Imaging} {Volume}, and {Multi}-{View} {Single} {Objective} {Light}-{Sheet} {Microscopy}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2020.09.22.309229},
	abstract = {Recent developments in Oblique Plane Microscopy (OPM) have shown that it can achieve high spatio-temporal resolution. Here we describe a single objective light-sheet microscope based on oblique plane illumination that achieves: (i) large field of view and high-resolution imaging via a custom remote focusing objective; (ii) fast volumetric imaging by means of
            light-sheet stabilised stage scanning
            – a novel scanning modality that extends the imaging volume without compromising imaging speed nor quality; (iii) multi-view imaging by alternating the orientation of light-sheet illumination and detection to improve the image quality on large samples; (iv) simpler design and ergonomics by remote placement of coverslips to allow inverted imaging, enabling imaging across scales in a high-throughput format. Overall, we achieved a resolution of 450 nm laterally and 2 μm axially and a field of view of 3000 μm × 800 μm × 300 μm. We demonstrate the speed, field of view, resolution and versatility of our novel instrument by imaging various systems, including zebrafish whole brain activity,
            Drosophila
            egg chamber development, and zebrafish development – up to nine embryos simultaneously.},
	language = {en},
	urldate = {2021-07-26},
	institution = {Developmental Biology},
	author = {Yang, Bin and Lange, Merlin and Millett-Sikking, Alfred and Solak, Ahmet Can and Kumar, Shruthi Vijay and Wang, Wanpeng and Kobayashi, Hirofumi and McCarroll, Matthew N. and Whitehead, Lachlan W. and Fiolka, Reto P. and Kornberg, Thomas B. and York, Andrew G. and Royer, Loic A.},
	month = sep,
	year = {2020},
	doi = {10.1101/2020.09.22.309229},
	file = {Yang et al. - 2020 - High-Resolution, Large Imaging Volume, and Multi-V.pdf:/home/zwerg/Zotero/storage/ICERH7E9/Yang et al. - 2020 - High-Resolution, Large Imaging Volume, and Multi-V.pdf:application/pdf},
}

@article{bird_3d_2021,
	title = {{3D} {Scene} {Compression} through {Entropy} {Penalized} {Neural} {Representation} {Functions}},
	doi = {10.1109/PCS50896.2021.9477505},
	abstract = {Some forms of novel visual media enable the viewer to explore a 3D scene from essentially arbitrary viewpoints, by interpolating between a discrete set of original views. Compared to 2D imagery, these types of applications require much larger amounts of storage space, which we seek to reduce. Existing approaches for compressing 3D scenes are often based on a separation of compression and rendering: each of the original views is compressed using traditional 2D image formats; the receiver decompresses the views and then performs the rendering. We unify these steps by directly compressing an implicit representation of the scene, a function that maps spatial coordinates to a radiance vector field, which can then be queried to render arbitrary viewpoints. The function is implemented as a neural network and jointly trained for reconstruction as well as compressibility, in an end-to-end manner, with the use of an entropy penalty on the parameters. Our method significantly outperforms a state-of-the-art conventional approach for scene compression, achieving simultaneously higher quality reconstructions and lower bitrates. Furthermore, we show that the performance at lower bitrates can be improved by jointly representing multiple scenes using a soft form of parameter sharing.},
	journal = {2021 Picture Coding Symposium (PCS)},
	author = {Bird, Thomas and Ball'e, Johannes and Singh, Saurabh and Chou, P.},
	year = {2021},
}

@article{bird_3d_2021-1,
	title = {{3D} {Scene} {Compression} through {Entropy} {Penalized} {Neural} {Representation} {Functions}},
	url = {http://arxiv.org/abs/2104.12456},
	abstract = {Some forms of novel visual media enable the viewer to explore a 3D scene from arbitrary viewpoints, by interpolating between a discrete set of original views. Compared to 2D imagery, these types of applications require much larger amounts of storage space, which we seek to reduce. Existing approaches for compressing 3D scenes are based on a separation of compression and rendering: each of the original views is compressed using traditional 2D image formats; the receiver decompresses the views and then performs the rendering. We unify these steps by directly compressing an implicit representation of the scene, a function that maps spatial coordinates to a radiance vector ﬁeld, which can then be queried to render arbitrary viewpoints. The function is implemented as a neural network and jointly trained for reconstruction as well as compressibility, in an end-to-end manner, with the use of an entropy penalty on the parameters. Our method significantly outperforms a state-of-the-art conventional approach for scene compression, achieving simultaneously higher quality reconstructions and lower bitrates. Furthermore, we show that the performance at lower bitrates can be improved by jointly representing multiple scenes using a soft form of parameter sharing.},
	language = {en},
	urldate = {2021-08-04},
	journal = {arXiv:2104.12456 [cs, eess]},
	author = {Bird, Thomas and Ballé, Johannes and Singh, Saurabh and Chou, Philip A.},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.12456},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: accepted (in an abridged format) as a contribution to the Learning-based Image Coding special session of the Picture Coding Symposium 2021},
	file = {Bird et al. - 2021 - 3D Scene Compression through Entropy Penalized Neu.pdf:/home/zwerg/Zotero/storage/BKTIUX9D/Bird et al. - 2021 - 3D Scene Compression through Entropy Penalized Neu.pdf:application/pdf},
}

@article{bird_3d_2021-2,
	title = {{3D} {Scene} {Compression} through {Entropy} {Penalized} {Neural} {Representation} {Functions}},
	url = {http://arxiv.org/abs/2104.12456},
	abstract = {Some forms of novel visual media enable the viewer to explore a 3D scene from arbitrary viewpoints, by interpolating between a discrete set of original views. Compared to 2D imagery, these types of applications require much larger amounts of storage space, which we seek to reduce. Existing approaches for compressing 3D scenes are based on a separation of compression and rendering: each of the original views is compressed using traditional 2D image formats; the receiver decompresses the views and then performs the rendering. We unify these steps by directly compressing an implicit representation of the scene, a function that maps spatial coordinates to a radiance vector ﬁeld, which can then be queried to render arbitrary viewpoints. The function is implemented as a neural network and jointly trained for reconstruction as well as compressibility, in an end-to-end manner, with the use of an entropy penalty on the parameters. Our method significantly outperforms a state-of-the-art conventional approach for scene compression, achieving simultaneously higher quality reconstructions and lower bitrates. Furthermore, we show that the performance at lower bitrates can be improved by jointly representing multiple scenes using a soft form of parameter sharing.},
	language = {en},
	urldate = {2021-08-04},
	journal = {arXiv:2104.12456 [cs, eess]},
	author = {Bird, Thomas and Ballé, Johannes and Singh, Saurabh and Chou, Philip A.},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.12456},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: accepted (in an abridged format) as a contribution to the Learning-based Image Coding special session of the Picture Coding Symposium 2021},
	file = {Bird et al. - 2021 - 3D Scene Compression through Entropy Penalized Neu.pdf:/home/zwerg/Zotero/storage/Q4J79QZX/Bird et al. - 2021 - 3D Scene Compression through Entropy Penalized Neu.pdf:application/pdf},
}

@article{lin_barf_2021,
	title = {{BARF}: {Bundle}-{Adjusting} {Neural} {Radiance} {Fields}},
	shorttitle = {{BARF}},
	abstract = {Neural Radiance Fields (NeRF) [30] have recently gained a surge of interest within the computer vision community for its power to synthesize photorealistic novel views of realworld scenes. One limitation of NeRF, however, is its requirement of accurate camera poses to learn the scene representations. In this paper, we propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from imperfect (or even unknown) camera poses — the joint problem of learning neural 3D representations and registering camera frames. We establish a theoretical connection to classical image alignment and show that coarse-to-fine registration is also applicable to NeRF. Furthermore, we show that naïvely applying positional encoding in NeRF has a negative impact on registration with a synthesis-based objective. Experiments on synthetic and real-world data show that BARF can effectively optimize the neural scene representations and resolve large camera pose misalignment at the same time. This enables view synthesis and localization of video sequences from unknown camera poses, opening up new avenues for visual localization systems (e.g. SLAM) and potential applications for dense 3D mapping and reconstruction.},
	journal = {ArXiv},
	author = {Lin, Chen-Hsuan and Ma, Wei-Chiu and Torralba, A. and Lucey, S.},
	year = {2021},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/3VAI5A9N/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf:application/pdf},
}

@article{gavish_optimal_2014,
	title = {The {Optimal} {Hard} {Threshold} for {Singular} {Values} is 4/sqrt(3)},
	url = {http://arxiv.org/abs/1305.5870},
	abstract = {We consider recovery of low-rank matrices from noisy data by hard thresholding of singular values, where singular values below a prescribed threshold \${\textbackslash}lambda\$ are set to 0. We study the asymptotic MSE in a framework where the matrix size is large compared to the rank of the matrix to be recovered, and the signal-to-noise ratio of the low-rank piece stays constant. The AMSE-optimal choice of hard threshold, in the case of n-by-n matrix in noise level {\textbackslash}sigma, is simply \$(4/{\textbackslash}sqrt\{3\}) {\textbackslash}sqrt\{n\}{\textbackslash}sigma {\textbackslash}approx 2.309 {\textbackslash}sqrt\{n\}{\textbackslash}sigma\$ when \${\textbackslash}sigma\$ is known, or simply \$2.858{\textbackslash}cdot y\_\{med\}\$ when \${\textbackslash}sigma\$ is unknown, where \$y\_\{med\}\$ is the median empirical singular value. For nonsquare \$m\$ by \$n\$ matrices with \$m {\textbackslash}neq n\$, these thresholding coefficients are replaced with different provided constants. In our asymptotic framework, this thresholding rule adapts to unknown rank and to unknown noise level in an optimal manner: it is always better than hard thresholding at any other value, no matter what the matrix is that we are trying to recover, and is always better than ideal Truncated SVD (TSVD), which truncates at the true rank of the low-rank matrix we are trying to recover. Hard thresholding at the recommended value to recover an n-by-n matrix of rank r guarantees an AMSE at most \$3nr{\textbackslash}sigma{\textasciicircum}2\$. In comparison, the guarantee provided by TSVD is \$5nr{\textbackslash}sigma{\textasciicircum}2\$, the guarantee provided by optimally tuned singular value soft thresholding is \$6nr{\textbackslash}sigma{\textasciicircum}2\$, and the best guarantee achievable by any shrinkage of the data singular values is \$2nr{\textbackslash}sigma{\textasciicircum}2\$. Empirical evidence shows that these AMSE properties of the \$4/{\textbackslash}sqrt\{3\}\$ thresholding rule remain valid even for relatively small n, and that performance improvement over TSVD and other shrinkage rules is substantial, turning it into the practical hard threshold of choice.},
	urldate = {2021-08-07},
	journal = {arXiv:1305.5870 [stat]},
	author = {Gavish, Matan and Donoho, David L.},
	month = jun,
	year = {2014},
	note = {arXiv: 1305.5870},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/JZQ9I2ZJ/Gavish and Donoho - 2014 - The Optimal Hard Threshold for Singular Values is .pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/VWBJQMM8/1305.html:text/html},
}

@article{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2021-08-07},
	journal = {arXiv:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv: 2010.11929},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/V5D6WDEH/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/KWCMB8XP/2010.html:text/html},
}

@misc{noauthor_clip_2021,
	title = {{CLIP}: {Connecting} {Text} and {Images}},
	shorttitle = {{CLIP}},
	url = {https://openai.com/blog/clip/},
	abstract = {We’re introducing a neural network called CLIP which efficiently learns visual concepts from natural language supervision.},
	language = {en},
	urldate = {2021-08-07},
	journal = {OpenAI},
	month = jan,
	year = {2021},
	file = {Snapshot:/home/zwerg/Zotero/storage/A89GJD2Z/clip.html:text/html},
}

@article{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {https://arxiv.org/abs/2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2021-08-07},
	journal = {arXiv:2103.00020 [cs]},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv: 2103.00020},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/ZXHD9TCI/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/XNA5JNPY/2103.html:text/html},
}

@article{chen_generative_nodate-1,
	title = {Generative {Pretraining} from {Pixels}},
	abstract = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we ﬁnd that a GPT-2 scale model learns strong image representations as measured by linear probing, ﬁne-tuning, and low-data classiﬁcation. On CIFAR-10, we achieve 96.3\% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0\% accuracy with full ﬁnetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0\% top-1 accuracy on a linear probe of our features.},
	language = {en},
	author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeff and Jun, Heewoo and Dhariwal, Prafulla and Luan, David and Sutskever, Ilya},
	pages = {12},
	file = {Chen et al. - Generative Pretraining from Pixels.pdf:/home/zwerg/Zotero/storage/WDNWIEA9/Chen et al. - Generative Pretraining from Pixels.pdf:application/pdf},
}

@article{wang_backpropagation-friendly_nodate,
	title = {Backpropagation-{Friendly} {Eigendecomposition}},
	abstract = {Eigendecomposition (ED) is widely used in deep networks. However, the backpropagation of its results tends to be numerically unstable, whether using ED directly or approximating it with the Power Iteration method, particularly when dealing with large matrices. While this can be mitigated by partitioning the data in small and arbitrary groups, doing so has no theoretical basis and makes its impossible to exploit the power of ED to the full.},
	language = {en},
	author = {Wang, Wei and Dang, Zheng and Hu, Yinlin and Fua, Pascal and Salzmann, Mathieu},
	pages = {9},
	file = {Wang et al. - Backpropagation-Friendly Eigendecomposition.pdf:/home/zwerg/Zotero/storage/8E5EMN74/Wang et al. - Backpropagation-Friendly Eigendecomposition.pdf:application/pdf},
}

@article{anderson_generalized_1992,
	title = {Generalized {QR} factorization and its applications},
	volume = {162-164},
	issn = {00243795},
	url = {https://linkinghub.elsevier.com/retrieve/pii/002437959290379O},
	doi = {10.1016/0024-3795(92)90379-O},
	abstract = {The purpose of this paper is to reintroduce the generalized QR factorization with or without pivoting of two matrices A and B having the same number of rows. When B is square and nonsingular, the factorization implicitly gives the orthogonal factorization of B-‘A. Continuing the work of Paige and Hammarling, we discuss the different forms of the factorization from the point of view of general-purpose software development.},
	language = {en},
	urldate = {2021-08-07},
	journal = {Linear Algebra and its Applications},
	author = {Anderson, E. and Bai, Z. and Dongarra, J.},
	month = feb,
	year = {1992},
	pages = {243--271},
	file = {Anderson et al. - 1992 - Generalized QR factorization and its applications.pdf:/home/zwerg/Zotero/storage/DI39U5QS/Anderson et al. - 1992 - Generalized QR factorization and its applications.pdf:application/pdf},
}

@misc{noauthor_so_nodate,
	title = {So you want to build an embedded {Linux} system?},
	url = {https://jaycarlson.net/embedded-linux/},
	language = {en-US},
	urldate = {2021-08-07},
	journal = {Jay Carlson},
}

@misc{noauthor_jetson_2021,
	title = {Jetson {Nano} machine learning projects you need to try!},
	url = {https://www.seeedstudio.com/blog/2021/02/02/jetson-nano-machine-learning-projects-you-need-to-try/},
	abstract = {NVIDIA's Jetson Nano has great GPU capabilities which makes it not only a popular choice for Machine Learning (ML), it is also often used for gaming and CUDA based computations. With their newest release of NVIDIA® Jetson Nano™ 2GB Developer Kit, pricing at only \$59, makes it even more affordable than its predecessor, NVIDIA Jetson Nano Developer Kit (\$99).},
	language = {en-US},
	urldate = {2021-08-07},
	journal = {Latest open tech from seeed studio},
	month = feb,
	year = {2021},
}

@article{mitchell_web_nodate,
	title = {Web {Scraping} with {Python}: {Collecting} {Data} from the {Modern} {Web}},
	language = {en},
	author = {Mitchell, Ryan},
	pages = {354},
	file = {PyWebScrapingBook.pdf:/home/zwerg/Zotero/storage/UR9IPUZQ/PyWebScrapingBook.pdf:application/pdf},
}

@article{chang_toward_2020,
	title = {Toward {Universal} {Stripe} {Removal} via {Wavelet}-{Based} {Deep} {Convolutional} {Neural} {Network}},
	volume = {58},
	issn = {0196-2892, 1558-0644},
	url = {https://ieeexplore.ieee.org/document/8936525/},
	doi = {10.1109/TGRS.2019.2957153},
	abstract = {Stripe noise from different remote sensing imaging systems varies considerably in terms of response, length, angle, and periodicity. Due to the complex distributions of different stripes, the destriping results of previous methods may be oversmoothed or contain residual stripe. To overcome this key problem, we provide a comprehensive analysis of existing destriping methods and propose a deep convolutional neural network (CNN) for handling various kinds of stripes. Moreover, previous methods individually model the stripe or the image priors, which may lose the relationship between them. In this article, a two-stream CNN is designed to simultaneously model the stripe and image, which better facilitates distinguishing them from each other. Moreover, we incorporate the wavelet into our CNN model for better directional feature representation. Therefore, the CNN learns the discriminative representation from the external data set, while the wavelet models the internal directionality of the stripe, in which both the internal and external priors are beneﬁcial to the destriping task. In addition, the wavelet extracts the multiscale information with a larger receptive ﬁeld for global contextual information modeling; thus, we can better distinguish the stripe from the similar image line pattern structures. The proposed method has been extensively evaluated on a number of data sets and outperforms the stateof-the-art methods by substantially a large margin in terms of quantitative and qualitative assessments, speed, and robustness. Index Terms—Convolutional neural network (CNN), destriping, image decomposition, wavelet.},
	language = {en},
	number = {4},
	urldate = {2021-08-17},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Chang, Yi and Chen, Meiya and Yan, Luxin and Zhao, Xi-Le and Li, Yi and Zhong, Sheng},
	month = apr,
	year = {2020},
	pages = {2880--2897},
	file = {Chang et al. - 2020 - Toward Universal Stripe Removal via Wavelet-Based .pdf:/home/zwerg/Zotero/storage/4XIIMGFV/Chang et al. - 2020 - Toward Universal Stripe Removal via Wavelet-Based .pdf:application/pdf},
}

@article{he_phase_2021,
	title = {Phase {Transition} of {Interstellar} {CO} {Ice}},
	volume = {915},
	issn = {2041-8205, 2041-8213},
	url = {https://iopscience.iop.org/article/10.3847/2041-8213/ac0a7c},
	doi = {10.3847/2041-8213/ac0a7c},
	language = {en},
	number = {1},
	urldate = {2021-08-20},
	journal = {The Astrophysical Journal Letters},
	author = {He, Jiao and Toriello, Francis E. and Emtiaz, Shahnewaz M. and Henning, Thomas and Vidali, Gianfranco},
	month = jul,
	year = {2021},
	pages = {L23},
	file = {He et al. - 2021 - Phase Transition of Interstellar CO Ice.pdf:/home/zwerg/Zotero/storage/KH5AFLH5/He et al. - 2021 - Phase Transition of Interstellar CO Ice.pdf:application/pdf},
}

@article{bailer-jones_applications_nodate,
	title = {Applications of machine learning in astronomy},
	abstract = {We developed a source detection algorithm based on the Minimal Spanning Tree (MST), that is a graph-theoretical method useful for ﬁnding clusters in a given set of points. This algorithm is applied to γ-ray bidimensional images where the points correspond to the arrival direction of photons, and the possible sources are associated with the regions where they clusterize. Some ﬁlters to select these clusters and to reduce the spurious detections are introduced. An empirical study of the statistical properties of MST on random ﬁelds is carried in order to derive some criteria to estimate the best ﬁlter values. We introduce also two parameters useful to verify the goodness of candidate sources. To show how the MST algorithm works in the practice, we present an application to an EGRET observation of the Virgo ﬁeld, at high galactic latitude and with a low and rather uniform background, in which several sources are detected.},
	language = {en},
	author = {Bailer-Jones, Coryn},
	pages = {24},
	file = {Bailer-Jones - Applications of machine learning in astronomy.pdf:/home/zwerg/Zotero/storage/YSXXK5KH/Bailer-Jones - Applications of machine learning in astronomy.pdf:application/pdf},
}

@article{fergus_machine_nodate,
	title = {Machine {Learning} for {Astronomy}},
	language = {en},
	author = {Fergus, Rob},
	pages = {80},
	file = {Fergus - Machine Learning for Astronomy.pdf:/home/zwerg/Zotero/storage/IRE8FV23/Fergus - Machine Learning for Astronomy.pdf:application/pdf},
}

@misc{noauthor_how_nodate,
	title = {How {Machine} {Learning} is {Used} in {Astronomy} {\textbar} {Hacker} {Noon}},
	url = {https://hackernoon.com/how-machine-learning-used-in-astronomy-g4d73yot},
	abstract = {Is Astronomy data science?},
	language = {en},
	urldate = {2021-08-28},
}

@article{lu_multi-omics_2021,
	title = {Multi-omics reveals clinically relevant proliferative drive associated with {mTOR}-{MYC}-{OXPHOS} activity in chronic lymphocytic leukemia},
	volume = {2},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {2662-1347},
	url = {https://www.nature.com/articles/s43018-021-00216-6},
	doi = {10.1038/s43018-021-00216-6},
	abstract = {Chronic lymphocytic leukemia (CLL) has a complex pattern of driver mutations and much of its clinical diversity remains unexplained. We devised a method for simultaneous subgroup discovery across multiple data types and applied it to genomic, transcriptomic, DNA methylation and ex vivo drug response data from 217 patients with CLL. We uncovered a biological axis of heterogeneity strongly associated with clinical behavior and orthogonal to known biomarkers. We validated its presence and clinical relevance in four independent cohorts (n = 547 patients). We found that this axis captures the proliferative drive (PD) of CLL cells, as it associates with lymphocyte doubling rate, global hypomethylation, accumulation of driver aberrations and response to pro-proliferative stimuli. CLL–PD was linked to the activation of mTOR–MYC–oxidative phosphorylation through transcriptomic, proteomic and single-cell resolution analysis. CLL–PD is a key determinant of disease outcome in CLL. Our multi-table integration approach may be applicable to other tumors whose inter-individual differences are currently unexplained.},
	language = {en},
	number = {8},
	urldate = {2021-09-15},
	journal = {Nature Cancer},
	author = {Lu, Junyan and Cannizzaro, Ester and Meier-Abt, Fabienne and Scheinost, Sebastian and Bruch, Peter-Martin and Giles, Holly A. R. and Lütge, Almut and Hüllein, Jennifer and Wagner, Lena and Giacopelli, Brian and Nadeu, Ferran and Delgado, Julio and Campo, Elías and Mangolini, Maurizio and Ringshausen, Ingo and Böttcher, Martin and Mougiakakos, Dimitrios and Jacobs, Andrea and Bodenmiller, Bernd and Dietrich, Sascha and Oakes, Christopher C. and Zenz, Thorsten and Huber, Wolfgang},
	month = aug,
	year = {2021},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 8
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Cancer;Chronic lymphocytic leukaemia;Tumour biomarkers;Tumour heterogeneity
Subject\_term\_id: cancer;chronic-lymphocytic-leukaemia;tumour-biomarkers;tumour-heterogeneity},
	pages = {853--864},
	file = {Snapshot:/home/zwerg/Zotero/storage/H33K6TK2/s43018-021-00216-6.html:text/html},
}

@article{armenteros_deeploc_nodate,
	title = {{DeepLoc}: prediction of protein subcellular localization using deep learning},
	abstract = {Motivation: The prediction of eukaryotic protein subcellular localization is a well-studied topic in bioinformatics due to its relevance in proteomics research. Many machine learning methods have been successfully applied in this task, but in most of them, predictions rely on annotation of homologues from knowledge databases. For novel proteins where no annotated homologues exist, and for predicting the effects of sequence variants, it is desirable to have methods for predicting protein properties from sequence information only.},
	language = {en},
	author = {Armenteros, Jose Juan Almagro and Sønderby, Casper Kaae and Sønderby, Søren Kaae and Nielsen, Henrik and Winther, Ole},
	pages = {9},
	file = {Armenteros et al. - DeepLoc prediction of protein subcellular localiz.pdf:/home/zwerg/Zotero/storage/UX8AHVP7/Armenteros et al. - DeepLoc prediction of protein subcellular localiz.pdf:application/pdf},
}

@misc{noauthor_deeploc_nodate,
	title = {{DeepLoc}: prediction of protein subcellular localization using deep learning {\textbar} {Bioinformatics} {\textbar} {Oxford} {Academic}},
	url = {https://academic.oup.com/bioinformatics/article/33/21/3387/3931857},
	urldate = {2021-09-15},
}

@article{frome_devise_nodate,
	title = {{DeViSE}: {A} {Deep} {Visual}-{Semantic} {Embedding} {Model}},
	abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difﬁculty of acquiring sufﬁcient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18\% across thousands of novel labels never seen by the visual model.},
	language = {en},
	author = {Frome, Andrea and Corrado, Greg S and Shlens, Jonathon and Bengio, Samy and Dean, Jeffrey and Ranzato, Marc’Aurelio and Mikolov, Tomas},
	pages = {11},
	file = {Frome et al. - DeViSE A Deep Visual-Semantic Embedding Model.pdf:/home/zwerg/Zotero/storage/4ST55QUK/Frome et al. - DeViSE A Deep Visual-Semantic Embedding Model.pdf:application/pdf},
}

@article{chelba_one_2014,
	title = {One {Billion} {Word} {Benchmark} for {Measuring} {Progress} in {Statistical} {Language} {Modeling}},
	url = {http://arxiv.org/abs/1312.3005},
	abstract = {We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35\% reduction in perplexity, or 10\% reduction in cross-entropy (bits), over that baseline. The benchmark is available as a code.google.com project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models.},
	urldate = {2021-09-14},
	journal = {arXiv:1312.3005 [cs]},
	author = {Chelba, Ciprian and Mikolov, Tomas and Schuster, Mike and Ge, Qi and Brants, Thorsten and Koehn, Phillipp and Robinson, Tony},
	month = mar,
	year = {2014},
	note = {arXiv: 1312.3005},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accompanied by a code.google.com project allowing anyone to generate the benchmark data, and use it to compare their language model against the ones described in the paper},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/LHCE39PZ/Chelba et al. - 2014 - One Billion Word Benchmark for Measuring Progress .pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/DMVJNCA6/1312.html:text/html},
}

@article{raman_geometric_2007,
	title = {Geometric approach to segmentation and protein localization in cell culture assays},
	volume = {225},
	issn = {0022-2720},
	doi = {10.1111/j.1365-2818.2007.01712.x},
	abstract = {Cell-based fluorescence imaging assays are heterogeneous and require the collection of a large number of images for detailed quantitative analysis. Complexities arise as a result of variation in spatial nonuniformity, shape, overlapping compartments and scale (size). A new technique and methodology has been developed and tested for delineating subcellular morphology and partitioning overlapping compartments at multiple scales. This system is packaged as an integrated software platform for quantifying images that are obtained through fluorescence microscopy. Proposed methods are model based, leveraging geometric shape properties of subcellular compartments and corresponding protein localization. From the morphological perspective, convexity constraint is imposed to delineate and partition nuclear compartments. From the protein localization perspective, radial symmetry is imposed to localize punctate protein events at submicron resolution. Convexity constraint is imposed against boundary information, which are extracted through a combination of zero-crossing and gradient operator. If the convexity constraint fails for the boundary then positive curvature maxima are localized along the contour and the entire blob is partitioned into disjointed convex objects representing individual nuclear compartment, by enforcing geometric constraints. Nuclear compartments provide the context for protein localization, which may be diffuse or punctate. Punctate signal are localized through iterative voting and radial symmetries for improved reliability and robustness. The technique has been tested against 196 images that were generated to study centrosome abnormalities. Corresponding computed representations are compared against manual counts for validation.},
	language = {eng},
	number = {Pt 1},
	journal = {Journal of Microscopy},
	author = {Raman, S. and Maxwell, C. A. and Barcellos-Hoff, M. H. and Parvin, B.},
	month = jan,
	year = {2007},
	pmid = {17286692},
	keywords = {Algorithms, Female, Humans, Cell Line, Proteins, Software, Image Processing, Computer-Assisted, Microscopy, Fluorescence, Cell Culture Techniques, Models, Biological, Subcellular Fractions},
	pages = {22--30},
}

@misc{noauthor_rostlabprot_bert_nodate,
	title = {Rostlab/prot\_bert · {Hugging} {Face}},
	url = {https://huggingface.co/Rostlab/prot_bert},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2021-09-06},
	file = {Snapshot:/home/zwerg/Zotero/storage/4XBP3WI9/prot_bert.html:text/html},
}

@article{carter_robust_2018,
	title = {Robust long-read native {DNA} sequencing using the {ONT} {CsgG} {Nanopore} system},
	volume = {2},
	issn = {2398-502X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5426553/},
	doi = {10.12688/wellcomeopenres.11246.3},
	abstract = {Background: The ability to obtain long read lengths during DNA sequencing has several potentially important practical applications. Especially long read lengths have been reported using the Nanopore sequencing method, currently commercially available from Oxford Nanopore Technologies (ONT). However, early reports have demonstrated only limited levels of combined throughput and sequence accuracy. Recently, ONT released a new CsgG pore sequencing system as well as a 250b/s translocation chemistry with potential for improvements.
Methods: We made use of such components on ONTs miniature ‘MinION’ device and sequenced native genomic DNA obtained from the near haploid cancer cell line HAP1. Analysis of our data was performed utilising recently described computational tools tailored for nanopore/long-read sequencing outputs, and here we present our key findings.
Results: From a single sequencing run, we obtained {\textasciitilde}240,000 high-quality mapped reads, comprising a total of {\textasciitilde}2.3 billion bases. A mean read length of 9.6kb and an N50 of {\textasciitilde}17kb was achieved, while sequences mapped to reference with a mean identity of 85\%. Notably, we obtained {\textasciitilde}68X coverage of the mitochondrial genome and were able to achieve a mean consensus identity of 99.8\% for sequenced mtDNA reads.
Conclusions: With improved sequencing chemistries already released and higher-throughput instruments in the pipeline, this early study suggests that ONT CsgG-based sequencing may be a useful option for potential practical long-read applications with relevance to complex genomes.},
	urldate = {2021-09-04},
	journal = {Wellcome Open Research},
	author = {Carter, Jean-Michel and Hussain, Shobbir},
	month = aug,
	year = {2018},
	pmid = {28503666},
	pmcid = {PMC5426553},
	pages = {23},
	file = {PubMed Central Full Text PDF:/home/zwerg/Zotero/storage/VEBGUYJC/Carter and Hussain - 2018 - Robust long-read native DNA sequencing using the O.pdf:application/pdf},
}

@article{van_der_verren_dual_2020,
	title = {A dual constriction biological nanopore resolves homonucleotide sequences with high fidelity},
	volume = {38},
	issn = {1087-0156},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7610451/},
	doi = {10.1038/s41587-020-0570-8},
	abstract = {Single-molecule long-read DNA sequencing with biological nanopores is fast and high-throughput but suffers reduced accuracy in homonucleotide stretches. We now combine the CsgG nanopore with the 35-residue N-terminal region of its extracellular interaction partner CsgF to produce a dual-constriction pore with improved signal and basecalling accuracy for homopolymer regions. The electron cryo-microscopy structure of CsgG in complex with full-length CsgF shows that the 33 N-terminal residues of CsgF bind inside the β-barrel of the pore, forming a defined second constriction. In complexes of CsgG bound to a 35-residue CsgF constriction peptide, the second constriction is separated from the primary constriction by {\textasciitilde}25 Å. We find that both constrictions contribute to electrical signal modulation upon ssDNA translocation. DNA sequencing using a prototype CsgG:CsgF protein pore with two constrictions improved single-read accuracy by 25 to 70 \% in homopolymers up to 9 nucleotides long.},
	number = {12},
	urldate = {2021-09-04},
	journal = {Nature biotechnology},
	author = {Van der Verren, Sander E. and Van Gerven, Nani and Jonckheere, Wim and Hambley, Richard and Singh, Pratik and Kilgour, John and Jordan, Michael and Wallace, E. Jayne and Jayasinghe, Lakmal and Remaut, Han},
	month = dec,
	year = {2020},
	pmid = {32632300},
	pmcid = {PMC7610451},
	pages = {1415--1420},
	file = {PubMed Central Full Text PDF:/home/zwerg/Zotero/storage/TSUYID3H/Van der Verren et al. - 2020 - A dual constriction biological nanopore resolves h.pdf:application/pdf},
}

@article{merity_regularizing_2017-1,
	title = {Regularizing and {Optimizing} {LSTM} {Language} {Models}},
	url = {http://arxiv.org/abs/1708.02182},
	abstract = {Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.},
	urldate = {2021-09-04},
	journal = {arXiv:1708.02182 [cs]},
	author = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.02182},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/P4Q22SWA/Merity et al. - 2017 - Regularizing and Optimizing LSTM Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/UQ3F55PK/1708.html:text/html},
}

@article{vaswani_attention_2017-1,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2021-09-16},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/8SLKXXCH/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/KQRPZNXG/1706.html:text/html},
}

@article{lample_large_2019,
	title = {Large {Memory} {Layers} with {Product} {Keys}},
	url = {http://arxiv.org/abs/1907.05242},
	abstract = {This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes.},
	language = {en},
	urldate = {2021-09-16},
	journal = {arXiv:1907.05242 [cs]},
	author = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc'Aurelio and Denoyer, Ludovic and Jégou, Hervé},
	month = dec,
	year = {2019},
	note = {arXiv: 1907.05242},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Advances in Neural Information Processing Systems, 2019},
	file = {Lample et al. - 2019 - Large Memory Layers with Product Keys.pdf:/home/zwerg/Zotero/storage/EEVQLMVN/Lample et al. - 2019 - Large Memory Layers with Product Keys.pdf:application/pdf},
}

@article{lample_cross-lingual_2019,
	title = {Cross-lingual {Language} {Model} {Pretraining}},
	url = {http://arxiv.org/abs/1901.07291},
	abstract = {Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9\% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.},
	urldate = {2021-09-16},
	journal = {arXiv:1901.07291 [cs]},
	author = {Lample, Guillaume and Conneau, Alexis},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.07291
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/6FEAYEFI/Lample and Conneau - 2019 - Cross-lingual Language Model Pretraining.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/APDHU5M6/1901.html:text/html},
}

@article{conneau_unsupervised_2020,
	title = {Unsupervised {Cross}-lingual {Representation} {Learning} at {Scale}},
	url = {http://arxiv.org/abs/1911.02116},
	abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6\% average accuracy on XNLI, +13\% average F1 score on MLQA, and +2.4\% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7\% in XNLI accuracy for Swahili and 11.4\% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.},
	urldate = {2021-09-16},
	journal = {arXiv:1911.02116 [cs]},
	author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = apr,
	year = {2020},
	note = {arXiv: 1911.02116},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL 2020 (+ updated results)},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/PCARUDAU/Conneau et al. - 2020 - Unsupervised Cross-lingual Representation Learning.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/ZK3N6UF9/1911.html:text/html},
}

@article{zhai_attention_2021,
	title = {An {Attention} {Free} {Transformer}},
	url = {http://arxiv.org/abs/2105.14103},
	abstract = {We introduce Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention. In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion. This new operation has a memory complexity linear w.r.t. both the context size and the dimension of features, making it compatible to both large input and model sizes. We also introduce AFT-local and AFT-conv, two model variants that take advantage of the idea of locality and spatial weight sharing while maintaining global connectivity. We conduct extensive experiments on two autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image recognition task (ImageNet-1K classification). We show that AFT demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.},
	urldate = {2021-09-16},
	journal = {arXiv:2105.14103 [cs]},
	author = {Zhai, Shuangfei and Talbott, Walter and Srivastava, Nitish and Huang, Chen and Goh, Hanlin and Zhang, Ruixiang and Susskind, Josh},
	month = may,
	year = {2021},
	note = {arXiv: 2105.14103},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/7RY5JRNK/Zhai et al. - 2021 - An Attention Free Transformer.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/W9WH3265/2105.html:text/html},
}

@article{karimov_cnn_2021,
	title = {{CNN} with large memory layers},
	url = {http://arxiv.org/abs/2101.11685},
	abstract = {This work is centred around the recently proposed product key memory structure [46], implemented for a number of computer vision applications. The memory structure can be regarded as a simple computation primitive suitable to be augmented to nearly all neural network architectures. The memory block allows implementing sparse access to memory with square root complexity scaling with respect to the memory capacity. The latter scaling is possible due to the incorporation of Cartesian product space decomposition of the key space for the nearest neighbour search. We have tested the memory layer on the classiﬁcation, image reconstruction and relocalization problems and found that for some of those, the memory layers can provide signiﬁcant speed/accuracy improvement with the high utilization of the key-value elements, while others require more careful ﬁne-tuning and suﬀer from dying keys. To tackle the later problem we have introduced a simple technique of memory re-initialization which helps us to eliminate unused key-value pairs from the memory and engage them in training again. We have conducted various experiments and got improvements in speed and accuracy for classiﬁcation and PoseNet relocalization models.},
	language = {en},
	urldate = {2021-09-16},
	journal = {arXiv:2101.11685 [cs]},
	author = {Karimov, Rasul and Malkov, Yury and Iskakov, Karim and Lempitsky, Victor},
	month = apr,
	year = {2021},
	note = {arXiv: 2101.11685},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Master's dissertation paper},
	file = {Karimov et al. - 2021 - CNN with large memory layers.pdf:/home/zwerg/Zotero/storage/XZL3D69G/Karimov et al. - 2021 - CNN with large memory layers.pdf:application/pdf},
}

@misc{noauthor_end--end_nodate,
	title = {End-to-end {Object} {Detection} with {Transformers}},
	url = {https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task.},
	language = {en},
	urldate = {2021-09-16},
	file = {Snapshot:/home/zwerg/Zotero/storage/VEE65KFQ/end-to-end-object-detection-with-transformers.html:text/html},
}

@incollection{vedaldi_end--end_2020,
	address = {Cham},
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	volume = {12346},
	isbn = {978-3-030-58451-1 978-3-030-58452-8},
	url = {https://link.springer.com/10.1007/978-3-030-58452-8_13},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, eﬀectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a ﬁxed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the ﬁnal set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a uniﬁed manner. We show that it signiﬁcantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	language = {en},
	urldate = {2021-09-16},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58452-8_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {213--229},
	file = {Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf:/home/zwerg/Zotero/storage/6HSSZM4B/Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf:application/pdf},
}

@article{cohen_gauge_2019,
	title = {Gauge {Equivariant} {Convolutional} {Networks} and the {Icosahedral} {CNN}},
	url = {http://arxiv.org/abs/1902.04615},
	abstract = {The principle of equivariance to symmetry transformations enables a theoretically grounded approach to neural network architecture design. Equivariant networks have shown excellent performance and data efficiency on vision and medical imaging problems that exhibit symmetries. Here we show how this principle can be extended beyond global symmetries to local gauge transformations. This enables the development of a very general class of convolutional neural networks on manifolds that depend only on the intrinsic geometry, and which includes many popular methods from equivariant and geometric deep learning. We implement gauge equivariant CNNs for signals defined on the surface of the icosahedron, which provides a reasonable approximation of the sphere. By choosing to work with this very regular manifold, we are able to implement the gauge equivariant convolution using a single conv2d call, making it a highly scalable and practical alternative to Spherical CNNs. Using this method, we demonstrate substantial improvements over previous methods on the task of segmenting omnidirectional images and global climate patterns.},
	urldate = {2021-09-16},
	journal = {arXiv:1902.04615 [cs, stat]},
	author = {Cohen, Taco S. and Weiler, Maurice and Kicanaoglu, Berkay and Welling, Max},
	month = may,
	year = {2019},
	note = {arXiv: 1902.04615},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Proceedings of the International Conference on Machine Learning (ICML), 2019},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/83A999P3/Cohen et al. - 2019 - Gauge Equivariant Convolutional Networks and the I.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/8U4FL4MF/1902.html:text/html},
}

@article{wang_learning_2017,
	title = {Learning to reinforcement learn},
	url = {http://arxiv.org/abs/1611.05763},
	abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
	urldate = {2021-09-16},
	journal = {arXiv:1611.05763 [cs, stat]},
	author = {Wang, Jane X. and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
	month = jan,
	year = {2017},
	note = {arXiv: 1611.05763},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 17 pages, 7 figures, 1 table},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/FZBZISPR/Wang et al. - 2017 - Learning to reinforcement learn.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/GUL9JQPS/1611.html:text/html},
}

@article{wolterink_blood_2018,
	title = {Blood {Vessel} {Geometry} {Synthesis} using {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1804.04381},
	abstract = {Computationally synthesized blood vessels can be used for training and evaluation of medical image analysis applications. We propose a deep generative model to synthesize blood vessel geometries, with an application to coronary arteries in cardiac CT angiography (CCTA). In the proposed method, a Wasserstein generative adversarial network (GAN) consisting of a generator and a discriminator network is trained. While the generator tries to synthesize realistic blood vessel geometries, the discriminator tries to distinguish synthesized geometries from those of real blood vessels. Both real and synthesized blood vessel geometries are parametrized as 1D signals based on the central vessel axis. The generator can optionally be provided with an attribute vector to synthesize vessels with particular characteristics. The GAN was optimized using a reference database with parametrizations of 4,412 real coronary artery geometries extracted from CCTA scans. After training, plausible coronary artery geometries could be synthesized based on random vectors sampled from a latent space. A qualitative analysis showed strong similarities between real and synthesized coronary arteries. A detailed analysis of the latent space showed that the diversity present in coronary artery anatomy was accurately captured by the generator. Results show that Wasserstein generative adversarial networks can be used to synthesize blood vessel geometries.},
	urldate = {2021-10-01},
	journal = {arXiv:1804.04381 [cs]},
	author = {Wolterink, Jelmer M. and Leiner, Tim and Isgum, Ivana},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.04381},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Submitted to the 1st Conference on Medical Imaging with Deep Learning (MIDL2018), Amsterdam, The Netherlands (https://openreview.net/forum?id=SJ4N7isiG)},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/TANB2F6K/Wolterink et al. - 2018 - Blood Vessel Geometry Synthesis using Generative A.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/9SVLCCHI/1804.html:text/html},
}

@article{lee_feature2mass_2018,
	title = {{Feature2Mass}: {Visual} {Feature} {Processing} in {Latent} {Space} for {Realistic} {Labeled} {Mass} {Generation}},
	shorttitle = {{Feature2Mass}},
	url = {http://arxiv.org/abs/1809.06147},
	abstract = {This paper deals with a method for generating realistic labeled masses. Recently, there have been many attempts to apply deep learning to various bio-image computing fields including computer-aided detection and diagnosis. In order to learn deep network model to be well-behaved in bio-image computing fields, a lot of labeled data is required. However, in many bioimaging fields, the large-size of labeled dataset is scarcely available. Although a few researches have been dedicated to solving this problem through generative model, there are some problems as follows: 1) The generated bio-image does not seem realistic; 2) the variation of generated bio-image is limited; and 3) additional label annotation task is needed. In this study, we propose a realistic labeled bio-image generation method through visual feature processing in latent space. Experimental results have shown that mass images generated by the proposed method were realistic and had wide expression range of targeted mass characteristics.},
	urldate = {2021-10-01},
	journal = {arXiv:1809.06147 [cs]},
	author = {Lee, Jae-Hyeok and Kim, Seong Tae and Lee, Hakmin and Ro, Yong Man},
	month = nov,
	year = {2018},
	note = {arXiv: 1809.06147},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: This paper presented at ECCV 2018 Workshop},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/GE6IGVGP/Lee et al. - 2018 - Feature2Mass Visual Feature Processing in Latent .pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/VVYKI84B/1809.html:text/html},
}

@article{frid-adar_gan-based_2018,
	title = {{GAN}-based {Synthetic} {Medical} {Image} {Augmentation} for increased {CNN} {Performance} in {Liver} {Lesion} {Classification}},
	volume = {321},
	issn = {09252312},
	url = {http://arxiv.org/abs/1803.01229},
	doi = {10.1016/j.neucom.2018.09.013},
	abstract = {Deep learning methods, and in particular convolutional neural networks (CNNs), have led to an enormous breakthrough in a wide range of computer vision tasks, primarily by using large-scale annotated datasets. However, obtaining such datasets in the medical domain remains a challenge. In this paper, we present methods for generating synthetic medical images using recently presented deep learning Generative Adversarial Networks (GANs). Furthermore, we show that generated medical images can be used for synthetic data augmentation, and improve the performance of CNN for medical image classification. Our novel method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). We first exploit GAN architectures for synthesizing high quality liver lesion ROIs. Then we present a novel scheme for liver lesion classification using CNN. Finally, we train the CNN using classic data augmentation and our synthetic data augmentation and compare performance. In addition, we explore the quality of our synthesized examples using visualization and expert assessment. The classification performance using only classic data augmentation yielded 78.6\% sensitivity and 88.4\% specificity. By adding the synthetic data augmentation the results increased to 85.7\% sensitivity and 92.4\% specificity. We believe that this approach to synthetic data augmentation can generalize to other medical classification applications and thus support radiologists' efforts to improve diagnosis.},
	urldate = {2021-10-01},
	journal = {Neurocomputing},
	author = {Frid-Adar, Maayan and Diamant, Idit and Klang, Eyal and Amitai, Michal and Goldberger, Jacob and Greenspan, Hayit},
	month = dec,
	year = {2018},
	note = {arXiv: 1803.01229},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	pages = {321--331},
	annote = {Comment: Preprint submitted to Neurocomputing},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/VTHILRHJ/Frid-Adar et al. - 2018 - GAN-based Synthetic Medical Image Augmentation for.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/3GFSATZF/1803.html:text/html},
}

@article{radford_unsupervised_2016,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	language = {en},
	urldate = {2021-10-01},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = jan,
	year = {2016},
	note = {arXiv: 1511.06434},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Under review as a conference paper at ICLR 2016},
	file = {Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:/home/zwerg/Zotero/storage/QY3NLRJJ/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf},
}

@misc{naik_big_2021,
	title = {Big {Tech} \& {Their} {Favourite} {Deep} {Learning} {Techniques}},
	url = {https://analyticsindiamag.com/big-tech-their-favourite-deep-learning-techniques/},
	abstract = {Every week, the top AI labs globally -- Google, Facebook, Microsoft, Apple, etc. -- release tons of new research work, tools, datasets, models, libraries and frameworks in artificial intelligence (AI) and machine learning (ML).},
	language = {en-US},
	urldate = {2021-10-01},
	journal = {Analytics India Magazine},
	author = {Naik, Amit Raja},
	month = sep,
	year = {2021},
	file = {Snapshot:/home/zwerg/Zotero/storage/TLVIDJ8Q/big-tech-their-favourite-deep-learning-techniques.html:text/html},
}

@article{van_der_maaten_barnes-hut-sne_2013,
	title = {Barnes-{Hut}-{SNE}},
	url = {http://arxiv.org/abs/1301.3342},
	abstract = {The paper presents an O(N log N)-implementation of t-SNE -- an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in O(N{\textasciicircum}2). The new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm used by astronomers to perform N-body simulations - to approximate the forces between the corresponding points in the embedding. Our experiments show that the new algorithm, called Barnes-Hut-SNE, leads to substantial computational advantages over standard t-SNE, and that it makes it possible to learn embeddings of data sets with millions of objects.},
	urldate = {2021-09-29},
	journal = {arXiv:1301.3342 [cs, stat]},
	author = {van der Maaten, Laurens},
	month = mar,
	year = {2013},
	note = {arXiv: 1301.3342},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/KDBBKLDE/van der Maaten - 2013 - Barnes-Hut-SNE.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/S8RZDKG6/1301.html:text/html},
}

@article{radosavovic_designing_2020,
	title = {Designing {Network} {Design} {Spaces}},
	url = {http://arxiv.org/abs/2003.13678},
	abstract = {In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting ﬁndings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of ﬂop regimes. Under comparable training settings and ﬂops, the RegNet models outperform the popular EfﬁcientNet models while being up to 5× faster on GPUs.},
	language = {en},
	urldate = {2021-09-27},
	journal = {arXiv:2003.13678 [cs]},
	author = {Radosavovic, Ilija and Kosaraju, Raj Prateek and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.13678},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2020},
	file = {Radosavovic et al. - 2020 - Designing Network Design Spaces.pdf:/home/zwerg/Zotero/storage/CZFMVNWF/Radosavovic et al. - 2020 - Designing Network Design Spaces.pdf:application/pdf},
}

@article{ridnik_tresnet_2020,
	title = {{TResNet}: {High} {Performance} {GPU}-{Dedicated} {Architecture}},
	shorttitle = {{TResNet}},
	url = {http://arxiv.org/abs/2003.13630},
	abstract = {Many deep learning models, developed in recent years, reach higher ImageNet accuracy than ResNet50, with fewer or comparable FLOPS count. While FLOPs are often seen as a proxy for network efficiency, when measuring actual GPU training and inference throughput, vanilla ResNet50 is usually significantly faster than its recent competitors, offering better throughput-accuracy trade-off. In this work, we introduce a series of architecture modifications that aim to boost neural networks' accuracy, while retaining their GPU training and inference efficiency. We first demonstrate and discuss the bottlenecks induced by FLOPs-optimizations. We then suggest alternative designs that better utilize GPU structure and assets. Finally, we introduce a new family of GPU-dedicated models, called TResNet, which achieve better accuracy and efficiency than previous ConvNets. Using a TResNet model, with similar GPU throughput to ResNet50, we reach 80.8 top-1 accuracy on ImageNet. Our TResNet models also transfer well and achieve state-of-the-art accuracy on competitive single-label classification datasets such as Stanford cars (96.0\%), CIFAR-10 (99.0\%), CIFAR-100 (91.5\%) and Oxford-Flowers (99.1\%). They also perform well on multi-label classification and object detection tasks. Implementation is available at: https://github.com/mrT23/TResNet.},
	urldate = {2021-09-27},
	journal = {arXiv:2003.13630 [cs, eess]},
	author = {Ridnik, Tal and Lawen, Hussam and Noy, Asaf and Baruch, Emanuel Ben and Sharir, Gilad and Friedman, Itamar},
	month = aug,
	year = {2020},
	note = {arXiv: 2003.13630},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: 11 pages, 5 figures},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/3XR4SHFP/Ridnik et al. - 2020 - TResNet High Performance GPU-Dedicated Architectu.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/GXQTNHT4/2003.html:text/html},
}

@inproceedings{wang_cspnet_2020,
	address = {Seattle, WA, USA},
	title = {{CSPNet}: {A} {New} {Backbone} that can {Enhance} {Learning} {Capability} of {CNN}},
	isbn = {978-1-72819-360-1},
	shorttitle = {{CSPNet}},
	url = {https://ieeexplore.ieee.org/document/9150780/},
	doi = {10.1109/CVPRW50498.2020.00203},
	abstract = {Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, such success greatly relies on costly computation resources, which hinders people with cheap devices from appreciating the advanced technology. In this paper, we propose Cross Stage Partial Network (CSPNet) to mitigate the problem that previous works require heavy inference computations from the network architecture perspective. We attribute the problem to the duplicate gradient information within network optimization. The proposed networks respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage, which, in our experiments, reduces computations by 20\% with equivalent or even superior accuracy on the ImageNet dataset, and significantly outperforms state-of-the-art approaches in terms of AP50 on the MS COCO object detection dataset. The CSPNet is easy to implement and general enough to cope with architectures based on ResNet, ResNeXt, and DenseNet.},
	language = {en},
	urldate = {2021-09-27},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Wang, Chien-Yao and Mark Liao, Hong-Yuan and Wu, Yueh-Hua and Chen, Ping-Yang and Hsieh, Jun-Wei and Yeh, I-Hau},
	month = jun,
	year = {2020},
	pages = {1571--1580},
	file = {Wang et al. - 2020 - CSPNet A New Backbone that can Enhance Learning C.pdf:/home/zwerg/Zotero/storage/5MWYN9E7/Wang et al. - 2020 - CSPNet A New Backbone that can Enhance Learning C.pdf:application/pdf},
}

@article{merity_regularizing_2017-2,
	title = {Regularizing and {Optimizing} {LSTM} {Language} {Models}},
	url = {http://arxiv.org/abs/1708.02182},
	abstract = {Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.},
	urldate = {2021-09-27},
	journal = {arXiv:1708.02182 [cs]},
	author = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.02182},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/Y7ZH7CX9/Merity et al. - 2017 - Regularizing and Optimizing LSTM Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/EDCW9NTJ/1708.html:text/html},
}

@techreport{wagner_deep_2020,
	title = {Deep learning-enhanced light-field imaging with continuous validation},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.07.30.228924v1},
	abstract = {Light field microscopy (LFM) has emerged as a powerful tool for fast volumetric image acquisition in biology, but its effective throughput and widespread use has been hampered by a computationally demanding and artefact-prone image reconstruction process. Here, we present a novel framework consisting of a hybrid light-field light-sheet microscope and deep learning-based volume reconstruction, where single light-sheet acquisitions continuously serve as training data and validation for the convolutional neural network reconstructing the LFM volume. Our network delivers high-quality reconstructions at video-rate throughput and we demonstrate the capabilities of our approach by imaging medaka heart dynamics and zebrafish neural activity.},
	language = {en},
	urldate = {2021-09-27},
	author = {Wagner, Nils and Beuttenmueller, Fynn and Norlin, Nils and Gierten, Jakob and Wittbrodt, Joachim and Weigert, Martin and Hufnagel, Lars and Prevedel, Robert and Kreshuk, Anna},
	month = jul,
	year = {2020},
	doi = {10.1101/2020.07.30.228924},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {2020.07.30.228924},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/4KVP8G5V/Wagner et al. - 2020 - Deep learning-enhanced light-field imaging with co.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/TCMG2Y7S/2020.07.30.html:text/html},
}

@article{pinkard_pycro-manager_2021,
	title = {Pycro-{Manager}: open-source software for customized and reproducible microscope control},
	volume = {18},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc. part of Springer Nature},
	issn = {1548-7105},
	shorttitle = {Pycro-{Manager}},
	url = {https://www.nature.com/articles/s41592-021-01087-6},
	doi = {10.1038/s41592-021-01087-6},
	language = {en},
	number = {3},
	urldate = {2021-09-23},
	journal = {Nature Methods},
	author = {Pinkard, Henry and Stuurman, Nico and Ivanov, Ivan E. and Anthony, Nicholas M. and Ouyang, Wei and Li, Bin and Yang, Bin and Tsuchida, Mark A. and Chhun, Bryant and Zhang, Grace and Mei, Ryan and Anderson, Michael and Shepherd, Douglas P. and Hunt-Isaak, Ian and Dunn, Raymond L. and Jahr, Wiebke and Kato, Saul and Royer, Loïc A. and Thiagarajah, Jay R. and Eliceiri, Kevin W. and Lundberg, Emma and Mehta, Shalin B. and Waller, Laura},
	month = mar,
	year = {2021},
	note = {Number: 3
Primary\_atype: Correspondence
Publisher: Nature Publishing Group
Subject\_term: Data acquisition;Image processing;Microscopy;Optical imaging;Software
Subject\_term\_id: data-acquisition;image-processing;microscopy;optical-imaging;software},
	pages = {226--228},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/N3J6NK97/Pinkard et al. - 2021 - Pycro-Manager open-source software for customized.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/AKS7GT4X/s41592-021-01087-6.html:text/html},
}

@article{pinkard_micro-magellan_2016,
	title = {Micro-{Magellan}: open-source, sample-adaptive, acquisition software for optical microscopy},
	volume = {13},
	issn = {1548-7091, 1548-7105},
	shorttitle = {Micro-{Magellan}},
	url = {http://www.nature.com/articles/nmeth.3991},
	doi = {10.1038/nmeth.3991},
	language = {en},
	number = {10},
	urldate = {2021-09-23},
	journal = {Nature Methods},
	author = {Pinkard, Henry and Stuurman, Nico and Corbin, Kaitlin and Vale, Ronald and Krummel, Matthew F},
	month = oct,
	year = {2016},
	pages = {807--809},
	file = {Gen3_MicroMagellan_paper.pdf:/home/zwerg/Zotero/storage/YFL3P2RZ/Gen3_MicroMagellan_paper.pdf:application/pdf},
}

@article{tosi_autoscanj_2021,
	title = {{AutoScanJ}: {A} {Suite} of {ImageJ} {Scripts} for {Intelligent} {Microscopy}},
	volume = {1},
	issn = {2673-7647},
	shorttitle = {{AutoScanJ}},
	url = {https://www.frontiersin.org/article/10.3389/fbinf.2021.627626},
	doi = {10.3389/fbinf.2021.627626},
	abstract = {We developed AutoscanJ, a suite of ImageJ scripts enabling to image targets of interest by automatically driving a motorized microscope at the corresponding locations. For live samples, our software can sequentially detect biological events from their onset and further image them at high resolution, an action that would be impractical by user operation. For fixed samples, the software can dramatically reduce the amount of data acquired and the acquisition duration in situations where statistically few targets of interest are observed per field of view. AutoScanJ is compatible with motorized fluorescence microscopes controlled by Leica LAS AF/X or Micro-Manager. The software is straightforward to set up and new custom image analysis workflows to detect targets of interest can be simply implemented and shared with minimal efforts as independent ImageJ macro functions. We illustrate five different application scenarios with the system ranging from samples fixed on micropatterned surfaces to live cells undergoing several rounds of division. The target detection functions for these applications are provided and can be used as a starting point and a source of inspiration for new applications. Overall, AutoScanJ helps to optimize microscope usage by autonomous operation, and it opens up new experimental avenues by enabling the real-time detection and selective imaging of transient events in live microscopy.},
	urldate = {2021-09-23},
	journal = {Frontiers in Bioinformatics},
	author = {Tosi, Sébastien and Lladó, Anna and Bardia, Lídia and Rebollo, Elena and Godo, Anna and Stockinger, Petra and Colombelli, Julien},
	year = {2021},
	pages = {3},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/YRDGHCF3/Tosi et al. - 2021 - AutoScanJ A Suite of ImageJ Scripts for Intellige.pdf:application/pdf},
}

@inproceedings{pereira_authentication_2014,
	address = {Dallas, TX, USA},
	title = {An authentication and access control framework for {CoAP}-based {Internet} of {Things}},
	isbn = {978-1-4799-4032-5},
	url = {http://ieeexplore.ieee.org/document/7049308/},
	doi = {10.1109/IECON.2014.7049308},
	abstract = {Internet of Things (IoT) and Cyber-physical Systems (CPS) are two very hot research topics today, and more and more products are starting to appear on the market. Research has shown that the use of Service Oriented Architecture (SOA) can enable distributed application and devices to device communication, even on very resource constrained devices, and thus play an important role for IoT and CPS.},
	language = {en},
	urldate = {2021-09-23},
	booktitle = {{IECON} 2014 - 40th {Annual} {Conference} of the {IEEE} {Industrial} {Electronics} {Society}},
	publisher = {IEEE},
	author = {Pereira, Pablo Punal and Eliasson, Jens and Delsing, Jerker},
	month = oct,
	year = {2014},
	pages = {5293--5299},
	file = {Pereira et al. - 2014 - An authentication and access control framework for.pdf:/home/zwerg/Zotero/storage/2JZIKVTP/Pereira et al. - 2014 - An authentication and access control framework for.pdf:application/pdf},
}

@misc{noauthor_iot_nodate,
	title = {{IoT} {Standards} and protocols guide — protocols of the {Internet} of {Things}},
	url = {https://www.avsystem.com/blog/iot-protocols-and-standards/},
	abstract = {IoT protocols are a crucial part of the IoT technology stack \&mdash; without them, hardware would be rendered},
	language = {en-US, en-GB, en-AU, en-CA, de-DE, fr, ja, pt-BR},
	urldate = {2021-09-23},
	file = {Snapshot:/home/zwerg/Zotero/storage/S9CZ8QMW/iot-protocols-and-standards.html:text/html},
}

@misc{noauthor_iot_nodate-1,
	title = {{IoT} {Standards} \& {Protocols} {Guide} {\textbar} 2019 {Comparisons} on {Network}, {Wireless} {Comms}, {Security}, {Industrial}},
	url = {https://www.postscapes.com/internet-of-things-protocols},
	abstract = {Overviews of protocols involved in Internet of Things devices and applications. Help clarify with IoT layer technology stack graphics and head-to-head comparisons.},
	language = {en-US},
	urldate = {2021-09-23},
	journal = {Postscapes},
	file = {Snapshot:/home/zwerg/Zotero/storage/GIKCVDZE/internet-of-things-protocols.html:text/html},
}

@misc{noauthor_11_nodate,
	title = {11 {Internet} of {Things} ({IoT}) {Protocols} {You} {Need} to {Know} {About}},
	url = {https://www.rs-online.com/designspark/eleven-internet-of-things-iot-protocols-you-need-to-know-about},
	urldate = {2021-09-23},
	file = {11 Internet of Things (IoT) Protocols You Need to Know About:/home/zwerg/Zotero/storage/WDQGXHB8/eleven-internet-of-things-iot-protocols-you-need-to-know-about.html:text/html},
}

@misc{wang_siammask_2021,
	title = {{SiamMask}},
	copyright = {MIT},
	url = {https://github.com/foolwood/SiamMask},
	abstract = {[CVPR2019] Fast Online Object Tracking and Segmentation: A Unifying Approach},
	urldate = {2021-09-22},
	author = {Wang, Qiang},
	month = sep,
	year = {2021},
	note = {original-date: 2019-03-04T11:34:17Z},
	keywords = {computer-vision, cvpr2019, deep-learning, object-tracking, pytorch, read-time, video-object-segmentation, visual-tracking},
}

@article{dorkenwald_stochastic_nodate,
	title = {Stochastic {Image}-to-{Video} {Synthesis} {Using} {cINNs}},
	abstract = {Video understanding calls for a model to learn the characteristic interplay between static scene content and its dynamics: Given an image, the model must be able to predict a future progression of the portrayed scene and, conversely, a video should be explained in terms of its static image content and all the remaining characteristics not present in the initial frame. This naturally suggests a bijective mapping between the video domain and the static content as well as residual information. In contrast to common stochastic image-to-video synthesis, such a model does not merely generate arbitrary videos progressing the initial image. Given this image, it rather provides a one-to-one mapping between the residual vectors and the video with stochastic outcomes when sampling. The approach is naturally implemented using a conditional invertible neural network (cINN) that can explain videos by independently modelling static and other video characteristics, thus laying the basis for controlled video synthesis. Experiments on diverse video datasets demonstrate the effectiveness of our approach in terms of both the quality and diversity of the synthesized results. Our project page is available at https://bit.ly/3dg90fV .},
	language = {en},
	author = {Dorkenwald, Michael and Milbich, Timo and Blattmann, Andreas and Rombach, Robin and Derpanis, Konstantinos G and Ommer, Bjorn},
	pages = {12},
	file = {Dorkenwald et al. - Stochastic Image-to-Video Synthesis Using cINNs.pdf:/home/zwerg/Zotero/storage/6WTXKDS4/Dorkenwald et al. - Stochastic Image-to-Video Synthesis Using cINNs.pdf:application/pdf},
}

@article{daniel_soft-introvae_nodate,
	title = {Soft-{IntroVAE}: {Analyzing} and {Improving} the {Introspective} {Variational} {Autoencoder}},
	abstract = {The recently introduced introspective variational autoencoder (IntroVAE) exhibits outstanding image generations, and allows for amortized inference using an image encoder. The main idea in IntroVAE is to train a VAE adversarially, using the VAE encoder to discriminate between generated and real data samples. However, the original IntroVAE loss function relied on a particular hinge-loss formulation that is very hard to stabilize in practice, and its theoretical convergence analysis ignored important terms in the loss. In this work, we take a step towards better understanding of the IntroVAE model, its practical implementation, and its applications. We propose the Soft-IntroVAE, a modiﬁed IntroVAE that replaces the hinge-loss terms with a smooth exponential loss on generated samples. This change signiﬁcantly improves training stability, and also enables theoretical analysis of the complete algorithm. Interestingly, we show that the IntroVAE converges to a distribution that minimizes a sum of KL distance from the data distribution and an entropy term. We discuss the implications of this result, and demonstrate that it induces competitive image generation and reconstruction. Finally, we describe an application of Soft-IntroVAE to unsupervised image translation, and demonstrate compelling results. Code and additional information is available on the project website taldatech.github.io/soft- intro- vae- web.},
	language = {en},
	author = {Daniel, Tal and Tamar, Aviv},
	pages = {10},
	file = {Daniel and Tamar - Soft-IntroVAE Analyzing and Improving the Introsp.pdf:/home/zwerg/Zotero/storage/JA268XJF/Daniel and Tamar - Soft-IntroVAE Analyzing and Improving the Introsp.pdf:application/pdf},
}

@article{park_unsupervised_nodate,
	title = {Unsupervised {Hyperbolic} {Representation} {Learning} via {Message} {Passing} {Auto}-{Encoders}},
	abstract = {Most of the existing literature regarding hyperbolic embedding concentrate upon supervised learning, whereas the use of unsupervised hyperbolic embedding is less well explored. In this paper, we analyze how unsupervised tasks can beneﬁt from learned representations in hyperbolic space. To explore how well the hierarchical structure of unlabeled data can be represented in hyperbolic spaces, we design a novel hyperbolic message passing auto-encoder whose overall auto-encoding is performed in hyperbolic space. The proposed model conducts auto-encoding the networks via fully utilizing hyperbolic geometry in message passing. Through extensive quantitative and qualitative analyses, we validate the properties and beneﬁts of the unsupervised hyperbolic representations. Codes are available at https://github.com/junhocho/HGCAE.},
	language = {en},
	author = {Park, Jiwoong and Cho, Junho and Chang, Hyung Jin and Choi, Jin Young},
	pages = {11},
	file = {Park et al. - Unsupervised Hyperbolic Representation Learning vi.pdf:/home/zwerg/Zotero/storage/LSNG3B9W/Park et al. - Unsupervised Hyperbolic Representation Learning vi.pdf:application/pdf},
}

@article{joseph_towards_nodate,
	title = {Towards {Open} {World} {Object} {Detection}},
	language = {en},
	author = {Joseph, K J and Khan, Salman and Khan, Fahad Shahbaz and Balasubramanian, Vineeth N},
	pages = {11},
	file = {Joseph et al. - Towards Open World Object Detection.pdf:/home/zwerg/Zotero/storage/LDT9BLD6/Joseph et al. - Towards Open World Object Detection.pdf:application/pdf},
}

@article{saharia_image_2021-1,
	title = {Image {Super}-{Resolution} via {Iterative} {Refinement}},
	url = {http://arxiv.org/abs/2104.07636},
	abstract = {We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50\%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34\%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet.},
	urldate = {2021-09-22},
	journal = {arXiv:2104.07636 [cs, eess]},
	author = {Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
	month = jun,
	year = {2021},
	note = {arXiv: 2104.07636},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/2JZFJGYF/Saharia et al. - 2021 - Image Super-Resolution via Iterative Refinement.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/SARW87D3/2104.html:text/html},
}

@misc{noauthor_friendly_nodate,
	title = {A friendly introduction to machine learning compilers and optimizers},
	url = {https://huyenchip.com/2021/09/07/a-friendly-introduction-to-machine-learning-compilers-and-optimizers.html},
	urldate = {2021-09-22},
	file = {A friendly introduction to machine learning compilers and optimizers:/home/zwerg/Zotero/storage/PKYV2NP8/a-friendly-introduction-to-machine-learning-compilers-and-optimizers.html:text/html},
}

@article{tan_efficientnetv2_2021,
	title = {{EfficientNetV2}: {Smaller} {Models} and {Faster} {Training}},
	shorttitle = {{EfficientNetV2}},
	url = {http://arxiv.org/abs/2104.00298},
	abstract = {This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller. Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy. With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3\% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0\% accuracy while training 5x-11x faster using the same computing resources. Code will be available at https://github.com/google/automl/tree/master/efficientnetv2.},
	urldate = {2021-09-22},
	journal = {arXiv:2104.00298 [cs]},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = jun,
	year = {2021},
	note = {arXiv: 2104.00298},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICML 2021},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/LHGTLRMR/Tan and Le - 2021 - EfficientNetV2 Smaller Models and Faster Training.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/K48IJJQF/2104.html:text/html},
}

@article{dai_coatnet_2021,
	title = {{CoAtNet}: {Marrying} {Convolution} and {Attention} for {All} {Data} {Sizes}},
	shorttitle = {{CoAtNet}},
	url = {http://arxiv.org/abs/2106.04803},
	abstract = {Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets(pronounced "coat" nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0\% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56\% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88\% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.},
	urldate = {2021-09-22},
	journal = {arXiv:2106.04803 [cs]},
	author = {Dai, Zihang and Liu, Hanxiao and Le, Quoc V. and Tan, Mingxing},
	month = sep,
	year = {2021},
	note = {arXiv: 2106.04803},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/SV3SCH48/Dai et al. - 2021 - CoAtNet Marrying Convolution and Attention for Al.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/W6SNIV7B/2106.html:text/html},
}

@misc{noauthor_youtube-8m_nodate,
	title = {{YouTube}-{8M}: {A} {Large} and {Diverse} {Labeled} {Video} {Dataset} for {Video} {Understanding} {Research}},
	url = {https://research.google.com/youtube8m/},
	urldate = {2021-09-22},
}

@misc{noauthor_open_nodate-1,
	title = {Open {Images} {V6}},
	url = {https://storage.googleapis.com/openimages/web/index.html},
	urldate = {2021-09-22},
}

@article{lhoest_datasets_2021,
	title = {Datasets: {A} {Community} {Library} for {Natural} {Language} {Processing}},
	shorttitle = {Datasets},
	url = {http://arxiv.org/abs/2109.02846},
	abstract = {The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.},
	urldate = {2021-09-22},
	journal = {arXiv:2109.02846 [cs]},
	author = {Lhoest, Quentin and del Moral, Albert Villanova and Jernite, Yacine and Thakur, Abhishek and von Platen, Patrick and Patil, Suraj and Chaumond, Julien and Drame, Mariama and Plu, Julien and Tunstall, Lewis and Davison, Joe and Šaško, Mario and Chhablani, Gunjan and Malik, Bhavitvya and Brandeis, Simon and Scao, Teven Le and Sanh, Victor and Xu, Canwen and Patry, Nicolas and McMillan-Major, Angelina and Schmid, Philipp and Gugger, Sylvain and Delangue, Clément and Matussière, Théo and Debut, Lysandre and Bekman, Stas and Cistac, Pierric and Goehringer, Thibault and Mustar, Victor and Lagunas, François and Rush, Alexander M. and Wolf, Thomas},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.02846},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: EMNLP Demo 2021},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/U3T9YYPU/Lhoest et al. - 2021 - Datasets A Community Library for Natural Language.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/6AQ4TV2W/2109.html:text/html},
}

@misc{noauthor_transformer_nodate,
	title = {Transformer models - {Hugging} {Face} {Course}},
	url = {https://huggingface.co/course/chapter1},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2021-09-22},
}

@article{engelmann_points_nodate,
	title = {From {Points} to {Multi}-{Object} {3D} {Reconstruction}},
	abstract = {We propose a method to detect and reconstruct multiple 3D objects from a single RGB image. The key idea is to optimize for detection, alignment and shape jointly over all objects in the RGB image, while focusing on realistic and physically plausible reconstructions. To this end, we propose a key-point detector that localizes objects as center points and directly predicts all object properties, including 9-DoF bounding boxes and 3D shapes – all in a single forward pass. The proposed method formulates 3D shape reconstruction as a shape selection problem, i.e. it selects among exemplar shapes from a given database. This makes it agnostic to shape representations, which enables a lightweight reconstruction of realistic and visually-pleasing shapes based on CAD-models, while the training objective is formulated around point clouds and voxel representations. A collision-loss promotes non-intersecting objects, further increasing the reconstruction realism. Given the RGB image, the presented approach performs lightweight reconstruction in a single-stage, it is real-time capable, fully differentiable and end-to-end trainable. Our experiments compare multiple approaches for 9-DoF bounding box estimation, evaluate the novel shape-selection mechanism and compare to recent methods in terms of 3D bounding box estimation and 3D shape reconstruction quality.},
	language = {en},
	author = {Engelmann, Francis and Rematas, Konstantinos and Leibe, Bastian and Ferrari, Vittorio},
	pages = {10},
	file = {Engelmann et al. - From Points to Multi-Object 3D Reconstruction.pdf:/home/zwerg/Zotero/storage/7EJIBJN3/Engelmann et al. - From Points to Multi-Object 3D Reconstruction.pdf:application/pdf},
}

@article{ranjan_learning_nodate,
	title = {Learning {To} {Count} {Everything}},
	abstract = {Existing works on visual counting primarily focus on one speciﬁc category at a time, such as people, animals, and cells. In this paper, we are interested in counting everything, that is to count objects from any category given only a few annotated instances from that category. To this end, we pose counting as a few-shot regression task. To tackle this task, we present a novel method that takes a query image together with a few exemplar objects from the query image and predicts a density map for the presence of all objects of interest in the query image. We also present a novel adaptation strategy to adapt our network to any novel visual category at test time, using only a few exemplar objects from the novel category. We also introduce a dataset of 147 object categories containing over 6000 images that are suitable for the few-shot counting task. The images are annotated with two types of annotation, dots and bounding boxes, and they can be used for developing few-shot counting models. Experiments on this dataset shows that our method outperforms several state-of-the-art object detectors and few-shot counting approaches. Our code and dataset can be found at https://github.com/cvlab- stonybrook/ LearningToCountEverything.},
	language = {en},
	author = {Ranjan, Viresh and Sharma, Udbhav and Nguyen, Thu and Hoai, Minh},
	pages = {10},
	file = {Ranjan et al. - Learning To Count Everything.pdf:/home/zwerg/Zotero/storage/KVGH4PH3/Ranjan et al. - Learning To Count Everything.pdf:application/pdf},
}

@article{zeng_corrnet3d_nodate,
	title = {{CorrNet3D}: {Unsupervised} {End}-to-{End} {Learning} of {Dense} {Correspondence} for {3D} {Point} {Clouds}},
	abstract = {Motivated by the intuition that one can transform two aligned point clouds to each other more easily and meaningfully than a misaligned pair, we propose CorrNet3D –the ﬁrst unsupervised and end-to-end deep learning-based framework – to drive the learning of dense correspondence between 3D shapes by means of deformation-like reconstruction to overcome the need for annotated data. Speciﬁcally, CorrNet3D consists of a deep feature embedding module and two novel modules called correspondence indicator and symmetric deformer. Feeding a pair of raw point clouds, our model ﬁrst learns the pointwise features and passes them into the indicator to generate a learnable correspondence matrix used to permute the input pair. The symmetric deformer, with an additional regularized loss, transforms the two permuted point clouds to each other to drive the unsupervised learning of the correspondence. The extensive experiments on both synthetic and real-world datasets of rigid and non-rigid 3D shapes show our CorrNet3D outperforms state-of-the-art methods to a large extent, including those taking meshes as input. CorrNet3D is a ﬂexible framework in that it can be easily adapted to supervised learning if annotated data are available. The source code and pre-trained model will be available at https://github.com/ZENGYIMINGEAMON/CorrNet3D.git.},
	language = {en},
	author = {Zeng, Yiming and Qian, Yue and Zhu, Zhiyu and Hou, Junhui and Yuan, Hui and He, Ying},
	pages = {10},
	file = {Zeng et al. - CorrNet3D Unsupervised End-to-End Learning of Den.pdf:/home/zwerg/Zotero/storage/T57YWXET/Zeng et al. - CorrNet3D Unsupervised End-to-End Learning of Den.pdf:application/pdf},
}

@misc{noauthor_awesome-data-labeling_2021,
	title = {awesome-data-labeling},
	url = {https://github.com/heartexlabs/awesome-data-labeling},
	abstract = {A curated list of awesome data labeling tools},
	urldate = {2021-09-22},
	publisher = {Heartex},
	month = sep,
	year = {2021},
	note = {original-date: 2019-06-19T02:03:37Z},
	keywords = {deep-learning, 3d-annotation, annotation, annotation-tool, audio-annotation, audio-annotation-tool, awesome, awesome-list, bounding-box, data-labeling, image-annotation, image-labeling, image-labeling-tool, label-images, label-videos, labeling, labeling-tool, lidar, semantic-segmentation, video-annotation},
}

@misc{noauthor_iot_2019,
	title = {{IoT} {Protocols}: {MQTT}, {CoAP}, {XMPP}, {SOAP}, {UPnP}},
	shorttitle = {{IoT} {Protocols}},
	url = {https://sirinsoftware.com/blog/iot-protocols-mqtt-coap-xmpp-soap-upnp/},
	abstract = {IoT protocols are the magic that developers use to connect devices into one network and enable them to exchange data. Apart from sending and receiving data packages, these protocols ensure the network’s security and device compatibility.},
	language = {en},
	urldate = {2021-09-21},
	journal = {Sirin Software},
	month = mar,
	year = {2019},
	note = {Section: Blog},
	file = {Snapshot:/home/zwerg/Zotero/storage/8FCD24B6/iot-protocols-mqtt-coap-xmpp-soap-upnp.html:text/html},
}

@misc{dickson_computer_2021,
	title = {Computer vision and deep learning provide new ways to detect cyber threats},
	url = {https://bdtechtalks.com/2021/09/10/computer-vision-deep-learning-threat-detection/},
	abstract = {The combination of binary visualization and machine learning is a powerful technique that can provide new solutions to old problems. It is showing promise in cybersecurity, but it could also be applied to other domains.},
	language = {en-US},
	urldate = {2021-09-20},
	journal = {TechTalks},
	author = {Dickson, Ben},
	month = sep,
	year = {2021},
}

@article{nuzzi_impact_2021,
	title = {The {Impact} of {Artificial} {Intelligence} and {Deep} {Learning} in {Eye} {Diseases}: {A} {Review}},
	volume = {8},
	issn = {2296-858X},
	shorttitle = {The {Impact} of {Artificial} {Intelligence} and {Deep} {Learning} in {Eye} {Diseases}},
	url = {https://www.frontiersin.org/article/10.3389/fmed.2021.710329},
	doi = {10.3389/fmed.2021.710329},
	abstract = {Artificial intelligence (AI) is a subset of computer science dealing with the development and training of algorithms that try to replicate human intelligence. We report a clinical overview of the basic principles of AI that are fundamental to appreciating its application to ophthalmology practice. Here, we review the most common eye diseases, focusing on some of the potential challenges and limitations emerging with the development and application of this new technology into ophthalmology.},
	urldate = {2021-09-20},
	journal = {Frontiers in Medicine},
	author = {Nuzzi, Raffaele and Boscia, Giacomo and Marolo, Paola and Ricardi, Federico},
	year = {2021},
	pages = {1347},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/ABDGC4NZ/Nuzzi et al. - 2021 - The Impact of Artificial Intelligence and Deep Lea.pdf:application/pdf},
}

@article{kamimura_representation_2021,
	title = {Representation and inference of size control laws by neural-network-aided point processes},
	volume = {3},
	url = {https://link.aps.org/doi/10.1103/PhysRevResearch.3.033032},
	doi = {10.1103/PhysRevResearch.3.033032},
	abstract = {The regulation and coordination of cell growth and division are long-standing problems in cell physiology. Recent single-cell measurements that use microfluidic devices have provided quantitative time-series data on various physiological parameters of cells. To clarify the regulatory laws and associated relevant parameters, such as cell size, simple mathematical models have been constructed and tested based on their capabilities to reproduce the measured data. However, the models may fail to capture some aspects of data due to presumed assumptions or simplification, especially when the data are multidimensional. Furthermore, comparing a model and data for validation is not trivial when we handle noisy multidimensional data. Thus, to extract hidden laws from data, a novel method, which can handle and integrate noisy multidimensional data more flexibly and exhaustively than the conventional ones, is necessary and helpful. By using cell size control as an example, we demonstrate that this problem can be addressed by using a neural network (NN) method, originally developed for history-dependent temporal point processes. The NN can effectively segregate history-dependent deterministic factors and unexplainable noise from given data by flexibly representing the functional forms of the deterministic relation and noise distribution. By using this method, we represent and infer the birth and division cell size distributions of bacteria and fission yeast. Known size control mechanisms, such as the adder model, are revealed as the conditional dependence of the size distributions on history. Further, we show that the inferred NN model provides a better data representation for model searching than conventional descriptive statistics. Thus, the NN method can work as a powerful tool for processing noisy data to uncover hidden dynamic laws.},
	number = {3},
	urldate = {2021-09-20},
	journal = {Physical Review Research},
	author = {Kamimura, Atsushi and Kobayashi, Tetsuya J.},
	month = jul,
	year = {2021},
	note = {Publisher: American Physical Society},
	pages = {033032},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/ICS63289/Kamimura and Kobayashi - 2021 - Representation and inference of size control laws .pdf:application/pdf},
}

@article{cucchi_reservoir_nodate,
	title = {Reservoir computing with biocompatible organic electrochemical networks for brain-inspired biosignal classification},
	volume = {7},
	url = {https://www.science.org/doi/10.1126/sciadv.abh0693},
	doi = {10.1126/sciadv.abh0693},
	number = {34},
	urldate = {2021-09-20},
	journal = {Science Advances},
	author = {Cucchi, Matteo and Gruener, Christopher and Petrauskas, Lautaro and Steiner, Peter and Tseng, Hsin and Fischer, Axel and Penkovsky, Bogdan and Matthus, Christian and Birkholz, Peter and Kleemann, Hans and Leo, Karl},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eabh0693},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/GGQB4VI6/Cucchi et al. - Reservoir computing with biocompatible organic ele.pdf:application/pdf},
}

@article{henikoff_amino_1992,
	title = {Amino acid substitution matrices from protein blocks},
	volume = {89},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/89/22/10915},
	doi = {10.1073/pnas.89.22.10915},
	abstract = {Methods for alignment of protein sequences typically measure similarity by using a substitution matrix with scores for all possible exchanges of one amino acid with another. The most widely used matrices are based on the Dayhoff model of evolutionary rates. Using a different approach, we have derived substitution matrices from about 2000 blocks of aligned sequence segments characterizing more than 500 groups of related proteins. This led to marked improvements in alignments and in searches using queries from each of the groups.},
	language = {en},
	number = {22},
	urldate = {2021-09-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Henikoff, S. and Henikoff, J. G.},
	month = nov,
	year = {1992},
	pmid = {1438297},
	note = {Publisher: National Academy of Sciences
Section: Research Article},
	pages = {10915--10919},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/NKBTGDSH/Henikoff and Henikoff - 1992 - Amino acid substitution matrices from protein bloc.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/5ZW53QTS/10915.html:text/html},
}

@article{vig_multiscale_2019,
	title = {A {Multiscale} {Visualization} of {Attention} in the {Transformer} {Model}},
	url = {http://arxiv.org/abs/1906.05714},
	abstract = {The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.},
	urldate = {2021-09-17},
	journal = {arXiv:1906.05714 [cs]},
	author = {Vig, Jesse},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.05714},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	annote = {Comment: To appear in ACL 2019 (System Demonstrations). arXiv admin note: substantial text overlap with arXiv:1904.02679},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/9N3F84YD/Vig - 2019 - A Multiscale Visualization of Attention in the Tra.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/W9YRPWL8/1906.html:text/html},
}

@techreport{rao_transformer_2020,
	title = {Transformer protein language models are unsupervised structure learners},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.12.15.422761v1},
	abstract = {Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.1},
	language = {en},
	urldate = {2021-09-17},
	author = {Rao, Roshan and Meier, Joshua and Sercu, Tom and Ovchinnikov, Sergey and Rives, Alexander},
	month = dec,
	year = {2020},
	doi = {10.1101/2020.12.15.422761},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {2020.12.15.422761},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/RSTQ5NGL/Rao et al. - 2020 - Transformer protein language models are unsupervis.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/JUKKIPEC/2020.12.15.html:text/html},
}

@article{vashishth_attention_2019,
	title = {Attention {Interpretability} {Across} {NLP} {Tasks}},
	url = {http://arxiv.org/abs/1909.11218},
	abstract = {The attention layer in a neural network model provides insights into the model's reasoning behind its prediction, which are usually criticized for being opaque. Recently, seemingly contradictory viewpoints have emerged about the interpretability of attention weights (Jain \& Wallace, 2019; Vig \& Belinkov, 2019). Amid such confusion arises the need to understand attention mechanism more systematically. In this work, we attempt to fill this gap by giving a comprehensive explanation which justifies both kinds of observations (i.e., when is attention interpretable and when it is not). Through a series of experiments on diverse NLP tasks, we validate our observations and reinforce our claim of interpretability of attention through manual evaluation.},
	urldate = {2021-09-17},
	journal = {arXiv:1909.11218 [cs]},
	author = {Vashishth, Shikhar and Upadhyay, Shyam and Tomar, Gaurav Singh and Faruqui, Manaal},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.11218},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/QE49VSR6/Vashishth et al. - 2019 - Attention Interpretability Across NLP Tasks.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/PJHDPWGQ/1909.html:text/html},
}

@techreport{elnaggar_prottrans_2021,
	title = {{ProtTrans}: {Towards} {Cracking} the {Language} of {Life}’s {Code} {Through} {Self}-{Supervised} {Learning}},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{ProtTrans}},
	url = {https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores.{\textless}/p{\textgreater}{\textless}p{\textgreater}Dimensionality reduction revealed that the raw protein LM-\textit{embeddings} from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the \textit{embeddings} as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81\%-87\%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81\%) and membrane vs. water-soluble (2-state accuracy Q2=91\%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the \textit{grammar} of the \textit{language of life}. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2021-09-17},
	author = {Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and Bhowmik, Debsindhu and Rost, Burkhard},
	month = may,
	year = {2021},
	doi = {10.1101/2020.07.12.199554},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {2020.07.12.199554},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/H2M7ZE3J/Elnaggar et al. - 2021 - ProtTrans Towards Cracking the Language of Life’s.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/GFTQKJ9E/2020.07.12.199554v3.html:text/html},
}

@article{raffel_exploring_2020,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2021-09-17},
	journal = {arXiv:1910.10683 [cs, stat]},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = jul,
	year = {2020},
	note = {arXiv: 1910.10683},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Final version as published in JMLR},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/2SEW4JY2/Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/EAWHLYES/1910.html:text/html},
}

@article{li_visualbert_2019,
	title = {{VisualBERT}: {A} {Simple} and {Performant} {Baseline} for {Vision} and {Language}},
	shorttitle = {{VisualBERT}},
	url = {http://arxiv.org/abs/1908.03557},
	abstract = {We propose VisualBERT, a simple and flexible framework for modeling a broad range of vision-and-language tasks. VisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an associated input image with self-attention. We further propose two visually-grounded language model objectives for pre-training VisualBERT on image caption data. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with state-of-the-art models while being significantly simpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between verbs and image regions corresponding to their arguments.},
	urldate = {2021-09-17},
	journal = {arXiv:1908.03557 [cs]},
	author = {Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.03557},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	annote = {Comment: Work in Progress},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/TGB7IXIC/Li et al. - 2019 - VisualBERT A Simple and Performant Baseline for V.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/WMZG9IT4/1908.html:text/html},
}

@article{lan_albert_2020,
	title = {{ALBERT}: {A} {Lite} {BERT} for {Self}-supervised {Learning} of {Language} {Representations}},
	shorttitle = {{ALBERT}},
	url = {http://arxiv.org/abs/1909.11942},
	abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and {\textbackslash}squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
	urldate = {2021-09-17},
	journal = {arXiv:1909.11942 [cs]},
	author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	month = feb,
	year = {2020},
	note = {arXiv: 1909.11942},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/F26QM4UE/Lan et al. - 2020 - ALBERT A Lite BERT for Self-supervised Learning o.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/6DSQ63U6/1909.html:text/html},
}

@article{cafagna_what_2021,
	title = {What {Vision}-{Language} {Models} `{See}' when they {See} {Scenes}},
	url = {http://arxiv.org/abs/2109.07301},
	abstract = {Images can be described in terms of the objects they contain, or in terms of the types of scene or place that they instantiate. In this paper we address to what extent pretrained Vision and Language models can learn to align descriptions of both types with images. We compare 3 state-of-the-art models, VisualBERT, LXMERT and CLIP. We find that (i) V\&L models are susceptible to stylistic biases acquired during pretraining; (ii) only CLIP performs consistently well on both object- and scene-level descriptions. A follow-up ablation study shows that CLIP uses object-level information in the visual modality to align with scene-level textual descriptions.},
	urldate = {2021-09-17},
	journal = {arXiv:2109.07301 [cs]},
	author = {Cafagna, Michele and van Deemter, Kees and Gatt, Albert},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.07301},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/TGL4S6FZ/Cafagna et al. - 2021 - What Vision-Language Models `See' when they See Sc.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/IJUQBCQW/2109.html:text/html},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2021-09-17},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/VMPF7WJ3/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/MT7PR6IG/1810.html:text/html},
}

@article{lee-thorp_fnet_2021,
	title = {{FNet}: {Mixing} {Tokens} with {Fourier} {Transforms}},
	shorttitle = {{FNet}},
	url = {http://arxiv.org/abs/2105.03824},
	abstract = {We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that "mix" input tokens. These linear mixers, along with standard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text classification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97\% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80\% faster on GPUs and 70\% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the "efficient" Transformers on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.},
	urldate = {2021-09-17},
	journal = {arXiv:2105.03824 [cs]},
	author = {Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
	month = sep,
	year = {2021},
	note = {arXiv: 2105.03824},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: NB update: We were measuring speeds incorrectly. FNet trains 80\%/70\% faster than BERT on GPU/TPU (not 7x/2x as initially reported). Other speed metrics (LRA \& small models) are also updated, but relative differences are roughly the same. Context: While the Fourier layer itself is significantly faster than self-attention (see Appendix A.4), overall training speed is slowed by feed-forward layers},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/GS3HW2HH/Lee-Thorp et al. - 2021 - FNet Mixing Tokens with Fourier Transforms.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/IZW6KXDE/2105.html:text/html},
}

@article{jumper_highly_2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2021-10-04},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 7873
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computational biophysics;Machine learning;Protein structure predictions;Structural biology
Subject\_term\_id: computational-biophysics;machine-learning;protein-structure-predictions;structural-biology},
	pages = {583--589},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/LELUHTE3/Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf:application/pdf},
}

@article{tunyasuvunakool_highly_2021-1,
	title = {Highly accurate protein structure prediction for the human proteome},
	volume = {596},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03828-1},
	doi = {10.1038/s41586-021-03828-1},
	abstract = {Abstract
            
              Protein structures can provide invaluable information, both for reasoning about biological processes and for enabling interventions such as structure-based drug development or targeted mutagenesis. After decades of effort, 17\% of the total residues in human protein sequences are covered by an experimentally determined structure
              1
              . Here we markedly expand the structural coverage of the proteome by applying the state-of-the-art machine learning method, AlphaFold
              2
              , at a scale that covers almost the entire human proteome (98.5\% of human proteins). The resulting dataset covers 58\% of residues with a confident prediction, of which a subset (36\% of all residues) have very high confidence. We introduce several metrics developed by building on the AlphaFold model and use them to interpret the dataset, identifying strong multi-domain predictions as well as regions that are likely to be disordered. Finally, we provide some case studies to illustrate how high-quality predictions could be used to generate biological hypotheses. We are making our predictions freely available to the community and anticipate that routine large-scale and high-accuracy structure prediction will become an important tool that will allow new questions to be addressed from a structural perspective.},
	language = {en},
	number = {7873},
	urldate = {2021-10-04},
	journal = {Nature},
	author = {Tunyasuvunakool, Kathryn and Adler, Jonas and Wu, Zachary and Green, Tim and Zielinski, Michal and Žídek, Augustin and Bridgland, Alex and Cowie, Andrew and Meyer, Clemens and Laydon, Agata and Velankar, Sameer and Kleywegt, Gerard J. and Bateman, Alex and Evans, Richard and Pritzel, Alexander and Figurnov, Michael and Ronneberger, Olaf and Bates, Russ and Kohl, Simon A. A. and Potapenko, Anna and Ballard, Andrew J. and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Clancy, Ellen and Reiman, David and Petersen, Stig and Senior, Andrew W. and Kavukcuoglu, Koray and Birney, Ewan and Kohli, Pushmeet and Jumper, John and Hassabis, Demis},
	month = aug,
	year = {2021},
	pages = {590--596},
	file = {Tunyasuvunakool et al. - 2021 - Highly accurate protein structure prediction for t.pdf:/home/zwerg/Zotero/storage/YLLS599N/Tunyasuvunakool et al. - 2021 - Highly accurate protein structure prediction for t.pdf:application/pdf},
}

@article{johnson_billion-scale_2017,
	title = {Billion-scale similarity search with {GPUs}},
	url = {http://arxiv.org/abs/1702.08734},
	abstract = {Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy. We propose a design for k-selection that operates at up to 55\% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.},
	urldate = {2021-10-04},
	journal = {arXiv:1702.08734 [cs]},
	author = {Johnson, Jeff and Douze, Matthijs and Jégou, Hervé},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.08734},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Data Structures and Algorithms, Computer Science - Databases, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/GWPV4E82/Johnson et al. - 2017 - Billion-scale similarity search with GPUs.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/A65A7WS2/1702.html:text/html},
}

@misc{noauthor_home_nodate-2,
	title = {Home · {CannyLab}/tsne-cuda {Wiki}},
	url = {https://github.com/CannyLab/tsne-cuda},
	abstract = {GPU Accelerated t-SNE for CUDA with Python bindings - Home · CannyLab/tsne-cuda Wiki},
	language = {en},
	urldate = {2021-10-04},
	journal = {GitHub},
	file = {Snapshot:/home/zwerg/Zotero/storage/W78IBMDR/wiki.html:text/html},
}

@article{han_coming_2019,
	title = {The coming era of artificial intelligence in biological data science},
	volume = {20},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-019-3225-3},
	doi = {10.1186/s12859-019-3225-3},
	number = {22},
	urldate = {2021-10-04},
	journal = {BMC Bioinformatics},
	author = {Han, Henry and Liu, Wenbin},
	month = dec,
	year = {2019},
	pages = {712},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/P2RQDRJ7/Han and Liu - 2019 - The coming era of artificial intelligence in biolo.pdf:application/pdf},
}

@techreport{meier_language_2021,
	title = {Language models enable zero-shot prediction of the effects of mutations on protein function},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.07.09.450648v1},
	abstract = {Modeling the effect of sequence variation on function is a fundamental problem for understanding and designing proteins. Since evolution encodes information about function into patterns in protein sequences, unsupervised models of variant effects can be learned from sequence data. The approach to date has been to fit a model to a family of related sequences. The conventional setting is limited, since a new model must be trained for each prediction task. We show that using only zero-shot inference, without any supervision from experimental data or additional training, protein language models capture the functional effects of sequence variation, performing at state-of-the-art.},
	language = {en},
	urldate = {2021-10-04},
	author = {Meier, Joshua and Rao, Roshan and Verkuil, Robert and Liu, Jason and Sercu, Tom and Rives, Alexander},
	month = jul,
	year = {2021},
	doi = {10.1101/2021.07.09.450648},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {2021.07.09.450648},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/G7NHLJ8C/Meier et al. - 2021 - Language models enable zero-shot prediction of the.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/2EVBLYSU/2021.07.09.450648v1.html:text/html},
}

@techreport{rao_msa_2021,
	title = {{MSA} {Transformer}},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.02.12.430858v1},
	abstract = {Unsupervised protein language models trained across millions of diverse sequences learn structure and function of proteins. Protein language models studied to date have been trained to perform inference from individual sequences. The longstanding approach in computational biology has been to make inferences from a family of evolutionarily related sequences by fitting a model to each family independently. In this work we combine the two paradigms. We introduce a protein language model which takes as input a set of sequences in the form of a multiple sequence alignment. The model interleaves row and column attention across the input sequences and is trained with a variant of the masked language modeling objective across many protein families. The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models.},
	language = {en},
	urldate = {2021-10-04},
	author = {Rao, Roshan and Liu, Jason and Verkuil, Robert and Meier, Joshua and Canny, John F. and Abbeel, Pieter and Sercu, Tom and Rives, Alexander},
	month = feb,
	year = {2021},
	doi = {10.1101/2021.02.12.430858},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {2021.02.12.430858},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/LL4HPJJF/Rao et al. - 2021 - MSA Transformer.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/EAH9JTK3/2021.02.12.html:text/html},
}

@techreport{rao_transformer_2020-1,
	title = {Transformer protein language models are unsupervised structure learners},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.12.15.422761v1},
	abstract = {Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.1},
	language = {en},
	urldate = {2021-10-04},
	author = {Rao, Roshan and Meier, Joshua and Sercu, Tom and Ovchinnikov, Sergey and Rives, Alexander},
	month = dec,
	year = {2020},
	doi = {10.1101/2020.12.15.422761},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {2020.12.15.422761},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/TLEN9RKP/Rao et al. - 2020 - Transformer protein language models are unsupervis.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/YN9I2QL9/2020.12.15.html:text/html},
}

@techreport{bhattacharya_single_2020,
	title = {Single {Layers} of {Attention} {Suffice} to {Predict} {Protein} {Contacts}},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.12.21.423882v2},
	abstract = {The established approach to unsupervised protein contact prediction estimates co-evolving positions using undirected graphical models. This approach trains a Potts model on a Multiple Sequence Alignment, then predicts that the edges with highest weight correspond to contacts in the 3D structure. On the other hand, increasingly large Transformers are being pretrained on protein sequence databases but have demonstrated mixed results for downstream tasks, including contact prediction. This has sparked discussion about the role of scale and attention-based models in unsupervised protein representation learning. We argue that attention is a principled model of protein interactions, grounded in real properties of protein family data. We introduce a simplified attention layer, factored attention, and show that it achieves comparable performance to Potts models, while sharing parameters both within and across families. Further, we extract contacts from the attention maps of a pretrained Transformer and show they perform competitively with the other two approaches. This provides evidence that large-scale pretraining can learn meaningful protein features when presented with unlabeled and unaligned data. We contrast factored attention with the Transformer to indicate that the Transformer leverages hierarchical signal in protein family databases not captured by our single-layer models. This raises the exciting possibility for the development of powerful structured models of protein family databases.1},
	language = {en},
	urldate = {2021-10-04},
	author = {Bhattacharya, Nicholas and Thomas, Neil and Rao, Roshan and Dauparas, Justas and Koo, Peter K. and Baker, David and Song, Yun S. and Ovchinnikov, Sergey},
	month = dec,
	year = {2020},
	doi = {10.1101/2020.12.21.423882},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {2020.12.21.423882},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/RP2F3PZN/Bhattacharya et al. - 2020 - Single Layers of Attention Suffice to Predict Prot.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/I86YA57H/2020.12.21.html:text/html},
}

@article{rahmatbakhsh_bioinformatic_2021,
	title = {Bioinformatic {Analysis} of {Temporal} and {Spatial} {Proteome} {Alternations} {During} {Infections}},
	volume = {12},
	issn = {1664-8021},
	url = {https://www.frontiersin.org/article/10.3389/fgene.2021.667936},
	doi = {10.3389/fgene.2021.667936},
	abstract = {Microbial pathogens have evolved numerous mechanisms to hijack host’s systems, thus causing disease. This is mediated by alterations in the combined host-pathogen proteome in time and space. Mass spectrometry-based proteomics approaches have been developed and tailored to map disease progression. The result is complex multidimensional data that pose numerous analytic challenges for downstream interpretation. However, a systematic review of approaches for the downstream analysis of such data has been lacking in the field. In this review, we detail the steps of a typical temporal and spatial analysis, including data pre-processing steps (i.e., quality control, data normalization, the imputation of missing values, and dimensionality reduction), different statistical and machine learning approaches, validation, interpretation, and the extraction of biological information from mass spectrometry data. We also discuss current best practices for these steps based on a collection of independent studies to guide users in selecting the most suitable strategies for their dataset and analysis objectives. Moreover, we also compiled the list of commonly used R software packages for each step of the analysis. These could be easily integrated into one’s analysis pipeline. Furthermore, we guide readers through various analysis steps by applying these workflows to mock and host-pathogen interaction data from public datasets. The workflows presented in this review will serve as an introduction for data analysis novices, while also helping established users update their data analysis pipelines. We conclude the review by discussing future directions and developments in temporal and spatial proteomics and data analysis approaches. Data analysis codes, prepared for this review are available from https://github.com/BabuLab-UofR/TempSpac, where guidelines and sample datasets are also offered for testing purposes.},
	urldate = {2021-10-05},
	journal = {Frontiers in Genetics},
	author = {Rahmatbakhsh, Matineh and Gagarinova, Alla and Babu, Mohan},
	year = {2021},
	pages = {1155},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/KJNKYX32/Rahmatbakhsh et al. - 2021 - Bioinformatic Analysis of Temporal and Spatial Pro.pdf:application/pdf},
}

@patent{karlsson_method_2004,
	title = {A method in microscopy and a microscope, where subimages are recorded and puzzled in the same coordinate system to enable a precise positioning of the microscope stage},
	url = {https://patents.google.com/patent/EP1377865A1/en?oq=EP+1377865AI},
	nationality = {EP},
	language = {en},
	assignee = {Cellavision AB},
	number = {EP1377865A1},
	urldate = {2021-10-05},
	author = {Karlsson, Adam and Linderup, Erik},
	month = jan,
	year = {2004},
	keywords = {image, images, microscope, recorded, signal},
	annote = {Classifications
G06K9/00134: Acquisition, e.g. centering the image field
G02B21/26: Stages; Adjusting means therefor
G02B21/367: Control or image processing arrangements for digital or video microscopes providing an output produced by processing a plurality of individual source images, e.g. image tiling, montage, composite images, depth sectioning, image comparison
G06T7/33: Determination of transform parameters for the alignment of images, i.e. image registration using feature-based methods
G06K2009/2045: Image acquisition using multiple overlapping images
G06T2207/10016: Video; Image sequence
G06T2207/10056: Microscopic image},
}

@techreport{gomez-de-mariscal_deepimagej_2021,
	title = {{DeepImageJ}: {A} user-friendly environment to run deep learning models in {ImageJ}},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{DeepImageJ}},
	url = {https://www.biorxiv.org/content/10.1101/799270v3},
	abstract = {DeepImageJ is a user-friendly solution that enables the generic use of pre-trained deep learn ing (DL) models for biomedical image analysis in ImageJ. The deepImageJ environment gives access to the largest bioimage repository of pre-trained DL models (BioImage Model Zoo). Hence, non-experts can easily perform common image processing tasks in life-science research with DL-based tools including pixel and object classification, instance segmentation, denoising or virtual staining. DeepImageJ is compatible with existing state-of-the-art solutions and it is equipped with utility tools for developers to include new models. Very recently, several train ing frameworks have adopted the deepImageJ format to deploy their work in one of the most used software in the field (ImageJ). Beyond its direct use, we expect deepImageJ to contribute to the broader dissemination and reuse of DL models in life-sciences applications and bioimage informatics.},
	language = {en},
	urldate = {2021-10-05},
	author = {Gómez-de-Mariscal, Estibaliz and García-López-de-Haro, Carlos and Ouyang, Wei and Donati, Laurène and Lundberg, Emma and Unser, Michael and Muñoz-Barrutia, Arrate and Sage, Daniel},
	month = may,
	year = {2021},
	doi = {10.1101/799270},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {799270},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/76FKNCDV/Gómez-de-Mariscal et al. - 2021 - DeepImageJ A user-friendly environment to run dee.pdf:application/pdf},
}

@article{chen_bioinformatics_2020,
	title = {Bioinformatics {Methods} for {Mass} {Spectrometry}-{Based} {Proteomics} {Data} {Analysis}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1422-0067/21/8/2873},
	doi = {10.3390/ijms21082873},
	abstract = {Recent advances in mass spectrometry (MS)-based proteomics have enabled tremendous progress in the understanding of cellular mechanisms, disease progression, and the relationship between genotype and phenotype. Though many popular bioinformatics methods in proteomics are derived from other omics studies, novel analysis strategies are required to deal with the unique characteristics of proteomics data. In this review, we discuss the current developments in the bioinformatics methods used in proteomics and how they facilitate the mechanistic understanding of biological processes. We first introduce bioinformatics software and tools designed for mass spectrometry-based protein identification and quantification, and then we review the different statistical and machine learning methods that have been developed to perform comprehensive analysis in proteomics studies. We conclude with a discussion of how quantitative protein data can be used to reconstruct protein interactions and signaling networks.},
	language = {en},
	number = {8},
	urldate = {2021-10-05},
	journal = {International Journal of Molecular Sciences},
	author = {Chen, Chen and Hou, Jie and Tanner, John J. and Cheng, Jianlin},
	month = jan,
	year = {2020},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {machine learning, bioinformatics analysis, computational proteomics},
	pages = {2873},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/U3VM6SKW/Chen et al. - 2020 - Bioinformatics Methods for Mass Spectrometry-Based.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/7F4WV5UH/htm.html:text/html},
}

@article{otasek_cytoscape_2019,
	title = {Cytoscape {Automation}: empowering workflow-based network analysis},
	volume = {20},
	issn = {1474-760X},
	shorttitle = {Cytoscape {Automation}},
	url = {https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1758-4},
	doi = {10.1186/s13059-019-1758-4},
	language = {en},
	number = {1},
	urldate = {2021-10-06},
	journal = {Genome Biology},
	author = {Otasek, David and Morris, John H. and Bouças, Jorge and Pico, Alexander R. and Demchak, Barry},
	month = dec,
	year = {2019},
	pages = {185},
	file = {Full Text:/home/zwerg/Zotero/storage/WKGZM9VT/Otasek et al. - 2019 - Cytoscape Automation empowering workflow-based ne.pdf:application/pdf},
}

@techreport{griss_reactomegsa_2020,
	title = {{ReactomeGSA} - {Efficient} {Multi}-{Omics} {Comparative} {Pathway} {Analysis}},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.04.16.044958v1},
	abstract = {Pathway analyses are key methods to analyse ‘omics experiments. Nevertheless, integrating data from different ‘omics technologies and different species still requires considerable bioinformatics knowledge.
Here we present the novel ReactomeGSA resource for comparative pathway analyses of multi-omics datasets. ReactomeGSA can be used through Reactome’s existing web interface and the novel ReactomeGSA R Bioconductor package with explicit support for scRNA-seq data. Data from different species is automatically mapped to a common pathway space. Public data from ExpressionAtlas and Single Cell ExpressionAtlas can be directly integrated in the analysis. ReactomeGSA thereby greatly reduces the technical barrier for multi-omics, cross-species, comparative pathway analyses.
We used ReactomeGSA to characterise the role of B cells in anti-tumour immunity. We compared B cell rich and poor human cancer samples from five TCGA transcriptomics and two CPTAC proteomics studies. There, B cell-rich lung adenocarcinoma samples lack the otherwise present activation through NFkappaB. This may be linked to the presence of a specific subset of tumour associated IgG+ plasma cells that lack NFkappaB activation in scRNA-seq data from human melanoma. This showcases how ReactomeGSA can derive novel biomedical insights by integrating large multi-omics datasets.},
	language = {en},
	urldate = {2021-10-06},
	author = {Griss, Johannes and Viteri, Guilherme and Sidiropoulos, Konstantinos and Nguyen, Vy and Fabregat, Antonio and Hermjakob, Henning},
	month = apr,
	year = {2020},
	doi = {10.1101/2020.04.16.044958},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {2020.04.16.044958},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/GE7YX7VT/Griss et al. - 2020 - ReactomeGSA - Efficient Multi-Omics Comparative Pa.pdf:application/pdf},
}

@misc{griss_reactomegsa_2021,
	title = {{ReactomeGSA}: {Client} for the {Reactome} {Analysis} {Service} for comparative multi-omics gene set analysis},
	copyright = {MIT + file LICENSE},
	shorttitle = {{ReactomeGSA}},
	url = {https://bioconductor.org/packages/ReactomeGSA/},
	abstract = {The ReactomeGSA packages uses Reactome's online analysis service to perform a multi-omics gene set analysis. The main advantage of this package is, that the retrieved results can be visualized using REACTOME's powerful webapplication. Since Reactome's analysis service also uses R to perfrom the actual gene set analysis you will get similar results when using the same packages (such as limma and edgeR) locally. Therefore, if you only require a gene set analysis, different packages are more suited.},
	urldate = {2021-10-06},
	publisher = {Bioconductor version: Release (3.13)},
	author = {Griss, Johannes},
	year = {2021},
	doi = {10.18129/B9.bioc.ReactomeGSA},
	keywords = {Transcriptomics, Software, GeneExpression, GeneSetEnrichment, Proteomics, Reactome, SystemsBiology},
}

@techreport{gjerga_converting_2020,
	type = {preprint},
	title = {Converting networks to predictive logic models from perturbation signalling data with {CellNOpt}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2020.03.04.976852},
	abstract = {Abstract
          
            Motivation
            The molecular changes induced by perturbations such as drugs and ligands are highly informative of the intracellular wiring. Our capacity to generate large data-sets is increasing steadily as new experimental approaches are developed. A useful way to extract mechanistic insight from the data is by integrating them with a prior knowledge network of signalling to obtain dynamic models. Logic models scale better with network size than alternative kinetic models, while keeping the interpretation of the model simple, making them particularly suitable for large datasets.
          
          
            Results
            
              CellNOpt is a collection of Bioconductor R packages for building logic models from perturbation data and prior knowledge of signalling networks. We have recently developed new components and refined the existing ones. These updates include (i) an Integer Linear Programming (ILP) formulation which guarantees efficient optimisation for Boolean models, (ii) a probabilistic logic implementation for semi-quantitative datasets and (iii) the integration of MaBoSS, a stochastic Boolean simulator. Furthermore, we introduce Dynamic-Feeder, a tool to identify missing links not present in the prior knowledge. We have also implemented systematic
              post-hoc
              analyses to highlight the key components and parameters of our models. Finally, we provide an R-Shiny tool to run CellNOpt interactively.
            
          
          
            Availability
            
              R-package(s):
              https://github.com/saezlab/cellnopt
            
          
          
            Contact
            
              julio.saez@bioquant.uni-heidelberg.de
            
          
          
            Supplementary information
            Supplemental Text.},
	language = {en},
	urldate = {2021-10-06},
	institution = {Bioinformatics},
	author = {Gjerga, Enio and Trairatphisan, Panuwat and Gabor, Attila and Koch, Hermann and Chevalier, Celine and Ceccarelli, Francesco and Dugourd, Aurelien and Mitsos, Alexander and Saez-Rodriguez, Julio},
	month = mar,
	year = {2020},
	doi = {10.1101/2020.03.04.976852},
	file = {Full Text:/home/zwerg/Zotero/storage/K8MQA26P/Gjerga et al. - 2020 - Converting networks to predictive logic models fro.pdf:application/pdf},
}

@techreport{gjerga_converting_2020-1,
	title = {Converting networks to predictive logic models from perturbation signalling data with {CellNOpt}},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.03.04.976852v1},
	abstract = {Motivation The molecular changes induced by perturbations such as drugs and ligands are highly informative of the intracellular wiring. Our capacity to generate large data-sets is increasing steadily as new experimental approaches are developed. A useful way to extract mechanistic insight from the data is by integrating them with a prior knowledge network of signalling to obtain dynamic models. Logic models scale better with network size than alternative kinetic models, while keeping the interpretation of the model simple, making them particularly suitable for large datasets.
Results CellNOpt is a collection of Bioconductor R packages for building logic models from perturbation data and prior knowledge of signalling networks. We have recently developed new components and refined the existing ones. These updates include (i) an Integer Linear Programming (ILP) formulation which guarantees efficient optimisation for Boolean models, (ii) a probabilistic logic implementation for semi-quantitative datasets and (iii) the integration of MaBoSS, a stochastic Boolean simulator. Furthermore, we introduce Dynamic-Feeder, a tool to identify missing links not present in the prior knowledge. We have also implemented systematic post-hoc analyses to highlight the key components and parameters of our models. Finally, we provide an R-Shiny tool to run CellNOpt interactively.
Availability R-package(s): https://github.com/saezlab/cellnopt
Contact julio.saez\{at\}bioquant.uni-heidelberg.de
Supplementary information Supplemental Text.},
	language = {en},
	urldate = {2021-10-06},
	author = {Gjerga, Enio and Trairatphisan, Panuwat and Gabor, Attila and Koch, Hermann and Chevalier, Celine and Ceccarelli, Francesco and Dugourd, Aurelien and Mitsos, Alexander and Saez-Rodriguez, Julio},
	month = mar,
	year = {2020},
	doi = {10.1101/2020.03.04.976852},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {2020.03.04.976852},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/5P7Z3AP9/Gjerga et al. - 2020 - Converting networks to predictive logic models fro.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/LX78VVDN/2020.03.04.html:text/html},
}

@article{terfve_cellnoptr_2012,
	title = {{CellNOptR}: a flexible toolkit to train protein signaling networks to data using multiple logic formalisms},
	volume = {6},
	issn = {1752-0509},
	shorttitle = {{CellNOptR}},
	url = {https://doi.org/10.1186/1752-0509-6-133},
	doi = {10.1186/1752-0509-6-133},
	abstract = {Cells process signals using complex and dynamic networks. Studying how this is performed in a context and cell type specific way is essential to understand signaling both in physiological and diseased situations. Context-specific medium/high throughput proteomic data measured upon perturbation is now relatively easy to obtain but formalisms that can take advantage of these features to build models of signaling are still comparatively scarce.},
	number = {1},
	urldate = {2021-10-06},
	journal = {BMC Systems Biology},
	author = {Terfve, Camille and Cokelaer, Thomas and Henriques, David and MacNamara, Aidan and Goncalves, Emanuel and Morris, Melody K. and Iersel, Martijn van and Lauffenburger, Douglas A. and Saez-Rodriguez, Julio},
	month = oct,
	year = {2012},
	keywords = {Logic modeling, Perturbation data, Phosphoproteomics, Signaling networks, Systems biology},
	pages = {133},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/DAKNYQ3T/Terfve et al. - 2012 - CellNOptR a flexible toolkit to train protein sig.pdf:application/pdf},
}

@misc{noauthor_avoiding_nodate,
	title = {Avoiding a replication crisis in deep-learning-based bioimage analysis {\textbar} {Nature} {Methods}},
	url = {https://www.nature.com/articles/s41592-021-01284-3},
	urldate = {2021-10-06},
	file = {Avoiding a replication crisis in deep-learning-based bioimage analysis | Nature Methods:/home/zwerg/Zotero/storage/QP3RG8UR/s41592-021-01284-3.html:text/html;Full Text:/home/zwerg/Zotero/storage/G78XBMAR/Avoiding a replication crisis in deep-learning-bas.pdf:application/pdf},
}

@article{yung_solid_2017,
	title = {Solid {Sampling} with a {Diode} {Laser} for {Portable} {Ambient} {Mass} {Spectrometry}},
	volume = {89},
	issn = {0003-2700},
	url = {https://doi.org/10.1021/acs.analchem.7b01745},
	doi = {10.1021/acs.analchem.7b01745},
	abstract = {A hand-held diode laser is implemented for solid sampling in portable ambient mass spectrometry (MS). Specifically, a pseudocontinuous wave battery-powered surgical laser diode is employed for portable laser diode thermal desorption (LDTD) at 940 nm and compared with nanosecond pulsed laser ablation at 2940 nm. Postionization is achieved in both cases using atmospheric pressure photoionization (APPI). The laser ablation atmospheric pressure photoionization (LAAPPI) and LDTD-APPI mass spectra of sage leaves (Salvia officinalis) using a field-deployable quadrupole ion trap MS display many similar ion peaks, as do the mass spectra of membrane grown biofilms of Pseudomonas aeruginosa. These results indicate that LDTD-APPI method should be useful for in-field sampling of plant and microbial communities, for example, by portable ambient MS. The feasibility of many portable MS applications is facilitated by the availability of relatively low cost, portable, battery-powered diode lasers. LDTD could also be coupled with plasma- or electrospray-based ionization for the analysis of a variety of solid samples.},
	number = {14},
	urldate = {2021-10-06},
	journal = {Analytical Chemistry},
	author = {Yung, Yeni P. and Wickramasinghe, Raveendra and Vaikkinen, Anu and Kauppila, Tiina J. and Veryovkin, Igor V. and Hanley, Luke},
	month = jul,
	year = {2017},
	note = {Publisher: American Chemical Society},
	pages = {7297--7301},
	file = {ACS Full Text Snapshot:/home/zwerg/Zotero/storage/UR8MLIJI/acs.analchem.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/KNLD7GZR/Yung et al. - 2017 - Solid Sampling with a Diode Laser for Portable Amb.pdf:application/pdf},
}

@misc{noauthor_pride_nodate,
	title = {{PRIDE} - {Proteomics} {Identification} {Database}},
	url = {https://www.ebi.ac.uk/pride/archive},
	urldate = {2021-10-06},
	file = {PRIDE - Proteomics Identification Database:/home/zwerg/Zotero/storage/NFUW3LLN/archive.html:text/html},
}

@article{furlan_miniaturised_2019,
	title = {Miniaturised interaction proteomics on a microfluidic platform with ultra-low input requirements},
	volume = {10},
	doi = {10.1038/s41467-019-09533-y},
	abstract = {Essentially all cellular processes are orchestrated by protein-protein interactions (PPIs). In recent years, affinity purification coupled to mass spectrometry (AP-MS) has been the preferred method to identify cellular PPIs. Here we present a microfluidic-based AP-MS workflow, called on-chip AP-MS, to identify PPIs using minute amounts of input material. By using this automated platform we purify the human Cohesin, CCC and Mediator complexes from as little as 4 micrograms of input lysate, representing a 50─100-fold downscaling compared to regular microcentrifuge tube-based protocols. We show that our platform can be used to affinity purify tagged baits as well as native cellular proteins and their interaction partners. As such, our method holds great promise for future biological and clinical AP-MS applications in which sample amounts are limited.},
	journal = {Nature Communications},
	author = {Furlan, Cristina and Dirks, René and Thomas, Peter and Jones, Robert and Wang, Jing and Lynch, Mark and Marks, Hendrik and Vermeulen, Michiel},
	month = apr,
	year = {2019},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/4IE5Q6CM/Furlan et al. - 2019 - Miniaturised interaction proteomics on a microflui.pdf:application/pdf},
}

@article{borner_organellar_2020,
	title = {Organellar {Maps} {Through} {Proteomic} {Profiling} - {A} {Conceptual} {Guide}},
	volume = {19},
	doi = {10.1074/mcp.R120.001971},
	abstract = {Protein subcellular localization is an essential and highly regulated determinant of protein function. Major advances in mass spectrometry and imaging have allowed the development of powerful spatial proteomics approaches for determining protein localization at the whole cell scale. Here, a brief overview of current methods is presented, followed by a detailed discussion of organellar mapping through proteomic profiling. This relatively simple yet flexible approach is rapidly gaining popularity, due to its ability to capture the localizations of thousands of proteins in a single experiment. It can be used to generate high-resolution cell maps, and as a tool for monitoring protein localization dynamics. This review highlights the strengths and limitations of the approach, and provides guidance to designing and interpreting profiling experiments.},
	journal = {Molecular \& Cellular Proteomics},
	author = {Borner, Georg},
	month = apr,
	year = {2020},
	pages = {mcp.R120.001971},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/Z4J3S5SJ/Borner - 2020 - Organellar Maps Through Proteomic Profiling - A Co.pdf:application/pdf},
}

@article{rahmatbakhsh_bioinformatic_2021-1,
	title = {Bioinformatic {Analysis} of {Temporal} and {Spatial} {Proteome} {Alternations} {During} {Infections}},
	volume = {12},
	issn = {1664-8021},
	url = {https://www.frontiersin.org/article/10.3389/fgene.2021.667936},
	doi = {10.3389/fgene.2021.667936},
	abstract = {Microbial pathogens have evolved numerous mechanisms to hijack host’s systems, thus causing disease. This is mediated by alterations in the combined host-pathogen proteome in time and space. Mass spectrometry-based proteomics approaches have been developed and tailored to map disease progression. The result is complex multidimensional data that pose numerous analytic challenges for downstream interpretation. However, a systematic review of approaches for the downstream analysis of such data has been lacking in the field. In this review, we detail the steps of a typical temporal and spatial analysis, including data pre-processing steps (i.e., quality control, data normalization, the imputation of missing values, and dimensionality reduction), different statistical and machine learning approaches, validation, interpretation, and the extraction of biological information from mass spectrometry data. We also discuss current best practices for these steps based on a collection of independent studies to guide users in selecting the most suitable strategies for their dataset and analysis objectives. Moreover, we also compiled the list of commonly used R software packages for each step of the analysis. These could be easily integrated into one’s analysis pipeline. Furthermore, we guide readers through various analysis steps by applying these workflows to mock and host-pathogen interaction data from public datasets. The workflows presented in this review will serve as an introduction for data analysis novices, while also helping established users update their data analysis pipelines. We conclude the review by discussing future directions and developments in temporal and spatial proteomics and data analysis approaches. Data analysis codes, prepared for this review are available from https://github.com/BabuLab-UofR/TempSpac, where guidelines and sample datasets are also offered for testing purposes.},
	urldate = {2021-10-06},
	journal = {Frontiers in Genetics},
	author = {Rahmatbakhsh, Matineh and Gagarinova, Alla and Babu, Mohan},
	year = {2021},
	pages = {1155},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/Z2JKMJ5X/Rahmatbakhsh et al. - 2021 - Bioinformatic Analysis of Temporal and Spatial Pro.pdf:application/pdf},
}

@article{peterson_optimized_2021,
	title = {Optimized protocol for the identification of lipid droplet proteomes using proximity labeling proteomics in cultured human cells},
	volume = {2},
	doi = {10.1016/j.xpro.2021.100579},
	abstract = {Lipid droplets are endoplasmic reticulum-derived neutral lipid storage organelles that play critical roles in cellular lipid and energy homeostasis. Here, we present a protocol for the identification of high-confidence lipid droplet proteomes in a cell culture model. This approach overcomes limitations associated with standard biochemical fractionation techniques, employing an engineered ascorbate peroxidase (APEX2) to biotinylate endogenous lipid droplet proteins in living cells for subsequent purification and identification by proteomics.
For complete details on the use and execution of this protocol, please refer to Bersuker et al. (2018).},
	journal = {STAR Protocols},
	author = {Peterson, Clark and Deol, Kirandeep and To, Milton and Olzmann, James},
	month = jun,
	year = {2021},
	pages = {100579},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/YGMGMRX2/Peterson et al. - 2021 - Optimized protocol for the identification of lipid.pdf:application/pdf},
}

@techreport{anishchenko_novo_2020,
	title = {De novo protein design by deep network hallucination},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.07.22.211482v1},
	abstract = {There has been considerable recent progress in protein structure prediction using deep neural networks to infer distance constraints from amino acid residue co-evolution1–3. We investigated whether the information captured by such networks is sufficiently rich to generate new folded proteins with sequences unrelated to those of the naturally occuring proteins used in training the models. We generated random amino acid sequences, and input them into the trRosetta structure prediction network to predict starting distance maps, which as expected are quite featureless. We then carried out Monte Carlo sampling in amino acid sequence space, optimizing the contrast (KL-divergence) between the distance distributions predicted by the network and the background distribution. Optimization from different random starting points resulted in a wide range of proteins with diverse sequences and all alpha, all beta sheet, and mixed alpha-beta structures. We obtained synthetic genes encoding 129 of these network hallucinated sequences, expressed and purified the proteins in E coli, and found that 27 folded to monomeric stable structures with circular dichroism spectra consistent with the hallucinated structures. Thus deep networks trained to predict native protein structures from their sequences can be inverted to design new proteins, and such networks and methods should contribute, alongside traditional physically based models, to the de novo design of proteins with new functions.},
	language = {en},
	urldate = {2021-11-11},
	author = {Anishchenko, Ivan and Chidyausiku, Tamuka M. and Ovchinnikov, Sergey and Pellock, Samuel J. and Baker, David},
	month = jul,
	year = {2020},
	doi = {10.1101/2020.07.22.211482},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {2020.07.22.211482},
	file = {Full Text:/home/zwerg/Zotero/storage/789765CM/Anishchenko et al. - 2020 - De novo protein design by deep network hallucinati.pdf:application/pdf},
}

@article{pakhrin_deep_2021,
	title = {Deep {Learning}-{Based} {Advances} in {Protein} {Structure} {Prediction}},
	volume = {22},
	issn = {1422-0067},
	doi = {10.3390/ijms22115553},
	abstract = {Obtaining an accurate description of protein structure is a fundamental step toward understanding the underpinning of biology. Although recent advances in experimental approaches have greatly enhanced our capabilities to experimentally determine protein structures, the gap between the number of protein sequences and known protein structures is ever increasing. Computational protein structure prediction is one of the ways to fill this gap. Recently, the protein structure prediction field has witnessed a lot of advances due to Deep Learning (DL)-based approaches as evidenced by the success of AlphaFold2 in the most recent Critical Assessment of protein Structure Prediction (CASP14). In this article, we highlight important milestones and progresses in the field of protein structure prediction due to DL-based methods as observed in CASP experiments. We describe advances in various steps of protein structure prediction pipeline viz. protein contact map prediction, protein distogram prediction, protein real-valued distance prediction, and Quality Assessment/refinement. We also highlight some end-to-end DL-based approaches for protein structure prediction approaches. Additionally, as there have been some recent DL-based advances in protein structure determination using Cryo-Electron (Cryo-EM) microscopy based, we also highlight some of the important progress in the field. Finally, we provide an outlook and possible future research directions for DL-based approaches in the protein structure prediction arena.},
	language = {eng},
	number = {11},
	journal = {International Journal of Molecular Sciences},
	author = {Pakhrin, Subash C. and Shrestha, Bikash and Adhikari, Badri and Kc, Dukka B.},
	month = may,
	year = {2021},
	pmid = {34074028},
	pmcid = {PMC8197379},
	keywords = {deep learning, Deep Learning, Algorithms, Amino Acid Sequence, Models, Molecular, Proteins, Software, Computational Biology, Cryoelectron Microscopy, Databases, Protein, Neural Networks, Computer, Protein Conformation, protein contact map prediction, protein distance prediction, protein quality assessment, protein structure prediction, Sequence Analysis, Protein},
	pages = {5553},
	file = {Full Text:/home/zwerg/Zotero/storage/TE4AARN3/Pakhrin et al. - 2021 - Deep Learning-Based Advances in Protein Structure .pdf:application/pdf},
}

@article{baek_accurate_2021,
	title = {Accurate prediction of protein structures and interactions using a three-track neural network},
	volume = {373},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.abj8754},
	doi = {10.1126/science.abj8754},
	language = {en},
	number = {6557},
	urldate = {2021-11-11},
	journal = {Science},
	author = {Baek, Minkyung and DiMaio, Frank and Anishchenko, Ivan and Dauparas, Justas and Ovchinnikov, Sergey and Lee, Gyu Rie and Wang, Jue and Cong, Qian and Kinch, Lisa N. and Schaeffer, R. Dustin and Millán, Claudia and Park, Hahnbeom and Adams, Carson and Glassman, Caleb R. and DeGiovanni, Andy and Pereira, Jose H. and Rodrigues, Andria V. and van Dijk, Alberdina A. and Ebrecht, Ana C. and Opperman, Diederik J. and Sagmeister, Theo and Buhlheller, Christoph and Pavkov-Keller, Tea and Rathinaswamy, Manoj K. and Dalwadi, Udit and Yip, Calvin K. and Burke, John E. and Garcia, K. Christopher and Grishin, Nick V. and Adams, Paul D. and Read, Randy J. and Baker, David},
	month = aug,
	year = {2021},
	pages = {871--876},
	file = {Baek_etal_Science2021_RoseTTAFold.pdf:/home/zwerg/Zotero/storage/GXIM2R4U/Baek_etal_Science2021_RoseTTAFold.pdf:application/pdf},
}

@article{yang_improved_2020,
	title = {Improved protein structure prediction using predicted interresidue orientations},
	volume = {117},
	copyright = {© 2020 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/117/3/1496},
	doi = {10.1073/pnas.1914677117},
	abstract = {The prediction of interresidue contacts and distances from coevolutionary data using deep learning has considerably advanced protein structure prediction. Here, we build on these advances by developing a deep residual network for predicting interresidue orientations, in addition to distances, and a Rosetta-constrained energy-minimization protocol for rapidly and accurately generating structure models guided by these restraints. In benchmark tests on 13th Community-Wide Experiment on the Critical Assessment of Techniques for Protein Structure Prediction (CASP13)- and Continuous Automated Model Evaluation (CAMEO)-derived sets, the method outperforms all previously described structure-prediction methods. Although trained entirely on native proteins, the network consistently assigns higher probability to de novo-designed proteins, identifying the key fold-determining residues and providing an independent quantitative measure of the “ideality” of a protein structure. The method promises to be useful for a broad range of protein structure prediction and design problems.},
	language = {en},
	number = {3},
	urldate = {2021-11-11},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yang, Jianyi and Anishchenko, Ivan and Park, Hahnbeom and Peng, Zhenling and Ovchinnikov, Sergey and Baker, David},
	month = jan,
	year = {2020},
	pmid = {31896580},
	note = {Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {deep learning, protein structure prediction, protein contact prediction},
	pages = {1496--1503},
}

@article{norn_protein_2021,
	title = {Protein sequence design by conformational landscape optimization},
	volume = {118},
	copyright = {Copyright © 2021 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/118/11/e2017228118},
	doi = {10.1073/pnas.2017228118},
	abstract = {The protein design problem is to identify an amino acid sequence that folds to a desired structure. Given Anfinsen’s thermodynamic hypothesis of folding, this can be recast as finding an amino acid sequence for which the desired structure is the lowest energy state. As this calculation involves not only all possible amino acid sequences but also, all possible structures, most current approaches focus instead on the more tractable problem of finding the lowest-energy amino acid sequence for the desired structure, often checking by protein structure prediction in a second step that the desired structure is indeed the lowest-energy conformation for the designed sequence, and typically discarding a large fraction of designed sequences for which this is not the case. Here, we show that by backpropagating gradients through the transform-restrained Rosetta (trRosetta) structure prediction network from the desired structure to the input amino acid sequence, we can directly optimize over all possible amino acid sequences and all possible structures in a single calculation. We find that trRosetta calculations, which consider the full conformational landscape, can be more effective than Rosetta single-point energy estimations in predicting folding and stability of de novo designed proteins. We compare sequence design by conformational landscape optimization with the standard energy-based sequence design methodology in Rosetta and show that the former can result in energy landscapes with fewer alternative energy minima. We show further that more funneled energy landscapes can be designed by combining the strengths of the two approaches: the low-resolution trRosetta model serves to disfavor alternative states, and the high-resolution Rosetta model serves to create a deep energy minimum at the design target structure.},
	language = {en},
	number = {11},
	urldate = {2021-11-11},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Norn, Christoffer and Wicky, Basile I. M. and Juergens, David and Liu, Sirui and Kim, David and Tischer, Doug and Koepnick, Brian and Anishchenko, Ivan and Players, Foldit and Baker, David and Ovchinnikov, Sergey},
	month = mar,
	year = {2021},
	pmid = {33712545},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	keywords = {machine learning, energy landscape, protein design, sequence optimization, stability prediction},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/T5DQTQQA/Norn et al. - 2021 - Protein sequence design by conformational landscap.pdf:application/pdf},
}

@article{gupta_embodied_2021,
	title = {Embodied intelligence via learning and evolution},
	volume = {12},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-25874-z},
	doi = {10.1038/s41467-021-25874-z},
	abstract = {Abstract
            The intertwined processes of learning and evolution in complex environmental niches have resulted in a remarkable diversity of morphological forms. Moreover, many aspects of animal intelligence are deeply embodied in these evolved morphologies. However, the principles governing relations between environmental complexity, evolved morphology, and the learnability of intelligent control, remain elusive, because performing large-scale in silico experiments on evolution and learning is challenging. Here, we introduce Deep Evolutionary Reinforcement Learning (DERL): a computational framework which can evolve diverse agent morphologies to learn challenging locomotion and manipulation tasks in complex environments. Leveraging DERL we demonstrate several relations between environmental complexity, morphological intelligence and the learnability of control. First, environmental complexity fosters the evolution of morphological intelligence as quantified by the ability of a morphology to facilitate the learning of novel tasks. Second, we demonstrate a morphological Baldwin effect i.e., in our simulations evolution rapidly selects morphologies that learn faster, thereby enabling behaviors learned late in the lifetime of early ancestors to be expressed early in the descendants lifetime. Third, we suggest a mechanistic basis for the above relationships through the evolution of morphologies that are more physically stable and energy efficient, and can therefore facilitate learning and control.},
	language = {en},
	number = {1},
	urldate = {2021-11-10},
	journal = {Nature Communications},
	author = {Gupta, Agrim and Savarese, Silvio and Ganguli, Surya and Fei-Fei, Li},
	month = dec,
	year = {2021},
	pages = {5721},
	file = {Gupta et al. - 2021 - Embodied intelligence via learning and evolution.pdf:/home/zwerg/Zotero/storage/2BYHABAY/Gupta et al. - 2021 - Embodied intelligence via learning and evolution.pdf:application/pdf},
}

@article{eisenstein_microscopy_2021,
	title = {Microscopy made to order},
	volume = {18},
	issn = {1548-7091, 1548-7105},
	url = {https://www.nature.com/articles/s41592-021-01313-1},
	doi = {10.1038/s41592-021-01313-1},
	language = {en},
	number = {11},
	urldate = {2021-11-09},
	journal = {Nature Methods},
	author = {Eisenstein, Michael},
	month = nov,
	year = {2021},
	pages = {1277--1281},
	file = {Eisenstein - 2021 - Microscopy made to order.pdf:/home/zwerg/Zotero/storage/2SJZA9G3/Eisenstein - 2021 - Microscopy made to order.pdf:application/pdf},
}

@article{xiang_zooming_2021,
	title = {Zooming {SlowMo}: {An} {Efficient} {One}-{Stage} {Framework} for {Space}-{Time} {Video} {Super}-{Resolution}},
	shorttitle = {Zooming {SlowMo}},
	url = {http://arxiv.org/abs/2104.07473},
	abstract = {In this paper, we address the space-time video super-resolution, which aims at generating a high-resolution (HR) slow-motion video from a low-resolution (LR) and low frame rate (LFR) video sequence. A na{\textbackslash}"ive method is to decompose it into two sub-tasks: video frame interpolation (VFI) and video super-resolution (VSR). Nevertheless, temporal interpolation and spatial upscaling are intra-related in this problem. Two-stage approaches cannot fully make use of this natural property. Besides, state-of-the-art VFI or VSR deep networks usually have a large frame reconstruction module in order to obtain high-quality photo-realistic video frames, which makes the two-stage approaches have large models and thus be relatively time-consuming. To overcome the issues, we present a one-stage space-time video super-resolution framework, which can directly reconstruct an HR slow-motion video sequence from an input LR and LFR video. Instead of reconstructing missing LR intermediate frames as VFI models do, we temporally interpolate LR frame features of the missing LR frames capturing local temporal contexts by a feature temporal interpolation module. Extensive experiments on widely used benchmarks demonstrate that the proposed framework not only achieves better qualitative and quantitative performance on both clean and noisy LR frames but also is several times faster than recent state-of-the-art two-stage networks. The source code is released in https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020 .},
	urldate = {2021-11-04},
	journal = {arXiv:2104.07473 [cs, eess]},
	author = {Xiang, Xiaoyu and Tian, Yapeng and Zhang, Yulun and Fu, Yun and Allebach, Jan P. and Xu, Chenliang},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.07473},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Multimedia},
	annote = {Comment: Journal version of "Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution"(CVPR-2020). 14 pages, 14 figures},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/NQ5AMIIH/Xiang et al. - 2021 - Zooming SlowMo An Efficient One-Stage Framework f.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/RVURWQCL/2104.html:text/html},
}

@techreport{priessner_content-aware_2021,
	title = {Content-aware frame interpolation ({CAFI}): {Deep} {Learning}-based temporal super-resolution for fast bioimaging},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Content-aware frame interpolation ({CAFI})},
	url = {https://www.biorxiv.org/content/10.1101/2021.11.02.466664v1},
	abstract = {The development of high-resolution microscopes has made it possible to investigate cellular processes in 4D (3D over time). However, observing fast cellular dynamics remains challenging as a consequence of photobleaching and phototoxicity. These issues become increasingly problematic with the depth of the volume acquired and the speed of the biological events of interest. Here, we report the implementation of two content-aware frame interpolation (CAFI) deep learning networks, Zooming SlowMo (ZS) and Depth-Aware Video Frame Interpolation (DAIN), based on combinations of recurrent neural networks, that are highly suited for accurately predicting images in between image pairs, therefore improving the temporal resolution of image series as a post-acquisition analysis step. We show that CAFI predictions are capable of understanding the motion context of biological structures to perform better than standard interpolation methods. We benchmark CAFI's performance on six different datasets, obtained from three different microscopy modalities (point-scanning confocal, spinning-disc confocal and confocal brightfield microscopy). We demonstrate its capabilities for single-particle tracking methods applied to the study of lysosome trafficking. CAFI therefore allows for reduced light exposure and phototoxicity on the sample and extends the possibility of long-term live-cell imaging. Both DAIN and ZS as well as the training and testing data are made available for use by the wider community via the ZeroCostDL4Mic platform.},
	language = {en},
	urldate = {2021-11-04},
	author = {Priessner, Martin and Gaboriau, David C. A. and Sheridan, Arlo and Lenn, Tchern and Chubb, Jonathan R. and Manor, Uri and Compte, Ramon Vilar and Laine, Romain F.},
	month = nov,
	year = {2021},
	doi = {10.1101/2021.11.02.466664},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {2021.11.02.466664},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/RET5FKEA/Priessner et al. - 2021 - Content-aware frame interpolation (CAFI) Deep Lea.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/WCZHH2JK/2021.11.02.html:text/html},
}

@article{lee_feature2mass_2018-1,
	title = {{Feature2Mass}: {Visual} {Feature} {Processing} in {Latent} {Space} for {Realistic} {Labeled} {Mass} {Generation}},
	shorttitle = {{Feature2Mass}},
	url = {http://arxiv.org/abs/1809.06147},
	abstract = {This paper deals with a method for generating realistic labeled masses. Recently, there have been many attempts to apply deep learning to various bio-image computing fields including computer-aided detection and diagnosis. In order to learn deep network model to be well-behaved in bio-image computing fields, a lot of labeled data is required. However, in many bioimaging fields, the large-size of labeled dataset is scarcely available. Although a few researches have been dedicated to solving this problem through generative model, there are some problems as follows: 1) The generated bio-image does not seem realistic; 2) the variation of generated bio-image is limited; and 3) additional label annotation task is needed. In this study, we propose a realistic labeled bio-image generation method through visual feature processing in latent space. Experimental results have shown that mass images generated by the proposed method were realistic and had wide expression range of targeted mass characteristics.},
	urldate = {2021-11-04},
	journal = {arXiv:1809.06147 [cs]},
	author = {Lee, Jae-Hyeok and Kim, Seong Tae and Lee, Hakmin and Ro, Yong Man},
	month = nov,
	year = {2018},
	note = {arXiv: 1809.06147},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: This paper presented at ECCV 2018 Workshop},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/IE43RN6T/Lee et al. - 2018 - Feature2Mass Visual Feature Processing in Latent .pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/BPU5AMYG/1809.html:text/html},
}

@article{laine_imaging_2021,
	title = {Imaging in focus: {An} introduction to denoising bioimages in the era of deep learning},
	volume = {140},
	issn = {1357-2725},
	shorttitle = {Imaging in focus},
	url = {https://www.sciencedirect.com/science/article/pii/S1357272521001588},
	doi = {10.1016/j.biocel.2021.106077},
	abstract = {Fluorescence microscopy enables the direct observation of previously hidden dynamic processes of life, allowing profound insights into mechanisms of health and disease. However, imaging of live samples is fundamentally limited by the toxicity of the illuminating light and images are often acquired using low light conditions. As a consequence, images can become very noisy which severely complicates their interpretation. In recent years, deep learning (DL) has emerged as a very successful approach to remove this noise while retaining the useful signal. Unlike classical algorithms which use well-defined mathematical functions to remove noise, DL methods learn to denoise from example data, providing a powerful content-aware approach. In this review, we first describe the different types of noise that typically corrupt fluorescence microscopy images and introduce the denoising task. We then present the main DL-based denoising methods and their relative advantages and disadvantages. We aim to provide insights into how DL-based denoising methods operate and help users choose the most appropriate tools for their applications.},
	language = {en},
	urldate = {2021-10-29},
	journal = {The International Journal of Biochemistry \& Cell Biology},
	author = {Laine, Romain F. and Jacquemet, Guillaume and Krull, Alexander},
	month = nov,
	year = {2021},
	keywords = {Deep learning, Microscopy, Denoising, Live-cell imaging, Noise},
	pages = {106077},
}

@article{bimber_light-field_2019,
	title = {Light-{Field} {Microscopy}: {A} {Review}},
	volume = {4},
	issn = {2572942X},
	shorttitle = {Light-{Field} {Microscopy}},
	url = {http://www.jneurology.com/articles/lightfield-microscopy-a-review.html},
	doi = {10.29245/2572.942X/2019/1.1237},
	abstract = {Light-field microcopy (LFM) supports single-shot volumetric recording and instant 3D excitation. It is an emerging technology for fast wide-field 3D imaging of neuronal activity, and has the potential to enable scanless 3D photostimulation for applications in optogenetics. This article introduces the basics of light-field technology, reviews the LFM principles and current implementations, summarizes first applications for optical imaging and photostimulation of neural activity, and outlines alternatives that go beyond LFM.},
	language = {en},
	number = {1},
	urldate = {2021-10-29},
	journal = {Journal of Neurology \& Neuromedicine},
	author = {Bimber, Oliver and Schedl, David},
	month = jan,
	year = {2019},
	pages = {1--6},
	file = {Bimber and Schedl - 2019 - Light-Field Microscopy A Review.pdf:/home/zwerg/Zotero/storage/A9MCHCT5/Bimber and Schedl - 2019 - Light-Field Microscopy A Review.pdf:application/pdf},
}

@techreport{ulicna_automated_2020,
	title = {Automated deep lineage tree analysis using a {Bayesian} single cell tracking approach},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.09.10.276980v1},
	abstract = {Single-cell methods are beginning to reveal the intrinsic heterogeneity in cell populations, which arises from the interplay or deterministic and stochastic processes. For example, the molecular mechanisms of cell cycle control are well characterised, yet the observed distribution of cell cycle durations in a population of cells is heterogenous. This variability may be governed either by stochastic processes, inherited in a deterministic fashion, or some combination of both. Previous studies have shown poor correlations within lineages when observing direct ancestral relationships but remain correlated with immediate relatives. However, assessing longer-range dependencies amid noisy data requires significantly more observations, and demands the development of automated procedures for lineage tree reconstruction. Here, we developed an open-source Python library, btrack, to facilitate retrieval of deep lineage information from live-cell imaging data. We acquired 3,500 hours of time-lapse microscopy data of epithelial cells in culture and used our software to extract 22,519 fully annotated single-cell trajectories. Benchmarking tests, including lineage tree reconstruction assessments, demonstrate that our approach yields high-fidelity results and achieves state-of-the-art performance without the requirement for manual curation of the tracker output data. To demonstrate the robustness of our supervision-free cell tracking pipeline, we retrieve cell cycle durations and their extended inter- and intra-generational family relationships, for up to eight generations, and up to fourth cousin relationships. The extracted lineage tree dataset represents approximately two orders of magnitude more data, and longer-range dependencies, than in previous studies of cell cycle heritability. Our results extend the range of observed correlations and suggest that strong heritable cell cycling is present. We envisage that our approach could be extended with additional live-cell reporters to provide a detailed quantitative characterisation of biochemical and mechanical origins to cycling heterogeneity in cell populations.},
	language = {en},
	urldate = {2021-10-22},
	author = {Ulicna, Kristina and Vallardi, Giulia and Charras, Guillaume and Lowe, Alan R.},
	month = sep,
	year = {2020},
	doi = {10.1101/2020.09.10.276980},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {2020.09.10.276980},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/E8BPNUXP/Ulicna et al. - 2020 - Automated deep lineage tree analysis using a Bayes.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/QADMGLMZ/2020.09.10.html:text/html},
}

@misc{noauthor_nlp_2021,
	title = {{NLP} from {Scratch} with {PyTorch}, fastai, and {HuggingFace}},
	url = {https://amarsaini.github.io/Epoching-Blog/jupyter/nlp/pytorch/fastai/huggingface/2021/06/27/NLP-from-Scratch-with-PyTorch-FastAI-and-HuggingFace.html},
	abstract = {A technical NLP tutorial using a variety of libraries to show the different levels/layers of common NLP pipelines},
	language = {en},
	urldate = {2021-10-21},
	journal = {Epoching’s Blog},
	month = jun,
	year = {2021},
	file = {Snapshot:/home/zwerg/Zotero/storage/A65593RF/NLP-from-Scratch-with-PyTorch-FastAI-and-HuggingFace.html:text/html},
}

@article{shen_global_2020,
	title = {Global {Self}-{Attention} {Networks} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/2010.03019},
	abstract = {Recently, a series of works in computer vision have shown promising results on various image and video understanding tasks using self-attention. However, due to the quadratic computational and memory complexities of self-attention, these works either apply attention only to low-resolution feature maps in later stages of a deep network or restrict the receptive field of attention in each layer to a small local region. To overcome these limitations, this work introduces a new global self-attention module, referred to as the GSA module, which is efficient enough to serve as the backbone component of a deep network. This module consists of two parallel layers: a content attention layer that attends to pixels based only on their content and a positional attention layer that attends to pixels based on their spatial locations. The output of this module is the sum of the outputs of the two layers. Based on the proposed GSA module, we introduce new standalone global attention-based deep networks that use GSA modules instead of convolutions to model pixel interactions. Due to the global extent of the proposed GSA module, a GSA network has the ability to model long-range pixel interactions throughout the network. Our experimental results show that GSA networks outperform the corresponding convolution-based networks significantly on the CIFAR-100 and ImageNet datasets while using less parameters and computations. The proposed GSA networks also outperform various existing attention-based networks on the ImageNet dataset.},
	urldate = {2021-10-20},
	journal = {arXiv:2010.03019 [cs]},
	author = {Shen, Zhuoran and Bello, Irwan and Vemulapalli, Raviteja and Jia, Xuhui and Chen, Ching-Hui},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.03019},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/2LU9FEQW/Shen et al. - 2020 - Global Self-Attention Networks for Image Recogniti.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/XNHMVWTS/2010.html:text/html},
}

@article{li_fast_2020,
	title = {Fast {Video} {Object} {Segmentation} using the {Global} {Context} {Module}},
	url = {http://arxiv.org/abs/2001.11243},
	abstract = {We developed a real-time, high-quality semi-supervised video object segmentation algorithm. Its accuracy is on par with the most accurate, time-consuming online-learning model, while its speed is similar to the fastest template-matching method with sub-optimal accuracy. The core component of the model is a novel global context module that effectively summarizes and propagates information through the entire video. Compared to previous approaches that only use one frame or a few frames to guide the segmentation of the current frame, the global context module uses all past frames. Unlike the previous state-of-the-art space-time memory network that caches a memory at each spatio-temporal position, the global context module uses a fixed-size feature representation. Therefore, it uses constant memory regardless of the video length and costs substantially less memory and computation. With the novel module, our model achieves top performance on standard benchmarks at a real-time speed.},
	urldate = {2021-10-20},
	journal = {arXiv:2001.11243 [cs]},
	author = {Li, Yu and Shen, Zhuoran and Shan, Ying},
	month = jul,
	year = {2020},
	note = {arXiv: 2001.11243},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.6},
	annote = {Comment: To appear at ECCV 2020},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/9E6AJWXH/Li et al. - 2020 - Fast Video Object Segmentation using the Global Co.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/5YU6TZEK/2001.html:text/html},
}

@article{shen_efficient_2020,
	title = {Efficient {Attention}: {Attention} with {Linear} {Complexities}},
	shorttitle = {Efficient {Attention}},
	url = {http://arxiv.org/abs/1812.01243},
	abstract = {Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on high-resolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-of-the-art accuracies for stereo depth estimation on the Scene Flow dataset. Code is available at https://github.com/cmsflash/efficient-attention.},
	urldate = {2021-10-20},
	journal = {arXiv:1812.01243 [cs]},
	author = {Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng},
	month = nov,
	year = {2020},
	note = {arXiv: 1812.01243},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, I.4.6, I.2.10, I.2.6, I.4.8},
	annote = {Comment: To appear at WACV 2021},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/RUBBFBFH/Shen et al. - 2020 - Efficient Attention Attention with Linear Complex.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/RCYPA6NM/1812.html:text/html},
}

@article{politi_quantitative_2018,
	title = {Quantitative mapping of fluorescently tagged cellular proteins using {FCS}-calibrated four-dimensional imaging},
	volume = {13},
	issn = {1750-2799},
	doi = {10.1038/nprot.2018.040},
	abstract = {The ability to tag a protein at its endogenous locus with a fluorescent protein (FP) enables quantitative understanding of protein dynamics at the physiological level. Genome-editing technology has now made this powerful approach routinely applicable to mammalian cells and many other model systems, thereby opening up the possibility to systematically and quantitatively map the cellular proteome in four dimensions. 3D time-lapse confocal microscopy (4D imaging) is an essential tool for investigating spatial and temporal protein dynamics; however, it lacks the required quantitative power to make the kind of absolute and comparable measurements required for systems analysis. In contrast, fluorescence correlation spectroscopy (FCS) provides quantitative proteomic and biophysical parameters such as protein concentration, hydrodynamic radius, and oligomerization but lacks the capability for high-throughput application in 4D spatial and temporal imaging. Here we present an automated experimental and computational workflow that integrates both methods and delivers quantitative 4D imaging data in high throughput. These data are processed to yield a calibration curve relating the fluorescence intensities (FIs) of image voxels to the absolute protein abundance. The calibration curve allows the conversion of the arbitrary FIs to protein amounts for all voxels of 4D imaging stacks. Using our workflow, users can acquire and analyze hundreds of FCS-calibrated image series to map their proteins of interest in four dimensions. Compared with other protocols, the current protocol does not require additional calibration standards and provides an automated acquisition pipeline for FCS and imaging data. The protocol can be completed in 1 d.},
	language = {eng},
	number = {6},
	journal = {Nature Protocols},
	author = {Politi, Antonio Z. and Cai, Yin and Walther, Nike and Hossain, M. Julius and Koch, Birgit and Wachsmuth, Malte and Ellenberg, Jan},
	month = jun,
	year = {2018},
	pmid = {29844523},
	pmcid = {PMC6609853},
	keywords = {Proteins, Imaging, Three-Dimensional, Proteomics, Proteome, Automation, Laboratory, Cells, Gene Editing, Optical Imaging, Spatio-Temporal Analysis, Staining and Labeling},
	pages = {1445--1464},
	file = {Accepted Version:/home/zwerg/Zotero/storage/ISWACIWQ/Politi et al. - 2018 - Quantitative mapping of fluorescently tagged cellu.pdf:application/pdf},
}

@article{swedlow_ellenberg_nodate,
	title = {Ellenberg et al\_Bioimage\_archive},
	abstract = {Public data archives are the backbone of modern biological and biomedical research. While archives for biological molecules and structures are well-established, resources for imaging data do not yet cover the full range of spatial and temporal scales or application domains used by the scientific community. In the last few years, the technical barriers to building such resources have been solved and the first examples of scientific outputs from public image data resources, often through linkage to existing molecular resources, have been published. Using the successes of existing biomolecular resources as a guide, we present the rationale and principles for the construction of image data archives and databases that will be the foundation of the next revolution in biological and biomedical informatics and discovery.},
	language = {en},
	author = {Swedlow, Jason},
	pages = {13},
	file = {Swedlow - Ellenberg et al_Bioimage_archive.pdf:/home/zwerg/Zotero/storage/WGGM4VFJ/Swedlow - Ellenberg et al_Bioimage_archive.pdf:application/pdf},
}

@misc{noauthor_3dnatives_nodate,
	title = {{3Dnatives} {Lab}: {Testing} {MakerBot}'s {Method} {X} {CFE} - {3Dnatives}},
	url = {https://www.3dnatives.com/en/3dnatives-lab-makerbot-method-x-cfe-280920215/?mkt_tok=NDQ0LVpUTS04NjYAAAGAFL6LaSKsRXqo34vB7e3VzVFiYi7XbDx0Id1dMgJ3S2Sz1PGR5ZvJV5D5Nryy59gihMvv3HZAf_lb2dMt7sagcp1NX8ME7ymlchAuRSy4VN33DA#!},
	urldate = {2021-10-13},
}

@misc{noauthor_microrna_nodate,
	title = {A {microRNA} signature that correlates with cognition and is a target against cognitive decline},
	url = {https://www.embopress.org/doi/epdf/10.15252/emmm.202013659},
	urldate = {2021-10-13},
	doi = {10.15252/emmm.202013659},
	file = {Snapshot:/home/zwerg/Zotero/storage/XTFDQ7HR/emmm.html:text/html},
}

@article{noauthor_microrna_2021,
	title = {A {microRNA} signature that correlates with cognition and is a target against cognitive decline},
	volume = {n/a},
	issn = {1757-4676},
	url = {https://www.embopress.org/doi/full/10.15252/emmm.202013659},
	doi = {10.15252/emmm.202013659},
	abstract = {Abstract While some individuals age without pathological memory impairments, others develop age-associated cognitive diseases. Since changes in cognitive function develop slowly over time in these patients, they are often diagnosed at an advanced stage of molecular pathology, a time point when causative treatments fail. Thus, there is great need for the identification of inexpensive and minimal invasive approaches that could be used for screening with the aim to identify individuals at risk for cognitive decline that can then undergo further diagnostics and eventually stratified therapies. In this study, we use an integrative approach combining the analysis of human data and mechanistic studies in model systems to identify a circulating 3-microRNA signature that reflects key processes linked to neural homeostasis and inform about cognitive status. We furthermore provide evidence that expression changes in this signature represent multiple mechanisms deregulated in the aging and diseased brain and are a suitable target for RNA therapeutics.},
	number = {n/a},
	urldate = {2021-10-13},
	journal = {EMBO Molecular Medicine},
	month = oct,
	year = {2021},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {Alzheimer, biomarker, cognitive impairment, microRNA, RNA therapeutics},
	pages = {e13659},
}

@article{wei_finetuned_2021,
	title = {Finetuned {Language} {Models} {Are} {Zero}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2109.01652},
	abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially boosts zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of tasks and model scale are key components to the success of instruction tuning.},
	urldate = {2021-10-13},
	journal = {arXiv:2109.01652 [cs]},
	author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
	month = oct,
	year = {2021},
	note = {arXiv: 2109.01652
version: 2},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Version 2. Find list of changes in Appendix F (page 30)},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/9PL7LLAA/Wei et al. - 2021 - Finetuned Language Models Are Zero-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/KXGGDBUJ/2109.html:text/html},
}

@article{misra_end--end_2021,
	title = {An {End}-to-{End} {Transformer} {Model} for {3D} {Object} {Detection}},
	url = {http://arxiv.org/abs/2109.08141},
	abstract = {We propose 3DETR, an end-to-end Transformer based object detection model for 3D point clouds. Compared to existing detection methods that employ a number of 3D-specific inductive biases, 3DETR requires minimal modifications to the vanilla Transformer block. Specifically, we find that a standard Transformer with non-parametric queries and Fourier positional embeddings is competitive with specialized architectures that employ libraries of 3D-specific operators with hand-tuned hyperparameters. Nevertheless, 3DETR is conceptually simple and easy to implement, enabling further improvements by incorporating 3D domain knowledge. Through extensive experiments, we show 3DETR outperforms the well-established and highly optimized VoteNet baselines on the challenging ScanNetV2 dataset by 9.5\%. Furthermore, we show 3DETR is applicable to 3D tasks beyond detection, and can serve as a building block for future research.},
	urldate = {2021-10-13},
	journal = {arXiv:2109.08141 [cs]},
	author = {Misra, Ishan and Girdhar, Rohit and Joulin, Armand},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.08141},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted at ICCV 2021},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/UL6PKTPE/Misra et al. - 2021 - An End-to-End Transformer Model for 3D Object Dete.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/D2M5DXQZ/2109.html:text/html},
}

@misc{noauthor_pytorch_nodate,
	title = {{PyTorch} to {TensorFlow} {Lite} for deploying on {Arm} {Ethos}-{U55} and {U65} - {AI} and {ML} blog - {Arm} {Community} blogs - {Arm} {Community}},
	url = {https://community.arm.com/arm-community-blogs/b/ai-and-ml-blog/posts/pytorch-to-tensorflow-lite-for-deploying-on-arm-ethos-u55-and-u65},
	abstract = {How to convert models to run Ethos-U55 and Ethos-U65 for models trained in PyTorch.},
	language = {en},
	urldate = {2021-10-13},
}

@article{kunis_mdemic_2021,
	title = {{MDEmic}: a metadata annotation tool to facilitate management of {FAIR} image data in the bioimaging community},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {{MDEmic}},
	url = {https://www.nature.com/articles/s41592-021-01288-z},
	doi = {10.1038/s41592-021-01288-z},
	language = {en},
	urldate = {2021-10-13},
	journal = {Nature Methods},
	author = {Kunis, Susanne and Hänsch, Sebastian and Schmidt, Christian and Wong, Frances and Strambio-De-Castillia, Caterina and Weidtkamp-Peters, Stefanie},
	month = oct,
	year = {2021},
	note = {Primary\_atype: Correspondence
Publisher: Nature Publishing Group
Subject\_term: Databases;Software;Standards
Subject\_term\_id: databases;software;standards},
	pages = {1--2},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/K4SHLP9J/Kunis et al. - 2021 - MDEmic a metadata annotation tool to facilitate m.pdf:application/pdf},
}

@misc{noauthor_2021_nodate,
	title = {2021 {ML}, {AI}, \& {Data} {Landscape}},
	url = {https://airtable.com},
	abstract = {Explore the "2021 ML, AI, \& Data Landscape" view on Airtable.},
	urldate = {2021-10-11},
	journal = {Airtable},
}

@misc{noauthor_red_2021,
	title = {Red {Hot}: {The} 2021 {Machine} {Learning}, {AI} and {Data} ({MAD}) {Landscape}},
	shorttitle = {Red {Hot}},
	url = {https://mattturck.com/data2021/},
	abstract = {Full resolution version of the landscape image here







It’s been a hot, hot year in the world of data, machine learning and AI. 



Just when you thought it couldn’t grow any more explosively, the data/AI landscape just did: rapid pace of company creation, exciting new product and project launch},
	language = {en-US},
	urldate = {2021-10-11},
	journal = {Matt Turck},
	month = sep,
	year = {2021},
	note = {Section: AI},
	file = {2021 - Red Hot The 2021 Machine Learning, AI and Data (M.html:/home/zwerg/Zotero/storage/93LYSXYN/2021 - Red Hot The 2021 Machine Learning, AI and Data (M.html:text/html},
}

@article{wagner_deep_2021,
	title = {Deep learning-enhanced light-field imaging with continuous validation},
	volume = {18},
	issn = {1548-7091, 1548-7105},
	url = {http://www.nature.com/articles/s41592-021-01136-0},
	doi = {10.1038/s41592-021-01136-0},
	language = {en},
	number = {5},
	urldate = {2021-10-11},
	journal = {Nature Methods},
	author = {Wagner, Nils and Beuttenmueller, Fynn and Norlin, Nils and Gierten, Jakob and Boffi, Juan Carlos and Wittbrodt, Joachim and Weigert, Martin and Hufnagel, Lars and Prevedel, Robert and Kreshuk, Anna},
	month = may,
	year = {2021},
	pages = {557--563},
	file = {Wagner et al. - 2021 - Deep learning-enhanced light-field imaging with co.pdf:/home/zwerg/Zotero/storage/NQKQXRVF/Wagner et al. - 2021 - Deep learning-enhanced light-field imaging with co.pdf:application/pdf},
}

@article{suzek_uniref_2015,
	title = {{UniRef} clusters: a comprehensive and scalable alternative for improving sequence similarity searches},
	volume = {31},
	issn = {1367-4803},
	shorttitle = {{UniRef} clusters},
	url = {https://doi.org/10.1093/bioinformatics/btu739},
	doi = {10.1093/bioinformatics/btu739},
	abstract = {Motivation: UniRef databases provide full-scale clustering of UniProtKB sequences and are utilized for a broad range of applications, particularly similarity-based functional annotation. Non-redundancy and intra-cluster homogeneity in UniRef were recently improved by adding a sequence length overlap threshold. Our hypothesis is that these improvements would enhance the speed and sensitivity of similarity searches and improve the consistency of annotation within clusters.Results: Intra-cluster molecular function consistency was examined by analysis of Gene Ontology terms. Results show that UniRef clusters bring together proteins of identical molecular function in more than 97\% of the clusters, implying that clusters are useful for annotation and can also be used to detect annotation inconsistencies. To examine coverage in similarity results, BLASTP searches against UniRef50 followed by expansion of the hit lists with cluster members demonstrated advantages compared with searches against UniProtKB sequences; the searches are concise (∼7 times shorter hit list before expansion), faster (∼6 times) and more sensitive in detection of remote similarities (\&gt;96\% recall at e-value \&lt;0.0001). Our results support the use of UniRef clusters as a comprehensive and scalable alternative to native sequence databases for similarity searches and reinforces its reliability for use in functional annotation.Availability and implementation: Web access and file download from UniProt website at http://www.uniprot.org/uniref and ftp://ftp.uniprot.org/pub/databases/uniprot/uniref. BLAST searches against UniRef are available at http://www.uniprot.org/blast/Contact:huang@dbi.udel.edu},
	number = {6},
	urldate = {2021-10-08},
	journal = {Bioinformatics},
	author = {Suzek, Baris E. and Wang, Yuqi and Huang, Hongzhan and McGarvey, Peter B. and Wu, Cathy H. and {the UniProt Consortium}},
	month = mar,
	year = {2015},
	pages = {926--932},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/HKP7KKWM/Suzek et al. - 2015 - UniRef clusters a comprehensive and scalable alte.pdf:application/pdf},
}

@article{pinkard_deep_2019-1,
	title = {Deep learning for single-shot autofocus microscopy},
	volume = {6},
	issn = {2334-2536},
	url = {https://www.osapublishing.org/abstract.cfm?URI=optica-6-6-794},
	doi = {10.1364/OPTICA.6.000794},
	language = {en},
	number = {6},
	urldate = {2021-10-08},
	journal = {Optica},
	author = {Pinkard, Henry and Phillips, Zachary and Babakhani, Arman and Fletcher, Daniel A. and Waller, Laura},
	month = jun,
	year = {2019},
	pages = {794},
	file = {Pinkard et al. - 2019 - Deep learning for single-shot autofocus microscopy.pdf:/home/zwerg/Zotero/storage/YIF6BTVS/Pinkard et al. - 2019 - Deep learning for single-shot autofocus microscopy.pdf:application/pdf},
}

@article{luo_single-shot_2021,
	title = {Single-{Shot} {Autofocusing} of {Microscopy} {Images} {Using} {Deep} {Learning}},
	volume = {8},
	issn = {2330-4022, 2330-4022},
	url = {https://pubs.acs.org/doi/10.1021/acsphotonics.0c01774},
	doi = {10.1021/acsphotonics.0c01774},
	abstract = {Autofocusing is a critical step for high-quality microscopic imaging of specimens, especially for measurements that extend over time covering large ﬁelds of view. Autofocusing is generally practiced using two main approaches. Hardware-based optical autofocusing methods rely on additional distance sensors that are integrated with a microscopy system. Algorithmic autofocusing methods, on the other hand, regularly require axial scanning through the sample volume, leading to longer imaging times, which might also introduce phototoxicity and photobleaching on the sample. Here, we demonstrate a deep learning-based oﬄine autofocusing method, termed Deep-R, that is trained to rapidly and blindly autofocus a single-shot microscopy image of a specimen that is acquired at an arbitrary out-of-focus plane. We illustrate the eﬃcacy of Deep-R using various tissue sections that were imaged using ﬂuorescence and brightﬁeld microscopy modalities and demonstrate snapshot autofocusing under diﬀerent scenarios, such as a uniform axial defocus as well as a sample tilt within the ﬁeld-of-view. Our results reveal that Deep-R is signiﬁcantly faster when compared with standard online algorithmic autofocusing methods. This deep learning-based blind autofocusing framework opens up new opportunities for rapid microscopic imaging of large sample areas, also reducing the photon dose on the sample.},
	language = {en},
	number = {2},
	urldate = {2021-10-08},
	journal = {ACS Photonics},
	author = {Luo, Yilin and Huang, Luzhe and Rivenson, Yair and Ozcan, Aydogan},
	month = feb,
	year = {2021},
	pages = {625--638},
	file = {Luo et al. - 2021 - Single-Shot Autofocusing of Microscopy Images Usin.pdf:/home/zwerg/Zotero/storage/UUH2Q3K2/Luo et al. - 2021 - Single-Shot Autofocusing of Microscopy Images Usin.pdf:application/pdf},
}

@article{yang_assessing_2018,
	title = {Assessing microscope image focus quality with deep learning},
	volume = {19},
	issn = {1471-2105},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2087-4},
	doi = {10.1186/s12859-018-2087-4},
	abstract = {Background: Large image datasets acquired on automated microscopes typically have some fraction of low quality, out-of-focus images, despite the use of hardware autofocus systems. Identification of these images using automated image analysis with high accuracy is important for obtaining a clean, unbiased image dataset. Complicating this task is the fact that image focus quality is only well-defined in foreground regions of images, and as a result, most previous approaches only enable a computation of the relative difference in quality between two or more images, rather than an absolute measure of quality.
Results: We present a deep neural network model capable of predicting an absolute measure of image focus on a single image in isolation, without any user-specified parameters. The model operates at the image-patch level, and also outputs a measure of prediction certainty, enabling interpretable predictions. The model was trained on only 384 in-focus Hoechst (nuclei) stain images of U2OS cells, which were synthetically defocused to one of 11 absolute defocus levels during training. The trained model can generalize on previously unseen real Hoechst stain images, identifying the absolute image focus to within one defocus level (approximately 3 pixel blur diameter difference) with 95\% accuracy. On a simpler binary in/out-of-focus classification task, the trained model outperforms previous approaches on both Hoechst and Phalloidin (actin) stain images (F-scores of 0.89 and 0.86, respectively over 0.84 and 0.83), despite only having been presented Hoechst stain images during training. Lastly, we observe qualitatively that the model generalizes to two additional stains, Hoechst and Tubulin, of an unseen cell type (Human MCF-7) acquired on a different instrument.
Conclusions: Our deep neural network enables classification of out-of-focus microscope images with both higher accuracy and greater precision than previous approaches via interpretable patch-level focus and certainty predictions. The use of synthetically defocused images precludes the need for a manually annotated training dataset. The model also generalizes to different image and cell types. The framework for model training and image prediction is available as a free software library and the pre-trained model is available for immediate use in Fiji (ImageJ) and CellProfiler.},
	language = {en},
	number = {1},
	urldate = {2021-10-08},
	journal = {BMC Bioinformatics},
	author = {Yang, Samuel J. and Berndl, Marc and Michael Ando, D. and Barch, Mariya and Narayanaswamy, Arunachalam and Christiansen, Eric and Hoyer, Stephan and Roat, Chris and Hung, Jane and Rueden, Curtis T. and Shankar, Asim and Finkbeiner, Steven and Nelson, Philip},
	month = dec,
	year = {2018},
	pages = {77},
	file = {Yang et al. - 2018 - Assessing microscope image focus quality with deep.pdf:/home/zwerg/Zotero/storage/4NQVJCD7/Yang et al. - 2018 - Assessing microscope image focus quality with deep.pdf:application/pdf},
}

@inproceedings{shajkofci_deepfocus_2020,
	address = {Iowa City, IA, USA},
	title = {{DeepFocus}: {A} {Few}-{Shot} {Microscope} {Slide} {Auto}-{Focus} {Using} a {Sample} {Invariant} {CNN}-{Based} {Sharpness} {Function}},
	isbn = {978-1-5386-9330-8},
	shorttitle = {{DeepFocus}},
	url = {https://ieeexplore.ieee.org/document/9098331/},
	doi = {10.1109/ISBI45749.2020.9098331},
	abstract = {Autofocus (AF) methods are extensively used in biomicroscopy, for example to acquire timelapses, where the imaged objects tend to drift out of focus. AF algorithms determine an optimal distance by which to move the sample back into the focal plane. Current hardware-based methods require modifying the microscope and image-based algorithms either rely on many images to converge to the sharpest position or need training data and models speciﬁc to each instrument and imaging conﬁguration. Here we propose DeepFocus, an AF method we implemented as a Micro-Manager plugin, and characterize its Convolutional Neural Network (CNN)-based sharpness function, which we observed to be depth co-variant and sample-invariant. Sample invariance allows our AF algorithm to converge to an optimal axial position within as few as three iterations using a model trained once for use with a wide range of optical microscopes and a single instrument-dependent calibration stack acquisition of a ﬂat (but arbitrary) textured object. From experiments carried out both on synthetic and experimental data, we observed an average precision, given 3 measured images, of 0.30 ± 0.16 µm with a 10×, NA 0.3 objective. We foresee that this performance and low image number will help limit photodamage during acquisitions with light-sensitive samples.},
	language = {en},
	urldate = {2021-10-08},
	booktitle = {2020 {IEEE} 17th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	publisher = {IEEE},
	author = {Shajkofci, Adrian and Liebling, Michael},
	month = apr,
	year = {2020},
	pages = {164--168},
	file = {Shajkofci and Liebling - 2020 - DeepFocus A Few-Shot Microscope Slide Auto-Focus .pdf:/home/zwerg/Zotero/storage/DJZJ94XM/Shajkofci and Liebling - 2020 - DeepFocus A Few-Shot Microscope Slide Auto-Focus .pdf:application/pdf},
}

@article{thul_subcellular_2017,
	title = {A subcellular map of the human proteome},
	volume = {356},
	url = {https://www.science.org/lookup/doi/10.1126/science.aal3321},
	doi = {10.1126/science.aal3321},
	number = {6340},
	urldate = {2021-10-07},
	journal = {Science},
	author = {Thul, Peter J. and Åkesson, Lovisa and Wiking, Mikaela and Mahdessian, Diana and Geladaki, Aikaterini and Ait Blal, Hammou and Alm, Tove and Asplund, Anna and Björk, Lars and Breckels, Lisa M. and Bäckström, Anna and Danielsson, Frida and Fagerberg, Linn and Fall, Jenny and Gatto, Laurent and Gnann, Christian and Hober, Sophia and Hjelmare, Martin and Johansson, Fredric and Lee, Sunjae and Lindskog, Cecilia and Mulder, Jan and Mulvey, Claire M. and Nilsson, Peter and Oksvold, Per and Rockberg, Johan and Schutten, Rutger and Schwenk, Jochen M. and Sivertsson, Åsa and Sjöstedt, Evelina and Skogs, Marie and Stadler, Charlotte and Sullivan, Devin P. and Tegel, Hanna and Winsnes, Casper and Zhang, Cheng and Zwahlen, Martin and Mardinoglu, Adil and Pontén, Fredrik and von Feilitzen, Kalle and Lilley, Kathryn S. and Uhlén, Mathias and Lundberg, Emma},
	month = may,
	year = {2017},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eaal3321},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/M5939Z8E/Thul et al. - 2017 - A subcellular map of the human proteome.pdf:application/pdf},
}

@article{shah_deep-learning_2021,
	title = {Deep-learning based denoising and reconstruction of super-resolution structured illumination microscopy images},
	volume = {9},
	copyright = {\&\#169; 2021 Chinese Laser Press},
	issn = {2327-9125},
	url = {https://www.osapublishing.org/prj/abstract.cfm?uri=prj-9-5-B168},
	doi = {10.1364/PRJ.416437},
	abstract = {Super-resolution structured illumination microscopy (SR-SIM) provides an up to twofold enhanced spatial resolution of fluorescently labeled samples. The reconstruction of high-quality SR-SIM images critically depends on patterned illumination with high modulation contrast. Noisy raw image data (e.g., as a result of low excitation power or low exposure time), result in reconstruction artifacts. Here, we demonstrate deep-learning based SR-SIM image denoising that results in high-quality reconstructed images. A residual encoding\&\#x2013;decoding convolutional neural network (RED-Net) was used to successfully denoise computationally reconstructed noisy SR-SIM images. We also demonstrate the end-to-end deep-learning based denoising and reconstruction of raw SIM images into high-resolution SR-SIM images. Both image reconstruction methods prove to be very robust against image reconstruction artifacts and generalize very well across various noise levels. The combination of computational image reconstruction and subsequent denoising via RED-Net shows very robust performance during inference after training even if the microscope settings change.},
	language = {EN},
	number = {5},
	urldate = {2021-10-07},
	journal = {Photonics Research},
	author = {Shah, Zafran Hussain and Müller, Marcel and Wang, Tung-Cheng and Scheidig, Philip Maurice and Schneider, Axel and Schüttpelz, Mark and Huser, Thomas and Huser, Thomas and Schenck, Wolfram and Schenck, Wolfram},
	month = may,
	year = {2021},
	note = {Publisher: Optical Society of America},
	pages = {B168--B181},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/LJCA3MQT/Shah et al. - 2021 - Deep-learning based denoising and reconstruction o.pdf:application/pdf},
}

@article{hu_image_2021,
	title = {Image enhancement for fluorescence microscopy based on deep learning with prior knowledge of aberration},
	volume = {46},
	copyright = {© 2021 Optical Society of America},
	issn = {1539-4794},
	url = {https://www.osapublishing.org/ol/abstract.cfm?uri=ol-46-9-2055},
	doi = {10.1364/OL.418997},
	abstract = {In this Letter, we propose a deep learning method with prior knowledge of potential aberration to enhance the fluorescence microscopy without additional hardware. The proposed method could effectively reduce noise and improve the peak signal-to-noise ratio of the acquired images at high speed. The enhancement performance and generalization of this method is demonstrated on three commercial fluorescence microscopes. This work provides a computational alternative to overcome the degradation induced by the biological specimen, and it has the potential to be further applied in biological applications.},
	language = {EN},
	number = {9},
	urldate = {2021-10-07},
	journal = {Optics Letters},
	author = {Hu, Lejia and Hu, Lejia and Hu, Shuwen and Gong, Wei and Gong, Wei and Gong, Wei and Si, Ke and Si, Ke and Si, Ke},
	month = may,
	year = {2021},
	note = {Publisher: Optical Society of America},
	keywords = {Image processing, Fluorescence microscopy, Multiphoton microscopy, Image quality, Image enhancement, Fast Fourier transforms},
	pages = {2055--2058},
	file = {Snapshot:/home/zwerg/Zotero/storage/PKPN3SHD/abstract.html:text/html},
}

@article{wang_deep_2021,
	title = {Deep learning enables confocal laser-scanning microscopy with enhanced resolution},
	volume = {46},
	copyright = {© 2021 Optical Society of America},
	issn = {1539-4794},
	url = {https://www.osapublishing.org/ol/abstract.cfm?uri=ol-46-19-4932},
	doi = {10.1364/OL.440561},
	abstract = {Theoretical resolution enhancement of confocal laser-scanning microscopy (CLSM) is sacrificed for the best compromise between optical sectioning and the signal-to-noise ratio (SNR). The pixel reassignment reconstruction algorithm can improve the effective spatial resolution of CLSM to its theoretical limit. However, current implementations are not versatile and are time-consuming or technically complex. Here we present a parameter-free post-processing strategy for laser-scanning microscopy based on deep learning, which enables a spatial resolution enhancement by a factor of ∼1.3, compared to conventional CLSM. To speed up the training process for experimental data, transfer learning, combined with a hybrid dataset consisting of simulated synthetic and experimental images, is employed. The overall resolution and SNR improvement, validated by quantitative evaluation metrics, allowed us to correctly infer the fine structures of real experimental images.},
	language = {EN},
	number = {19},
	urldate = {2021-10-07},
	journal = {Optics Letters},
	author = {Wang, Weibo and Wang, Weibo and Wu, Biwei and Wu, Biwei and Zhang, Baoyuan and Zhang, Baoyuan and Ma, Jie and Ma, Jie and Tan, Jiubin and Tan, Jiubin},
	month = oct,
	year = {2021},
	note = {Publisher: Optical Society of America},
	keywords = {Image processing, Confocal microscopy, Spatial resolution, Confocal laser scanning microscopy, Optical signal to noise ratio, Reconstruction algorithms},
	pages = {4932--4935},
	file = {Snapshot:/home/zwerg/Zotero/storage/D9KM6NVI/abstract.html:text/html},
}

@misc{noauthor_bio-transformers_nodate,
	title = {Bio-transformers : {Documentation} and {Tutorial} — bio-transformers v0.1.14},
	url = {https://bio-transformers.readthedocs.io/en/latest/},
	urldate = {2021-11-12},
}

@misc{instadeep_bio-transformers_nodate,
	title = {bio-transformers: {Wrapper} on top of {ESM}/{Protbert} model in order to easily work with protein embedding},
	copyright = {Apache Software License},
	shorttitle = {bio-transformers},
	author = {Instadeep},
	keywords = {Scientific/Engineering - Artificial Intelligence, Scientific/Engineering - Bio-Informatics, Software Development},
}

@misc{alammar_illustrated_nodate,
	title = {The {Illustrated} {Transformer}},
	url = {https://jalammar.github.io/illustrated-transformer/},
	abstract = {Discussions:
Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)


Translations: Chinese (Simplified), French, Japanese, Korean, Russian, Spanish, Vietnamese

Watch: MIT’s Deep Learning State of the Art lecture referencing this post

In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.

The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.

2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:




A High-Level Look
Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.},
	urldate = {2021-11-12},
	author = {Alammar, Jay},
}

@article{zhang_deepmsa_2020,
	title = {{DeepMSA}: constructing deep multiple sequence alignment to improve contact prediction and fold-recognition for distant-homology proteins},
	volume = {36},
	issn = {1367-4811},
	shorttitle = {{DeepMSA}},
	doi = {10.1093/bioinformatics/btz863},
	abstract = {MOTIVATION: The success of genome sequencing techniques has resulted in rapid explosion of protein sequences. Collections of multiple homologous sequences can provide critical information to the modeling of structure and function of unknown proteins. There are however no standard and efficient pipeline available for sensitive multiple sequence alignment (MSA) collection. This is particularly challenging when large whole-genome and metagenome databases are involved.
RESULTS: We developed DeepMSA, a new open-source method for sensitive MSA construction, which has homologous sequences and alignments created from multi-sources of whole-genome and metagenome databases through complementary hidden Markov model algorithms. The practical usefulness of the pipeline was examined in three large-scale benchmark experiments based on 614 non-redundant proteins. First, DeepMSA was utilized to generate MSAs for residue-level contact prediction by six coevolution and deep learning-based programs, which resulted in an accuracy increase in long-range contacts by up to 24.4\% compared to the default programs. Next, multiple threading programs are performed for homologous structure identification, where the average TM-score of the template alignments has over 7.5\% increases with the use of the new DeepMSA profiles. Finally, DeepMSA was used for secondary structure prediction and resulted in statistically significant improvements in the Q3 accuracy. It is noted that all these improvements were achieved without re-training the parameters and neural-network models, demonstrating the robustness and general usefulness of the DeepMSA in protein structural bioinformatics applications, especially for targets without homologous templates in the PDB library.
AVAILABILITY AND IMPLEMENTATION: https://zhanglab.ccmb.med.umich.edu/DeepMSA/.
SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
	language = {eng},
	number = {7},
	journal = {Bioinformatics (Oxford, England)},
	author = {Zhang, Chengxin and Zheng, Wei and Mortuza, S. M. and Li, Yang and Zhang, Yang},
	month = apr,
	year = {2020},
	pmid = {31738385},
	pmcid = {PMC7141871},
	keywords = {Algorithms, Proteins, Software, Neural Networks, Computer, Sequence Analysis, Protein, Sequence Alignment},
	pages = {2105--2112},
}

@article{luo_ecnet_2021,
	title = {{ECNet} is an evolutionary context-integrated deep learning framework for protein engineering},
	volume = {12},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-25976-8},
	doi = {10.1038/s41467-021-25976-8},
	abstract = {Abstract
            Machine learning has been increasingly used for protein engineering. However, because the general sequence contexts they capture are not specific to the protein being engineered, the accuracy of existing machine learning algorithms is rather limited. Here, we report ECNet (evolutionary context-integrated neural network), a deep-learning algorithm that exploits evolutionary contexts to predict functional fitness for protein engineering. This algorithm integrates local evolutionary context from homologous sequences that explicitly model residue-residue epistasis for the protein of interest with the global evolutionary context that encodes rich semantic and structural features from the enormous protein sequence universe. As such, it enables accurate mapping from sequence to function and provides generalization from low-order mutants to higher-order mutants. We show that ECNet predicts the sequence-function relationship more accurately as compared to existing machine learning algorithms by using {\textasciitilde}50 deep mutational scanning and random mutagenesis datasets. Moreover, we used ECNet to guide the engineering of TEM-1 β-lactamase and identified variants with improved ampicillin resistance with high success rates.},
	language = {en},
	number = {1},
	urldate = {2021-11-13},
	journal = {Nature Communications},
	author = {Luo, Yunan and Jiang, Guangde and Yu, Tianhao and Liu, Yang and Vo, Lam and Ding, Hantian and Su, Yufeng and Qian, Wesley Wei and Zhao, Huimin and Peng, Jian},
	month = dec,
	year = {2021},
	pages = {5743},
	file = {Luo et al. - 2021 - ECNet is an evolutionary context-integrated deep l.pdf:/home/zwerg/Zotero/storage/CJ7563CT/Luo et al. - 2021 - ECNet is an evolutionary context-integrated deep l.pdf:application/pdf},
}

@article{hie_adaptive_2021,
	title = {Adaptive machine learning for protein engineering},
	url = {http://arxiv.org/abs/2106.05466},
	abstract = {Machine-learning models that learn from data to predict how protein sequence encodes function are emerging as a useful protein engineering tool. However, when using these models to suggest new protein designs, one must deal with the vast combinatorial complexity of protein sequences. Here, we review how to use a sequence-to-function machine-learning surrogate model to select sequences for experimental measurement. First, we discuss how to select sequences through a single round of machine-learning optimization. Then, we discuss sequential optimization, where the goal is to discover optimized sequences and improve the model across multiple rounds of training, optimization, and experimental measurement.},
	urldate = {2021-11-13},
	journal = {arXiv:2106.05466 [cs, q-bio]},
	author = {Hie, Brian L. and Yang, Kevin K.},
	month = jul,
	year = {2021},
	note = {arXiv: 2106.05466},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules, Quantitative Biology - Quantitative Methods},
	annote = {Comment: 9 pages, 2 figures},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/F52DVUYL/Hie and Yang - 2021 - Adaptive machine learning for protein engineering.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/GIMS6SX2/2106.html:text/html},
}

@article{parr_matrix_2018-1,
	title = {The {Matrix} {Calculus} {You} {Need} {For} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1802.01528},
	abstract = {This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don't worry if you get stuck at some point along the way---just go back and reread the previous section, and try writing down and working through some examples. And if you're still stuck, we're happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here. See related articles at http://explained.ai},
	urldate = {2021-11-16},
	journal = {arXiv:1802.01528 [cs, stat]},
	author = {Parr, Terence and Howard, Jeremy},
	month = jul,
	year = {2018},
	note = {arXiv: 1802.01528},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: PDF version of mobile/web friendly version http://explained.ai/matrix-calculus/index.html},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/QPXFYTGI/Parr and Howard - 2018 - The Matrix Calculus You Need For Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/AFNRRPYT/1802.html:text/html},
}

@article{ha_hypernetworks_2016,
	title = {{HyperNetworks}},
	url = {http://arxiv.org/abs/1609.09106},
	abstract = {This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.},
	urldate = {2021-11-17},
	journal = {arXiv:1609.09106 [cs]},
	author = {Ha, David and Dai, Andrew and Le, Quoc V.},
	month = dec,
	year = {2016},
	note = {arXiv: 1609.09106},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/7E3DKPF9/Ha et al. - 2016 - HyperNetworks.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/6SBZZUEN/1609.html:text/html},
}

@misc{noauthor_-device_nodate,
	title = {On-device {Panoptic} {Segmentation} for {Camera} {Using} {Transformers}},
	url = {https://machinelearning.apple.com/research/panoptic-segmentation},
	abstract = {Camera (in iOS and iPadOS) relies on a wide range of scene-understanding technologies to develop images. In particular, pixel-level…},
	language = {en-US},
	urldate = {2021-11-17},
	journal = {Apple Machine Learning Research},
	file = {Snapshot:/home/zwerg/Zotero/storage/3NZU73WG/panoptic-segmentation.html:text/html},
}

@article{carion_end--end_2020-1,
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	url = {http://arxiv.org/abs/2005.12872},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	urldate = {2021-11-17},
	journal = {arXiv:2005.12872 [cs]},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	month = may,
	year = {2020},
	note = {arXiv: 2005.12872},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/VDMYEG2X/Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/XBTHTACN/2005.html:text/html},
}

@article{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	urldate = {2021-11-17},
	journal = {arXiv:1711.05101 [cs, math]},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv: 1711.05101},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	annote = {Comment: Published as a conference paper at ICLR 2019},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/ACHDJZBG/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/I2QTL69E/1711.html:text/html},
}

@article{safran_quality_2016,
	title = {On the {Quality} of the {Initial} {Basin} in {Overspecified} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04210},
	abstract = {Deep learning, in the form of artificial neural networks, has achieved remarkable practical success in recent years, for a variety of difficult machine learning applications. However, a theoretical explanation for this remains a major open problem, since training neural networks involves optimizing a highly non-convex objective function, and is known to be computationally hard in the worst case. In this work, we study the {\textbackslash}emph\{geometric\} structure of the associated non-convex objective function, in the context of ReLU networks and starting from a random initialization of the network parameters. We identify some conditions under which it becomes more favorable to optimization, in the sense of (i) High probability of initializing at a point from which there is a monotonically decreasing path to a global minimum; and (ii) High probability of initializing at a basin (suitably defined) with a small minimal objective value. A common theme in our results is that such properties are more likely to hold for larger ("overspecified") networks, which accords with some recent empirical and theoretical observations.},
	urldate = {2021-11-17},
	journal = {arXiv:1511.04210 [cs, stat]},
	author = {Safran, Itay and Shamir, Ohad},
	month = jun,
	year = {2016},
	note = {arXiv: 1511.04210},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/7IRWF23K/Safran and Shamir - 2016 - On the Quality of the Initial Basin in Overspecifi.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/7B5G2EUI/1511.html:text/html},
}

@article{li_visualizing_2018,
	title = {Visualizing the {Loss} {Landscape} of {Neural} {Nets}},
	url = {http://arxiv.org/abs/1712.09913},
	abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
	urldate = {2021-11-17},
	journal = {arXiv:1712.09913 [cs, stat]},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	month = nov,
	year = {2018},
	note = {arXiv: 1712.09913},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: NIPS 2018 (extended version, 10.5 pages), code is available at https://github.com/tomgoldstein/loss-landscape},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/N6THRPUA/Li et al. - 2018 - Visualizing the Loss Landscape of Neural Nets.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/ELFWYRUX/1712.html:text/html},
}

@article{goodfellow_qualitatively_2015,
	title = {Qualitatively characterizing neural network optimization problems},
	url = {http://arxiv.org/abs/1412.6544},
	abstract = {Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.},
	urldate = {2021-11-17},
	journal = {arXiv:1412.6544 [cs, stat]},
	author = {Goodfellow, Ian J. and Vinyals, Oriol and Saxe, Andrew M.},
	month = may,
	year = {2015},
	note = {arXiv: 1412.6544},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/RRIFXMXE/Goodfellow et al. - 2015 - Qualitatively characterizing neural network optimi.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/ZXGDL4HF/1412.html:text/html},
}

@article{zhang_baseline_2010-2,
	title = {Baseline correction using adaptive iteratively reweighted penalized least squares},
	volume = {135},
	issn = {1364-5528},
	doi = {10.1039/b922045c},
	abstract = {Baseline drift always blurs or even swamps signals and deteriorates analytical results, particularly in multivariate analysis. It is necessary to correct baseline drift to perform further data analysis. Simple or modified polynomial fitting has been found to be effective to some extent. However, this method requires user intervention and is prone to variability especially in low signal-to-noise ratio environments. A novel algorithm named adaptive iteratively reweighted Penalized Least Squares (airPLS) that does not require any user intervention and prior information, such as peak detection etc., is proposed in this work. The method works by iteratively changing weights of sum squares errors (SSE) between the fitted baseline and original signals, and the weights of the SSE are obtained adaptively using the difference between the previously fitted baseline and the original signals. The baseline estimator is fast and flexible. Theory, implementation, and applications in simulated and real datasets are presented. The algorithm is implemented in R language and MATLAB, which is available as open source software (http://code.google.com/p/airpls).},
	language = {eng},
	number = {5},
	journal = {The Analyst},
	author = {Zhang, Zhi-Min and Chen, Shan and Liang, Yi-Zeng},
	month = may,
	year = {2010},
	pmid = {20419267},
	keywords = {Algorithms, Software, Least-Squares Analysis, Magnetic Resonance Spectroscopy, Models, Statistical, Spectrum Analysis, Raman},
	pages = {1138--1146},
}

@misc{noauthor_pdf_nodate-2,
	title = {[{PDF}] {Background} removal from spectra by designing and minimising a non-quadratic cost function {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/Background-removal-from-spectra-by-designing-and-a-Mazet-Carteret/045cadceeaea7436bd784220c1d997849cebff19},
	urldate = {2021-11-18},
	file = {[PDF] Background removal from spectra by designing and minimising a non-quadratic cost function | Semantic Scholar:/home/zwerg/Zotero/storage/MHQ4X8BB/045cadceeaea7436bd784220c1d997849cebff19.html:text/html},
}

@misc{brooks_features_2021,
	title = {Features},
	copyright = {MIT},
	url = {https://github.com/jsbroks/coco-annotator},
	abstract = {:pencil2: Web-based image segmentation tool for object detection, localization, and keypoints},
	urldate = {2021-11-19},
	author = {Brooks, Justin},
	month = nov,
	year = {2021},
	note = {original-date: 2018-09-03T21:38:31Z},
	keywords = {computer-vision, deep-learning, image-annotation, image-labeling, annotate-images, coco, coco-annotator, coco-format, datasets, detection, image-segmentation, label, machine-learning},
}

@misc{darrenl_labelimg_2021,
	title = {{LabelImg}},
	copyright = {MIT},
	url = {https://github.com/tzutalin/labelImg},
	abstract = {🖍️ LabelImg is a graphical image annotation tool and label object bounding boxes in images},
	urldate = {2021-11-19},
	author = {darrenl},
	month = nov,
	year = {2021},
	note = {original-date: 2015-09-17T01:33:59Z},
	keywords = {deep-learning, detection, annotations, image-classification, imagenet, python2, python3, recognition, tools},
}

@article{kappel_dynamics_2009,
	title = {Dynamics of chromatin structure and nuclear multiprotein complexes investigated by quantitative fluorescence live cell microscopy and computational modeling},
	url = {http://archiv.ub.uni-heidelberg.de/volltextserver/id/eprint/9786},
	doi = {10.11588/HEIDOK.00009786},
	abstract = {Biology has rapidly been transformed into a mainly data-driven, quantitative science. Demands on biological imaging are moving towards quantitative annotations of genes in vivo. In this work I have studied in detail the spatio-temporal distribution and the molecular interaction of protein ensembles as well as of multiprotein aggregates. I have provided the methodology to estimate biophysical parameters such as diffusion coefficients, anomalous diffusion and the free fraction in the binding equilibrium of protein ensembles using fluorescence photobleaching analysis and numcerical modeling and parameter estimation. On the side of protein complexes I have extended existing single particle tracking approaches to allow to automatically detect the exact timing of mobility changes of single particles in live cells. Here, I was able to provide quantitative parameters also on the diffusion coefficient, anomalous diffusion, velocity and chromatin interaction. The nuclear protein ensemble I studied was murine linker histone H1° fused to GFP. I was able to show that diffusion and binding of H1°-GFP to chromatin can be addressed using photobleaching analysis and numcerical modeling. I have thus obtained diffusion coefficients for wild-type H1° and seven point mutants with differential binding affinity ranging from D = 0.01 mm²/s (strongest binder) to D = 0.1 mm²/s (weakest binder). Likewise, I was able to estimate the free fraction to range from = 400 ppm to = 3000 ppm. Exemplary of large multiprotein complexes I chose PML nuclear bodies (PML NBs), named after their constituent promyelotic leukemia protein. I studied in detail their dynamic mobility during early mitosis, ranging from prophase to prometaphase. A dramatic global increase in PML NB mobility was found during this period with the diffusion coefficient increasing from D = 0.001 mm²/s at interphase to D = 0.005 mm²/s at prophase. Similarly, velocities increased from v = 0.7 mm/min to v = 1.4mm/min and concomittant with a loss in subdiffusive motion. I was able to establish loss of tethering to chromatin as the most likely reason behind this increase as opposed to material flow or chromatin condensation. Lastly, I was also able to relate the timing of the mobility increase to other important cellular events. The increase of PML NB mobility predominantly occured after nuclear entry of cyclin B1, which irreversibly commits the cell to mitosis, and before nuclear envelope breakdown (NEBD).},
	urldate = {2021-11-20},
	author = {Kappel, Norman Constantin},
	year = {2009},
	note = {Publisher: Heidelberg University Library},
	keywords = {570 Life sciences},
	file = {Dissertation_Norman_Constantin_Kappel.pdf:/home/zwerg/Zotero/storage/HM29EHFM/Dissertation_Norman_Constantin_Kappel.pdf:application/pdf},
}

@article{alvarez_bioactive_2021,
	title = {Bioactive scaffolds with enhanced supramolecular motion promote recovery from spinal cord injury},
	volume = {374},
	url = {https://www.science.org/doi/10.1126/science.abh3602},
	doi = {10.1126/science.abh3602},
	number = {6569},
	urldate = {2021-11-20},
	journal = {Science},
	author = {Álvarez, Z. and Kolberg-Edelbrock, A. N. and Sasselli, I. R. and Ortega, J. A. and Qiu, R. and Syrgiannis, Z. and Mirau, P. A. and Chen, F. and Chin, S. M. and Weigand, S. and Kiskinis, E. and Stupp, S. I.},
	month = nov,
	year = {2021},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {848--856},
}

@misc{noauthor_file_nodate,
	title = {File {Format} {Guide}},
	url = {https://www.ncbi.nlm.nih.gov/sra/docs/submitformats/},
	urldate = {2021-11-23},
	file = {File Format Guide:/home/zwerg/Zotero/storage/TCIS29QB/submitformats.html:text/html},
}

@article{lachmann_massive_2018,
	title = {Massive mining of publicly available {RNA}-seq data from human and mouse},
	volume = {9},
	copyright = {2018 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-018-03751-6},
	doi = {10.1038/s41467-018-03751-6},
	abstract = {RNA sequencing (RNA-seq) is the leading technology for genome-wide transcript quantification. However, publicly available RNA-seq data is currently provided mostly in raw form, a significant barrier for global and integrative retrospective analyses. ARCHS4 is a web resource that makes the majority of published RNA-seq data from human and mouse available at the gene and transcript levels. For developing ARCHS4, available FASTQ files from RNA-seq experiments from the Gene Expression Omnibus (GEO) were aligned using a cloud-based infrastructure. In total 187,946 samples are accessible through ARCHS4 with 103,083 mouse and 84,863 human. Additionally, the ARCHS4 web interface provides intuitive exploration of the processed data through querying tools, interactive visualization, and gene pages that provide average expression across cell lines and tissues, top co-expressed genes for each gene, and predicted biological functions and protein–protein interactions for each gene based on prior knowledge combined with co-expression.},
	language = {en},
	number = {1},
	urldate = {2021-11-23},
	journal = {Nature Communications},
	author = {Lachmann, Alexander and Torre, Denis and Keenan, Alexandra B. and Jagodnik, Kathleen M. and Lee, Hoyjin J. and Wang, Lily and Silverstein, Moshe C. and Ma’ayan, Avi},
	month = apr,
	year = {2018},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computer science;Data integration;Data processing
Subject\_term\_id: computer-science;data-integration;data-processing},
	keywords = {Computer science, Data integration, Data processing},
	pages = {1366},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/C2ABVZZP/Lachmann et al. - 2018 - Massive mining of publicly available RNA-seq data .pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/SIVNLMED/s41467-018-03751-6.html:text/html},
}

@misc{noauthor_notitle_nodate,
	url = {https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjX58mMu6v0AhVDuqQKHbu4CzUQFnoECBYQAQ&url=https%3A%2F%2Fwww.mdpi.com%2F1424-8220%2F20%2F19%2F5567%2Fpdf&usg=AOvVaw0wf5S7cNm8HXhrbbRZUCVZ},
	urldate = {2021-11-22},
	file = {https\://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjX58mMu6v0AhVDuqQKHbu4CzUQFnoECBYQAQ&url=https%3A%2F%2Fwww.mdpi.com%2F1424-8220%2F20%2F19%2F5567%2Fpdf&usg=AOvVaw0wf5S7cNm8HXhrbbRZUCVZ:/home/zwerg/Zotero/storage/GA2TDMJU/url.html:text/html},
}

@article{kollar_simnet_nodate,
	title = {{SimNet}: {Enabling} {Robust} {Unknown} {Object} {Manipulation} from {Pure} {Synthetic} {Data} via {Stereo}},
	abstract = {Robot manipulation of unknown objects in unstructured environments is a challenging problem due to the variety of shapes, materials, arrangements and lighting conditions. Even with large-scale real-world data collection, robust perception and manipulation of transparent and reﬂective objects across various lighting conditions remains challenging. To address these challenges we propose an approach to performing sim-to-real transfer of robotic perception. The underlying model, SimNet, is trained as a single multi-headed neural network using simulated stereo data as input and simulated object segmentation masks, 3D oriented bounding boxes (OBBs), object keypoints and disparity as output. A key component of SimNet is the incorporation of a learned stereo sub-network that predicts disparity. SimNet is evaluated on unknown object detection and deformable object keypoint detection and signiﬁcantly outperforms a baseline that uses a structured light RGB-D sensor. By inferring grasp positions using the OBB and keypoint predictions, SimNet can be used to perform end-to-end manipulation of unknown objects across our ﬂeet of Toyota HSR robots. In object grasping experiments, SimNet signiﬁcantly outperforms the RBG-D baseline on optically challenging objects, suggesting that SimNet can enable robust manipulation of unknown objects, including transparent objects, in novel environments. Additional visualizations and materials are located at https://tinyurl.com/simnet-corl.},
	language = {en},
	author = {Kollar, Thomas and Laskey, Michael and Stone, Kevin and Thananjeyan, Brijen and Tjersland, Mark},
	pages = {11},
	file = {Kollar et al. - SimNet Enabling Robust Unknown Object Manipulatio.pdf:/home/zwerg/Zotero/storage/LCFMP5ZJ/Kollar et al. - SimNet Enabling Robust Unknown Object Manipulatio.pdf:application/pdf},
}

@article{liu_non-local_2018,
	title = {Non-local mean filtering algorithm based on deep learning},
	volume = {232},
	doi = {10.1051/matecconf/201823203025},
	abstract = {Aimed at the problem that the traditional image denoising algorithm is not effective in noise reduction, a new image denoising method is proposed. The method combines deep learning and non-local mean filtering algorithms to denoise the noisy image to obtain better noise reduction effect. By comparing with Gaussian filtering algorithm, median filtering algorithm, bilateral filtering algorithm and early non-local mean filtering algorithm, the noise reduction effect of the new algorithm is better than the traditional method and the peak signal to noise ratio is compared with the early non-local mean algorithm. The performance is better.},
	journal = {MATEC Web of Conferences},
	author = {Liu, Baozhong and Liu, Jianbin},
	month = jan,
	year = {2018},
	pages = {03025},
	file = {Full Text:/home/zwerg/Zotero/storage/95MUNLHS/Liu and Liu - 2018 - Non-local mean filtering algorithm based on deep l.pdf:application/pdf},
}

@article{qin_multi-scale_2021,
	title = {A multi-scale map of cell structure fusing protein images and interactions},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-04115-9},
	doi = {10.1038/s41586-021-04115-9},
	abstract = {The cell is a multi-scale structure with modular organization across at least four orders of magnitude1. Two central approaches for mapping this structure—protein fluorescent imaging and protein biophysical association—each generate extensive datasets, but of distinct qualities and resolutions that are typically treated separately2,3. Here we integrate immunofluorescence images in the Human Protein Atlas4 with affinity purifications in BioPlex5 to create a unified hierarchical map of human cell architecture. Integration is achieved by configuring each approach as a general measure of protein distance, then calibrating the two measures using machine learning. The map, known as the multi-scale integrated cell (MuSIC 1.0), resolves 69 subcellular systems, of which approximately half are to our knowledge undocumented. Accordingly, we perform 134 additional affinity purifications and validate subunit associations for the majority of systems. The map reveals a pre-ribosomal RNA processing assembly and accessory factors, which we show govern rRNA maturation, and functional roles for SRRM1 and FAM120C in chromatin and RPS3A in splicing. By integration across scales, MuSIC increases the resolution of imaging while giving protein interactions a spatial dimension, paving the way to incorporate diverse types of data in proteome-wide cell maps.},
	language = {en},
	urldate = {2021-11-26},
	journal = {Nature},
	author = {Qin, Yue and Huttlin, Edward L. and Winsnes, Casper F. and Gosztyla, Maya L. and Wacheul, Ludivine and Kelly, Marcus R. and Blue, Steven M. and Zheng, Fan and Chen, Michael and Schaffer, Leah V. and Licon, Katherine and Bäckström, Anna and Vaites, Laura Pontano and Lee, John J. and Ouyang, Wei and Liu, Sophie N. and Zhang, Tian and Silva, Erica and Park, Jisoo and Pitea, Adriana and Kreisberg, Jason F. and Gygi, Steven P. and Ma, Jianzhu and Harper, J. Wade and Yeo, Gene W. and Lafontaine, Denis L. J. and Lundberg, Emma and Ideker, Trey},
	month = nov,
	year = {2021},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computational models;Data integration;Machine learning;Network topology;Proteome informatics
Subject\_term\_id: computational-models;data-integration;machine-learning;network-topology;proteome-informatics},
	keywords = {Machine learning, Network topology, Data integration, Computational models, Proteome informatics},
	pages = {1--7},
	file = {_.pdf:/home/zwerg/Zotero/storage/6XF8FSJY/_.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/Y5HYVRPI/s41586-021-04115-9.html:text/html},
}

@misc{noauthor_annotated_nodate,
	title = {The {Annotated} {Transformer}},
	url = {http://nlp.seas.harvard.edu/2018/04/03/attention.html},
	urldate = {2021-11-25},
	file = {The Annotated Transformer:/home/zwerg/Zotero/storage/6UICYLV5/attention.html:text/html},
}

@inproceedings{klein_opennmt_2017,
	address = {Vancouver, Canada},
	title = {{OpenNMT}: {Open}-{Source} {Toolkit} for {Neural} {Machine} {Translation}},
	shorttitle = {{OpenNMT}},
	url = {http://aclweb.org/anthology/P17-4012},
	doi = {10.18653/v1/P17-4012},
	language = {en},
	urldate = {2021-11-25},
	booktitle = {Proceedings of {ACL} 2017, {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Klein, Guillaume and Kim, Yoon and Deng, Yuntian and Senellart, Jean and Rush, Alexander},
	year = {2017},
	pages = {67--72},
	file = {Full Text:/home/zwerg/Zotero/storage/C62HXNY5/Klein et al. - 2017 - OpenNMT Open-Source Toolkit for Neural Machine Tr.pdf:application/pdf},
}

@article{devlin_bert_2019-1,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2021-11-25},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/E7BHD428/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/IMUVP2L2/1810.html:text/html},
}

@article{deepika_recent_2021,
	title = {Recent innovations in properties of nanostructured glasses and composites},
	volume = {16},
	issn = {1745-8080},
	url = {https://doi.org/10.1080/17458080.2021.1940221},
	doi = {10.1080/17458080.2021.1940221},
	abstract = {The thrust to invent new materials and technology has been ever increasing due to technological challenges in progressive world. Among new materials, nanomaterials possess superior optical, electrical, magnetic, mechanical, and thermal properties, which have made them suitable for a multitude of applications. The present review paper deals with recent advances in properties of nanostructured glasses and composites in terms optical, electrical, mechanical and thermal properties. A brief discussion has been done on fabrication method of nanostructured glasses. The review of optical properties shows that nanostructured glasses show both direct and indirect band gap and this tuning of band gap depends on extent of nano structuring of samples. The electrical properties also show enhancement in electrical conductivity on nano-structuring of glasses compared to their bulk counterparts. The changes in mechanical and thermal properties of nanostructured glasses and composites are attributed to many microstructural features like grain size and shape, their distribution, pores and their distribution, other flaws/defects and their distribution, surface condition, impurity level, second phases/dopants, stress, duration of its application and temperature effect on the samples. Literature reports that nano structuring leads to enhanced phonon boundary scattering which reduces the thermal conductivity.},
	number = {1},
	urldate = {2021-11-26},
	journal = {Journal of Experimental Nanoscience},
	author = {{Deepika} and Dixit, Manasvi and Singh, Hukum and Attia, M. S. and Amin, Mohammed A.},
	month = jan,
	year = {2021},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/17458080.2021.1940221},
	keywords = {electrical, Glasses, mechanical, nanostructured, optical, thermal},
	pages = {181--212},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/TR9R4CY4/Deepika et al. - 2021 - Recent innovations in properties of nanostructured.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/AC3RLCPF/17458080.2021.html:text/html},
}

@article{radtke_laser-lithography_2007,
	title = {Laser-lithography on non-planar surfaces},
	volume = {15},
	copyright = {\&\#169; 2007 Optical Society of America},
	issn = {1094-4087},
	url = {https://www.osapublishing.org/oe/abstract.cfm?uri=oe-15-3-1167},
	doi = {10.1364/OE.15.001167},
	abstract = {An extensively modified laser-lithography system specially developed for realization of micro-optical profiles on non-planar surfaces is presented. This extended system offers new possibilities of fabricating micro-optical elements without the technology related restriction of surface shape that existed so far. A diffractive lens on a convex spherical substrate is designed and fabricated as an example for hybrid achromatic refractive-diffractive elements to demonstrate the functionality of the system and the wide range of possible new applications.},
	language = {EN},
	number = {3},
	urldate = {2021-11-27},
	journal = {Optics Express},
	author = {Radtke, Daniela and Zeitner, Uwe D.},
	month = feb,
	year = {2007},
	note = {Publisher: Optical Society of America},
	pages = {1167--1174},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/4XYZU9I3/Radtke and Zeitner - 2007 - Laser-lithography on non-planar surfaces.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/JGE2H3XU/fulltext.html:text/html},
}

@article{gao_cortical_2019,
	title = {Cortical {Column} and {Whole} {Brain} {Imaging} with {Molecular} {Contrast} and {Nanoscale} {Resolution}},
	volume = {363},
	issn = {0036-8075},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6481610/},
	doi = {10.1126/science.aau8302},
	abstract = {Optical and electron microscopy have made tremendous inroads in
understanding the complexity of the brain. However, optical microscopy offer
insufficient resolution to reveal subcellular details and electron microscopy
lacks the throughput and molecular contrast to visualize specific molecular
constituents over mm-scale or larger dimensions. We combined expansion
microscopy and lattice light-sheet microscopy to image the nanoscale spatial
relationships between proteins across the thickness of the mouse cortex or the
entire Drosophila brain. These included synaptic proteins at
dendritic spines, myelination along axons, and presynaptic densities at
dopaminergic neurons in every fly brain region. The technology should enable
statistically rich, large scale studies of neural development, sexual
dimorphism, degree of stereotypy, and structural correlations to behavior or
neural activity, all with molecular contrast.},
	number = {6424},
	urldate = {2021-11-27},
	journal = {Science (New York, N.Y.)},
	author = {Gao, Ruixuan and Asano, Shoh M. and Upadhyayula, Srigokul and Pisarev, Igor and Milkie, Daniel E. and Liu, Tsung-Li and Singh, Ved and Graves, Austin and Huynh, Grace H. and Zhao, Yongxin and Bogovic, John and Colonell, Jennifer and Ott, Carolyn M. and Zugates, Christopher and Tappan, Susan and Rodriguez, Alfredo and Mosaliganti, Kishore R. and Sheu, Shu-Hsien and Pasolli, H. Amalia and Pang, Song and Xu, C. Shan and Megason, Sean G. and Hess, Harald and Lippincott-Schwartz, Jennifer and Hantman, Adam and Rubin, Gerald M. and Kirchhausen, Tom and Saalfeld, Stephan and Aso, Yoshinori and Boyden, Edward S. and Betzig, Eric},
	month = jan,
	year = {2019},
	pmid = {30655415},
	pmcid = {PMC6481610},
	pages = {eaau8302},
	file = {PubMed Central Full Text PDF:/home/zwerg/Zotero/storage/9VN2S8E6/Gao et al. - 2019 - Cortical Column and Whole Brain Imaging with Molec.pdf:application/pdf},
}

@misc{noauthor_cortical_nodate,
	title = {Cortical minicolumn - {Wikipedia}},
	url = {https://en.wikipedia.org/wiki/Cortical_minicolumn},
	urldate = {2021-11-27},
	file = {Cortical minicolumn - Wikipedia:/home/zwerg/Zotero/storage/75LIFZB5/Cortical_minicolumn.html:text/html},
}

@misc{noauthor_cortical_2021,
	title = {Cortical column},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Cortical_column&oldid=1044565983},
	abstract = {A cortical column, also called hypercolumn, macrocolumn, functional column or sometimes cortical module, is a group of neurons in the cortex of the brain that can be successively penetrated by a probe inserted perpendicularly to the cortical surface, and which have nearly identical receptive fields. Neurons within a minicolumn (microcolumn) encode similar features, whereas a hypercolumn "denotes a unit containing a full set of values for any given set of receptive field parameters". A cortical module is defined as either synonymous with a hypercolumn (Mountcastle) or as a tissue block of multiple overlapping hypercolumns.The columnar hypothesis states that the cortex is composed of discrete, modular columns of neurons, characterized by a consistent connectivity profile.It is still unclear what precisely is meant by the term, and it does not correspond to any single structure within the cortex. It has been impossible to find a canonical microcircuit that corresponds to the cortical column, and no genetic mechanism has been deciphered that designates how to construct a column. However, the columnar organization hypothesis is currently the most widely adopted to explain the cortical processing of information.},
	language = {en},
	urldate = {2021-11-27},
	journal = {Wikipedia},
	month = sep,
	year = {2021},
	note = {Page Version ID: 1044565983},
	file = {Snapshot:/home/zwerg/Zotero/storage/S94684TV/index.html:text/html},
}

@misc{noauthor_synaptotropic_2020,
	title = {Synaptotropic hypothesis},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Synaptotropic_hypothesis&oldid=984794708},
	abstract = {The synaptotropic hypothesis, also called the synaptotrophic hypothesis, is a neurobiological hypothesis of neuronal growth and synapse formation. The hypothesis was first formulated by J.E. Vaughn in 1988, and remains a focus of current research efforts. The synaptotropic hypothesis proposes that input from a presynaptic to a postsynaptic cell (and maturation of excitatory synaptic inputs) eventually can change the course of synapse formation at dendritic and axonal arbors. This synapse formation is required for the development of neuronal structure in the functioning brain.},
	language = {en},
	urldate = {2021-11-27},
	journal = {Wikipedia},
	month = oct,
	year = {2020},
	note = {Page Version ID: 984794708},
	file = {Snapshot:/home/zwerg/Zotero/storage/CSGPZJZ2/index.html:text/html},
}

@misc{noauthor_spinal_nodate,
	title = {Spinal cord injury},
	url = {https://www.who.int/news-room/fact-sheets/detail/spinal-cord-injury},
	abstract = {WHO fact sheet on spinal cord injury provides key facts, understanding, demographic trends, mortality, health economic consequences, prevention and WHO response.},
	language = {en},
	urldate = {2021-11-28},
	file = {Snapshot:/home/zwerg/Zotero/storage/U5DTY2ZQ/spinal-cord-injury.html:text/html},
}

@misc{noauthor_wso_global_stroke_fact_sheetpdf_nodate,
	title = {{WSO}\_Global\_Stroke\_Fact\_Sheet.pdf},
	url = {https://www.world-stroke.org/assets/downloads/WSO_Global_Stroke_Fact_Sheet.pdf},
	urldate = {2021-11-28},
	file = {WSO_Global_Stroke_Fact_Sheet.pdf:/home/zwerg/Zotero/storage/CRU62L74/WSO_Global_Stroke_Fact_Sheet.pdf:application/pdf},
}

@misc{noauthor_nvidia_2020,
	title = {{NVIDIA} {Omniverse} {Platform}},
	url = {https://developer.nvidia.com/nvidia-omniverse-platform},
	abstract = {- Announcing NVIDIA Omniverse Replicator, a simulation framework for physically accurate synthetic data. LEARN MORE Develop with NVIDIA Omniverse NVIDIA Omniverse™ is a scalable, multi-GPU real-time reference development platform for 3D simulation and design collaboration, and based on Pixar's Universal Scene Description and NVIDIA RTX™ technology. Download Open Beta NVIDIA Omniverse is built from the ground up to be easily extensible and customizable with a modular development framework.},
	language = {en},
	urldate = {2021-11-28},
	journal = {NVIDIA Developer},
	month = apr,
	year = {2020},
	file = {Snapshot:/home/zwerg/Zotero/storage/EGAHYWZD/nvidia-omniverse-platform.html:text/html},
}

@article{lin_what_2009,
	title = {What are the basic concepts of temporal, contrast, and spatial resolution in cardiac {CT}?},
	volume = {3},
	issn = {1934-5925},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4752333/},
	doi = {10.1016/j.jcct.2009.07.003},
	abstract = {An imaging instrument can be characterized by its spatial resolution, contrast resolution, and temporal resolution. The capabilities of computed tomography (CT) relative to other cardiac imaging modalities can be understood in these terms. The purpose of this review is to characterize the spatial, contrast, and temporal resolutions of cardiac CT in practical terms.},
	number = {6},
	urldate = {2021-11-28},
	journal = {Journal of cardiovascular computed tomography},
	author = {Lin, Eugene and Alessio, Adam},
	year = {2009},
	pmid = {19717355},
	pmcid = {PMC4752333},
	pages = {403--408},
	file = {PubMed Central Full Text PDF:/home/zwerg/Zotero/storage/XCFGXSQ6/Lin and Alessio - 2009 - What are the basic concepts of temporal, contrast,.pdf:application/pdf},
}

@article{buxhoeveden_minicolumn_2002,
	title = {The minicolumn hypothesis in neuroscience},
	volume = {125},
	issn = {0006-8950},
	url = {https://doi.org/10.1093/brain/awf110},
	doi = {10.1093/brain/awf110},
	abstract = {The minicolumn is a continuing source of research and debate more than half a century after it was identified as a component of brain organization. The minicolumn is a sophisticated local network that contains within it the elements for redundancy and plasticity. Although it is sometimes compared to subcortical nuclei, the design of the minicolumn is a distinctive form of module that has evolved specifically in the neocortex. It unites the horizontal and vertical components of cortex within the same cortical space. Minicolumns are often considered highly repetitive, even clone‐like, units. However, they display considerable heterogeneity between areas and species, perhaps even within a given macrocolumn. Despite a growing recognition of the anatomical basis of the cortical minicolumn, as well as its physiological properties, the potential of the minicolumn has not been exploited in fields such as comparative neuroanatomy, abnormalities of the brain and mind, and evolution.},
	number = {5},
	urldate = {2021-11-28},
	journal = {Brain},
	author = {Buxhoeveden, Daniel P. and Casanova, Manuel F.},
	month = may,
	year = {2002},
	pages = {935--951},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/ZR5S37DJ/Buxhoeveden and Casanova - 2002 - The minicolumn hypothesis in neuroscience.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/YNP3YHNI/328135.html:text/html},
}

@misc{noauthor_cortical_2021-1,
	title = {Cortical minicolumn},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Cortical_minicolumn&oldid=1040682347},
	abstract = {A cortical minicolumn is a vertical column through the cortical layers of the brain. Neurons within the microcolumn "receive common inputs, have common outputs, are interconnected, and may well constitute a fundamental computational unit of the cerebral cortex". Minicolumns comprise perhaps 80–120 neurons, except in the primate primary visual cortex (V1), where there are typically more than twice the number.  There are about 2×108 minicolumns in humans. From calculations, the diameter of a minicolumn is about 28–40 μm. Minicolumns grow from progenitor cells within the embryo and contain neurons within multiple layers (2–6) of the cortex.Many sources support the existence of minicolumns, especially Mountcastle, with strong evidence reviewed by Buxhoeveden and Casanova who conclude "...  the minicolumn must be considered a strong model for cortical organization" and "[the minicolumn is] the most basic and consistent template by which the neocortex organizes its neurones, pathways, and intrinsic circuits".
Cortical minicolumns can also be called cortical microcolumns. Cells in 50 μm minicolumn all have the same receptive field; adjacent minicolumns may have different fields.Although many studies have observed Neurons structured into cortical columns, it is still widely disputed as to whether they serve a purpose or are even the smallest functional unit of the cortex, or whether Neurons are simply packed in such a way purely for efficient space usage and that the columns otherwise have no other role in Brain function.},
	language = {en},
	urldate = {2021-11-28},
	journal = {Wikipedia},
	month = aug,
	year = {2021},
	note = {Page Version ID: 1040682347},
	file = {Snapshot:/home/zwerg/Zotero/storage/T9JQ767K/index.html:text/html},
}

@misc{studio_approach_nodate,
	title = {Approach},
	url = {https://neuralink.com/approach/},
	abstract = {Developing ultra high bandwidth brain-machine interfaces to connect humans and computers.},
	language = {en},
	urldate = {2021-11-28},
	journal = {Neuralink},
	author = {Studio, Play},
	file = {Snapshot:/home/zwerg/Zotero/storage/CQH4MN4X/approach.html:text/html},
}

@misc{noauthor_looking_nodate,
	title = {Looking {Glass} {Factory}. {The} {World}'s {Leading} {Holographic} {Display} {Company}. - {Looking} {Glass} {Factory}},
	url = {https://lookingglassfactory.com/},
	urldate = {2021-11-28},
	file = {Looking Glass Factory. The World's Leading Holographic Display Company. - Looking Glass Factory:/home/zwerg/Zotero/storage/MYMA2L2F/lookingglassfactory.com.html:text/html},
}

@misc{noauthor_photomask_nodate,
	title = {Photomask {Metrology} {Solutions}},
	url = {https://www.zeiss.com/semiconductor-manufacturing-technology/products-solutions/photomask-solutions/mask-metrology.html},
	abstract = {Photomask Metrology Solutions by ZEISS for registration metrology on photomasks},
	language = {en},
	urldate = {2021-11-28},
	file = {Snapshot:/home/zwerg/Zotero/storage/9H3RKREL/mask-metrology.html:text/html},
}

@article{raveh_bayesian_2021,
	title = {Bayesian metamodeling of complex biological systems across varying representations},
	volume = {118},
	doi = {10.1073/pnas.2104559118},
	abstract = {Comprehensive modeling of a whole cell requires an integration of vast amounts of information on various aspects of the cell and its parts. To divide and conquer this task, we introduce Bayesian metamodeling, a general approach to modeling complex systems by integrating a collection of heterogeneous input models. Each input model can in principle be based on any type of data and can describe a different aspect of the modeled system using any mathematical representation, scale, and level of granularity. These input models are 1) converted to a standardized statistical representation relying on probabilistic graphical models, 2) coupled by modeling their mutual relations with the physical world, and 3) finally harmonized with respect to each other. To illustrate Bayesian metamodeling, we provide a proof-of-principle metamodel of glucose-stimulated insulin secretion by human pancreatic β-cells. The input models include a coarse-grained spatiotemporal simulation of insulin vesicle trafficking, docking, and exocytosis; a molecular network model of glucose-stimulated insulin secretion signaling; a network model of insulin metabolism; a structural model of glucagon-like peptide-1 receptor activation; a linear model of a pancreatic cell population; and ordinary differential equations for systemic postprandial insulin response. Metamodeling benefits from decentralized computing, while often producing a more accurate, precise, and complete model that contextualizes input models as well as resolves conflicting information. We anticipate Bayesian metamodeling will facilitate collaborative science by providing a framework for sharing expertise, resources, data, and models, as exemplified by the Pancreatic β-Cell Consortium.},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Raveh, Barak and Sun, Liping and White, Kate and Sanyal, Tanmoy and Tempkin, Jeremy and Zheng, Dongqing and Bharath, Kala and Singla, Jitin and Wang, Chenxi and Zhao, Jihui and Li, Angdi and Graham, Nicholas and Kesselman, Carl and Stevens, Raymond and Sali, Andrej},
	month = aug,
	year = {2021},
	pages = {e2104559118},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/FFBMYWIB/Raveh et al. - 2021 - Bayesian metamodeling of complex biological system.pdf:application/pdf},
}

@article{noauthor_afid_nodate,
	title = {{AFid}: a tool for automated identification and exclusion of  autofluorescent objects from microscopy images},
	file = {_.pdf:/home/zwerg/Zotero/storage/LPPIC9LS/_.pdf:application/pdf},
}

@article{royer_practical_2018-1,
	title = {A practical guide to adaptive light-sheet microscopy},
	volume = {13},
	issn = {1754-2189, 1750-2799},
	url = {http://www.nature.com/articles/s41596-018-0043-4},
	doi = {10.1038/s41596-018-0043-4},
	language = {en},
	number = {11},
	urldate = {2021-11-30},
	journal = {Nature Protocols},
	author = {Royer, Loïc A. and Lemon, William C. and Chhetri, Raghav K. and Keller, Philipp J.},
	month = nov,
	year = {2018},
	pages = {2462--2500},
	file = {Royer et al. - 2018 - A practical guide to adaptive light-sheet microsco.pdf:/home/zwerg/Zotero/storage/6ZQCLZWP/Royer et al. - 2018 - A practical guide to adaptive light-sheet microsco.pdf:application/pdf},
}

@article{rivenson_toward_2018-1,
	title = {Toward a {Thinking} {Microscope}: {Deep} {Learning} in {Optical} {Microscopy} and {Image} {Reconstruction}},
	volume = {29},
	issn = {1047-6938, 1541-3721},
	shorttitle = {Toward a {Thinking} {Microscope}},
	url = {http://arxiv.org/abs/1805.08970},
	doi = {10.1364/OPN.29.7.000034},
	abstract = {We discuss recently emerging applications of the state-of-art deep learning methods on optical microscopy and microscopic image reconstruction, which enable new transformations among different modes and modalities of microscopic imaging, driven entirely by image data. We believe that deep learning will fundamentally change both the hardware and image reconstruction methods used in optical microscopy in a holistic manner.},
	number = {7},
	urldate = {2021-11-30},
	journal = {Optics and Photonics News},
	author = {Rivenson, Yair and Ozcan, Aydogan},
	month = jul,
	year = {2018},
	note = {arXiv: 1805.08970},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, I.2.10, I.2.6, 68T01, 68T05, 68U10, 62M45, 78M32, 92C50, 92C55, 94A08, I.2, I.2.1, I.3, I.3.3, I.4.3, I.4.4, I.4.9, J.3, Physics - Optics},
	pages = {34},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/2R6BEVXU/Rivenson and Ozcan - 2018 - Toward a Thinking Microscope Deep Learning in Opt.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/V7M6IJDZ/1805.html:text/html},
}

@article{rivenson_phase_2018,
	title = {Phase recovery and holographic image reconstruction using deep learning in neural networks},
	volume = {7},
	issn = {2047-7538},
	url = {http://arxiv.org/abs/1705.04286},
	doi = {10.1038/lsa.2017.141},
	abstract = {Phase recovery from intensity-only measurements forms the heart of coherent imaging techniques and holography. Here we demonstrate that a neural network can learn to perform phase recovery and holographic image reconstruction after appropriate training. This deep learning-based approach provides an entirely new framework to conduct holographic imaging by rapidly eliminating twin-image and self-interference related spatial artifacts. Compared to existing approaches, this neural network based method is significantly faster to compute, and reconstructs improved phase and amplitude images of the objects using only one hologram, i.e., requires less number of measurements in addition to being computationally faster. We validated this method by reconstructing phase and amplitude images of various samples, including blood and Pap smears, and tissue sections. These results are broadly applicable to any phase recovery problem, and highlight that through machine learning challenging problems in imaging science can be overcome, providing new avenues to design powerful computational imaging systems.},
	number = {2},
	urldate = {2021-11-30},
	journal = {Light: Science \& Applications},
	author = {Rivenson, Yair and Zhang, Yibo and Gunaydin, Harun and Teng, Da and Ozcan, Aydogan},
	month = feb,
	year = {2018},
	note = {arXiv: 1705.04286},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, I.2.10, I.2.6, I.2, I.2.1, I.4.9, Physics - Optics, 68T01, 68T05, 68U10, 62M45, 78M32, 92C55, 94A08, I.4.5, Physics - Applied Physics},
	pages = {17141--17141},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/5XXS5A74/Rivenson et al. - 2018 - Phase recovery and holographic image reconstructio.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/38ALZYBR/1705.html:text/html},
}

@article{ren_hrnet_2018,
	title = {{HRNet}: an end-to-end deep learning framework for digital holographic reconstruction},
	shorttitle = {{HRNet}},
	doi = {10.1117/1.AP.1.1.015002},
	abstract = {Abstract. Digital holography records the entire wavefront of an object, including amplitude and phase. To reconstruct the object numerically, we can backpropagate the hologram with Fresnel–Kirchhoff integral-based algorithms such as the angular spectrum method and the convolution method. Although effective, these techniques require prior knowledge, such as the object distance, the incident angle between the two beams, and the source wavelength. Undesirable zero-order and twin images have to be removed by an additional filtering operation, which is usually manual and consumes more time in off-axis configuration. In addition, for phase imaging, the phase aberration has to be compensated, and subsequently an unwrapping step is needed to recover the true object thickness. The former either requires additional hardware or strong assumptions, whereas the phase unwrapping algorithms are often sensitive to noise and distortion. Furthermore, for a multisectional object, an all-in-focus image and depth map are desired for many applications, but current approaches tend to be computationally demanding. We propose an end-to-end deep learning framework, HRNet, to tackle these holographic reconstruction problems. Through this data-driven approach, we show that it is possible to reconstruct a noise-free image that does not require any prior knowledge and can handle phase imaging as well as depth map generation.},
	author = {Ren, Zhenbo and Xu, Zhimin and Lam, E.},
	year = {2018},
}

@article{tian_computational_2015,
	title = {Computational illumination for high-speed in vitro {Fourier} ptychographic microscopy},
	volume = {2},
	copyright = {\&\#169; 2015 Optical Society of America},
	issn = {2334-2536},
	url = {https://www.osapublishing.org/optica/abstract.cfm?uri=optica-2-10-904},
	doi = {10.1364/OPTICA.2.000904},
	abstract = {We demonstrate a new computational illumination technique that achieves a large space\&\#x2013;bandwidth\&\#x2013;time product, for quantitative phase imaging of unstained live samples in vitro. Microscope lenses can have either a large field of view (FOV) or high resolution, and not both. Fourier ptychographic microscopy (FPM) is a new computational imaging technique that circumvents this limit by fusing information from multiple images taken with different illumination angles. The result is a gigapixel-scale image having both a wide FOV and high resolution, i.e., a large space\&\#x2013;bandwidth product. FPM has enormous potential for revolutionizing microscopy and has already found application in digital pathology. However, it suffers from long acquisition times (of the order of minutes), limiting throughput. Faster capture times would not only improve the imaging speed, but also allow studies of live samples, where motion artifacts degrade results. In contrast to fixed (e.g.,\&\#xA0;pathology) slides, live samples are continuously evolving at various spatial and temporal scales. Here, we present a new source coding scheme, along with real-time hardware control, to achieve 0.8\&\#xA0;NA resolution across a 4\&\#xD7; FOV with subsecond capture times. We propose an improved algorithm and a new initialization scheme, which allow robust phase reconstruction over long time-lapse experiments. We present the first FPM results for both growing and confluent in vitro cell cultures, capturing videos of subcellular dynamical phenomena in popular cell lines undergoing division and migration. Our method opens up FPM to applications with live samples, for observing rare events in both space and time.},
	language = {EN},
	number = {10},
	urldate = {2021-11-30},
	journal = {Optica},
	author = {Tian, Lei and Liu, Ziji and Yeh, Li-Hao and Chen, Michael and Zhong, Jingshan and Waller, Laura},
	month = oct,
	year = {2015},
	note = {Publisher: Optical Society of America},
	pages = {904--911},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/8CSEAED2/Tian et al. - 2015 - Computational illumination for high-speed in vitro.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/IQUCX3LZ/fulltext.html:text/html},
}

@article{gutzler_coherent_2010,
	title = {Coherent aperture-synthesis, wide-field, high-resolution holographic microscopy of biological tissue},
	volume = {35},
	copyright = {© 2010 Optical Society of America},
	issn = {1539-4794},
	url = {https://www.osapublishing.org/ol/abstract.cfm?uri=ol-35-8-1136},
	doi = {10.1364/OL.35.001136},
	abstract = {We show for the first time, to our knowledge, high-resolution wide-field images of biological samples recorded using coherent aperture-synthesis Fourier holography. To achieve this, we combined off-axis plane-wave polarized illumination with an axial sample rotation and polarization-sensitive collection of backscattered light. We synthesized 180 Fourier holograms using an efficient postdetection phase-matching correlation scheme. The result was an annular spatial frequency-space synthetic aperture (NA=0.93) with an effective area 25 times larger than that due to a single hologram. A high-resolution high-contrast microscopic reconstruction of biological tissue was computed over a sample area of 9 mm2 from holograms acquired at 34 mm working distance.},
	language = {EN},
	number = {8},
	urldate = {2021-11-30},
	journal = {Optics Letters},
	author = {Gutzler, Thomas and Hillman, Timothy R. and Alexandrov, Sergey A. and Sampson, David D.},
	month = apr,
	year = {2010},
	note = {Publisher: Optical Society of America},
	keywords = {Biological imaging, Holographic microscopy, Synthetic aperture imaging, Image quality, Aperture synthesis, Spatial frequency},
	pages = {1136--1138},
}

@article{tippie_high-resolution_2011,
	title = {High-resolution synthetic-aperture digital holography with digital phase and pupil correction},
	volume = {19},
	copyright = {\&\#169; 2011 OSA},
	issn = {1094-4087},
	url = {https://www.osapublishing.org/oe/abstract.cfm?uri=oe-19-13-12027},
	doi = {10.1364/OE.19.012027},
	abstract = {A 218 mega-pixel synthetic aperture was collected by raster scanning a CCD detector in a digital holography imaging experiment. Frames were mosaicked together using a two-step cross-correlation registration. Phase correction using sharpness metrics were utilized to achieve diffraction-limited resolution.},
	language = {EN},
	number = {13},
	urldate = {2021-11-30},
	journal = {Optics Express},
	author = {Tippie, Abbie E. and Kumar, Abhishek and Fienup, James R.},
	month = jun,
	year = {2011},
	note = {Publisher: Optical Society of America},
	pages = {12027--12038},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/5XDVR3MD/Tippie et al. - 2011 - High-resolution synthetic-aperture digital hologra.pdf:application/pdf},
}

@article{zheng_wide-field_2013,
	title = {Wide-field, high-resolution {Fourier} ptychographic microscopy},
	volume = {7},
	copyright = {2013 Nature Publishing Group},
	issn = {1749-4893},
	url = {https://www.nature.com/articles/nphoton.2013.187},
	doi = {10.1038/nphoton.2013.187},
	abstract = {We report an imaging method, termed Fourier ptychographic microscopy (FPM), which iteratively stitches together a number of variably illuminated, low-resolution intensity images in Fourier space to produce a wide-field, high-resolution complex sample image. By adopting a wavefront correction strategy, the FPM method can also correct for aberrations and digitally extend a microscope's depth of focus beyond the physical limitations of its optics.As a demonstration, we built a microscope prototype with a half-pitch resolution of 0.78 µm, a field of view of ∼120 mm2 and a resolution-invariant depth of focus of 0.3 mm (characterized at 632 nm). Gigapixel colour images of histology slides verify successful FPM operation. The reported imaging procedure transforms the general challenge of high-throughput, high-resolution microscopy from one that is coupled to the physical limitations of the system's optics to one that is solvable through computation.},
	language = {en},
	number = {9},
	urldate = {2021-11-30},
	journal = {Nature Photonics},
	author = {Zheng, Guoan and Horstmeyer, Roarke and Yang, Changhuei},
	month = sep,
	year = {2013},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 9
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Imaging and sensing
Subject\_term\_id: imaging-and-sensing},
	keywords = {Imaging and sensing},
	pages = {739--745},
	file = {Accepted Version:/home/zwerg/Zotero/storage/DUCB38BU/Zheng et al. - 2013 - Wide-field, high-resolution Fourier ptychographic .pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/R6G8LDB5/nphoton.2013.html:text/html},
}

@article{yeh_experimental_2015,
	title = {Experimental robustness of {Fourier} {Ptychography} phase retrieval algorithms},
	volume = {23},
	issn = {1094-4087},
	url = {http://arxiv.org/abs/1511.02986},
	doi = {10.1364/OE.23.033214},
	abstract = {Fourier ptychography is a new computational microscopy technique that provides gigapixel-scale intensity and phase images with both wide field-of-view and high resolution. By capturing a stack of low-resolution images under different illumination angles, a nonlinear inverse algorithm can be used to computationally reconstruct the high-resolution complex field. Here, we compare and classify multiple proposed inverse algorithms in terms of experimental robustness. We find that the main sources of error are noise, aberrations and mis-calibration (i.e. model mis-match). Using simulations and experiments, we demonstrate that the choice of cost function plays a critical role, with amplitude-based cost functions performing better than intensity-based ones. The reason for this is that Fourier ptychography datasets consist of images from both brightfield and darkfield illumination, representing a large range of measured intensities. Both noise (e.g. Poisson noise) and model mis-match errors are shown to scale with intensity. Hence, algorithms that use an appropriate cost function will be more tolerant to both noise and model mis-match. Given these insights, we propose a global Newton's method algorithm which is robust and computationally efficient. Finally, we discuss the impact of procedures for algorithmic correction of aberrations and mis-calibration.},
	number = {26},
	urldate = {2021-11-30},
	journal = {Optics Express},
	author = {Yeh, Li-Hao and Dong, Jonathan and Zhong, Jingshan and Tian, Lei and Chen, Michael and Tang, Gongguo and Soltanolkotabi, Mahdi and Waller, Laura},
	month = dec,
	year = {2015},
	note = {arXiv: 1511.02986},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Physics - Optics},
	pages = {33214},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/IJTZ6R9P/Yeh et al. - 2015 - Experimental robustness of Fourier Ptychography ph.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/FSFPHAPZ/1511.html:text/html},
}

@article{horstmeyer_solving_2015,
	title = {Solving ptychography with a convex relaxation},
	volume = {17},
	issn = {1367-2630},
	url = {https://doi.org/10.1088/1367-2630/17/5/053044},
	doi = {10.1088/1367-2630/17/5/053044},
	abstract = {Ptychography is a powerful computational imaging technique that transforms a collection of low-resolution images into a high-resolution sample reconstruction. Unfortunately, algorithms that currently solve this reconstruction problem lack stability, robustness, and theoretical guarantees. Recently, convex optimization algorithms have improved the accuracy and reliability of several related reconstruction efforts. This paper proposes a convex formulation of the ptychography problem. This formulation has no local minima, it can be solved using a wide range of algorithms, it can incorporate appropriate noise models, and it can include multiple a priori constraints. The paper considers a specific algorithm, based on low-rank factorization, whose runtime and memory usage are near-linear in the size of the output image. Experiments demonstrate that this approach offers a 25\% lower background variance on average than alternating projections, the ptychographic reconstruction algorithm that is currently in widespread use.},
	language = {en},
	number = {5},
	urldate = {2021-11-30},
	journal = {New Journal of Physics},
	author = {Horstmeyer, Roarke and Chen, Richard Y. and Ou, Xiaoze and Ames, Brendan and Tropp, Joel A. and Yang, Changhuei},
	month = may,
	year = {2015},
	note = {Publisher: IOP Publishing},
	pages = {053044},
	file = {IOP Full Text PDF:/home/zwerg/Zotero/storage/UNNSML5X/Horstmeyer et al. - 2015 - Solving ptychography with a convex relaxation.pdf:application/pdf},
}

@article{zheng_wide-field_2013-1,
	title = {Wide-field, high-resolution {Fourier} ptychographic microscopy},
	volume = {7},
	issn = {1749-4885, 1749-4893},
	url = {http://www.nature.com/articles/nphoton.2013.187},
	doi = {10.1038/nphoton.2013.187},
	abstract = {In this article, we report an imaging method, termed Fourier ptychographic microscopy (FPM), which iteratively stitches together a number of variably illuminated, low-resolution intensity images in Fourier space to produce a wide-field, high-resolution complex sample image. By adopting a wavefront correction strategy, the FPM method can also correct for aberrations and digitally extend a microscope's depth-of-focus beyond the physical limitations of its optics. As a demonstration, we built a microscope prototype with a resolution of 0.78 μm, a field-of-view of approximately 120 mm2, and a resolution-invariant depth-of-focus of 0.3 mm (characterized at 632 nm). Gigapixel color images of histology slides verify FPM's successful operation. The reported imaging procedure transforms the general challenge of high-throughput, highresolution microscopy from one that is coupled to the physical limitations of the system's optics to one that is solvable through computation.},
	language = {en},
	number = {9},
	urldate = {2021-11-30},
	journal = {Nature Photonics},
	author = {Zheng, Guoan and Horstmeyer, Roarke and Yang, Changhuei},
	month = sep,
	year = {2013},
	pages = {739--745},
	file = {Zheng et al. - 2013 - Wide-field, high-resolution Fourier ptychographic .pdf:/home/zwerg/Zotero/storage/ZDH2QPG8/Zheng et al. - 2013 - Wide-field, high-resolution Fourier ptychographic .pdf:application/pdf},
}

@article{zheng_wide-field_2013-2,
	title = {Wide-field, high-resolution {Fourier} ptychographic microscopy},
	volume = {7},
	issn = {1749-4885, 1749-4893},
	url = {http://arxiv.org/abs/1405.0226},
	doi = {10.1038/nphoton.2013.187},
	abstract = {In this article, we report an imaging method, termed Fourier ptychographic microscopy (FPM), which iteratively stitches together a number of variably illuminated, low-resolution intensity images in Fourier space to produce a wide-field, high-resolution complex sample image. By adopting a wavefront correction strategy, the FPM method can also correct for aberrations and digitally extend a microscope's depth-of-focus beyond the physical limitations of its optics. As a demonstration, we built a microscope prototype with a resolution of 0.78 um, a field-of-view of {\textasciitilde}120 mm2, and a resolution-invariant depth-of-focus of 0.3 mm (characterized at 632 nm). Gigapixel colour images of histology slides verify FPM's successful operation. The reported imaging procedure transforms the general challenge of high-throughput, high-resolution microscopy from one that is coupled to the physical limitations of the system's optics to one that is solvable through computation.},
	number = {9},
	urldate = {2021-11-30},
	journal = {Nature Photonics},
	author = {Zheng, Guoan and Horstmeyer, Roarke and Yang, Changhuei},
	month = sep,
	year = {2013},
	note = {arXiv: 1405.0226},
	keywords = {Physics - Optics},
	pages = {739--745},
	annote = {Comment: 11 pages, 4 figures. Nature Photonics,2013},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/CSJ5GC2R/Zheng et al. - 2013 - Wide-field, high-resolution Fourier ptychographic .pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/77H5ALF8/1405.html:text/html},
}

@article{sun_stability_2018-1,
	title = {Stability of {Scattering} {Decoder} {For} {Nonlinear} {Diffractive} {Imaging}},
	url = {http://arxiv.org/abs/1806.08015},
	abstract = {The problem of image reconstruction under multiple light scattering is usually formulated as a regularized non-convex optimization. A deep learning architecture, Scattering Decoder (ScaDec), was recently proposed to solve this problem in a purely data-driven fashion. The proposed method was shown to substantially outperform optimization-based baselines and achieve state-of-the-art results. In this paper, we thoroughly test the robustness of ScaDec to different permittivity contrasts, number of transmissions, and input signal-to-noise ratios. The results on high-fidelity simulated datasets show that the performance of ScaDec is stable in different settings.},
	urldate = {2021-11-30},
	journal = {arXiv:1806.08015 [cs]},
	author = {Sun, Yu and Kamilov, Ulugbek S.},
	month = dec,
	year = {2018},
	note = {arXiv: 1806.08015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: in Proceedings of iTWIST'18, Paper-ID: 31, Marseille, France, November, 21-23, 2018},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/ZM29HXDZ/Sun and Kamilov - 2018 - Stability of Scattering Decoder For Nonlinear Diff.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/ZDR5J9EL/1806.html:text/html},
}

@article{sun_efficient_2018,
	title = {Efficient and accurate inversion of multiple scattering with deep learning},
	volume = {26},
	issn = {1094-4087},
	url = {http://arxiv.org/abs/1803.06594},
	doi = {10.1364/OE.26.014678},
	abstract = {Image reconstruction under multiple light scattering is crucial in a number of applications such as diffraction tomography. The reconstruction problem is often formulated as a nonconvex optimization, where a nonlinear measurement model is used to account for multiple scattering and regularization is used to enforce prior constraints on the object. In this paper, we propose a powerful alternative to this optimizationbased view of image reconstruction by designing and training a deep convolutional neural network that can invert multiple scattered measurements to produce a high-quality image of the refractive index. Our results on both simulated and experimental datasets show that the proposed approach is substantially faster and achieves higher imaging quality compared to the state-of-the-art methods based on optimization.},
	language = {en},
	number = {11},
	urldate = {2021-11-30},
	journal = {Optics Express},
	author = {Sun, Yu and Xia, Zhihao and Kamilov, Ulugbek S.},
	month = may,
	year = {2018},
	note = {arXiv: 1803.06594},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {14678},
	file = {Sun et al. - 2018 - Efficient and accurate inversion of multiple scatt.pdf:/home/zwerg/Zotero/storage/7E6EVD8T/Sun et al. - 2018 - Efficient and accurate inversion of multiple scatt.pdf:application/pdf},
}

@article{waller_machine_2015,
	title = {Machine learning for {3D} microscopy},
	volume = {523},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/523416a},
	doi = {10.1038/523416a},
	abstract = {Artificial neural networks have been combined with microscopy to visualize the 3D structure of biological cells. This could lead to solutions for difficult imaging problems, such as the multiple scattering of light.},
	language = {en},
	number = {7561},
	urldate = {2021-11-30},
	journal = {Nature},
	author = {Waller, Laura and Tian, Lei},
	month = jul,
	year = {2015},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7561
Primary\_atype: News \& Views
Publisher: Nature Publishing Group
Subject\_term: Applied physics;Mathematics and computing;Microscopy;Optics and photonics
Subject\_term\_id: applied-physics;mathematics-and-computing;microscopy;optics-and-photonics},
	keywords = {Microscopy, Applied physics, Mathematics and computing, Optics and photonics},
	pages = {416--417},
	file = {Snapshot:/home/zwerg/Zotero/storage/GEBXNICT/523416a.html:text/html},
}

@article{monakhova_untrained_2021,
	title = {Untrained networks for compressive lensless photography},
	volume = {29},
	issn = {1094-4087},
	url = {http://arxiv.org/abs/2103.07609},
	doi = {10.1364/OE.424075},
	abstract = {Compressive lensless imagers enable novel applications in an extremely compact device, requiring only a phase or amplitude mask placed close to the sensor. They have been demonstrated for 2D and 3D microscopy, single-shot video, and single-shot hyperspectral imaging; in each of these cases, a compressive-sensing-based inverse problem is solved in order to recover a 3D data-cube from a 2D measurement. Typically, this is accomplished using convex optimization and hand-picked priors. Alternatively, deep learning-based reconstruction methods offer the promise of better priors, but require many thousands of ground truth training pairs, which can be difficult or impossible to acquire. In this work, we propose the use of untrained networks for compressive image recovery. Our approach does not require any labeled training data, but instead uses the measurement itself to update the network weights. We demonstrate our untrained approach on lensless compressive 2D imaging as well as single-shot high-speed video recovery using the camera's rolling shutter, and single-shot hyperspectral imaging. We provide simulation and experimental verification, showing that our method results in improved image quality over existing methods.},
	number = {13},
	urldate = {2021-11-30},
	journal = {Optics Express},
	author = {Monakhova, Kristina and Tran, Vi and Kuo, Grace and Waller, Laura},
	month = jun,
	year = {2021},
	note = {arXiv: 2103.07609},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Physics - Optics},
	pages = {20913},
	annote = {Comment: 17 pages, 8 figures},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/9IIV8N63/Monakhova et al. - 2021 - Untrained networks for compressive lensless photog.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/XA8Z9AM3/2103.html:text/html},
}

@article{yanny_miniscope3d_2020,
	title = {{Miniscope3D}: optimized single-shot miniature {3D} fluorescence microscopy},
	volume = {9},
	issn = {2047-7538},
	shorttitle = {{Miniscope3D}},
	url = {http://arxiv.org/abs/2010.05382},
	doi = {10.1038/s41377-020-00403-7},
	abstract = {Miniature fluorescence microscopes are a standard tool in systems biology. However, widefield miniature microscopes capture only 2D information, and modifications that enable 3D capabilities increase the size and weight and have poor resolution outside a narrow depth range. Here, we achieve the 3D capability by replacing the tube lens of a conventional 2D Miniscope with an optimized multifocal phase mask at the objective's aperture stop. Placing the phase mask at the aperture stop significantly reduces the size of the device, and varying the focal lengths enables a uniform resolution across a wide depth range. The phase mask encodes the 3D fluorescence intensity into a single 2D measurement, and the 3D volume is recovered by solving a sparsity-constrained inverse problem. We provide methods for designing and fabricating the phase mask and an efficient forward model that accounts for the field-varying aberrations in miniature objectives. We demonstrate a prototype that is 17 mm tall and weighs 2.5 grams, achieving 2.76 \${\textbackslash}mu\$m lateral, and 15 \${\textbackslash}mu\$m axial resolution across most of the 900x700x390 \${\textbackslash}mu m{\textasciicircum}3\$ volume at 40 volumes per second. The performance is validated experimentally on resolution targets, dynamic biological samples, and mouse brain tissue. Compared with existing miniature single-shot volume-capture implementations, our system is smaller and lighter and achieves a more than 2x better lateral and axial resolution throughout a 10x larger usable depth range. Our microscope design provides single-shot 3D imaging for applications where a compact platform matters, such as volumetric neural imaging in freely moving animals and 3D motion studies of dynamic samples in incubators and lab-on-a-chip devices.},
	number = {1},
	urldate = {2021-11-30},
	journal = {Light: Science \& Applications},
	author = {Yanny, Kyrollos and Antipa, Nick and Liberti, William and Dehaeck, Sam and Monakhova, Kristina and Liu, Fanglin Linda and Shen, Konlin and Ng, Ren and Waller, Laura},
	month = dec,
	year = {2020},
	note = {arXiv: 2010.05382},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Physics - Optics},
	pages = {171},
	annote = {Comment: Published with Nature Springer in Light: Science and Applications},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/UU94NBJG/Yanny et al. - 2020 - Miniscope3D optimized single-shot miniature 3D fl.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/PAHJW5SB/2010.html:text/html},
}

@article{monakhova_spectral_2020,
	title = {Spectral {DiffuserCam}: lensless snapshot hyperspectral imaging with a spectral filter array},
	volume = {7},
	issn = {2334-2536},
	shorttitle = {Spectral {DiffuserCam}},
	url = {http://arxiv.org/abs/2006.08565},
	doi = {10.1364/OPTICA.397214},
	abstract = {Hyperspectral imaging is useful for applications ranging from medical diagnostics to agricultural crop monitoring; however, traditional scanning hyperspectral imagers are prohibitively slow and expensive for widespread adoption. Snapshot techniques exist but are often confined to bulky benchtop setups or have low spatio-spectral resolution. In this paper, we propose a novel, compact, and inexpensive computational camera for snapshot hyperspectral imaging. Our system consists of a tiled spectral filter array placed directly on the image sensor and a diffuser placed close to the sensor. Each point in the world maps to a unique pseudorandom pattern on the spectral filter array, which encodes multiplexed spatio-spectral information. By solving a sparsity-constrained inverse problem, we recover the hyperspectral volume with sub-super-pixel resolution. Our hyperspectral imaging framework is flexible and can be designed with contiguous or non-contiguous spectral filters that can be chosen for a given application. We provide theory for system design, demonstrate a prototype device, and present experimental results with high spatio-spectral resolution.},
	number = {10},
	urldate = {2021-11-30},
	journal = {Optica},
	author = {Monakhova, Kristina and Yanny, Kyrollos and Aggarwal, Neerja and Waller, Laura},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.08565},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Physics - Optics},
	pages = {1298},
	annote = {Comment: 10 pages, 10 figures, Optica},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/H36PNKLQ/Monakhova et al. - 2020 - Spectral DiffuserCam lensless snapshot hyperspect.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/5H6FYWLQ/2006.html:text/html},
}

@article{monakhova_learned_2019,
	title = {Learned reconstructions for practical mask-based lensless imaging},
	volume = {27},
	issn = {1094-4087},
	url = {http://arxiv.org/abs/1908.11502},
	doi = {10.1364/OE.27.028075},
	abstract = {Mask-based lensless imagers are smaller and lighter than traditional lensed cameras. In these imagers, the sensor does not directly record an image of the scene; rather, a computational algorithm reconstructs it. Typically, mask-based lensless imagers use a model-based reconstruction approach that suffers from long compute times and a heavy reliance on both system calibration and heuristically chosen denoisers. In this work, we address these limitations using a bounded-compute, trainable neural network to reconstruct the image. We leverage our knowledge of the physical system by unrolling a traditional model-based optimization algorithm, whose parameters we optimize using experimentally gathered ground-truth data. Optionally, images produced by the unrolled network are then fed into a jointly-trained denoiser. As compared to traditional methods, our architecture achieves better perceptual image quality and runs 20x faster, enabling interactive previewing of the scene. We explore a spectrum between model-based and deep learning methods, showing the benefits of using an intermediate approach. Finally, we test our network on images taken in the wild with a prototype mask-based camera, demonstrating that our network generalizes to natural images.},
	number = {20},
	urldate = {2021-11-30},
	journal = {Optics Express},
	author = {Monakhova, Kristina and Yurtsever, Joshua and Kuo, Grace and Antipa, Nick and Yanny, Kyrollos and Waller, Laura},
	month = sep,
	year = {2019},
	note = {arXiv: 1908.11502},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	pages = {28075},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/KKP2TZSF/Monakhova et al. - 2019 - Learned reconstructions for practical mask-based l.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/EWA44PJ6/1908.html:text/html},
}

@article{liu_fourier_2020,
	title = {Fourier {DiffuserScope}: {Single}-shot {3D} {Fourier} light field microscopy with a diffuser},
	volume = {28},
	issn = {1094-4087},
	shorttitle = {Fourier {DiffuserScope}},
	url = {http://arxiv.org/abs/2006.16343},
	doi = {10.1364/OE.400876},
	abstract = {Light field microscopy (LFM) uses a microlens array (MLA) near the sensor plane of a microscope to achieve single-shot 3D imaging of a sample without any moving parts. Unfortunately, the 3D capability of LFM comes with a significant loss of lateral resolution at the focal plane. Placing the MLA near the pupil plane of the microscope, instead of the image plane, can mitigate the artifacts and provide an efficient forward model, at the expense of field-of-view (FOV). Here, we demonstrate improved resolution across a large volume with Fourier DiffuserScope, which uses a diffuser in the pupil plane to encode 3D information, then computationally reconstructs the volume by solving a sparsity-constrained inverse problem. Our diffuser consists of randomly placed microlenses with varying focal lengths; the random positions provide a larger FOV compared to a conventional MLA, and the diverse focal lengths improve the axial depth range. To predict system performance based on diffuser parameters, we for the first time establish a theoretical framework and design guidelines, which are verified by numerical simulations, then build an experimental system that achieves \${\textless} 3\$ um lateral and \$4\$ um axial resolution over a \$1000 {\textbackslash}times 1000 {\textbackslash}times 280\$ um\${\textasciicircum}3\$ volume. Our diffuser design outperforms the MLA used in LFM, providing more uniform resolution over a larger volume, both laterally and axially.},
	number = {20},
	urldate = {2021-12-01},
	journal = {Optics Express},
	author = {Liu, Fanglin Linda and Kuo, Grace and Antipa, Nick and Yanny, Kyrollos and Waller, Laura},
	month = sep,
	year = {2020},
	note = {arXiv: 2006.16343},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Physics - Optics},
	pages = {28969},
	annote = {Comment: 19 pages, 7 figures},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/XP7GMRHL/Liu et al. - 2020 - Fourier DiffuserScope Single-shot 3D Fourier ligh.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/RLDU7EM4/2006.html:text/html},
}

@article{ulyanov_deep_2020,
	title = {Deep {Image} {Prior}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	url = {http://arxiv.org/abs/1711.10925},
	doi = {10.1007/s11263-020-01303-4},
	abstract = {Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity. Code and supplementary material are available at https://dmitryulyanov.github.io/deep\_image\_prior .},
	number = {7},
	urldate = {2021-12-01},
	journal = {International Journal of Computer Vision},
	author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
	month = jul,
	year = {2020},
	note = {arXiv: 1711.10925},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	pages = {1867--1888},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/9I5VWRYE/Ulyanov et al. - 2020 - Deep Image Prior.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/UJ7APJ22/1711.html:text/html},
}

@article{heckel_deep_2019,
	title = {Deep {Decoder}: {Concise} {Image} {Representations} from {Untrained} {Non}-convolutional {Networks}},
	shorttitle = {Deep {Decoder}},
	url = {http://arxiv.org/abs/1810.03982},
	abstract = {Deep neural networks, in particular convolutional neural networks, have become highly effective tools for compressing images and solving inverse problems including denoising, inpainting, and reconstruction from few and noisy measurements. This success can be attributed in part to their ability to represent and generate natural images well. Contrary to classical tools such as wavelets, image-generating deep neural networks have a large number of parameters---typically a multiple of their output dimension---and need to be trained on large datasets. In this paper, we propose an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The deep decoder has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality. This underparameterization enables the deep decoder to compress images into a concise set of network weights, which we show is on par with wavelet-based thresholding. Further, underparameterization provides a barrier to overfitting, allowing the deep decoder to have state-of-the-art performance for denoising. The deep decoder is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations.},
	urldate = {2021-12-01},
	journal = {arXiv:1810.03982 [cs, stat]},
	author = {Heckel, Reinhard and Hand, Paul},
	month = feb,
	year = {2019},
	note = {arXiv: 1810.03982},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: International Conference on Learning Representations 2019},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/LWZED9WX/Heckel and Hand - 2019 - Deep Decoder Concise Image Representations from U.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/B3CCCBGF/1810.html:text/html},
}

@article{liu_image_2018,
	title = {Image {Restoration} using {Total} {Variation} {Regularized} {Deep} {Image} {Prior}},
	url = {http://arxiv.org/abs/1810.12864},
	abstract = {In the past decade, sparsity-driven regularization has led to significant improvements in image reconstruction. Traditional regularizers, such as total variation (TV), rely on analytical models of sparsity. However, increasingly the field is moving towards trainable models, inspired from deep learning. Deep image prior (DIP) is a recent regularization framework that uses a convolutional neural network (CNN) architecture without data-driven training. This paper extends the DIP framework by combining it with the traditional TV regularization. We show that the inclusion of TV leads to considerable performance gains when tested on several traditional restoration tasks such as image denoising and deblurring.},
	urldate = {2021-12-01},
	journal = {arXiv:1810.12864 [cs]},
	author = {Liu, Jiaming and Sun, Yu and Xu, Xiaojian and Kamilov, Ulugbek S.},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.12864},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/VTRTLAWE/Liu et al. - 2018 - Image Restoration using Total Variation Regularize.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/YSFZXMB2/1810.html:text/html},
}

@article{antipa_diffusercam_2017,
	title = {{DiffuserCam}: {Lensless} {Single}-exposure {3D} {Imaging}},
	shorttitle = {{DiffuserCam}},
	url = {http://arxiv.org/abs/1710.02134},
	abstract = {We demonstrate a compact and easy-to-build computational camera for single-shot 3D imaging. Our lensless system consists solely of a diffuser placed in front of a standard image sensor. Every point within the volumetric field-of-view projects a unique pseudorandom pattern of caustics on the sensor. By using a physical approximation and simple calibration scheme, we solve the large-scale inverse problem in a computationally efficient way. The caustic patterns enable compressed sensing, which exploits sparsity in the sample to solve for more 3D voxels than pixels on the 2D sensor. Our 3D voxel grid is chosen to match the experimentally measured two-point optical resolution across the field-of-view, resulting in 100 million voxels being reconstructed from a single 1.3 megapixel image. However, the effective resolution varies significantly with scene content. Because this effect is common to a wide range of computational cameras, we provide new theory for analyzing resolution in such systems.},
	urldate = {2021-12-01},
	journal = {arXiv:1710.02134 [cs]},
	author = {Antipa, Nick and Kuo, Grace and Heckel, Reinhard and Mildenhall, Ben and Bostan, Emrah and Ng, Ren and Waller, Laura},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.02134},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: The first two authors contributed equally},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/LIB4TU3I/Antipa et al. - 2017 - DiffuserCam Lensless Single-exposure 3D Imaging.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/6Y87JJ78/1710.html:text/html},
}

@article{monakhova_spectral_2020-1,
	title = {Spectral {DiffuserCam}: lensless snapshot hyperspectral imaging with a spectral filter array},
	volume = {7},
	issn = {2334-2536},
	shorttitle = {Spectral {DiffuserCam}},
	url = {http://arxiv.org/abs/2006.08565},
	doi = {10.1364/OPTICA.397214},
	abstract = {Hyperspectral imaging is useful for applications ranging from medical diagnostics to agricultural crop monitoring; however, traditional scanning hyperspectral imagers are prohibitively slow and expensive for widespread adoption. Snapshot techniques exist but are often confined to bulky benchtop setups or have low spatio-spectral resolution. In this paper, we propose a novel, compact, and inexpensive computational camera for snapshot hyperspectral imaging. Our system consists of a tiled spectral filter array placed directly on the image sensor and a diffuser placed close to the sensor. Each point in the world maps to a unique pseudorandom pattern on the spectral filter array, which encodes multiplexed spatio-spectral information. By solving a sparsity-constrained inverse problem, we recover the hyperspectral volume with sub-super-pixel resolution. Our hyperspectral imaging framework is flexible and can be designed with contiguous or non-contiguous spectral filters that can be chosen for a given application. We provide theory for system design, demonstrate a prototype device, and present experimental results with high spatio-spectral resolution.},
	number = {10},
	urldate = {2021-12-01},
	journal = {Optica},
	author = {Monakhova, Kristina and Yanny, Kyrollos and Aggarwal, Neerja and Waller, Laura},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.08565},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Physics - Optics},
	pages = {1298},
	annote = {Comment: 10 pages, 10 figures, Optica},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/LSXW8KDV/Monakhova et al. - 2020 - Spectral DiffuserCam lensless snapshot hyperspect.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/9KTNVLAX/2006.html:text/html},
}

@article{mahendran_understanding_2014,
	title = {Understanding {Deep} {Image} {Representations} by {Inverting} {Them}},
	url = {http://arxiv.org/abs/1412.0035},
	abstract = {Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
	urldate = {2021-12-01},
	journal = {arXiv:1412.0035 [cs]},
	author = {Mahendran, Aravindh and Vedaldi, Andrea},
	month = nov,
	year = {2014},
	note = {arXiv: 1412.0035},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/QJYTVNBD/Mahendran and Vedaldi - 2014 - Understanding Deep Image Representations by Invert.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/JITIUNJF/1412.html:text/html},
}

@inproceedings{zhao_learning-based_2021,
	title = {Learning-{Based} {Image} {Transport} {Through} {Disordered} {Optical} {Fibers} {With} {Transverse} {Anderson} {Localization}},
	doi = {10.3389/fphy.2021.710351},
	abstract = {Fiber-optic imaging systems play a unique role in biomedical imaging and clinical practice due to their flexibilities of performing imaging deep into tissues and organs with minimized penetration damage. Their imaging performance is often limited by the waveguide mode properties of conventional optical fibers and the image reconstruction method, which restrains the enhancement of imaging quality, transport robustness, system size, and illumination compatibility. The emerging disordered Anderson localizing optical fibers circumvent these difficulties by their intriguing properties of the transverse Anderson localization of light, such as single-mode-like behavior, wavelength independence, and high mode density. To go beyond the performance limit of conventional system, there is a growing interest in integrating the disordered Anderson localizing optical fiber with deep learning algorithms. Novel imaging platforms based on this concept have been explored recently to make the best of Anderson localization fibers. Here, we review recent developments of Anderson localizing optical fibers and focus on the latest progress in deep-learning-based imaging applications using these fibers.},
	booktitle = {Frontiers in {Physics}},
	author = {Zhao, Jian and Hu, Xiaowen and Gausmann, Stefan and Antonio-Lopez, J. and Correa, R. A. and Schülzgen, A.},
	year = {2021},
	file = {Full Text:/home/zwerg/Zotero/storage/HH36EUQ2/Zhao et al. - 2021 - Learning-Based Image Transport Through Disordered .pdf:application/pdf},
}

@article{monakhova_supplementary_2021,
	title = {Supplementary document for {Untrained} networks for compressive lensless photography - 5245911.pdf},
	url = {https://opticapublishing.figshare.com/articles/journal_contribution/Supplementary_document_for_Untrained_networks_for_compressive_lensless_photography_-_5245911_pdf/14709501/3},
	doi = {10.6084/m9.figshare.14709501.v3},
	abstract = {Supplemental Document},
	language = {en},
	urldate = {2021-12-01},
	author = {Monakhova, Kristina and Tran, Vi and Kuo, Grace and Waller, Laura},
	month = jun,
	year = {2021},
	note = {Publisher: Optica Publishing Group},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/J8JRLGTA/Monakhova et al. - 2021 - Supplementary document for Untrained networks for .pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/39ELMJJG/14709501.html:text/html},
}

@inproceedings{kuo_diffusercam_2017,
	title = {{DiffuserCam}: {Diffuser}-{Based} {Lensless} {Cameras}},
	copyright = {© 2017 Optical Society of America},
	shorttitle = {{DiffuserCam}},
	url = {https://www.osapublishing.org/abstract.cfm?uri=COSI-2017-CTu3B.2},
	doi = {10.1364/COSI.2017.CTu3B.2},
	abstract = {We propose a lensless, diffuser-based camera with a simple calibration scheme. We investigate the resolution, field of view, and depth of field of our system, and we show results from two prototypes.},
	language = {EN},
	urldate = {2021-12-02},
	booktitle = {Imaging and {Applied} {Optics} 2017 ({3D}, {AIO}, {COSI}, {IS}, {MATH}, {pcAOP}) (2017), paper {CTu3B}.2},
	publisher = {Optical Society of America},
	author = {Kuo, Grace and Antipa, Nick and Ng, Ren and Waller, Laura},
	month = jun,
	year = {2017},
	keywords = {Point spread function, Spatial resolution, Image reconstruction, Calibration, Computer simulation, First order optics},
	pages = {CTu3B.2},
	file = {Snapshot:/home/zwerg/Zotero/storage/VYJFN647/abstract.html:text/html},
}

@misc{noauthor_engineering_nodate,
	title = {Engineering {Textbooks}},
	url = {https://open.umn.edu/opentextbooks/subjects/engineering},
	abstract = {Engineering},
	language = {en},
	urldate = {2021-12-09},
	journal = {Open Textbook Library},
	file = {Snapshot:/home/zwerg/Zotero/storage/98MLYIDI/engineering.html:text/html},
}

@misc{newman_hyperparameter_2021,
	title = {Hyperparameter {Tuning} with {KerasTuner} and {TensorFlow}},
	url = {https://towardsdatascience.com/hyperparameter-tuning-with-kerastuner-and-tensorflow-c4a4d690b31a},
	abstract = {Understand best practices to optimize your model’s architecture and hyperparameters with KerasTuner and TensorFlow},
	language = {en},
	urldate = {2022-01-04},
	journal = {Medium},
	author = {Newman, Luke},
	month = aug,
	year = {2021},
	file = {Snapshot:/home/zwerg/Zotero/storage/AGGDPGTA/hyperparameter-tuning-with-kerastuner-and-tensorflow-c4a4d690b31a.html:text/html},
}

@article{frei_engineered_2021,
	title = {Engineered {HaloTag} variants for fluorescence lifetime multiplexing},
	issn = {1548-7091, 1548-7105},
	url = {https://www.nature.com/articles/s41592-021-01341-x},
	doi = {10.1038/s41592-021-01341-x},
	abstract = {Abstract
            Self-labeling protein tags such as HaloTag are powerful tools that can label fusion proteins with synthetic fluorophores for use in fluorescence microscopy. Here we introduce HaloTag variants with either increased or decreased brightness and fluorescence lifetime compared with HaloTag7 when labeled with rhodamines. Combining these HaloTag variants enabled live-cell fluorescence lifetime multiplexing of three cellular targets in one spectral channel using a single fluorophore and the generation of a fluorescence lifetime-based biosensor. Additionally, the brightest HaloTag variant showed up to 40\% higher brightness in live-cell imaging applications.},
	language = {en},
	urldate = {2021-12-28},
	journal = {Nature Methods},
	author = {Frei, Michelle S. and Tarnawski, Miroslaw and Roberti, M. Julia and Koch, Birgit and Hiblot, Julien and Johnsson, Kai},
	month = dec,
	year = {2021},
	file = {s41592-021-01341-x.pdf:/home/zwerg/Zotero/storage/XIACS43S/s41592-021-01341-x.pdf:application/pdf},
}

@misc{noauthor_flax_nodate,
	title = {Flax documentation — {Flax} documentation},
	url = {https://flax.readthedocs.io/en/latest/index.html},
	urldate = {2021-12-27},
	file = {Flax documentation — Flax documentation:/home/zwerg/Zotero/storage/8YNYVH92/index.html:text/html},
}

@misc{noauthor_using_nodate,
	title = {Using {AI} to bring children’s drawings to life},
	url = {https://ai.facebook.com/blog/using-ai-to-bring-childrens-drawings-to-life/},
	abstract = {Does your child love to draw? Ever wished that characters in their drawings could “come to life” and move around the page? Using AI, we’ve developed automatic animation that can bring children’s one-of-a-kind characters to life!},
	language = {en},
	urldate = {2021-12-23},
}

@misc{noauthor_efficient_nodate,
	title = {Efficient {PyTorch}: {Tensor} {Memory} {Format} {Matters}},
	shorttitle = {Efficient {PyTorch}},
	url = {https://pytorch.org/blog/tensor-memory-format-matters/},
	abstract = {Ensuring the right memory format for your inputs can significantly impact the running time of your PyTorch vision models. When in doubt, choose a Channels Last memory format.},
	language = {en},
	urldate = {2021-12-23},
}

@misc{noauthor_personalized_2018,
	title = {Personalized {Cancer} {Vaccines} to {Conquer} {Cancer}},
	url = {https://www.parkerici.org/the-latest/personalized-cancer-vaccines-to-conquer-cancer/},
	abstract = {A Q\&A with Parker Institute Scientist Catherine Wu, MD Catherine Wu, MD, a Parker Institute for Cancer Immunotherapy investigator and oncologist at Dana-Farber Cancer Institute, is one of the world’s foremost experts on cancer vaccines. In 2017, she published a seminal paper showing that a new type of personalized cancer vaccine works, keeping four of … Continued},
	language = {en-US},
	urldate = {2021-12-20},
	journal = {Parker Institute for Cancer Immunotherapy},
	month = apr,
	year = {2018},
	file = {Snapshot:/home/zwerg/Zotero/storage/T3D6CGP7/personalized-cancer-vaccines-to-conquer-cancer.html:text/html},
}

@article{acinas_deep_2021,
	title = {Deep ocean metagenomes provide insight into the metabolic architecture of bathypelagic microbial communities},
	volume = {4},
	issn = {2399-3642},
	url = {http://www.nature.com/articles/s42003-021-02112-2},
	doi = {10.1038/s42003-021-02112-2},
	abstract = {Abstract
            
              The deep sea, the largest ocean’s compartment, drives planetary-scale biogeochemical cycling. Yet, the functional exploration of its microbial communities lags far behind other environments. Here we analyze 58 metagenomes from tropical and subtropical deep oceans to generate the Malaspina Gene Database. Free-living or particle-attached lifestyles drive functional differences in bathypelagic prokaryotic communities, regardless of their biogeography. Ammonia and CO oxidation pathways are enriched in the free-living microbial communities and dissimilatory nitrate reduction to ammonium and H
              2
              oxidation pathways in the particle-attached, while the Calvin Benson-Bassham cycle is the most prevalent inorganic carbon fixation pathway in both size fractions. Reconstruction of the Malaspina Deep Metagenome-Assembled Genomes reveals unique non-cyanobacterial diazotrophic bacteria and chemolithoautotrophic prokaryotes. The widespread potential to grow both autotrophically and heterotrophically suggests that mixotrophy is an ecologically relevant trait in the deep ocean. These results expand our understanding of the functional microbial structure and metabolic capabilities of the largest Earth aquatic ecosystem.},
	language = {en},
	number = {1},
	urldate = {2021-12-17},
	journal = {Communications Biology},
	author = {Acinas, Silvia G. and Sánchez, Pablo and Salazar, Guillem and Cornejo-Castillo, Francisco M. and Sebastián, Marta and Logares, Ramiro and Royo-Llonch, Marta and Paoli, Lucas and Sunagawa, Shinichi and Hingamp, Pascal and Ogata, Hiroyuki and Lima-Mendez, Gipsi and Roux, Simon and González, José M. and Arrieta, Jesús M. and Alam, Intikhab S. and Kamau, Allan and Bowler, Chris and Raes, Jeroen and Pesant, Stéphane and Bork, Peer and Agustí, Susana and Gojobori, Takashi and Vaqué, Dolors and Sullivan, Matthew B. and Pedrós-Alió, Carlos and Massana, Ramon and Duarte, Carlos M. and Gasol, Josep M.},
	month = dec,
	year = {2021},
	pages = {604},
	file = {Acinas et al. - 2021 - Deep ocean metagenomes provide insight into the me.pdf:/home/zwerg/Zotero/storage/L32XPGP6/Acinas et al. - 2021 - Deep ocean metagenomes provide insight into the me.pdf:application/pdf},
}

@misc{noauthor_deepbacs_nodate,
	title = {{DeepBacs}: {Bacterial} image analysis using open-source deep learning approaches {\textbar} {bioRxiv}},
	url = {https://www.biorxiv.org/content/10.1101/2021.11.03.467152v1},
	urldate = {2021-12-17},
}

@techreport{narayanasamy_fast_2021,
	title = {Fast {DNA}-{PAINT} imaging using a deep neural network},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.11.20.469366v1},
	abstract = {DNA points accumulation for imaging in nanoscale topography (DNA-PAINT) is a super-resolution technique with relatively easy-to-implement multi-target imaging. However, image acquisition is slow as sufficient statistical data has to be generated from spatio-temporally isolated single emitters. Here, we trained the neural network (NN) DeepSTORM to predict fluorophore positions from high emitter density DNA-PAINT data. This achieves image acquisition in one minute. We demonstrate multi-color super-resolution imaging of structure-conserved semi-thin neuronal tissue and imaging of large samples. This improvement can be integrated into any single-molecule microscope and enables fast single-molecule super-resolution microscopy.},
	language = {en},
	urldate = {2021-12-17},
	author = {Narayanasamy, Kaarjel K. and Rahm, Johanna V. and Tourani, Siddharth and Heilemann, Mike},
	month = nov,
	year = {2021},
	doi = {10.1101/2021.11.20.469366},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {2021.11.20.469366},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/ADPJQYGJ/Narayanasamy et al. - 2021 - Fast DNA-PAINT imaging using a deep neural network.pdf:application/pdf},
}

@misc{dickson_neural_2021,
	title = {Neural networks can hide malware, and scientists are worried},
	url = {https://thenextweb.com/news/neural-networks-hide-malware-scientists-worried-syndication},
	abstract = {As deep learning becomes ingrained in applications we use every day, we need to think how to protect users against their emerging threats.},
	language = {en},
	urldate = {2021-12-17},
	journal = {TNW {\textbar} Neural},
	author = {Dickson, Ben},
	month = dec,
	year = {2021},
	note = {Section: neural},
}

@misc{noauthor_children_2021,
	title = {Children as {Social} {Robot} {Designers}},
	url = {https://spectrum.ieee.org/social-robots-children},
	abstract = {What happens when you let kids design their own social robot from scratch},
	language = {en},
	urldate = {2021-12-17},
	journal = {IEEE Spectrum},
	month = dec,
	year = {2021},
}

@misc{noauthor_computed_nodate,
	title = {Computed structures of core eukaryotic protein complexes},
	url = {https://www.science.org/doi/10.1126/science.abm4805},
	urldate = {2021-12-17},
}

@misc{noauthor_few-fs_nodate,
	title = {Few-fs resolution of a photoactive protein traversing a conical intersection {\textbar} {Nature}},
	url = {https://www.nature.com/articles/s41586-021-04050-9?utm_medium=affiliate&utm_source=commission_junction&utm_campaign=3_nsn6445_deeplink_PID100017430&utm_content=deeplink},
	urldate = {2021-12-17},
}

@misc{noauthor_better_nodate,
	title = {Better {Images} of {AI}},
	url = {https://betterimagesofai.org/images},
	urldate = {2021-12-17},
	file = {Better Images of AI:/home/zwerg/Zotero/storage/ABI9QQAY/images.html:text/html},
}

@article{ha_collective_2021,
	title = {Collective {Intelligence} for {Deep} {Learning}: {A} {Survey} of {Recent} {Developments}},
	shorttitle = {Collective {Intelligence} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2111.14377},
	abstract = {In the past decade, we have witnessed the rise of deep learning to dominate the field of artificial intelligence. Advances in artificial neural networks alongside corresponding advances in hardware accelerators with large memory capacity, together with the availability of large datasets enabled researchers and practitioners alike to train and deploy sophisticated neural network models that achieve state-of-the-art performance on tasks across several fields spanning computer vision, natural language processing, and reinforcement learning. However, as these neural networks become bigger, more complex, and more widely used, fundamental problems with current deep learning models become more apparent. State-of-the-art deep learning models are known to suffer from issues that range from poor robustness, inability to adapt to novel task settings, to requiring rigid and inflexible configuration assumptions. Ideas from collective intelligence, in particular concepts from complex systems such as self-organization, emergent behavior, swarm optimization, and cellular systems tend to produce solutions that are robust, adaptable, and have less rigid assumptions about the environment configuration. It is therefore natural to see these ideas incorporated into newer deep learning methods. In this review, we will provide a historical context of neural network research's involvement with complex systems, and highlight several active areas in modern deep learning research that incorporate the principles of collective intelligence to advance its current capabilities. To facilitate a bi-directional flow of ideas, we also discuss work that utilize modern deep learning models to help advance complex systems research. We hope this review can serve as a bridge between complex systems and deep learning communities to facilitate the cross pollination of ideas and foster new collaborations across disciplines.},
	urldate = {2021-12-17},
	journal = {arXiv:2111.14377 [cs]},
	author = {Ha, David and Tang, Yujin},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.14377},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/JQCUUPK2/Ha and Tang - 2021 - Collective Intelligence for Deep Learning A Surve.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/ZL5XBXTV/2111.html:text/html},
}

@article{reinke_intrinsically_2020,
	title = {Intrinsically {Motivated} {Discovery} of {Diverse} {Patterns} in {Self}-{Organizing} {Systems}},
	url = {http://arxiv.org/abs/1908.06663},
	abstract = {In many complex dynamical systems, artificial or natural, one can observe self-organization of patterns emerging from local rules. Cellular automata, like the Game of Life (GOL), have been widely used as abstract models enabling the study of various aspects of self-organization and morphogenesis, such as the emergence of spatially localized patterns. However, findings of self-organized patterns in such models have so far relied on manual tuning of parameters and initial states, and on the human eye to identify interesting patterns. In this paper, we formulate the problem of automated discovery of diverse self-organized patterns in such high-dimensional complex dynamical systems, as well as a framework for experimentation and evaluation. Using a continuous GOL as a testbed, we show that recent intrinsically-motivated machine learning algorithms (POP-IMGEPs), initially developed for learning of inverse models in robotics, can be transposed and used in this novel application area. These algorithms combine intrinsically-motivated goal exploration and unsupervised learning of goal space representations. Goal space representations describe the interesting features of patterns for which diverse variations should be discovered. In particular, we compare various approaches to define and learn goal space representations from the perspective of discovering diverse spatially localized patterns. Moreover, we introduce an extension of a state-of-the-art POP-IMGEP algorithm which incrementally learns a goal representation using a deep auto-encoder, and the use of CPPN primitives for generating initialization parameters. We show that it is more efficient than several baselines and equally efficient as a system pre-trained on a hand-made database of patterns identified by human experts.},
	urldate = {2021-12-17},
	journal = {arXiv:1908.06663 [cs, stat]},
	author = {Reinke, Chris and Etcheverry, Mayalen and Oudeyer, Pierre-Yves},
	month = feb,
	year = {2020},
	note = {arXiv: 1908.06663},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 29 pages, 19 figure, ICLR 2020 conference paper, associated website: https://automated-discovery.github.io/},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/WK3364CY/Reinke et al. - 2020 - Intrinsically Motivated Discovery of Diverse Patte.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/UHNKZQSF/1908.html:text/html},
}

@book{noauthor_self-assembling_2021,
	title = {The {Self}-{Assembling} {Brain}},
	isbn = {978-0-691-18122-6},
	url = {https://press.princeton.edu/books/hardcover/9780691181226/the-self-assembling-brain},
	abstract = {What neurobiology and artificial intelligence tell us about how the brain builds itself},
	language = {en},
	urldate = {2021-12-17},
	month = may,
	year = {2021},
}

@article{variengien_towards_2021,
	title = {Towards self-organized control: {Using} neural cellular automata to robustly control a cart-pole agent},
	shorttitle = {Towards self-organized control},
	url = {http://arxiv.org/abs/2106.15240},
	abstract = {Neural cellular automata (Neural CA) are a recent framework used to model biological phenomena emerging from multicellular organisms. In these systems, artificial neural networks are used as update rules for cellular automata. Neural CA are end-to-end differentiable systems where the parameters of the neural network can be learned to achieve a particular task. In this work, we used neural CA to control a cart-pole agent. The observations of the environment are transmitted in input cells, while the values of output cells are used as a readout of the system. We trained the model using deep-Q learning, where the states of the output cells were used as the Q-value estimates to be optimized. We found that the computing abilities of the cellular automata were maintained over several hundreds of thousands of iterations, producing an emergent stable behavior in the environment it controls for thousands of steps. Moreover, the system demonstrated life-like phenomena such as a developmental phase, regeneration after damage, stability despite a noisy environment, and robustness to unseen disruption such as input deletion.},
	urldate = {2021-12-17},
	journal = {arXiv:2106.15240 [cs]},
	author = {Variengien, Alexandre and Nichele, Stefano and Glover, Tom and Pontes-Filho, Sidney},
	month = jul,
	year = {2021},
	note = {arXiv: 2106.15240},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/CFHMQPXM/Variengien et al. - 2021 - Towards self-organized control Using neural cellu.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/C2HMI7F5/2106.html:text/html},
}

@article{tang_sensory_2021,
	title = {The {Sensory} {Neuron} as a {Transformer}: {Permutation}-{Invariant} {Neural} {Networks} for {Reinforcement} {Learning}},
	shorttitle = {The {Sensory} {Neuron} as a {Transformer}},
	url = {http://arxiv.org/abs/2109.02869},
	abstract = {In complex systems, we often observe complex global behavior emerge from a collection of agents interacting with each other in their environment, with each individual agent acting only on locally available information, without knowing the full picture. Such systems have inspired development of artificial intelligence algorithms in areas such as swarm optimization and cellular automata. Motivated by the emergence of collective behavior from complex cellular systems, we build systems that feed each sensory input from the environment into distinct, but identical neural networks, each with no fixed relationship with one another. We show that these sensory networks can be trained to integrate information received locally, and through communication via an attention mechanism, can collectively produce a globally coherent policy. Moreover, the system can still perform its task even if the ordering of its inputs is randomly permuted several times during an episode. These permutation invariant systems also display useful robustness and generalization properties that are broadly applicable. Interactive demo and videos of our results: https://attentionneuron.github.io/},
	urldate = {2021-12-17},
	journal = {arXiv:2109.02869 [cs]},
	author = {Tang, Yujin and Ha, David},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.02869},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: To appear at the 35th Conference on Neural Information Processing Systems (NeurIPS 2021). Selected for a spotlight presentation},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/ZZII7RGQ/Tang and Ha - 2021 - The Sensory Neuron as a Transformer Permutation-I.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/C58M873P/2109.html:text/html},
}

@misc{noauthor_future_nodate,
	title = {The {Future} of {Artificial} {Intelligence} is {Self}-{Organizing} and {Self}-{Assembling} – {Sebastian} {Risi}},
	url = {https://sebastianrisi.com/self_assembling_ai/},
	language = {en-US},
	urldate = {2021-12-17},
	file = {Snapshot:/home/zwerg/Zotero/storage/MDU3A2NP/self_assembling_ai.html:text/html},
}

@article{pedersen_evolving_2021,
	title = {Evolving and {Merging} {Hebbian} {Learning} {Rules}: {Increasing} {Generalization} by {Decreasing} the {Number} of {Rules}},
	shorttitle = {Evolving and {Merging} {Hebbian} {Learning} {Rules}},
	url = {http://arxiv.org/abs/2104.07959},
	doi = {10.1145/3449639.3459317},
	abstract = {Generalization to out-of-distribution (OOD) circumstances after training remains a challenge for artificial agents. To improve the robustness displayed by plastic Hebbian neural networks, we evolve a set of Hebbian learning rules, where multiple connections are assigned to a single rule. Inspired by the biological phenomenon of the genomic bottleneck, we show that by allowing multiple connections in the network to share the same local learning rule, it is possible to drastically reduce the number of trainable parameters, while obtaining a more robust agent. During evolution, by iteratively using simple K-Means clustering to combine rules, our Evolve and Merge approach is able to reduce the number of trainable parameters from 61,440 to 1,920, while at the same time improving robustness, all without increasing the number of generations used. While optimization of the agents is done on a standard quadruped robot morphology, we evaluate the agents' performances on slight morphology modifications in a total of 30 unseen morphologies. Our results add to the discussion on generalization, overfitting and OOD adaptation. To create agents that can adapt to a wider array of unexpected situations, Hebbian learning combined with a regularising "genomic bottleneck" could be a promising research direction.},
	urldate = {2021-12-17},
	journal = {Proceedings of the Genetic and Evolutionary Computation Conference},
	author = {Pedersen, Joachim Winther and Risi, Sebastian},
	month = jun,
	year = {2021},
	note = {arXiv: 2104.07959},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	pages = {892--900},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/NEBPH85B/Pedersen and Risi - 2021 - Evolving and Merging Hebbian Learning Rules Incre.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/FQFSUILH/2104.html:text/html},
}

@article{najarro_meta-learning_2021,
	title = {Meta-{Learning} through {Hebbian} {Plasticity} in {Random} {Networks}},
	url = {http://arxiv.org/abs/2007.02686},
	abstract = {Lifelong learning and adaptability are two defining aspects of biological agents. Modern reinforcement learning (RL) approaches have shown significant progress in solving complex tasks, however once training is concluded, the found solutions are typically static and incapable of adapting to new information or perturbations. While it is still not completely understood how biological brains learn and adapt so efficiently from experience, it is believed that synaptic plasticity plays a prominent role in this process. Inspired by this biological mechanism, we propose a search method that, instead of optimizing the weight parameters of neural networks directly, only searches for synapse-specific Hebbian learning rules that allow the network to continuously self-organize its weights during the lifetime of the agent. We demonstrate our approach on several reinforcement learning tasks with different sensory modalities and more than 450K trainable plasticity parameters. We find that starting from completely random weights, the discovered Hebbian rules enable an agent to navigate a dynamical 2D-pixel environment; likewise they allow a simulated 3D quadrupedal robot to learn how to walk while adapting to morphological damage not seen during training and in the absence of any explicit reward or error signal in less than 100 timesteps. Code is available at https://github.com/enajx/HebbianMetaLearning.},
	urldate = {2021-12-17},
	journal = {arXiv:2007.02686 [cs]},
	author = {Najarro, Elias and Risi, Sebastian},
	month = mar,
	year = {2021},
	note = {arXiv: 2007.02686},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: v4: Typo in equation in 3.1 corrected. v3: Bug that made diagonal patterns appear has been fixed. Simulations have been re-run and plots updated. v2: Figures 1, 7 and Table 1 updated, new results on 4.1 added, typos corrected, references added},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/HJNWAM7L/Najarro and Risi - 2021 - Meta-Learning through Hebbian Plasticity in Random.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/NGQGEJF3/2007.html:text/html},
}

@article{kirsch_meta_2021,
	title = {Meta {Learning} {Backpropagation} {And} {Improving} {It}},
	url = {http://arxiv.org/abs/2012.14905},
	abstract = {Many concepts have been proposed for meta learning with neural networks (NNs), e.g., NNs that learn to reprogram fast weights, Hebbian plasticity, learned learning rules, and meta recurrent NNs. Our Variable Shared Meta Learning (VSML) unifies the above and demonstrates that simple weight-sharing and sparsity in an NN is sufficient to express powerful learning algorithms (LAs) in a reusable fashion. A simple implementation of VSML where the weights of a neural network are replaced by tiny LSTMs allows for implementing the backpropagation LA solely by running in forward-mode. It can even meta learn new LAs that differ from online backpropagation and generalize to datasets outside of the meta training distribution without explicit gradient calculation. Introspection reveals that our meta learned LAs learn through fast association in a way that is qualitatively different from gradient descent.},
	urldate = {2021-12-17},
	journal = {arXiv:2012.14905 [cs, stat]},
	author = {Kirsch, Louis and Schmidhuber, Jürgen},
	month = oct,
	year = {2021},
	note = {arXiv: 2012.14905},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Updated to the NeurIPS 2021 camera ready},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/5UBN53R4/Kirsch and Schmidhuber - 2021 - Meta Learning Backpropagation And Improving It.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/7PIDUEQ5/2012.html:text/html},
}

@article{zador_critique_2019,
	title = {A critique of pure learning and what artificial neural networks can learn from animal brains},
	volume = {10},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/s41467-019-11786-6},
	doi = {10.1038/s41467-019-11786-6},
	language = {en},
	number = {1},
	urldate = {2021-12-17},
	journal = {Nature Communications},
	author = {Zador, Anthony M.},
	month = dec,
	year = {2019},
	pages = {3770},
	file = {Zador - 2019 - A critique of pure learning and what artificial ne.pdf:/home/zwerg/Zotero/storage/ATW9GTNI/Zador - 2019 - A critique of pure learning and what artificial ne.pdf:application/pdf},
}

@misc{noauthor_critique_nodate,
	title = {A critique of pure learning and what artificial neural networks can learn from animal brains {\textbar} {Nature} {Communications}},
	url = {https://www.nature.com/articles/s41467-019-11786-6},
	urldate = {2021-12-17},
	file = {A critique of pure learning and what artificial neural networks can learn from animal brains | Nature Communications:/home/zwerg/Zotero/storage/F58E58CK/s41467-019-11786-6.html:text/html},
}

@article{horibe_regenerating_2021,
	title = {Regenerating {Soft} {Robots} through {Neural} {Cellular} {Automata}},
	url = {http://arxiv.org/abs/2102.02579},
	abstract = {Morphological regeneration is an important feature that highlights the environmental adaptive capacity of biological systems. Lack of this regenerative capacity significantly limits the resilience of machines and the environments they can operate in. To aid in addressing this gap, we develop an approach for simulated soft robots to regrow parts of their morphology when being damaged. Although numerical simulations using soft robots have played an important role in their design, evolving soft robots with regenerative capabilities have so far received comparable little attention. Here we propose a model for soft robots that regenerate through a neural cellular automata. Importantly, this approach only relies on local cell information to regrow damaged components, opening interesting possibilities for physical regenerable soft robots in the future. Our approach allows simulated soft robots that are damaged to partially regenerate their original morphology through local cell interactions alone and regain some of their ability to locomote. These results take a step towards equipping artificial systems with regenerative capacities and could potentially allow for more robust operations in a variety of situations and environments. The code for the experiments in this paper is available at: {\textbackslash}url\{github.com/KazuyaHoribe/RegeneratingSoftRobots\}.},
	urldate = {2021-12-17},
	journal = {arXiv:2102.02579 [cs, q-bio]},
	author = {Horibe, Kazuya and Walker, Kathryn and Risi, Sebastian},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.02579},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics, Quantitative Biology - Populations and Evolution},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/DHSETN7I/Horibe et al. - 2021 - Regenerating Soft Robots through Neural Cellular A.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/5KKKAXRN/2102.html:text/html},
}

@misc{noauthor_210308737_nodate,
	title = {[2103.08737] {Growing} {3D} {Artefacts} and {Functional} {Machines} with {Neural} {Cellular} {Automata}},
	url = {https://arxiv.org/abs/2103.08737},
	urldate = {2021-12-17},
	file = {[2103.08737] Growing 3D Artefacts and Functional Machines with Neural Cellular Automata:/home/zwerg/Zotero/storage/FXZTCPX7/2103.html:text/html},
}

@inproceedings{anonymous_variational_2021,
	title = {Variational {Neural} {Cellular} {Automata}},
	url = {https://openreview.net/forum?id=7fFO4cMBx_9},
	abstract = {In nature, the process of cellular growth and differentiation has lead to an amazing diversity of organisms --- algae, starfish, giant sequoia, tardigrades, and orcas are all created by the same...},
	language = {en},
	urldate = {2021-12-17},
	author = {Anonymous},
	month = sep,
	year = {2021},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/FN5D4XFH/Anonymous - 2021 - Variational Neural Cellular Automata.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/UHIIEDZY/forum.html:text/html},
}

@article{gilpin_cellular_2019,
	title = {Cellular automata as convolutional neural networks},
	volume = {100},
	issn = {2470-0045, 2470-0053},
	url = {http://arxiv.org/abs/1809.02942},
	doi = {10.1103/PhysRevE.100.032402},
	abstract = {Deep learning techniques have recently demonstrated broad success in predicting complex dynamical systems ranging from turbulence to human speech, motivating broader questions about how neural networks encode and represent dynamical rules. We explore this problem in the context of cellular automata (CA), simple dynamical systems that are intrinsically discrete and thus difficult to analyze using standard tools from dynamical systems theory. We show that any CA may readily be represented using a convolutional neural network with a network-in-network architecture. This motivates our development of a general convolutional multilayer perceptron architecture, which we find can learn the dynamical rules for arbitrary CA when given videos of the CA as training data. In the limit of large network widths, we find that training dynamics are nearly identical across replicates, and that common patterns emerge in the structure of networks trained on different CA rulesets. We train ensembles of networks on randomly-sampled CA, and we probe how the trained networks internally represent the CA rules using an information-theoretic technique based on distributions of layer activation patterns. We find that CA with simpler rule tables produce trained networks with hierarchical structure and layer specialization, while more complex CA produce shallower representations---illustrating how the underlying complexity of the CA's rules influences the specificity of these internal representations. Our results suggest how the entropy of a physical process can affect its representation when learned by neural networks.},
	number = {3},
	urldate = {2021-12-17},
	journal = {Physical Review E},
	author = {Gilpin, William},
	month = sep,
	year = {2019},
	note = {arXiv: 1809.02942},
	keywords = {Computer Science - Neural and Evolutionary Computing, Condensed Matter - Disordered Systems and Neural Networks, Nonlinear Sciences - Cellular Automata and Lattice Gases, Physics - Computational Physics},
	pages = {032402},
	annote = {Comment: 8 pages, 4 figures (+Appendix)},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/MR8236HZ/Gilpin - 2019 - Cellular automata as convolutional neural networks.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/N2YETFV8/1809.html:text/html},
}

@article{jumper_highly_2021-1,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2021-12-17},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 7873
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computational biophysics;Machine learning;Protein structure predictions;Structural biology
Subject\_term\_id: computational-biophysics;machine-learning;protein-structure-predictions;structural-biology},
	keywords = {Machine learning, Computational biophysics, Protein structure predictions, Structural biology},
	pages = {583--589},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/9SZUNAZE/Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf:application/pdf},
}

@misc{noauthor_exploring_nodate,
	title = {Exploring the beauty of pure mathematics in novel ways},
	url = {https://deepmind.com/blog/article/exploring-the-beauty-of-pure-mathematics-in-novel-ways},
	abstract = {Discovering new patterns in the fields of topology and representation theory with machine learning},
	language = {ALL},
	urldate = {2021-12-15},
	journal = {Deepmind},
	file = {Snapshot:/home/zwerg/Zotero/storage/57426URK/exploring-the-beauty-of-pure-mathematics-in-novel-ways.html:text/html},
}

@article{hua_cytoimagenet_2021,
	title = {{CytoImageNet}: {A} large-scale pretraining dataset for bioimage transfer learning},
	shorttitle = {{CytoImageNet}},
	url = {http://arxiv.org/abs/2111.11646},
	abstract = {Motivation: In recent years, image-based biological assays have steadily become high-throughput, sparking a need for fast automated methods to extract biologically-meaningful information from hundreds of thousands of images. Taking inspiration from the success of ImageNet, we curate CytoImageNet, a large-scale dataset of openly-sourced and weakly-labeled microscopy images (890K images, 894 classes). Pretraining on CytoImageNet yields features that are competitive to ImageNet features on downstream microscopy classification tasks. We show evidence that CytoImageNet features capture information not available in ImageNet-trained features. The dataset is made available at https://www.kaggle.com/stanleyhua/cytoimagenet.},
	urldate = {2021-12-15},
	journal = {arXiv:2111.11646 [cs, q-bio]},
	author = {Hua, Stanley Bryan Z. and Lu, Alex X. and Moses, Alan M.},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.11646
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Quantitative Methods},
	annote = {Comment: Accepted paper at NeurIPS 2021 Learning Meaningful Representations for Life (LMRL) Workshop},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/8WQRZGTS/Hua et al. - 2021 - CytoImageNet A large-scale pretraining dataset fo.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/UEHF2BGA/2111.html:text/html},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {CytoImageNet} {Dataset}},
	url = {https://paperswithcode.com/dataset/cytoimagenet},
	abstract = {CytoImageNet is a large-scale pretraining dataset of microscopy images (890K, 894 classes). In the paper, CytoImageNet pretraining yielded features competitive to and different from ImageNet pretrained features on downstream microscopy tasks.


It was constructed from 40 openly available microscopy datasets. 
Weak labels (from experimental metadata) were assigned to each image in the dataset.
Images are of varying sizes.

The primary purpose of the dataset is to be used for pretraining as a pretext task for learning useful bioimage representations. However, it may be used for validation or exploratory analysis.},
	language = {en},
	urldate = {2021-12-15},
	file = {Snapshot:/home/zwerg/Zotero/storage/6FB5HMGC/cytoimagenet.html:text/html},
}

@misc{noauthor_react_nodate,
	title = {React {Native} vs {Vue} {Native} {\textbar} {What} are the differences?},
	url = {https://stackshare.io/stackups/react-native-vs-vue-native},
	abstract = {"Learn once write everywhere", "Cross platform" and "Javascript" are the key factors why developers consider React Native; whereas "Can use v-if, v-model and so on", "Can be use with vuex and vue-router" and "Use .vue file instead of .js" are the primary reasons why Vue Native is favored.},
	language = {en},
	urldate = {2021-12-15},
	journal = {StackShare},
}

@misc{noauthor_styleguide_nodate,
	title = {styleguide},
	url = {https://google.github.io/styleguide/pyguide.html},
	abstract = {Style guides for Google-originated open-source projects},
	language = {en-US},
	urldate = {2021-12-15},
	journal = {styleguide},
	file = {Snapshot:/home/zwerg/Zotero/storage/339IQ5CY/pyguide.html:text/html},
}

@article{giovannucci_caiman_nodate,
	title = {{CaImAn} an open source tool for scalable calcium imaging data analysis},
	volume = {8},
	issn = {2050-084X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6342523/},
	doi = {10.7554/eLife.38173},
	abstract = {Advances in fluorescence microscopy enable monitoring larger brain areas in-vivo with finer time resolution. The resulting data rates require reproducible analysis pipelines that are reliable, fully automated, and scalable to datasets generated over the course of months. We present CaImAn, an open-source library for calcium imaging data analysis. CaImAn provides automatic and scalable methods to address problems common to pre-processing, including motion correction, neural activity identification, and registration across different sessions of data collection. It does this while requiring minimal user intervention, with good scalability on computers ranging from laptops to high-performance computing clusters. CaImAn is suitable for two-photon and one-photon imaging, and also enables real-time analysis on streaming data. To benchmark the performance of CaImAn we collected and combined a corpus of manual annotations from multiple labelers on nine mouse two-photon datasets. We demonstrate that CaImAn achieves near-human performance in detecting locations of active neurons., The human brain contains billions of cells called neurons that rapidly carry information from one part of the brain to another. Progress in medical research and healthcare is hindered by the difficulty in understanding precisely which neurons are active at any given time. New brain imaging techniques and genetic tools allow researchers to track the activity of thousands of neurons in living animals over many months. However, these experiments produce large volumes of data that researchers currently have to analyze manually, which can take a long time and generate irreproducible results., There is a need to develop new computational tools to analyze such data. The new tools should be able to operate on standard computers rather than just specialist equipment as this would limit the use of the solutions to particularly well-funded research teams. Ideally, the tools should also be able to operate in real-time as several experimental and therapeutic scenarios, like the control of robotic limbs, require this. To address this need, Giovannucci et al. developed a new software package called CaImAn to analyze brain images on a large scale., Firstly, the team developed algorithms that are suitable to analyze large sets of data on laptops and other standard computing equipment. These algorithms were then adapted to operate online in real-time. To test how well the new software performs against manual analysis by human researchers, Giovannucci et al. asked several trained human annotators to identify active neurons that were round or donut-shaped in several sets of imaging data from mouse brains. Each set of data was independently analyzed by three or four researchers who then discussed any neurons they disagreed on to generate a ‘consensus annotation’. Giovannucci et al. then used CaImAn to analyze the same sets of data and compared the results to the consensus annotations. This demonstrated that CaImAn is nearly as good as human researchers at identifying active neurons in brain images., CaImAn provides a quicker method to analyze large sets of brain imaging data and is currently used by over a hundred laboratories across the world. The software is open source, meaning that it is freely-available and that users are encouraged to customize it and collaborate with other users to develop it further.},
	urldate = {2021-12-13},
	journal = {eLife},
	author = {Giovannucci, Andrea and Friedrich, Johannes and Gunn, Pat and Kalfon, Jérémie and Brown, Brandon L and Koay, Sue Ann and Taxidis, Jiannis and Najafi, Farzaneh and Gauthier, Jeffrey L and Zhou, Pengcheng and Khakh, Baljit S and Tank, David W and Chklovskii, Dmitri B and Pnevmatikakis, Eftychios A},
	pmid = {30652683},
	pmcid = {PMC6342523},
	pages = {e38173},
	file = {PubMed Central Full Text PDF:/home/zwerg/Zotero/storage/K26AUDD9/Giovannucci et al. - CaImAn an open source tool for scalable calcium im.pdf:application/pdf},
}

@article{padilla-parra_quantitative_2008-1,
	title = {Quantitative {FRET} analysis by fast acquisition time domain {FLIM} at high spatial resolution in living cells},
	volume = {95},
	issn = {1542-0086},
	doi = {10.1529/biophysj.108.131276},
	abstract = {Quantitative analysis in Förster resonance energy transfer (FRET) experiments in live cells for protein interaction studies is still a challenging issue. In a two-component system (FRET and no FRET donor species), fitting of fluorescence lifetime imaging microscopy (FLIM) data gives the fraction of donor molecules involved in FRET (f(D)) and the intrinsic transfer efficiency. But when fast FLIM acquisitions are used to monitor dynamic changes in protein-protein interactions at high spatial and temporal resolutions in living cells, photon statistics and time resolution are limited. In this case, fitting procedures are not reliable, even for single lifetime donors. We introduce the new concept of a minimal fraction of donor molecules involved in FRET (mf(D)), coming from the mathematical minimization of f(D). We find particular advantage in the use of mf(D) because it can be obtained without fitting procedures and it is derived directly from FLIM data. mf(D) constitutes an interesting quantitative parameter for live cell studies because it is related to the minimal relative concentration of interacting proteins. For multi-lifetime donors, the process of fitting complex fluorescence decays to find at least four reliable lifetimes is a near impossible task. Here, mf(D) extension for multi-lifetime donors is the only quantitative determinant. We applied this methodology for imaging the interaction between the bromodomains of TAF(II250) and acetylated histones H4 in living cells at high resolution. We show the existence of discrete acetylated chromatin domains where the minimal fraction of bromodomain interacting with acetylated H4 oscillates from 0.26 to 0.36 and whose size is smaller than half of one micron cube. We demonstrate that mf(D) by itself is a useful tool to investigate quantitatively protein interactions in live cells, especially when using fast FRET-FLIM acquisition times.},
	language = {eng},
	number = {6},
	journal = {Biophysical Journal},
	author = {Padilla-Parra, Sergi and Audugé, Nicolas and Coppey-Moisan, Maïté and Tramier, Marc},
	month = sep,
	year = {2008},
	pmid = {18539634},
	pmcid = {PMC2527249},
	keywords = {Humans, Cell Line, Microscopy, Fluorescence, Time Factors, Cell Survival, Green Fluorescent Proteins, Protein Binding, Cells, Acetylation, Chromatin, Fluorescence Resonance Energy Transfer, Histone Acetyltransferases, Histones, Photons, Protein Structure, Tertiary, TATA-Binding Protein Associated Factors, Transcription Factor TFIID},
	pages = {2976--2988},
	file = {Full Text:/home/zwerg/Zotero/storage/KLNZ5KAS/Padilla-Parra et al. - 2008 - Quantitative FRET analysis by fast acquisition tim.pdf:application/pdf},
}

@misc{noauthor_nvidia_2021,
	title = {{NVIDIA} brings collaborative {AI} to healthcare},
	url = {https://ai-med.io/more-news/nvidia-brings-collaborative-ai-to-healthcare/},
	abstract = {New open-source software provides a common computing foundation for federated learning, accelerating AI in healthcare.},
	language = {en-GB},
	urldate = {2021-12-09},
	journal = {AIMed},
	month = nov,
	year = {2021},
}

@article{lavin_simulation_2021,
	title = {Simulation {Intelligence}: {Towards} a {New} {Generation} of {Scientific} {Methods}},
	shorttitle = {Simulation {Intelligence}},
	url = {http://arxiv.org/abs/2112.03235},
	abstract = {The original "Seven Motifs" set forth a roadmap of essential methods for the field of scientific computing, where a motif is an algorithmic method that captures a pattern of computation and data movement. We present the "Nine Motifs of Simulation Intelligence", a roadmap for the development and integration of the essential algorithms necessary for a merger of scientific computing, scientific simulation, and artificial intelligence. We call this merger simulation intelligence (SI), for short. We argue the motifs of simulation intelligence are interconnected and interdependent, much like the components within the layers of an operating system. Using this metaphor, we explore the nature of each layer of the simulation intelligence operating system stack (SI-stack) and the motifs therein: (1) Multi-physics and multi-scale modeling; (2) Surrogate modeling and emulation; (3) Simulation-based inference; (4) Causal modeling and inference; (5) Agent-based modeling; (6) Probabilistic programming; (7) Differentiable programming; (8) Open-ended optimization; (9) Machine programming. We believe coordinated efforts between motifs offers immense opportunity to accelerate scientific discovery, from solving inverse problems in synthetic biology and climate science, to directing nuclear energy experiments and predicting emergent behavior in socioeconomic settings. We elaborate on each layer of the SI-stack, detailing the state-of-art methods, presenting examples to highlight challenges and opportunities, and advocating for specific ways to advance the motifs and the synergies from their combinations. Advancing and integrating these technologies can enable a robust and efficient hypothesis-simulation-analysis type of scientific method, which we introduce with several use-cases for human-machine teaming and automated science.},
	urldate = {2021-12-09},
	journal = {arXiv:2112.03235 [cs]},
	author = {Lavin, Alexander and Zenil, Hector and Paige, Brooks and Krakauer, David and Gottschlich, Justin and Mattson, Tim and Anandkumar, Anima and Choudry, Sanjay and Rocki, Kamil and Baydin, Atılım Güneş and Prunkl, Carina and Paige, Brooks and Isayev, Olexandr and Peterson, Erik and McMahon, Peter L. and Macke, Jakob and Cranmer, Kyle and Zhang, Jiaxin and Wainwright, Haruko and Hanuka, Adi and Veloso, Manuela and Assefa, Samuel and Zheng, Stephan and Pfeffer, Avi},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.03235},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computational Engineering, Finance, and Science, Computer Science - Mathematical Software},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/8ZT7URKV/Lavin et al. - 2021 - Simulation Intelligence Towards a New Generation .pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/VNS3MH4Z/2112.html:text/html},
}

@article{ba_layer_2016,
	title = {Layer {Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	urldate = {2021-12-08},
	journal = {arXiv:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.06450},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/XPM6AY7X/Ba et al. - 2016 - Layer Normalization.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/T8TXIVGX/1607.html:text/html},
}

@article{swedlow_global_2021,
	title = {A global view of standards for open image data formats and repositories},
	volume = {18},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-021-01113-7},
	doi = {10.1038/s41592-021-01113-7},
	abstract = {Imaging technologies are used throughout the life and biomedical sciences to understand mechanisms in biology and diagnosis and therapy in animal and human medicine. We present criteria for globally applicable guidelines for open image data tools and resources for the rapidly developing fields of biological and biomedical imaging.},
	language = {en},
	number = {12},
	urldate = {2021-12-07},
	journal = {Nature Methods},
	author = {Swedlow, Jason R. and Kankaanpää, Pasi and Sarkans, Ugis and Goscinski, Wojtek and Galloway, Graham and Malacrida, Leonel and Sullivan, Ryan P. and Härtel, Steffen and Brown, Claire M. and Wood, Christopher and Keppler, Antje and Paina, Federica and Loos, Ben and Zullino, Sara and Longo, Dario Livio and Aime, Silvio and Onami, Shuichi},
	month = dec,
	year = {2021},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 12
Primary\_atype: Comments \& Opinion
Publisher: Nature Publishing Group
Subject\_term: Data publication and archiving;Research data
Subject\_term\_id: data-publication-and-archiving;research-data},
	keywords = {Research data, Data publication and archiving},
	pages = {1440--1446},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/F23I2YE3/Swedlow et al. - 2021 - A global view of standards for open image data for.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/AYKLN3NK/s41592-021-01113-7.html:text/html},
}

@misc{noauthor_ome-ngff_nodate,
	title = {{OME}-{NGFF}: a next-generation file format for expanding bioimaging data-access strategies {\textbar} {Nature} {Methods}},
	url = {https://www.nature.com/articles/s41592-021-01326-w},
	urldate = {2021-12-07},
	file = {OME-NGFF\: a next-generation file format for expanding bioimaging data-access strategies | Nature Methods:/home/zwerg/Zotero/storage/ZTKQC2E2/s41592-021-01326-w.html:text/html},
}

@misc{noauthor_best_nodate,
	title = {Best practices and tools for reporting reproducible fluorescence microscopy methods {\textbar} {Nature} {Methods}},
	url = {https://www.nature.com/articles/s41592-021-01156-w},
	urldate = {2021-12-07},
	file = {Best practices and tools for reporting reproducible fluorescence microscopy methods | Nature Methods:/home/zwerg/Zotero/storage/X8TKGY2B/s41592-021-01156-w.html:text/html},
}

@article{sarkans_rembi_2021,
	title = {{REMBI}: {Recommended} {Metadata} for {Biological} {Images}—enabling reuse of microscopy data in biology},
	volume = {18},
	copyright = {2021 Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {{REMBI}},
	url = {https://www.nature.com/articles/s41592-021-01166-8},
	doi = {10.1038/s41592-021-01166-8},
	abstract = {Bioimaging data have significant potential for reuse, but unlocking this potential requires systematic archiving of data and metadata in public databases. We propose draft metadata guidelines to begin addressing the needs of diverse communities within light and electron microscopy. We hope this publication and the proposed Recommended Metadata for Biological Images (REMBI) will stimulate discussions about their implementation and future extension.},
	language = {en},
	number = {12},
	urldate = {2021-12-07},
	journal = {Nature Methods},
	author = {Sarkans, Ugis and Chiu, Wah and Collinson, Lucy and Darrow, Michele C. and Ellenberg, Jan and Grunwald, David and Hériché, Jean-Karim and Iudin, Andrii and Martins, Gabriel G. and Meehan, Terry and Narayan, Kedar and Patwardhan, Ardan and Russell, Matthew Robert Geoffrey and Saibil, Helen R. and Strambio-De-Castillia, Caterina and Swedlow, Jason R. and Tischer, Christian and Uhlmann, Virginie and Verkade, Paul and Barlow, Mary and Bayraktar, Omer and Birney, Ewan and Catavitello, Cesare and Cawthorne, Christopher and Wagner-Conrad, Stephan and Duke, Elizabeth and Paul-Gilloteaux, Perrine and Gustin, Emmanuel and Harkiolaki, Maria and Kankaanpää, Pasi and Lemberger, Thomas and McEntyre, Jo and Moore, Josh and Nicholls, Andrew W. and Onami, Shuichi and Parkinson, Helen and Parsons, Maddy and Romanchikova, Marina and Sofroniew, Nicholas and Swoger, Jim and Utz, Nadine and Voortman, Lenard M. and Wong, Frances and Zhang, Peijun and Kleywegt, Gerard J. and Brazma, Alvis},
	month = dec,
	year = {2021},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 12
Primary\_atype: Comments \& Opinion
Publisher: Nature Publishing Group
Subject\_term: Data integration;Molecular imaging;Optical imaging
Subject\_term\_id: data-integration;molecular-imaging;optical-imaging},
	keywords = {Molecular imaging, Data integration, Optical imaging},
	pages = {1418--1422},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/MK58HHJC/Sarkans et al. - 2021 - REMBI Recommended Metadata for Biological Images—.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/VZ4EB2YD/s41592-021-01166-8.html:text/html},
}

@misc{noauthor_mdemic_nodate,
	title = {{MDEmic}: a metadata annotation tool to facilitate management of {FAIR} image data in the bioimaging community {\textbar} {Nature} {Methods}},
	url = {https://www.nature.com/articles/s41592-021-01288-z},
	urldate = {2021-12-07},
	file = {MDEmic\: a metadata annotation tool to facilitate management of FAIR image data in the bioimaging community | Nature Methods:/home/zwerg/Zotero/storage/8M45FMMZ/s41592-021-01288-z.html:text/html},
}

@misc{noauthor_methodsj2_nodate,
	title = {{MethodsJ2}: a software tool to capture metadata and generate comprehensive microscopy methods text {\textbar} {Nature} {Methods}},
	url = {https://www.nature.com/articles/s41592-021-01290-5},
	urldate = {2021-12-07},
	file = {MethodsJ2\: a software tool to capture metadata and generate comprehensive microscopy methods text | Nature Methods:/home/zwerg/Zotero/storage/CA4V5P39/s41592-021-01290-5.html:text/html},
}

@article{rigano_micro-meta_2021,
	title = {Micro-{Meta} {App}: an interactive tool for collecting microscopy metadata based on community specifications},
	copyright = {2021 The Author(s)},
	issn = {1548-7105},
	shorttitle = {Micro-{Meta} {App}},
	url = {https://www.nature.com/articles/s41592-021-01315-z},
	doi = {10.1038/s41592-021-01315-z},
	abstract = {For quality, interpretation, reproducibility and sharing value, microscopy images should be accompanied by detailed descriptions of the conditions that were used to produce them. Micro-Meta App is an intuitive, highly interoperable, open-source software tool that was developed in the context of the 4D Nucleome (4DN) consortium and is designed to facilitate the extraction and collection of relevant microscopy metadata as specified by the recent 4DN-BINA-OME tiered-system of Microscopy Metadata specifications. In addition to substantially lowering the burden of quality assurance, the visual nature of Micro-Meta App makes it particularly suited for training purposes.},
	language = {en},
	urldate = {2021-12-07},
	journal = {Nature Methods},
	author = {Rigano, Alessandro and Ehmsen, Shannon and Öztürk, Serkan Utku and Ryan, Joel and Balashov, Alexander and Hammer, Mathias and Kirli, Koray and Boehm, Ulrike and Brown, Claire M. and Bellve, Karl and Chambers, James J. and Cosolo, Andrea and Coleman, Robert A. and Faklaris, Orestis and Fogarty, Kevin E. and Guilbert, Thomas and Hamacher, Anna B. and Itano, Michelle S. and Keeley, Daniel P. and Kunis, Susanne and Lacoste, Judith and Laude, Alex and Ma, Willa Y. and Marcello, Marco and Montero-Llopis, Paula and Nelson, Glyn and Nitschke, Roland and Pimentel, Jaime A. and Weidtkamp-Peters, Stefanie and Park, Peter J. and Alver, Burak H. and Grunwald, David and Strambio-De-Castillia, Caterina},
	month = dec,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Confocal microscopy;Data publication and archiving;Software;Standards;Wide-field fluorescence microscopy
Subject\_term\_id: confocal-microscopy;data-publication-and-archiving;software;standards;wide-field-fluorescence-microscopy},
	keywords = {Software, Confocal microscopy, Data publication and archiving, Standards, Wide-field fluorescence microscopy},
	pages = {1--7},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/PJ3MUEMF/Rigano et al. - 2021 - Micro-Meta App an interactive tool for collecting.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/AK4IUN82/s41592-021-01315-z.html:text/html},
}

@misc{fallisch_special_2021,
	title = {Special {Issue} in {Nature} {Methods} released},
	url = {https://quarep.org/special-issue-in-nature-methods-released/},
	abstract = {Today, December 3, 2021, a special edition of Nature Methods was published containing an amazing selection of publications aimed at improving the quality of res},
	language = {en-US},
	urldate = {2021-12-07},
	journal = {QUAREP},
	author = {Fallisch, Arne},
	month = dec,
	year = {2021},
	file = {Snapshot:/home/zwerg/Zotero/storage/CC9M7EE7/special-issue-in-nature-methods-released.html:text/html},
}

@article{hammer_towards_2021,
	title = {Towards community-driven metadata standards for light microscopy: tiered specifications extending the {OME} model},
	volume = {18},
	copyright = {2021 Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {Towards community-driven metadata standards for light microscopy},
	url = {https://www.nature.com/articles/s41592-021-01327-9},
	doi = {10.1038/s41592-021-01327-9},
	abstract = {Rigorous record-keeping and quality control are required to ensure the quality, reproducibility and value of imaging data. The 4DN Initiative and BINA here propose light Microscopy Metadata Specifications that extend the OME Data Model, scale with experimental intent and complexity, and make it possible for scientists to create comprehensive records of imaging experiments.},
	language = {en},
	number = {12},
	urldate = {2021-12-07},
	journal = {Nature Methods},
	author = {Hammer, Mathias and Huisman, Maximiliaan and Rigano, Alessandro and Boehm, Ulrike and Chambers, James J. and Gaudreault, Nathalie and North, Alison J. and Pimentel, Jaime A. and Sudar, Damir and Bajcsy, Peter and Brown, Claire M. and Corbett, Alexander D. and Faklaris, Orestis and Lacoste, Judith and Laude, Alex and Nelson, Glyn and Nitschke, Roland and Farzam, Farzin and Smith, Carlas S. and Grunwald, David and Strambio-De-Castillia, Caterina},
	month = dec,
	year = {2021},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 12
Primary\_atype: Comments \& Opinion
Publisher: Nature Publishing Group
Subject\_term: Data acquisition;Data publication and archiving;Fluorescence imaging;Image processing;Standards
Subject\_term\_id: data-acquisition;data-publication-and-archiving;fluorescence-imaging;image-processing;standards},
	keywords = {Image processing, Fluorescence imaging, Data publication and archiving, Standards, Data acquisition},
	pages = {1427--1440},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/I6294R4P/Hammer et al. - 2021 - Towards community-driven metadata standards for li.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/ECC343HN/s41592-021-01327-9.html:text/html},
}

@article{boehm_quarep-limi_2021,
	title = {{QUAREP}-{LiMi}: a community endeavor to advance quality assessment and reproducibility in light microscopy},
	volume = {18},
	copyright = {2021 Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {{QUAREP}-{LiMi}},
	url = {https://www.nature.com/articles/s41592-021-01162-y},
	doi = {10.1038/s41592-021-01162-y},
	abstract = {The community-driven initiative Quality Assessment and Reproducibility for Instruments \& Images in Light Microscopy (QUAREP-LiMi) wants to improve reproducibility for light microscopy image data through quality control (QC) management of instruments and images. It aims for a common set of QC guidelines for hardware calibration and image acquisition, management and analysis.},
	language = {en},
	number = {12},
	urldate = {2021-12-07},
	journal = {Nature Methods},
	author = {Boehm, Ulrike and Nelson, Glyn and Brown, Claire M. and Bagley, Steve and Bajcsy, Peter and Bischof, Johanna and Dauphin, Aurelien and Dobbie, Ian M. and Eriksson, John E. and Faklaris, Orestis and Fernandez-Rodriguez, Julia and Ferrand, Alexia and Gelman, Laurent and Gheisari, Ali and Hartmann, Hella and Kukat, Christian and Laude, Alex and Mitkovski, Miso and Munck, Sebastian and North, Alison J. and Rasse, Tobias M. and Resch-Genger, Ute and Schuetz, Lucas C. and Seitz, Arne and Strambio-De-Castillia, Caterina and Swedlow, Jason R. and Nitschke, Roland},
	month = dec,
	year = {2021},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 12
Primary\_atype: Comments \& Opinion
Publisher: Nature Publishing Group
Subject\_term: Microscopy;Research data
Subject\_term\_id: microscopy;research-data},
	keywords = {Microscopy, Research data},
	pages = {1423--1426},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/9VKAMNE4/Boehm et al. - 2021 - QUAREP-LiMi a community endeavor to advance quali.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/H89JHLBC/s41592-021-01162-y.html:text/html},
}

@article{valizadegan_exominer_2021,
	title = {{ExoMiner}: {A} {Highly} {Accurate} and {Explainable} {Deep} {Learning} {Classifier} to {Mine} {Exoplanets}},
	shorttitle = {{ExoMiner}},
	url = {http://arxiv.org/abs/2111.10009},
	abstract = {The kepler and TESS missions have generated over 100,000 potential transit signals that must be processed in order to create a catalog of planet candidates. During the last few years, there has been a growing interest in using machine learning to analyze these data in search of new exoplanets. Different from the existing machine learning works, ExoMiner, the proposed deep learning classifier in this work, mimics how domain experts examine diagnostic tests to vet a transit signal. ExoMiner is a highly accurate, explainable, and robust classifier that 1) allows us to validate 301 new exoplanets from the MAST Kepler Archive and 2) is general enough to be applied across missions such as the on-going TESS mission. We perform an extensive experimental study to verify that ExoMiner is more reliable and accurate than the existing transit signal classifiers in terms of different classification and ranking metrics. For example, for a fixed precision value of 99\%, ExoMiner retrieves 93.6\% of all exoplanets in the test set (i.e., recall=0.936) while this rate is 76.3\% for the best existing classifier. Furthermore, the modular design of ExoMiner favors its explainability. We introduce a simple explainability framework that provides experts with feedback on why ExoMiner classifies a transit signal into a specific class label (e.g., planet candidate or not planet candidate).},
	urldate = {2021-12-06},
	journal = {arXiv:2111.10009 [astro-ph]},
	author = {Valizadegan, Hamed and Martinho, Miguel and Wilkens, Laurent S. and Jenkins, Jon M. and Smith, Jeffrey and Caldwell, Douglas A. and Twicken, Joseph D. and Gerum, Pedro C. and Walia, Nikash and Hausknecht, Kaylie and Lubin, Noa Y. and Bryson, Stephen T. and Oza, Nikunj C.},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.10009},
	keywords = {Computer Science - Machine Learning, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Earth and Planetary Astrophysics, J.2, I.2.6},
	annote = {Comment: Accepted for Publication in Astrophysical Journals, November 20201},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/SXB4SWAM/Valizadegan et al. - 2021 - ExoMiner A Highly Accurate and Explainable Deep L.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/KDQILYTC/2111.html:text/html},
}

@article{croquet_automated_nodate,
	title = {Automated landmarking for palatal shape analysis using geometric deep learning},
	volume = {n/a},
	issn = {1601-6343},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ocr.12513},
	doi = {10.1111/ocr.12513},
	abstract = {Objectives To develop and evaluate a geometric deep-learning network to automatically place seven palatal landmarks on digitized maxillary dental casts. Settings and Sample Population The sample comprised individuals with permanent dentition of various ethnicities. The network was trained from manual landmark annotations on 732 dental casts and evaluated on 104 dental casts. Materials and Methods A geometric deep-learning network was developed to hierarchically learn features from point-clouds representing the 3D surface of each cast. These features predict the locations of seven palatal landmarks. Results Repeat-measurement reliability was {\textless}0.3 mm for all landmarks on all casts. Accuracy is promising. The proportion of test subjects with errors less than 2 mm was between 0.93 and 0.68, depending on the landmark. Unusually shaped and large palates generate the highest errors. There was no evidence for a difference in mean palatal shape estimated from manual compared to the automatic landmarking. The automatic landmarking reduces sample variation around the mean and reduces measurements of palatal size. Conclusions The automatic landmarking method shows excellent repeatability and promising accuracy, which can streamline patient assessment and research studies. However, landmark indications should be subject to visual quality control.},
	language = {en},
	number = {n/a},
	urldate = {2021-12-06},
	journal = {Orthodontics \& Craniofacial Research},
	author = {Croquet, Balder and Matthews, Harold and Mertens, Jules and Fan, Yi and Nauwelaers, Nele and Mahdi, Soha and Hoskens, Hanne and El Sergani, Ahmed and Xu, Tianmin and Vandermeulen, Dirk and Bronstein, Michael and Marazita, Mary and Weinberg, Seth and Claes, Peter},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ocr.12513},
	keywords = {3D shape analysis, automatic landmarking, geometric deep learning, palate},
	file = {Snapshot:/home/zwerg/Zotero/storage/C59TIB7Y/ocr.html:text/html},
}

@article{hawthorne_sequence--sequence_2021,
	title = {{SEQUENCE}-{TO}-{SEQUENCE} {PIANO} {TRANSCRIPTION} {WITH} {TRANSFORMERS}},
	abstract = {Automatic Music Transcription has seen signiﬁcant progress in recent years by training custom deep neural networks on large datasets. However, these models have required extensive domain-speciﬁc design of network architectures, input/output representations, and complex decoding schemes. In this work, we show that equivalent performance can be achieved using a generic encoderdecoder Transformer with standard decoding methods. We demonstrate that the model can learn to translate spectrogram inputs directly to MIDI-like output events for several transcription tasks. This sequence-to-sequence approach simpliﬁes transcription by jointly modeling audio features and language-like output dependencies, thus removing the need for task-speciﬁc architectures. These results point toward possibilities for creating new Music Information Retrieval models by focusing on dataset creation and labeling rather than custom model design.},
	language = {en},
	author = {Hawthorne, Curtis and Simon, Ian and Swavely, Rigel and Manilow, Ethan and Engel, Jesse},
	year = {2021},
	pages = {8},
	file = {Hawthorne et al. - 2021 - SEQUENCE-TO-SEQUENCE PIANO TRANSCRIPTION WITH TRAN.pdf:/home/zwerg/Zotero/storage/FNFQUZS6/Hawthorne et al. - 2021 - SEQUENCE-TO-SEQUENCE PIANO TRANSCRIPTION WITH TRAN.pdf:application/pdf},
}

@misc{noauthor_real-time_nodate,
	title = {Real-time machine learning: challenges and solutions},
	url = {https://huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html},
	urldate = {2022-01-05},
	file = {Real-time machine learning\: challenges and solutions:/home/zwerg/Zotero/storage/D6PTCJIZ/real-time-machine-learning-challenges-and-solutions.html:text/html},
}

@inproceedings{nguyen_dataset_2021,
	title = {Dataset {Distillation} with {Infinitely} {Wide} {Convolutional} {Networks}},
	url = {https://openreview.net/forum?id=hXWPpJedrVP},
	abstract = {We achieve state of the art dataset distillation results on a variety of datasets using large-scale distributed metalearning with neural kernels.},
	language = {en},
	urldate = {2022-01-05},
	author = {Nguyen, Timothy and Novak, Roman and Xiao, Lechao and Lee, Jaehoon},
	month = may,
	year = {2021},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/3KM79L2F/Nguyen et al. - 2021 - Dataset Distillation with Infinitely Wide Convolut.pdf:application/pdf},
}

@misc{khan_setting_2022,
	title = {Setting up an {MLOps} pipeline with {Microsoft} {Azure}},
	url = {https://heartbeat.comet.ml/setting-up-an-ml-ops-pipeline-with-microsoft-azure-2e9e7f04a16b},
	abstract = {How to build end-to-end CI/CD pipeline for your ML workflows by leveraging Microsoft’s Azure Machine Learning platform},
	language = {en},
	urldate = {2022-01-05},
	journal = {Medium},
	author = {Khan, Jamshed},
	month = jan,
	year = {2022},
	file = {Snapshot:/home/zwerg/Zotero/storage/XMA8EJQK/setting-up-an-ml-ops-pipeline-with-microsoft-azure-2e9e7f04a16b.html:text/html},
}

@misc{noauthor_ml_2021,
	title = {{ML} {Model} {Registry}: {What} {It} {Is}, {Why} {It} {Matters}, {How} to {Implement} {It}},
	shorttitle = {{ML} {Model} {Registry}},
	url = {https://neptune.ai/blog/ml-model-registry},
	abstract = {Why do you have to know more about model registry? If you were once the only data scientist on your team you can probably relate to this: you start working on a machine learning project and perform a series of experiments that produce various models (and artifacts) that you “track” through non-standard naming conventions. Since […]},
	language = {en-US},
	urldate = {2022-01-05},
	journal = {neptune.ai},
	month = dec,
	year = {2021},
	file = {Snapshot:/home/zwerg/Zotero/storage/4IPFACJW/ml-model-registry.html:text/html},
}

@article{lee_vision_2021,
	title = {Vision {Transformer} for {Small}-{Size} {Datasets}},
	url = {http://arxiv.org/abs/2112.13492},
	abstract = {Recently, the Vision Transformer (ViT), which applied the transformer structure to the image classification task, has outperformed convolutional neural networks. However, the high performance of the ViT results from pre-training using a large-size dataset such as JFT-300M, and its dependence on a large dataset is interpreted as due to low locality inductive bias. This paper proposes Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA), which effectively solve the lack of locality inductive bias and enable it to learn from scratch even on small-size datasets. Moreover, SPT and LSA are generic and effective add-on modules that are easily applicable to various ViTs. Experimental results show that when both SPT and LSA were applied to the ViTs, the performance improved by an average of 2.96\% in Tiny-ImageNet, which is a representative small-size dataset. Especially, Swin Transformer achieved an overwhelming performance improvement of 4.08\% thanks to the proposed SPT and LSA.},
	urldate = {2022-01-05},
	journal = {arXiv:2112.13492 [cs]},
	author = {Lee, Seung Hoon and Lee, Seunghyun and Song, Byung Cheol},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.13492
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/IJJ65W4G/Lee et al. - 2021 - Vision Transformer for Small-Size Datasets.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/PMEKYC6D/2112.html:text/html},
}

@misc{bissuel_hyper-parameter_2019,
	title = {Hyper-parameter optimization algorithms: a short review},
	shorttitle = {Hyper-parameter optimization algorithms},
	url = {https://medium.com/criteo-engineering/hyper-parameter-optimization-algorithms-2fe447525903},
	abstract = {At Criteo AI labs, we are using more and more complex machine learning (ML) models every day. This means that we have many more…},
	language = {en},
	urldate = {2022-01-05},
	journal = {Criteo R\&D Blog},
	author = {Bissuel, Aloïs},
	month = apr,
	year = {2019},
	file = {Snapshot:/home/zwerg/Zotero/storage/GD997V5D/hyper-parameter-optimization-algorithms-2fe447525903.html:text/html},
}

@misc{sankaranarayanan_pdfpdf_nodate,
	title = {pdf.pdf},
	url = {https://openreview.net/pdf?id=iMSjopcOn0p},
	urldate = {2022-01-05},
	author = {Sankaranarayanan, Sriram},
	file = {pdf.pdf:/home/zwerg/Zotero/storage/JEA6US9G/pdf.pdf:application/pdf},
}

@article{jumper_protein_2022,
	title = {Protein structure predictions to atomic accuracy with {AlphaFold}},
	volume = {19},
	issn = {1548-7091, 1548-7105},
	url = {https://www.nature.com/articles/s41592-021-01362-6},
	doi = {10.1038/s41592-021-01362-6},
	language = {en},
	number = {1},
	urldate = {2022-01-12},
	journal = {Nature Methods},
	author = {Jumper, John and Hassabis, Demis},
	month = jan,
	year = {2022},
	pages = {11--12},
	file = {Jumper and Hassabis - 2022 - Protein structure predictions to atomic accuracy w.pdf:/home/zwerg/Zotero/storage/SY48463Q/Jumper and Hassabis - 2022 - Protein structure predictions to atomic accuracy w.pdf:application/pdf},
}

@techreport{baharlou_afid_2019,
	title = {{AFid}: {A} tool for automated identification and exclusion of autofluorescent objects from microscopy images},
	copyright = {© 2019, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	shorttitle = {{AFid}},
	url = {https://www.biorxiv.org/content/10.1101/566315v2},
	abstract = {Autofluorescence is a long-standing problem that has hindered the analysis of images of tissues acquired by fluorescence microscopy. Current approaches to mitigate autofluorescence in tissue are lab-based and involve either chemical treatment of sections or specialized instrumentation and software to ‘unmix’ autofluorescent signals. Importantly, these approaches are pre-emptive and there are currently no methods to deal with autofluorescence in acquired fluorescence microscopy images. To address this, we developed Autofluorescence Identifier (AFid). AFid identifies autofluorescent pixels as discrete objects in multi-channel images post acquisition. These objects can then be tagged for exclusion from downstream analysis. We validated AFid using images of FFPE human colorectal tissue stained for common immune markers. Further, we demonstrate its utility for image analysis where its implementation allows the accurate measurement of HIV-Dendritic Cell interactions in a colorectal explant model of HIV transmission.
Availability and implementation https://ellispatrick.github.io/AFid
Contact ellis.patrick\{at\}sydney.edu.au},
	language = {en},
	urldate = {2022-01-12},
	author = {Baharlou, Heeva and Canete, Nicolas P. and Bertram, Kirstie M. and Sandgren, Kerrie J. and Cunningham, Anthony L. and Harman, Andrew N. and Patrick, Ellis},
	month = nov,
	year = {2019},
	doi = {10.1101/566315},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	pages = {566315},
	file = {Full Text:/home/zwerg/Zotero/storage/7KGSUCSZ/Baharlou et al. - 2019 - AFid A tool for automated identification and excl.pdf:application/pdf;Full Text PDF:/home/zwerg/Zotero/storage/B6VJ9QZZ/Baharlou et al. - 2019 - AFid A tool for automated identification and excl.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/PIW42RUS/566315v2.html:text/html},
}

@inproceedings{choi_channel_2020,
	title = {Channel {Attention} {Is} {All} {You} {Need} for {Video} {Frame} {Interpolation}},
	doi = {10.1609/AAAI.V34I07.6693},
	abstract = {A simple but effective deep neural network for video frame interpolation, which is end-to-end trainable and is free from a motion estimation network component, and achieves outstanding performance compared to the existing models with a component for optical flow computation. Prevailing video frame interpolation techniques rely heavily on optical flow estimation and require additional model complexity and computational cost; it is also susceptible to error propagation in challenging scenarios with large motion and heavy occlusion. To alleviate the limitation, we propose a simple but effective deep neural network for video frame interpolation, which is end-to-end trainable and is free from a motion estimation network component. Our algorithm employs a special feature reshaping operation, referred to as PixelShuffle, with a channel attention, which replaces the optical flow computation module. The main idea behind the design is to distribute the information in a feature map into multiple channels and extract motion information by attending the channels for pixel-level frame synthesis. The model given by this principle turns out to be effective in the presence of challenging motion and occlusion. We construct a comprehensive evaluation benchmark and demonstrate that the proposed approach achieves outstanding performance compared to the existing models with a component for optical flow computation.},
	booktitle = {{AAAI}},
	author = {Choi, Myungsub and Kim, Heewon and Han, Bohyung and Xu, N. and Lee, Kyoung Mu},
	year = {2020},
	file = {Full Text:/home/zwerg/Zotero/storage/TUS2H9CI/Choi et al. - 2020 - Channel Attention Is All You Need for Video Frame .pdf:application/pdf},
}

@misc{noauthor_cambrian-ai_nodate,
	title = {Cambrian-{AI} {Research} {LLC}},
	url = {https://cambrian-ai.com/},
	abstract = {Cambrian-AI Research assists AI Semiconductor companies and their investors to improve decision-making and communicate their value proposition.},
	language = {en-US},
	urldate = {2022-01-21},
	journal = {Cambrian AI Research},
}

@misc{cutrale_hybrid_nodate,
	title = {Hybrid {Unmixing} {Software}},
	url = {https://bioimage.usc.edu/documents/README_HyU_20211220_FC_DK.pdf},
	urldate = {2022-01-26},
	author = {Cutrale, Francesco},
	file = {README_HyU_20211220_FC_DK.pdf:/home/zwerg/Zotero/storage/C2AULKWS/README_HyU_20211220_FC_DK.pdf:application/pdf},
}

@techreport{chiang_hyu_2022,
	type = {preprint},
	title = {{HyU}: {Hybrid} {Unmixing} for longitudinal in vivo imaging of low signal to noise fluorescence},
	shorttitle = {{HyU}},
	url = {https://www.researchsquare.com/article/rs-1073331/v1},
	abstract = {Abstract
          The expanded application of fluorescence imaging in biomedical and biological research towards more complex systems and geometries requires tools that can analyze a multitude of components at widely varying time- and length-scales.  The major challenge in such complex imaging experiments is to cleanly separate multiple fluorescent labels with overlapping spectra from one another and background autofluorescence, without perturbing the sample with high levels of light.  Thus, there is a requirement for efficient and robust analysis tools capable of quantitatively separating these signals.  
In response, we have combined multispectral fluorescence microscopy with hyperspectral phasors and linear unmixing to create Hybrid Unmixing (HyU).  Here we demonstrate its capabilities in the dynamic imaging of multiple fluorescent labels in live, developing zebrafish embryos.  HyU is more sensitive to low light levels of fluorescence compared to conventional linear unmixing approaches, permitting better multiplexed volumetric imaging over time, with less bleaching.  HyU can also simultaneously image both bright exogenous and dim endogenous labels because of its high dynamic range.  This allows studies of cellular behaviors, tagged components, and cell metabolism within the same specimen, offering a powerful window into the orchestrated complexity of biological systems.},
	language = {en},
	urldate = {2022-01-26},
	institution = {In Review},
	author = {Chiang, Hsiao and Koo, Daniel and Kitano, Masahiro and Unruh, Jay and Trinh, Le and Fraser, Scott and Cutrale, Francesco},
	month = jan,
	year = {2022},
	doi = {10.21203/rs.3.rs-1073331/v1},
	file = {Chiang et al. - 2022 - HyU Hybrid Unmixing for longitudinal in vivo imag.pdf:/home/zwerg/Zotero/storage/MKHZRASM/Chiang et al. - 2022 - HyU Hybrid Unmixing for longitudinal in vivo imag.pdf:application/pdf},
}

@article{roding_highly_2019,
	title = {A {Highly} {Accurate} {Pixel}-{Based} {FRAP} {Model} {Based} on {Spectral}-{Domain} {Numerical} {Methods}},
	volume = {116},
	doi = {10.1016/j.bpj.2019.02.023},
	abstract = {We introduce a new, to our knowledge, numerical model based on spectral methods for analysis of fluorescence recovery after photobleaching data. The model covers pure diffusion and diffusion and binding (reaction-diffusion) with immobile binding sites, as well as arbitrary bleach region shapes. Fitting of the model is supported using both conventional recovery-curve-based estimation and pixel-based estimation, in which all individual pixels in the data are utilized. The model explicitly accounts for multiple bleach frames, diffusion (and binding) during bleaching, and bleaching during imaging. To our knowledge, no other fluorescence recovery after photobleaching framework incorporates all these model features and estimation methods. We thoroughly validate the model by comparison to stochastic simulations of particle dynamics and find it to be highly accurate. We perform simulation studies to compare recovery-curve-based estimation and pixel-based estimation in realistic settings and show that pixel-based estimation is the better method for parameter estimation as well as for distinguishing pure diffusion from diffusion and binding. We show that accounting for multiple bleach frames is important and that the effect of neglecting this is qualitatively different for the two estimation methods. We perform a simple experimental validation showing that pixel-based estimation provides better agreement with literature values than recovery-curve-based estimation and that accounting for multiple bleach frames improves the result. Further, the software developed in this work is freely available online.},
	journal = {Biophysical Journal},
	author = {Röding, Magnus and Lacroix, Leander and Krona, Annika and Gebäck, Tobias and Lorén, Niklas},
	month = mar,
	year = {2019},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/X92WUQSU/Röding et al. - 2019 - A Highly Accurate Pixel-Based FRAP Model Based on .pdf:application/pdf},
}

@article{skarstrom_deepfrap_2020,
	title = {{DeepFRAP}: {Fast} fluorescence recovery after photobleaching data analysis using deep neural networks},
	volume = {282},
	shorttitle = {{DeepFRAP}},
	doi = {10.1111/jmi.12989},
	abstract = {Conventional analysis of fluorescence recovery after photobleaching (FRAP) data for diffusion coefficient estimation typically involves fitting an analytical or numerical FRAP model to the recovery curve data using non‐linear least squares. Depending on the model, this can be time consuming, especially for batch analysis of large numbers of data sets and if multiple initial guesses for the parameter vector are used to ensure convergence. In this work, we develop a completely new approach, DeepFRAP, utilizing machine learning for parameter estimation in FRAP. From a numerical FRAP model developed in previous work, we generate a very large set of simulated recovery curve data with realistic noise levels. The data are used for training different deep neural network regression models for prediction of several parameters, most importantly the diffusion coefficient. The neural networks are extremely fast and can estimate the parameters orders of magnitude faster than least squares. The performance of the neural network estimation framework is compared to conventional least squares estimation on simulated data, and found to be strikingly similar. Also, a simple experimental validation is performed, demonstrating excellent agreement between the two methods. We make the data and code used publicly available to facilitate further development of machine learning‐based estimation in FRAP.

Lay description
Fluorescence recovery after photobleaching (FRAP) is one of the most frequently used methods for microscopy‐based diffusion measurements and broadly used in materials science, pharmaceutics, food science and cell biology. In a FRAP experiment, a laser is used to photobleach fluorescent particles in a region. By analysing the recovery of the fluorescence intensity due to the diffusion of still fluorescent particles, the diffusion coefficient and other parameters can be estimated. Typically, a confocal laser scanning microscope (CLSM) is used to image the time evolution of the recovery, and a model is fit using least squares to obtain parameter estimates. In this work, we introduce a new, fast and accurate method for analysis of data from FRAP. The new method is based on using artificial neural networks to predict parameter values, such as the diffusion coefficient, effectively circumventing classical least squares fitting. This leads to a dramatic speed‐up, especially noticeable when analysing large numbers of FRAP data sets, while still producing results in excellent agreement with least squares. Further, the neural network estimates can be used as very good initial guesses for least squares estimation in order to make the least squares optimization convergence much faster than it otherwise would. This provides for obtaining, for example, diffusion coefficients as soon as possible, spending minimal time on data analysis. In this fashion, the proposed method facilitates efficient use of the experimentalist's time which is the main motivation to our approach. The concept is demonstrated on pure diffusion. However, the concept can easily be extended to the diffusion and binding case. The concept is likely to be useful in all application areas of FRAP, including diffusion in cells, gels and solutions.},
	journal = {Journal of Microscopy},
	author = {Skärström, Victor and Krona, Annika and Lorén, Niklas and Röding, Magnus},
	month = nov,
	year = {2020},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/TGIYC5AQ/Skärström et al. - 2020 - DeepFRAP Fast fluorescence recovery after photobl.pdf:application/pdf},
}

@article{brock_high-performance_2021,
	title = {High-{Performance} {Large}-{Scale} {Image} {Recognition} {Without} {Normalization}},
	url = {http://arxiv.org/abs/2102.06171},
	abstract = {Batch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5\%. In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when finetuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2\%. Our code is available at https://github.com/deepmind/ deepmind-research/tree/master/nfnets},
	urldate = {2022-02-04},
	journal = {arXiv:2102.06171 [cs, stat]},
	author = {Brock, Andrew and De, Soham and Smith, Samuel L. and Simonyan, Karen},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.06171},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/NJ2EDKRS/Brock et al. - 2021 - High-Performance Large-Scale Image Recognition Wit.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/2BY6SBWH/2102.html:text/html},
}

@article{jo_puzzle-cam_2021,
	title = {Puzzle-{CAM}: {Improved} localization via matching partial and full features},
	shorttitle = {Puzzle-{CAM}},
	url = {http://arxiv.org/abs/2101.11253},
	doi = {10.1109/ICIP42928.2021.9506058},
	abstract = {Weakly-supervised semantic segmentation (WSSS) is introduced to narrow the gap for semantic segmentation performance from pixel-level supervision to image-level supervision. Most advanced approaches are based on class activation maps (CAMs) to generate pseudo-labels to train the segmentation network. The main limitation of WSSS is that the process of generating pseudo-labels from CAMs that use an image classifier is mainly focused on the most discriminative parts of the objects. To address this issue, we propose Puzzle-CAM, a process that minimizes differences between the features from separate patches and the whole image. Our method consists of a puzzle module and two regularization terms to discover the most integrated region in an object. Puzzle-CAM can activate the overall region of an object using image-level supervision without requiring extra parameters. \% In experiments, Puzzle-CAM outperformed previous state-of-the-art methods using the same labels for supervision on the PASCAL VOC 2012 test dataset. In experiments, Puzzle-CAM outperformed previous state-of-the-art methods using the same labels for supervision on the PASCAL VOC 2012 dataset. Code associated with our experiments is available at https://github.com/OFRIN/PuzzleCAM.},
	urldate = {2022-02-04},
	journal = {2021 IEEE International Conference on Image Processing (ICIP)},
	author = {Jo, Sanghyun and Yu, In-Jae},
	month = sep,
	year = {2021},
	note = {arXiv: 2101.11253},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	pages = {639--643},
	annote = {Comment: Accepted to ICIP 2021},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/PMJ3673L/Jo and Yu - 2021 - Puzzle-CAM Improved localization via matching par.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/JBNZT4JC/2101.html:text/html},
}

@article{kim_discriminative_2021,
	title = {Discriminative {Region} {Suppression} for {Weakly}-{Supervised} {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/2103.07246},
	abstract = {Weakly-supervised semantic segmentation (WSSS) using image-level labels has recently attracted much attention for reducing annotation costs. Existing WSSS methods utilize localization maps from the classification network to generate pseudo segmentation labels. However, since localization maps obtained from the classifier focus only on sparse discriminative object regions, it is difficult to generate high-quality segmentation labels. To address this issue, we introduce discriminative region suppression (DRS) module that is a simple yet effective method to expand object activation regions. DRS suppresses the attention on discriminative regions and spreads it to adjacent non-discriminative regions, generating dense localization maps. DRS requires few or no additional parameters and can be plugged into any network. Furthermore, we introduce an additional learning strategy to give a self-enhancement of localization maps, named localization map refinement learning. Benefiting from this refinement learning, localization maps are refined and enhanced by recovering some missing parts or removing noise itself. Due to its simplicity and effectiveness, our approach achieves mIoU 71.4\% on the PASCAL VOC 2012 segmentation benchmark using only image-level labels. Extensive experiments demonstrate the effectiveness of our approach. The code is available at https://github.com/qjadud1994/DRS.},
	urldate = {2022-02-04},
	journal = {arXiv:2103.07246 [cs]},
	author = {Kim, Beomyoung and Han, Sangeun and Kim, Junmo},
	month = apr,
	year = {2021},
	note = {arXiv: 2103.07246},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: AAAI 2021, Accepted},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/VVMYJLA8/Kim et al. - 2021 - Discriminative Region Suppression for Weakly-Super.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/TEK3NAJE/2103.html:text/html},
}

@misc{noauthor_ai_2022,
	title = {{AI} and {Machine} {Learning} {Salaries} {Drop}},
	url = {https://spectrum.ieee.org/software-engineer-salary},
	abstract = {Overall, 2021 was a good year for tech professionals in the U.S., with the average salary up 6.9 percent to \$104,566, according to online recruitment firm Dice. Dice released these numbers last month as part of its annual Tech Salary Report.},
	language = {en},
	urldate = {2022-02-08},
	journal = {IEEE Spectrum},
	month = feb,
	year = {2022},
	file = {Snapshot:/home/zwerg/Zotero/storage/A7U2FGIG/software-engineer-salary.html:text/html},
}

@misc{noauthor_cellpose_nodate,
	title = {Cellpose 2.0: how to train your own model {\textbar} {bioRxiv}},
	url = {https://www.biorxiv.org/content/10.1101/2022.04.01.486764v1?rss=1},
	urldate = {2022-04-08},
}

@article{eschweiler_robust_2022,
	title = {Robust {3D} {Cell} {Segmentation}: {Extending} the {View} of {Cellpose}},
	shorttitle = {Robust {3D} {Cell} {Segmentation}},
	url = {http://arxiv.org/abs/2105.00794},
	abstract = {Increasing data set sizes of 3D microscopy imaging experiments demand for an automation of segmentation processes to be able to extract meaningful biomedical information. Due to the shortage of annotated 3D image data that can be used for machine learning-based approaches, 3D segmentation approaches are required to be robust and to generalize well to unseen data. The Cellpose approach proposed by Stringer et al. proved to be such a generalist approach for cell instance segmentation tasks. In this paper, we extend the Cellpose approach to improve segmentation accuracy on 3D image data and we further show how the formulation of the gradient maps can be simplified while still being robust and reaching similar segmentation accuracy. The code is publicly available and was integrated into two established open-source applications that allow using the 3D extension of Cellpose without any programming knowledge.},
	urldate = {2022-04-08},
	journal = {arXiv:2105.00794 [cs, eess]},
	author = {Eschweiler, Dennis and Smith, Richard S. and Stegmaier, Johannes},
	month = feb,
	year = {2022},
	note = {arXiv: 2105.00794},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/VYU5HAPG/Eschweiler et al. - 2022 - Robust 3D Cell Segmentation Extending the View of.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/LWC58YV7/2105.html:text/html},
}

@article{hollandi_nucleus_2022,
	title = {Nucleus segmentation: towards automated solutions},
	volume = {32},
	issn = {0962-8924, 1879-3088},
	shorttitle = {Nucleus segmentation},
	url = {https://www.cell.com/trends/cell-biology/abstract/S0962-8924(21)00251-8},
	doi = {10.1016/j.tcb.2021.12.004},
	language = {English},
	number = {4},
	urldate = {2022-04-08},
	journal = {Trends in Cell Biology},
	author = {Hollandi, Reka and Moshkov, Nikita and Paavolainen, Lassi and Tasnadi, Ervin and Piccinini, Filippo and Horvath, Peter},
	month = apr,
	year = {2022},
	pmid = {35067424},
	note = {Publisher: Elsevier},
	keywords = {deep learning, microscopy, image processing, nucleus segmentation, oncology, single-cell analysis},
	pages = {295--310},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/ZGNY8F63/Hollandi et al. - 2022 - Nucleus segmentation towards automated solutions.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/YAERGABG/S0962-8924(21)00251-8.html:text/html},
}

@misc{noauthor_introducing_nodate-1,
	title = {Introducing {Mephisto}: {A} new platform for more open, collaborative data collection},
	url = {https://ai.facebook.com/blog/introducing-mephisto-a-new-platform-for-more-open-collaborative-data-collection/},
	urldate = {2022-04-06},
}

@article{palla_squidpy_2022,
	title = {Squidpy: a scalable framework for spatial omics analysis {\textbar} {Nature} {Methods}},
	volume = {19},
	issn = {1548-7105},
	shorttitle = {Squidpy},
	url = {https://www.nature.com/articles/s41592-021-01358-2},
	doi = {10.1038/s41592-021-01358-2},
	abstract = {Spatial omics data are advancing the study of tissue organization and cellular communication at an unprecedented scale. Flexible tools are required to store, integrate and visualize the large diversity of spatial omics data. Here, we present Squidpy, a Python framework that brings together tools from omics and image analysis to enable scalable description of spatial molecular data, such as transcriptome or multivariate proteins. Squidpy provides efficient infrastructure and numerous analysis methods that allow to efficiently store, manipulate and interactively visualize spatial omics data. Squidpy is extensible and can be interfaced with a variety of already existing libraries for the scalable analysis of spatial omics data.},
	number = {2},
	urldate = {2022-03-28},
	journal = {Nature Methods},
	author = {Palla, Giovanni and Spitzer, Hannah and Klein, Michal and Fischer, David and Schaar, Anna Christina and Kuemmerle, Louis Benedikt and Rybakov, Sergei and Ibarra, Ignacio L. and Holmberg, Olle and Virshup, Isaac and Lotfollahi, Mohammad and Richter, Sabrina and Theis, Fabian J.},
	month = feb,
	year = {2022},
	pages = {171--178},
	file = {Palla et al. - 2022 - Squidpy a scalable framework for spatial omics analysis  Nature Methods.pdf:/home/zwerg/Zotero/storage/4FNHVXLR/Palla et al. - 2022 - Squidpy a scalable framework for spatial omics analysis  Nature Methods.pdf:application/pdf},
}

@misc{noauthor_opencv_nodate,
	title = {{OpenCV}: {Feature} {Detection} and {Description}},
	url = {https://docs.opencv.org/3.4/db/d27/tutorial_py_table_of_contents_feature2d.html},
	urldate = {2022-03-20},
}

@article{rublee_orb_2011,
	title = {{ORB}: {An} efficient alternative to {SIFT} or {SURF}},
	shorttitle = {{ORB}},
	doi = {10.1109/ICCV.2011.6126544},
	abstract = {This paper proposes a very fast binary descriptor based on BRIEF, called ORB, which is rotation invariant and resistant to noise, and demonstrates through experiments how ORB is at two orders of magnitude faster than SIFT, while performing as well in many situations. Feature matching is at the base of many computer vision problems, such as object recognition or structure from motion. Current methods rely on costly descriptors for detection and matching. In this paper, we propose a very fast binary descriptor based on BRIEF, called ORB, which is rotation invariant and resistant to noise. We demonstrate through experiments how ORB is at two orders of magnitude faster than SIFT, while performing as well in many situations. The efficiency is tested on several real-world applications, including object detection and patch-tracking on a smart phone.},
	journal = {2011 International Conference on Computer Vision},
	author = {Rublee, Ethan and Rabaud, V. and Konolige, K. and Bradski, G.},
	year = {2011},
}

@article{zhang_fully_2018,
	title = {Fully {Convolutional} {Adaptation} {Networks} for {Semantic} {Segmentation}},
	url = {https://arxiv.org/abs/1804.08286v1},
	doi = {10.48550/arXiv.1804.08286},
	abstract = {The recent advances in deep neural networks have convincingly demonstrated high capability in learning vision models on large datasets. Nevertheless, collecting expert labeled datasets especially with pixel-level annotations is an extremely expensive process. An appealing alternative is to render synthetic data (e.g., computer games) and generate ground truth automatically. However, simply applying the models learnt on synthetic images may lead to high generalization error on real images due to domain shift. In this paper, we facilitate this issue from the perspectives of both visual appearance-level and representation-level domain adaptation. The former adapts source-domain images to appear as if drawn from the "style" in the target domain and the latter attempts to learn domain-invariant representations. Specifically, we present Fully Convolutional Adaptation Networks (FCAN), a novel deep architecture for semantic segmentation which combines Appearance Adaptation Networks (AAN) and Representation Adaptation Networks (RAN). AAN learns a transformation from one domain to the other in the pixel space and RAN is optimized in an adversarial learning manner to maximally fool the domain discriminator with the learnt source and target representations. Extensive experiments are conducted on the transfer from GTA5 (game videos) to Cityscapes (urban street scenes) on semantic segmentation and our proposal achieves superior results when comparing to state-of-the-art unsupervised adaptation techniques. More remarkably, we obtain a new record: mIoU of 47.5\% on BDDS (drive-cam videos) in an unsupervised setting.},
	language = {en},
	urldate = {2022-03-20},
	author = {Zhang, Yiheng and Qiu, Zhaofan and Yao, Ting and Liu, Dong and Mei, Tao},
	month = apr,
	year = {2018},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/VQE3DGVQ/Zhang et al. - 2018 - Fully Convolutional Adaptation Networks for Semant.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/42FKGGPV/1804.html:text/html},
}

@article{liu_fully_2016,
	title = {Fully {Convolutional} {Attention} {Networks} for {Fine}-{Grained} {Recognition}},
	url = {https://arxiv.org/abs/1603.06765v4},
	doi = {10.48550/arXiv.1603.06765},
	abstract = {Fine-grained recognition is challenging due to its subtle local inter-class differences versus large intra-class variations such as poses. A key to address this problem is to localize discriminative parts to extract pose-invariant features. However, ground-truth part annotations can be expensive to acquire. Moreover, it is hard to define parts for many fine-grained classes. This work introduces Fully Convolutional Attention Networks (FCANs), a reinforcement learning framework to optimally glimpse local discriminative regions adaptive to different fine-grained domains. Compared to previous methods, our approach enjoys three advantages: 1) the weakly-supervised reinforcement learning procedure requires no expensive part annotations; 2) the fully-convolutional architecture speeds up both training and testing; 3) the greedy reward strategy accelerates the convergence of the learning. We demonstrate the effectiveness of our method with extensive experiments on four challenging fine-grained benchmark datasets, including CUB-200-2011, Stanford Dogs, Stanford Cars and Food-101.},
	language = {en},
	urldate = {2022-03-20},
	author = {Liu, Xiao and Xia, Tian and Wang, Jiang and Yang, Yi and Zhou, Feng and Lin, Yuanqing},
	month = mar,
	year = {2016},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/R3RUDZ7M/Liu et al. - 2016 - Fully Convolutional Attention Networks for Fine-Gr.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/6IXRTH5M/1603.html:text/html},
}

@article{ding_scaling_2022,
	title = {Scaling {Up} {Your} {Kernels} to 31x31: {Revisiting} {Large} {Kernel} {Design} in {CNNs}},
	shorttitle = {Scaling {Up} {Your} {Kernels} to 31x31},
	url = {http://arxiv.org/abs/2203.06717},
	abstract = {We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances of vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, e.g., applying re-parameterized large depth-wise convolutions, to design efficient high-performance large-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN architecture whose kernel size is as large as 31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the performance gap between CNNs and ViTs, e.g., achieving comparable or superior results than Swin Transformer on ImageNet and a few typical downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, obtaining 87.8\% top-1 accuracy on ImageNet and 56.0\% mIoU on ADE20K, which is very competitive among the state-of-the-arts with similar model sizes. Our study further reveals that, in contrast to small-kernel CNNs, large-kernel CNNs have much larger effective receptive fields, and higher shape bias rather than texture bias. Code \& models at https://github.com/megvii-research/RepLKNet.},
	urldate = {2022-03-20},
	journal = {arXiv:2203.06717 [cs]},
	author = {Ding, Xiaohan and Zhang, Xiangyu and Zhou, Yizhuang and Han, Jungong and Ding, Guiguang and Sun, Jian},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.06717},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by CVPR 2022},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/JJCNX548/Ding et al. - 2022 - Scaling Up Your Kernels to 31x31 Revisiting Large.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/ZG265M5K/2203.html:text/html},
}

@article{zhu_dftr_2022,
	title = {{DFTR}: {Depth}-supervised {Hierarchical} {Feature} {Fusion} {Transformer} for {Salient} {Object} {Detection}},
	shorttitle = {{DFTR}},
	url = {http://arxiv.org/abs/2203.06429},
	abstract = {Automated salient object detection (SOD) plays an increasingly crucial role in many computer vision applications. Although existing frameworks achieve impressive SOD performances especially with the development of deep learning techniques, their performances still have room for improvement. In this work, we propose a novel pure Transformer-based SOD framework, namely Depth-supervised hierarchical feature Fusion TRansformer (DFTR), to further improve the accuracy of both RGB and RGB-D SOD. The proposed DFTR involves three primary improvements: 1) The backbone of feature encoder is switched from a convolutional neural network to a Swin Transformer for more effective feature extraction; 2) We propose a multi-scale feature aggregation (MFA) module to fully exploit the multi-scale features encoded by the Swin Transformer in a coarse-to-fine manner; 3) Following recent studies, we formulate an auxiliary task of depth map prediction and use the ground-truth depth maps as extra supervision signals for network learning. To enable bidirectional information flow between saliency and depth branches, a novel multi-task feature fusion (MFF) module is integrated into our DFTR. We extensively evaluate the proposed DFTR on ten benchmarking datasets. Experimental results show that our DFTR consistently outperforms the existing state-of-the-art methods for both RGB and RGB-D SOD tasks. The code and model will be released.},
	urldate = {2022-03-20},
	journal = {arXiv:2203.06429 [cs]},
	author = {Zhu, Heqin and Sun, Xu and Li, Yuexiang and Ma, Kai and Zhou, S. Kevin and Zheng, Yefeng},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.06429},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 9 pages, 4 figures, 4 tables},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/IWFI8LPA/Zhu et al. - 2022 - DFTR Depth-supervised Hierarchical Feature Fusion.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/WGUZGD9B/2203.html:text/html},
}

@article{paul_vision_2021-1,
	title = {Vision {Transformers} are {Robust} {Learners}},
	url = {http://arxiv.org/abs/2105.07581},
	abstract = {Transformers, composed of multiple self-attention layers, hold strong promises toward a generic learning primitive applicable to different data modalities, including the recent breakthroughs in computer vision achieving state-of-the-art (SOTA) standard accuracy. What remains largely unexplored is their robustness evaluation and attribution. In this work, we study the robustness of the Vision Transformer (ViT) against common corruptions and perturbations, distribution shifts, and natural adversarial examples. We use six different diverse ImageNet datasets concerning robust classification to conduct a comprehensive performance comparison of ViT models and SOTA convolutional neural networks (CNNs), Big-Transfer. Through a series of six systematically designed experiments, we then present analyses that provide both quantitative and qualitative indications to explain why ViTs are indeed more robust learners. For example, with fewer parameters and similar dataset and pre-training combinations, ViT gives a top-1 accuracy of 28.10\% on ImageNet-A which is 4.3x higher than a comparable variant of BiT. Our analyses on image masking, Fourier spectrum sensitivity, and spread on discrete cosine energy spectrum reveal intriguing properties of ViT attributing to improved robustness. Code for reproducing our experiments is available at https://git.io/J3VO0.},
	urldate = {2022-03-20},
	journal = {arXiv:2105.07581 [cs]},
	author = {Paul, Sayak and Chen, Pin-Yu},
	month = dec,
	year = {2021},
	note = {arXiv: 2105.07581},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to AAAI 2022. Sayak Paul and Pin-Yu Chen contributed equally to this work. Code available at https://github.com/sayakpaul/robustness-vit},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/PYFA6KS5/Paul and Chen - 2021 - Vision Transformers are Robust Learners.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/3YQAAY3E/2105.html:text/html},
}

@article{fan_sunet_2022,
	title = {{SUNet}: {Swin} {Transformer} {UNet} for {Image} {Denoising}},
	shorttitle = {{SUNet}},
	url = {http://arxiv.org/abs/2202.14009},
	abstract = {Image restoration is a challenging ill-posed problem which also has been a long-standing issue. In the past few years, the convolution neural networks (CNNs) almost dominated the computer vision and had achieved considerable success in different levels of vision tasks including image restoration. However, recently the Swin Transformer-based model also shows impressive performance, even surpasses the CNN-based methods to become the state-of-the-art on high-level vision tasks. In this paper, we proposed a restoration model called SUNet which uses the Swin Transformer layer as our basic block and then is applied to UNet architecture for image denoising. The source code and pre-trained models are available at https://github.com/FanChiMao/SUNet.},
	urldate = {2022-03-20},
	journal = {arXiv:2202.14009 [cs, eess]},
	author = {Fan, Chi-Mao and Liu, Tsung-Jung and Liu, Kuan-Hsien},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.14009},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/PIZ85UAA/Fan et al. - 2022 - SUNet Swin Transformer UNet for Image Denoising.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/ZXB3GNDD/2202.html:text/html},
}

@article{paul_vision_2021-2,
	title = {Vision {Transformers} are {Robust} {Learners}},
	url = {http://arxiv.org/abs/2105.07581},
	abstract = {Transformers, composed of multiple self-attention layers, hold strong promises toward a generic learning primitive applicable to different data modalities, including the recent breakthroughs in computer vision achieving state-of-the-art (SOTA) standard accuracy. What remains largely unexplored is their robustness evaluation and attribution. In this work, we study the robustness of the Vision Transformer (ViT) against common corruptions and perturbations, distribution shifts, and natural adversarial examples. We use six different diverse ImageNet datasets concerning robust classification to conduct a comprehensive performance comparison of ViT models and SOTA convolutional neural networks (CNNs), Big-Transfer. Through a series of six systematically designed experiments, we then present analyses that provide both quantitative and qualitative indications to explain why ViTs are indeed more robust learners. For example, with fewer parameters and similar dataset and pre-training combinations, ViT gives a top-1 accuracy of 28.10\% on ImageNet-A which is 4.3x higher than a comparable variant of BiT. Our analyses on image masking, Fourier spectrum sensitivity, and spread on discrete cosine energy spectrum reveal intriguing properties of ViT attributing to improved robustness. Code for reproducing our experiments is available at https://git.io/J3VO0.},
	urldate = {2022-03-20},
	journal = {arXiv:2105.07581 [cs]},
	author = {Paul, Sayak and Chen, Pin-Yu},
	month = dec,
	year = {2021},
	note = {arXiv: 2105.07581},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to AAAI 2022. Sayak Paul and Pin-Yu Chen contributed equally to this work. Code available at https://github.com/sayakpaul/robustness-vit},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/E2HU2TFS/Paul and Chen - 2021 - Vision Transformers are Robust Learners.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/7F7SZTE7/2105.html:text/html},
}

@article{zou_devil_2022,
	title = {The {Devil} {Is} in the {Details}: {Window}-based {Attention} for {Image} {Compression}},
	shorttitle = {The {Devil} {Is} in the {Details}},
	url = {http://arxiv.org/abs/2203.08450},
	abstract = {Learned image compression methods have exhibited superior rate-distortion performance than classical image compression standards. Most existing learned image compression models are based on Convolutional Neural Networks (CNNs). Despite great contributions, a main drawback of CNN based model is that its structure is not designed for capturing local redundancy, especially the non-repetitive textures, which severely affects the reconstruction quality. Therefore, how to make full use of both global structure and local texture becomes the core problem for learning-based image compression. Inspired by recent progresses of Vision Transformer (ViT) and Swin Transformer, we found that combining the local-aware attention mechanism with the global-related feature learning could meet the expectation in image compression. In this paper, we first extensively study the effects of multiple kinds of attention mechanisms for local features learning, then introduce a more straightforward yet effective window-based local attention block. The proposed window-based attention is very flexible which could work as a plug-and-play component to enhance CNN and Transformer models. Moreover, we propose a novel Symmetrical TransFormer (STF) framework with absolute transformer blocks in the down-sampling encoder and up-sampling decoder. Extensive experimental evaluations have shown that the proposed method is effective and outperforms the state-of-the-art methods. The code is publicly available at https://github.com/Googolxx/STF.},
	urldate = {2022-03-20},
	journal = {arXiv:2203.08450 [cs]},
	author = {Zou, Renjie and Song, Chunfeng and Zhang, Zhaoxiang},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.08450},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by CVPR 2022},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/9M293Y3M/Zou et al. - 2022 - The Devil Is in the Details Window-based Attentio.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/TE9AJW3I/2203.html:text/html},
}

@article{liu_swin_2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	shorttitle = {Swin {Transformer}},
	url = {https://arxiv.org/abs/2103.14030},
	doi = {10.1109/ICCV48922.2021.00986},
	abstract = {A hierarchical Transformer whose representation is computed with Shifted windows, which has the flexibility to model at various scales and has linear computational complexity with respect to image size and will prove beneficial for all-MLP architectures. This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.},
	journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, B.},
	year = {2021},
	file = {Submitted Version:/home/zwerg/Zotero/storage/8PINAT5G/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf:application/pdf},
}

@article{wang_when_2022,
	title = {When {Shift} {Operation} {Meets} {Vision} {Transformer}: {An} {Extremely} {Simple} {Alternative} to {Attention} {Mechanism}},
	shorttitle = {When {Shift} {Operation} {Meets} {Vision} {Transformer}},
	url = {http://arxiv.org/abs/2201.10801},
	abstract = {Attention mechanism has been widely believed as the key to success of vision transformers (ViTs), since it provides a flexible and powerful way to model spatial relationships. However, is the attention mechanism truly an indispensable part of ViT? Can it be replaced by some other alternatives? To demystify the role of attention mechanism, we simplify it into an extremely simple case: ZERO FLOP and ZERO parameter. Concretely, we revisit the shift operation. It does not contain any parameter or arithmetic calculation. The only operation is to exchange a small portion of the channels between neighboring features. Based on this simple operation, we construct a new backbone network, namely ShiftViT, where the attention layers in ViT are substituted by shift operations. Surprisingly, ShiftViT works quite well in several mainstream tasks, e.g., classification, detection, and segmentation. The performance is on par with or even better than the strong baseline Swin Transformer. These results suggest that the attention mechanism might not be the vital factor that makes ViT successful. It can be even replaced by a zero-parameter operation. We should pay more attentions to the remaining parts of ViT in the future work. Code is available at github.com/microsoft/SPACH.},
	urldate = {2022-03-19},
	journal = {arXiv:2201.10801 [cs]},
	author = {Wang, Guangting and Zhao, Yucheng and Tang, Chuanxin and Luo, Chong and Zeng, Wenjun},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.10801},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: accepted by AAAI-22},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/SYCGKSYK/Wang et al. - 2022 - When Shift Operation Meets Vision Transformer An .pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/CTBY8JYM/2201.html:text/html},
}

@misc{noauthor_freenome_nodate,
	title = {Freenome — {Uses} {Its} {Multiomics} {Platform} for {Early} {Cancer} {Detection}},
	url = {https://www.freenome.com/},
	urldate = {2022-03-10},
	file = {Freenome — Uses Its Multiomics Platform for Early Cancer Detection:/home/zwerg/Zotero/storage/IVL5H4MP/www.freenome.com.html:text/html},
}

@misc{noauthor_generative_nodate,
	title = {Generative {Biology}},
	url = {https://generatebiomedicines.com/generative-biology},
	abstract = {Generate Biomedicines is a new kind of therapeutics company—existing at the intersection of biology, machine learning, and biological engineering.},
	language = {en-US},
	urldate = {2022-03-10},
	journal = {Generate Biomedicines},
	file = {Snapshot:/home/zwerg/Zotero/storage/QL7DLZWJ/generative-biology.html:text/html},
}

@misc{noauthor_wollit_nodate,
	title = {Wollit: {Improve} your credit score while you sleep},
	shorttitle = {Wollit},
	url = {https://www.wollit.com/},
	abstract = {Cheapest credit builder in the UK! Build your credit score with all 3 UK credit reference agencies and unlock your hidden Credit Profile. Get started in 2 minutes.},
	urldate = {2022-03-10},
	file = {Snapshot:/home/zwerg/Zotero/storage/NM69Z55I/www.wollit.com.html:text/html},
}

@misc{noauthor_steadypay_nodate,
	title = {{SteadyPay} - {We} top you up. {When} you're paid less than usual. {No} interest.},
	url = {https://www.steadypay.co/},
	urldate = {2022-03-10},
	file = {SteadyPay - We top you up. When you're paid less than usual. No interest.:/home/zwerg/Zotero/storage/HEP7IA7T/www.steadypay.co.html:text/html},
}

@misc{noauthor_payactiv_nodate,
	title = {Payactiv: {Bringing} {Financial} {Wellness} and {Freedom} to {All}},
	shorttitle = {Payactiv},
	url = {https://www.payactiv.com/},
	abstract = {An essential app to access, save, spend for all livelihood needs. An easy solution to bring financial wellness, choices, and freedom to all.},
	language = {en-US},
	urldate = {2022-03-10},
	journal = {Payactiv},
	file = {Snapshot:/home/zwerg/Zotero/storage/LH4SQLTD/www.payactiv.com.html:text/html},
}

@misc{noauthor_stability_nodate,
	title = {Stability and {Protection} for {Independent} {Workers}},
	url = {https://www.trezeo.com/pricing/},
	abstract = {Trezeo provides a unique range of financial stability services, protection products, benefits and perks for Independent Workers. Build your Membership.},
	language = {en-GB},
	urldate = {2022-03-10},
	journal = {Trezeo},
	file = {Snapshot:/home/zwerg/Zotero/storage/97S5DFXL/pricing.html:text/html},
}

@article{pomarico_statistical_2022,
	title = {Statistical distortion of supervised learning predictions in optical microscopy induced by image compression},
	volume = {12},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-07445-4},
	doi = {10.1038/s41598-022-07445-4},
	abstract = {Abstract
            The growth of data throughput in optical microscopy has triggered the extensive use of supervised learning (SL) models on compressed datasets for automated analysis. Investigating the effects of image compression on SL predictions is therefore pivotal to assess their reliability, especially for clinical use. We quantify the statistical distortions induced by compression through the comparison of predictions on compressed data to the raw predictive uncertainty, numerically estimated from the raw noise statistics measured via sensor calibration. Predictions on cell segmentation parameters are altered by up to 15\% and more than 10 standard deviations after 16-to-8 bits pixel depth reduction and 10:1 JPEG compression. JPEG formats with higher compression ratios show significantly larger distortions. Interestingly, a recent metrologically accurate algorithm, offering up to 10:1 compression ratio, provides a prediction spread equivalent to that stemming from raw noise. The method described here allows to set a lower bound to the predictive uncertainty of a SL task and can be generalized to determine the statistical distortions originated from a variety of processing pipelines in AI-assisted fields.},
	language = {en},
	number = {1},
	urldate = {2022-03-08},
	journal = {Scientific Reports},
	author = {Pomarico, Enrico and Schmidt, Cédric and Chays, Florian and Nguyen, David and Planchette, Arielle and Tissot, Audrey and Roux, Adrien and Pagès, Stéphane and Batti, Laura and Clausen, Christoph and Lasser, Theo and Radenovic, Aleksandra and Sanguinetti, Bruno and Extermann, Jérôme},
	month = dec,
	year = {2022},
	pages = {3464},
}

@article{konstantinos_towards_2020,
	title = {Towards {ML} {Engineering}: {A} {Brief} {History} {Of} {TensorFlow} {Extended} ({TFX})},
	shorttitle = {Towards {ML} {Engineering}},
	url = {http://arxiv.org/abs/2010.02013},
	abstract = {Software Engineering, as a discipline, has matured over the past 5+ decades. The modern world heavily depends on it, so the increased maturity of Software Engineering was an eventuality. Practices like testing and reliable technologies help make Software Engineering reliable enough to build industries upon. Meanwhile, Machine Learning (ML) has also grown over the past 2+ decades. ML is used more and more for research, experimentation and production workloads. ML now commonly powers widely-used products integral to our lives. But ML Engineering, as a discipline, has not widely matured as much as its Software Engineering ancestor. Can we take what we have learned and help the nascent field of applied ML evolve into ML Engineering the way Programming evolved into Software Engineering [1]? In this article we will give a whirlwind tour of Sibyl [2] and TensorFlow Extended (TFX) [3], two successive end-to-end (E2E) ML platforms at Alphabet. We will share the lessons learned from over a decade of applied ML built on these platforms, explain both their similarities and their differences, and expand on the shifts (both mental and technical) that helped us on our journey. In addition, we will highlight some of the capabilities of TFX that help realize several aspects of ML Engineering. We argue that in order to unlock the gains ML can bring, organizations should advance the maturity of their ML teams by investing in robust ML infrastructure and promoting ML Engineering education. We also recommend that before focusing on cutting-edge ML modeling techniques, product leaders should invest more time in adopting interoperable ML platforms for their organizations. In closing, we will also share a glimpse into the future of TFX.},
	urldate = {2022-04-17},
	journal = {arXiv:2010.02013 [cs]},
	author = {Konstantinos and Katsiapis and Karmarkar, Abhijit and Altay, Ahmet and Zaks, Aleksandr and Polyzotis, Neoklis and Ramesh, Anusha and Mathes, Ben and Vasudevan, Gautam and Giannoumis, Irene and Wilkiewicz, Jarek and Simsa, Jiri and Hong, Justin and Trott, Mitch and Lutz, Noé and Dournov, Pavel A. and Crowe, Robert and Sirajuddin, Sarah and Warkentin, Tris Brian and Li, Zhitao},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.02013},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	annote = {Comment: 16 pages},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/WCGJUUGN/Konstantinos et al. - 2020 - Towards ML Engineering A Brief History Of TensorF.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/XQFENT7Y/2010.html:text/html},
}

@article{paleyes_challenges_2021,
	title = {Challenges in {Deploying} {Machine} {Learning}: a {Survey} of {Case} {Studies}},
	shorttitle = {Challenges in {Deploying} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2011.09926},
	abstract = {In recent years, machine learning has received increased interest both as an academic research field and as a solution for real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. Our survey shows that practitioners face challenges at each stage of the deployment. The goal of this paper is to layout a research agenda to explore approaches addressing these challenges.},
	urldate = {2022-04-17},
	journal = {arXiv:2011.09926 [cs]},
	author = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.},
	month = jan,
	year = {2021},
	note = {arXiv: 2011.09926},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: The ML-Retrospectives, Surveys \& Meta-Analyses Workshop, NeurIPS 2020; v2. updated with typo fixes and new references},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/LE5HEGWB/Paleyes et al. - 2021 - Challenges in Deploying Machine Learning a Survey.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/5SFQXZLD/2011.html:text/html},
}

@article{sculley_hidden_nodate,
	title = {Hidden {Technical} {Debt} in {Machine} {Learning} {Systems}},
	abstract = {Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we ﬁnd it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-speciﬁc risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, conﬁguration issues, changes in the external world, and a variety of system-level anti-patterns.},
	language = {en},
	author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-François and Dennison, Dan},
	pages = {9},
	file = {Sculley et al. - Hidden Technical Debt in Machine Learning Systems.pdf:/home/zwerg/Zotero/storage/UQ7KZN2W/Sculley et al. - Hidden Technical Debt in Machine Learning Systems.pdf:application/pdf},
}

@misc{noauthor_apache_nodate,
	title = {Apache {Beam}},
	url = {https://beam.apache.org/},
	urldate = {2022-04-17},
	file = {Apache Beam:/home/zwerg/Zotero/storage/65ZX3ZT8/beam.apache.org.html:text/html},
}

@misc{noauthor_rules_nodate,
	title = {Rules of {Machine} {Learning}: {\textbar} {ML} {Universal} {Guides}},
	shorttitle = {Rules of {Machine} {Learning}},
	url = {https://developers.google.com/machine-learning/guides/rules-of-ml},
	language = {en},
	urldate = {2022-04-17},
	journal = {Google Developers},
	file = {Snapshot:/home/zwerg/Zotero/storage/BX4BWATY/rules-of-ml.html:text/html},
}

@misc{noauthor_apache_nodate-1,
	title = {Apache {Arrow}},
	url = {https://arrow.apache.org/},
	abstract = {A cross-language development platform for in-memory analytics},
	language = {en-US},
	urldate = {2022-04-17},
	journal = {Apache Arrow},
}

@article{mitchell_model_2019,
	title = {Model {Cards} for {Model} {Reporting}},
	url = {http://arxiv.org/abs/1810.03993},
	doi = {10.1145/3287560.3287596},
	abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related AI technology, increasing transparency into how well AI technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
	urldate = {2022-04-17},
	journal = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
	month = jan,
	year = {2019},
	note = {arXiv: 1810.03993},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {220--229},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/N7DURDNW/Mitchell et al. - 2019 - Model Cards for Model Reporting.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/3QQVIHSZ/1810.html:text/html},
}

@article{jo_puzzle-cam_2021-1,
	title = {Puzzle-{CAM}: {Improved} localization via matching partial and full features},
	shorttitle = {Puzzle-{CAM}},
	url = {http://arxiv.org/abs/2101.11253},
	doi = {10.1109/ICIP42928.2021.9506058},
	abstract = {Weakly-supervised semantic segmentation (WSSS) is introduced to narrow the gap for semantic segmentation performance from pixel-level supervision to image-level supervision. Most advanced approaches are based on class activation maps (CAMs) to generate pseudo-labels to train the segmentation network. The main limitation of WSSS is that the process of generating pseudo-labels from CAMs that use an image classifier is mainly focused on the most discriminative parts of the objects. To address this issue, we propose Puzzle-CAM, a process that minimizes differences between the features from separate patches and the whole image. Our method consists of a puzzle module and two regularization terms to discover the most integrated region in an object. Puzzle-CAM can activate the overall region of an object using image-level supervision without requiring extra parameters. \% In experiments, Puzzle-CAM outperformed previous state-of-the-art methods using the same labels for supervision on the PASCAL VOC 2012 test dataset. In experiments, Puzzle-CAM outperformed previous state-of-the-art methods using the same labels for supervision on the PASCAL VOC 2012 dataset. Code associated with our experiments is available at https://github.com/OFRIN/PuzzleCAM.},
	urldate = {2022-04-19},
	journal = {2021 IEEE International Conference on Image Processing (ICIP)},
	author = {Jo, Sanghyun and Yu, In-Jae},
	month = sep,
	year = {2021},
	note = {arXiv: 2101.11253},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	pages = {639--643},
	annote = {Comment: Accepted to ICIP 2021},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/3HAA8T97/Jo and Yu - 2021 - Puzzle-CAM Improved localization via matching par.pdf:application/pdf},
}

@article{ramesh_hierarchical_2022,
	title = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
	url = {http://arxiv.org/abs/2204.06125},
	abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
	urldate = {2022-04-19},
	journal = {arXiv:2204.06125 [cs]},
	author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	month = apr,
	year = {2022},
	note = {arXiv: 2204.06125},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/G4CGXUVK/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/4S53FDWR/2204.html:text/html},
}

@misc{noauthor_nvidias_2022,
	title = {Nvidia’s {Next} {GPU} {Shows} {That} {Transformers} {Are} {Transforming} {AI}},
	url = {https://spectrum.ieee.org/nvidias-next-gpu-shows-that-transformers-are-transforming-ai},
	abstract = {Transformers, the type of neural network behind OpenAI's GPT-3 and other big natural language processors, are quickly becoming some of the most important in industry—and likely to spread to other areas of AI. Nvidia's new Hopper H100 is proof that the AI accelerator maker is a believer.},
	language = {en},
	urldate = {2022-04-19},
	journal = {IEEE Spectrum},
	month = apr,
	year = {2022},
	note = {Section: Artificial Intelligence},
	file = {Snapshot:/home/zwerg/Zotero/storage/FPSCX8XD/nvidias-next-gpu-shows-that-transformers-are-transforming-ai.html:text/html},
}

@misc{noauthor_deepdetect_nodate,
	title = {{DeepDetect}},
	url = {https://www.deepdetect.com/},
	abstract = {DeepDetect is an Open-Source Deep Learning platform made by Jolibrain's scientists for the Enterprise},
	language = {en-us},
	urldate = {2022-04-19},
	journal = {DeepDetect by JoliBrain},
}

@misc{noauthor_mlpack_nodate,
	title = {mlpack - {Documentation}},
	url = {https://www.mlpack.org/docs.html},
	urldate = {2022-04-19},
}

@misc{noauthor_opennn_nodate,
	title = {{OpenNN} {\textbar} {Open} {Neural} {Networks} {Library}},
	url = {https://www.opennn.net/},
	urldate = {2022-04-19},
}

@misc{noauthor_gradio_nodate,
	title = {gradio 2.9.0b8 on {PyPI} - {Libraries}.io},
	url = {https://libraries.io/pypi/gradio},
	urldate = {2022-04-21},
}

@article{zou_pseudoseg_2021,
	title = {{PseudoSeg}: {Designing} {Pseudo} {Labels} for {Semantic} {Segmentation}},
	shorttitle = {{PseudoSeg}},
	url = {http://arxiv.org/abs/2010.09713},
	abstract = {Recent advances in semi-supervised learning (SSL) demonstrate that a combination of consistency regularization and pseudo-labeling can effectively improve image classification accuracy in the low-data regime. Compared to classification, semantic segmentation tasks require much more intensive labeling costs. Thus, these tasks greatly benefit from data-efficient training methods. However, structured outputs in segmentation render particular difficulties (e.g., designing pseudo-labeling and augmentation) to apply existing SSL strategies. To address this problem, we present a simple and novel re-design of pseudo-labeling to generate well-calibrated structured pseudo labels for training with unlabeled or weakly-labeled data. Our proposed pseudo-labeling strategy is network structure agnostic to apply in a one-stage consistency training framework. We demonstrate the effectiveness of the proposed pseudo-labeling strategy in both low-data and high-data regimes. Extensive experiments have validated that pseudo labels generated from wisely fusing diverse sources and strong data augmentation are crucial to consistency training for segmentation. The source code is available at https://github.com/googleinterns/wss.},
	urldate = {2022-04-20},
	journal = {arXiv:2010.09713 [cs]},
	author = {Zou, Yuliang and Zhang, Zizhao and Zhang, Han and Li, Chun-Liang and Bian, Xiao and Huang, Jia-Bin and Pfister, Tomas},
	month = mar,
	year = {2021},
	note = {arXiv: 2010.09713},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICLR 2021. Project page: https://yuliang.vision/pseudo\_seg/ Code: https://github.com/googleinterns/wss},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/2JX9AL98/Zou et al. - 2021 - PseudoSeg Designing Pseudo Labels for Semantic Se.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/95TPZJY9/2010.html:text/html},
}

@article{kim_discriminative_2021-1,
	title = {Discriminative {Region} {Suppression} for {Weakly}-{Supervised} {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/2103.07246},
	abstract = {Weakly-supervised semantic segmentation (WSSS) using image-level labels has recently attracted much attention for reducing annotation costs. Existing WSSS methods utilize localization maps from the classification network to generate pseudo segmentation labels. However, since localization maps obtained from the classifier focus only on sparse discriminative object regions, it is difficult to generate high-quality segmentation labels. To address this issue, we introduce discriminative region suppression (DRS) module that is a simple yet effective method to expand object activation regions. DRS suppresses the attention on discriminative regions and spreads it to adjacent non-discriminative regions, generating dense localization maps. DRS requires few or no additional parameters and can be plugged into any network. Furthermore, we introduce an additional learning strategy to give a self-enhancement of localization maps, named localization map refinement learning. Benefiting from this refinement learning, localization maps are refined and enhanced by recovering some missing parts or removing noise itself. Due to its simplicity and effectiveness, our approach achieves mIoU 71.4\% on the PASCAL VOC 2012 segmentation benchmark using only image-level labels. Extensive experiments demonstrate the effectiveness of our approach. The code is available at https://github.com/qjadud1994/DRS.},
	urldate = {2022-04-20},
	journal = {arXiv:2103.07246 [cs]},
	author = {Kim, Beomyoung and Han, Sangeun and Kim, Junmo},
	month = apr,
	year = {2021},
	note = {arXiv: 2103.07246},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: AAAI 2021, Accepted},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/QKAS9ZZB/Kim et al. - 2021 - Discriminative Region Suppression for Weakly-Super.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/WPTTP3D2/2103.html:text/html},
}

@article{zhang_resnest_2020,
	title = {{ResNeSt}: {Split}-{Attention} {Networks}},
	shorttitle = {{ResNeSt}},
	url = {http://arxiv.org/abs/2004.08955},
	abstract = {It is well known that featuremap attention and multi-path representation are important for visual recognition. In this paper, we present a modularized architecture, which applies the channel-wise attention on different network branches to leverage their success in capturing cross-feature interactions and learning diverse representations. Our design results in a simple and unified computation block, which can be parameterized using only a few variables. Our model, named ResNeSt, outperforms EfficientNet in accuracy and latency trade-off on image classification. In addition, ResNeSt has achieved superior transfer learning results on several public benchmarks serving as the backbone, and has been adopted by the winning entries of COCO-LVIS challenge. The source code for complete system and pretrained models are publicly available.},
	urldate = {2022-04-20},
	journal = {arXiv:2004.08955 [cs]},
	author = {Zhang, Hang and Wu, Chongruo and Zhang, Zhongyue and Zhu, Yi and Lin, Haibin and Zhang, Zhi and Sun, Yue and He, Tong and Mueller, Jonas and Manmatha, R. and Li, Mu and Smola, Alexander},
	month = dec,
	year = {2020},
	note = {arXiv: 2004.08955
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/VUEZISMA/Zhang et al. - 2020 - ResNeSt Split-Attention Networks.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/H3NYQ393/2004.html:text/html},
}

@article{araslanov_single-stage_2020,
	title = {Single-{Stage} {Semantic} {Segmentation} from {Image} {Labels}},
	url = {http://arxiv.org/abs/2005.08104},
	abstract = {Recent years have seen a rapid growth in new approaches improving the accuracy of semantic segmentation in a weakly supervised setting, i.e. with only image-level labels available for training. However, this has come at the cost of increased model complexity and sophisticated multi-stage training procedures. This is in contrast to earlier work that used only a single stage \$-\$ training one segmentation network on image labels \$-\$ which was abandoned due to inferior segmentation accuracy. In this work, we first define three desirable properties of a weakly supervised method: local consistency, semantic fidelity, and completeness. Using these properties as guidelines, we then develop a segmentation-based network model and a self-supervised training scheme to train for semantic masks from image-level annotations in a single stage. We show that despite its simplicity, our method achieves results that are competitive with significantly more complex pipelines, substantially outperforming earlier single-stage methods.},
	urldate = {2022-04-20},
	journal = {arXiv:2005.08104 [cs]},
	author = {Araslanov, Nikita and Roth, Stefan},
	month = may,
	year = {2020},
	note = {arXiv: 2005.08104},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear at CVPR 2020; minor corrections in Eq. (9). Code: https://github.com/visinf/1-stage-wseg},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/8KZS4BTJ/Araslanov and Roth - 2020 - Single-Stage Semantic Segmentation from Image Labe.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/2A59JNIT/2005.html:text/html},
}

@inproceedings{pan_unveiling_2021,
	title = {Unveiling the {Potential} of {Structure} {Preserving} for {Weakly} {Supervised} {Object} {Localization}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Pan_Unveiling_the_Potential_of_Structure_Preserving_for_Weakly_Supervised_Object_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-04-20},
	author = {Pan, Xingjia and Gao, Yingguo and Lin, Zhiwen and Tang, Fan and Dong, Weiming and Yuan, Haolei and Huang, Feiyue and Xu, Changsheng},
	year = {2021},
	pages = {11642--11651},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/GFGPB2X4/Pan et al. - 2021 - Unveiling the Potential of Structure Preserving fo.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/6955E4L9/Pan_Unveiling_the_Potential_of_Structure_Preserving_for_Weakly_Supervised_Object_CVPR_2021_pape.html:text/html},
}

@article{jo_recurseed_2022,
	title = {{RecurSeed} and {CertainMix} for {Weakly} {Supervised} {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/2204.06754},
	abstract = {Although weakly supervised semantic segmentation using only image-level labels (WSSS-IL) is potentially useful, its low performance and implementation complexity still limit its application. The main causes are (a) non-detection and (b) false-detection phenomena: (a) The class activation maps refined from existing WSSS-IL methods still only represent partial regions for large-scale objects, and (b) for small-scale objects, over-activation causes them to deviate from the object edges. We propose RecurSeed which alternately reduces non- and false-detections through recursive iterations, thereby implicitly finding an optimal junction that minimizes both errors. To maximize the effectiveness of RecurSeed, we also propose a novel data augmentation (DA) approach called CertainMix, which virtually creates object masks and further expresses their edges in combining the segmentation results, thereby obtaining a new DA method effectively reflecting object existence reliability through the spatial information. We achieved new state-of-the-art performances on both the PASCAL VOC 2012 and MS COCO 2014 benchmarks (VOC val 72.4\%, COCO val 45.0\%). The code is available at https://github.com/OFRIN/RecurSeed\_and\_CertainMix.},
	urldate = {2022-04-20},
	journal = {arXiv:2204.06754 [cs]},
	author = {Jo, Sang Hyun and Yu, In Jae and Kim, Kyung-Su},
	month = apr,
	year = {2022},
	note = {arXiv: 2204.06754
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/3EEBDG2P/Jo et al. - 2022 - RecurSeed and CertainMix for Weakly Supervised Sem.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/9RJZ8P44/2204.html:text/html},
}

@misc{ng_andrew_machine_nodate,
	title = {Machine learning {Engineering} for {Production} - {Lecture} {Notes} week 1},
	url = {https://discourse-cloud-file-uploads.s3.dualstack.us-west-2.amazonaws.com/dlai/original/2X/9/95589262880194306f98de4a9353e16f010afc8d.pdf},
	urldate = {2022-04-30},
	author = {Ng, Andrew},
	file = {Ng, Andrew - Machine learning Engineering for Production - Lect.pdf:/home/zwerg/Zotero/storage/GSHS9SMV/Ng, Andrew - Machine learning Engineering for Production - Lect.pdf:application/pdf},
}

@misc{ng_andrew_machine_nodate-1,
	title = {Machine learning {Engineering} for {Production} - {Lecture} {Notes} week 2},
	url = {https://discourse-cloud-file-uploads.s3.dualstack.us-west-2.amazonaws.com/dlai/original/2X/b/bafea8c78386ea890508f0aa9c151f0207e9f4d7.pdf},
	urldate = {2022-04-30},
	author = {Ng, Andrew},
	file = {Ng, Andrew - Machine learning Engineering for Production - Lect.pdf:/home/zwerg/Zotero/storage/CMJJKV3M/Ng, Andrew - Machine learning Engineering for Production - Lect.pdf:application/pdf},
}

@misc{ng_andrew_machine_nodate-2,
	title = {Machine learning {Engineering} for {Production} - {Lecture} {Notes} week 3},
	url = {https://discourse-cloud-file-uploads.s3.dualstack.us-west-2.amazonaws.com/dlai/original/2X/d/d25bd5c53abf835d3d07a5360e118b58bc8cdfc2.pdf},
	urldate = {2022-04-30},
	author = {Ng, Andrew},
	file = {Ng, Andrew - Machine learning Engineering for Production - Lect.pdf:/home/zwerg/Zotero/storage/G43STY98/Ng, Andrew - Machine learning Engineering for Production - Lect.pdf:application/pdf},
}

@misc{university_3_2020,
	title = {3 - {Baselines}},
	url = {https://blog.ml.cmu.edu/2020/08/31/3-baselines/},
	abstract = {In this article, we will discuss what a baseline is and where it fits in our data analysis projects. We will see that there are two different types of baselines, one which refers to a simple model, and another which refers to the best model from previous works. A baseline guides our selection of mor},
	language = {en-US},
	urldate = {2022-05-01},
	journal = {Machine Learning Blog {\textbar} ML@CMU {\textbar} Carnegie Mellon University},
	author = {University, Carnegie Mellon, Machine Learning Department},
	month = aug,
	year = {2020},
	note = {Section: Educational},
	file = {Snapshot:/home/zwerg/Zotero/storage/7TPSZ3T5/3-baselines.html:text/html},
}

@misc{noauthor_responsible_2021,
	title = {Responsible {Machine} {Learning} with {Error} {Analysis}},
	url = {https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/responsible-machine-learning-with-error-analysis/ba-p/2141774},
	abstract = {Overview    Website: ErrorAnalysis.ai Github repository: https://github.com/microsoft/responsible-ai-widgets/   Machine Learning (ML) teams who deploy models in the real world often face the challenges of conducting rigorous performance evaluation and testing for ML models. How often do we read clai...},
	language = {en},
	urldate = {2022-05-01},
	journal = {TECHCOMMUNITY.MICROSOFT.COM},
	month = feb,
	year = {2021},
	note = {Section: AI - Machine Learning Blog},
	file = {Snapshot:/home/zwerg/Zotero/storage/KNCZPDQ3/2141774.html:text/html},
}

@misc{noauthor_ml_2020,
	title = {{ML} {Experiment} {Tracking}: {What} {It} {Is}, {Why} {It} {Matters}, and {How} to {Implement} {It}},
	shorttitle = {{ML} {Experiment} {Tracking}},
	url = {https://neptune.ai/blog/ml-experiment-tracking},
	abstract = {Let me share a story that I’ve heard too many times. ”… We were developing an ML model with my team, we ran a lot of experiments and got promising results… …unfortunately, we couldn’t tell exactly what performed best because we forgot to save some model parameters and dataset versions… …after a few weeks, we […]},
	language = {en-US},
	urldate = {2022-05-01},
	journal = {neptune.ai},
	month = nov,
	year = {2020},
	file = {Snapshot:/home/zwerg/Zotero/storage/HBV4V4QA/ml-experiment-tracking.html:text/html},
}

@article{brundage_toward_2020,
	title = {Toward {Trustworthy} {AI} {Development}: {Mechanisms} for {Supporting} {Verifiable} {Claims}},
	shorttitle = {Toward {Trustworthy} {AI} {Development}},
	url = {http://arxiv.org/abs/2004.07213},
	abstract = {With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.},
	urldate = {2022-05-01},
	journal = {arXiv:2004.07213 [cs]},
	author = {Brundage, Miles and Avin, Shahar and Wang, Jasmine and Belfield, Haydn and Krueger, Gretchen and Hadfield, Gillian and Khlaaf, Heidy and Yang, Jingying and Toner, Helen and Fong, Ruth and Maharaj, Tegan and Koh, Pang Wei and Hooker, Sara and Leung, Jade and Trask, Andrew and Bluemke, Emma and Lebensold, Jonathan and O'Keefe, Cullen and Koren, Mark and Ryffel, Théo and Rubinovitz, J. B. and Besiroglu, Tamay and Carugati, Federica and Clark, Jack and Eckersley, Peter and de Haas, Sarah and Johnson, Maritza and Laurie, Ben and Ingerman, Alex and Krawczuk, Igor and Askell, Amanda and Cammarota, Rosario and Lohn, Andrew and Krueger, David and Stix, Charlotte and Henderson, Peter and Graham, Logan and Prunkl, Carina and Martin, Bianca and Seger, Elizabeth and Zilberman, Noa and hÉigeartaigh, Seán Ó and Kroeger, Frens and Sastry, Girish and Kagan, Rebecca and Weller, Adrian and Tse, Brian and Barnes, Elizabeth and Dafoe, Allan and Scharre, Paul and Herbert-Voss, Ariel and Rasser, Martijn and Sodhani, Shagun and Flynn, Carrick and Gilbert, Thomas Krendl and Dyer, Lisa and Khan, Saif and Bengio, Yoshua and Anderljung, Markus},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.07213
version: 2},
	keywords = {Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/WCUATPYZ/Brundage et al. - 2020 - Toward Trustworthy AI Development Mechanisms for .pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/VD2KPTEH/2004.html:text/html},
}

@article{nakkiran_deep_2019-2,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	shorttitle = {Deep {Double} {Descent}},
	url = {http://arxiv.org/abs/1912.02292},
	abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	urldate = {2022-05-01},
	journal = {arXiv:1912.02292 [cs, stat]},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.02292},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: G.K. and Y.B. contributed equally},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/NSWEBFKC/Nakkiran et al. - 2019 - Deep Double Descent Where Bigger Models and More .pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/9QBWBS7Z/1912.html:text/html},
}

@misc{noauthor_how_nodate-1,
	title = {How to program with {Bash}: {Logical} operators and shell expansions {\textbar} {Opensource}.com},
	shorttitle = {How to program with {Bash}},
	url = {https://opensource.com/article/19/10/programming-bash-logical-operators-shell-expansions},
	abstract = {Explore how the principles behind open source--collaboration, transparency, and rapid prototyping--are proven catalysts for innovation.},
	language = {en},
	urldate = {2022-05-01},
}

@article{wang_deep_2019-1,
	title = {Deep learning enables cross-modality super-resolution in fluorescence microscopy},
	volume = {16},
	issn = {1548-7091},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7276094/},
	doi = {10.1038/s41592-018-0239-0},
	abstract = {We present deep-learning-enabled super-resolution across different fluorescence microscopy modalities. This data-driven approach does not require numerical modeling of the imaging process or the estimation of a point-spread-function, and is based on training a generative adversarial network (GAN) to transform diffraction-limited input images into super-resolved ones. Using this framework, we improve the resolution of wide-field images acquired with low-numerical-aperture objectives, matching the resolution that is acquired using high-numerical-aperture objectives. We also demonstrate cross-modality super-resolution, transforming confocal microscopy images to match the resolution acquired with a stimulated emission depletion (STED) microscope. We further demonstrate that total internal reflection fluorescence (TIRF) microscopy images of subcellular structures within cells and tissues can be transformed to match the results obtained with a TIRF-based structured illumination microscope. The deep network rapidly outputs these super-resolved images, without any iterations or parameter search, and could serve to democratize super-resolution imaging.},
	number = {1},
	urldate = {2022-05-06},
	journal = {Nature methods},
	author = {Wang, Hongda and Rivenson, Yair and Jin, Yiyin and Wei, Zhensong and Gao, Ronald and Günaydin, Harun and Bentolila, Laurent A. and Kural, Comert and Ozcan, Aydogan},
	month = jan,
	year = {2019},
	pmid = {30559434},
	pmcid = {PMC7276094},
	pages = {103--110},
	file = {Accepted Version:/home/zwerg/Zotero/storage/GCQ43T34/Wang et al. - 2019 - Deep learning enables cross-modality super-resolut.pdf:application/pdf},
}

@article{siddique_u-net_nodate,
	title = {U-{Net} and its variants for medical image segmentation: theory and applications},
	abstract = {U-net is an image segmentation technique developed primarily for medical image analysis that can precisely segment images using a scarce amount of training data. These traits provide U-net with a very high utility within the medical imaging community and have resulted in extensive adoption of U-net as the primary tool for segmentation tasks in medical imaging. The success of U-net is evident in its widespread use in all major image modalities from CT scans and MRI to X-rays and microscopy. Furthermore, while U-net is largely a segmentation tool, there have been instances of the use of U-net in other applications. As U-net’s potential is still increasing, in this review we look at the various developments that have been made in the U-net architecture and provide observations on recent trends. We examine the various innovations that have been made in deep learning and discuss how these tools facilitate U-net. Furthermore, we look at image modalities and application areas where U-net has been applied.},
	language = {en},
	author = {Siddique, Nahian and Sidike, Paheding and Elkin, Colin and Devabhaktuni, Vijay},
	pages = {42},
	file = {Siddique et al. - U-Net and its variants for medical image segmentat.pdf:/home/zwerg/Zotero/storage/FGPAPDVU/Siddique et al. - U-Net and its variants for medical image segmentat.pdf:application/pdf},
}

@inproceedings{chen_dual_2017,
	title = {Dual {Path} {Networks}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/f7e0b956540676a129760a3eae309294-Abstract.html},
	abstract = {In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26\% smaller model size, 25\% less computational cost and 8\% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications.},
	urldate = {2022-05-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Yunpeng and Li, Jianan and Xiao, Huaxin and Jin, Xiaojie and Yan, Shuicheng and Feng, Jiashi},
	year = {2017},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/WG4SV4KR/Chen et al. - 2017 - Dual Path Networks.pdf:application/pdf},
}

@article{hui_dual-path_2021,
	title = {Dual-{Path} {Attention} {Compensation} {U}-{Net} for {Stroke} {Lesion} {Segmentation}},
	volume = {2021},
	issn = {1687-5273},
	doi = {10.1155/2021/7552185},
	abstract = {For the segmentation task of stroke lesions, using the attention U-Net model based on the self-attention mechanism can suppress irrelevant regions in an input image while highlighting salient features useful for specific tasks. However, when the lesion is small and the lesion contour is blurred, attention U-Net may generate wrong attention coefficient maps, leading to incorrect segmentation results. To cope with this issue, we propose a dual-path attention compensation U-Net (DPAC-UNet) network, which consists of a primary network and auxiliary path network. Both networks are attention U-Net models and identical in structure. The primary path network is the core network that performs accurate lesion segmentation and outputting of the final segmentation result. The auxiliary path network generates auxiliary attention compensation coefficients and sends them to the primary path network to compensate for and correct possible attention coefficient errors. To realize the compensation mechanism of DPAC-UNet, we propose a weighted binary cross-entropy Tversky (WBCE-Tversky) loss to train the primary path network to achieve accurate segmentation and propose another compound loss function called tolerance loss to train the auxiliary path network to generate auxiliary compensation attention coefficient maps with expanded coverage area to perform compensate operations. We conducted segmentation experiments using the 239 MRI scans of the anatomical tracings of lesions after stroke (ATLAS) dataset to evaluate the performance and effectiveness of our method. The experimental results show that the DSC score of the proposed DPAC-UNet network is 6\% higher than the single-path attention U-Net. It is also higher than the existing segmentation methods of the related literature. Therefore, our method demonstrates powerful abilities in the application of stroke lesion segmentation.},
	language = {eng},
	journal = {Computational Intelligence and Neuroscience},
	author = {Hui, Haisheng and Zhang, Xueying and Wu, Zelin and Li, Fenlian},
	year = {2021},
	pmid = {34504522},
	pmcid = {PMC8423551},
	keywords = {Stroke, Humans, Image Processing, Computer-Assisted, Magnetic Resonance Imaging},
	pages = {7552185},
	file = {Full Text:/home/zwerg/Zotero/storage/7X8D4G3A/Hui et al. - 2021 - Dual-Path Attention Compensation U-Net for Stroke .pdf:application/pdf},
}

@article{phong_illumination_1975,
	title = {Illumination for {Computer} {Generated} {Pictures}},
	volume = {18},
	abstract = {The quality of computer generated images of threedimensional scenes depends on the shading technique used to paint the objects on the cathode-ray tube screen. The shading algorithm itself depends in part on the method for modeling the object, which also determines the hidden surface algorithm. The various methods of object modeling, shading, and hidden surface removal are thus strongly interconnected. Several shading techniques corresponding to different methods of object modeling and the related hidden surface algorithms are presented here. Human visual perception and the fundamental laws of optics are considered in the development of a shading rule that provides better quality and increased realism in generated images.},
	language = {en},
	number = {6},
	author = {Phong, Bui Tuong},
	year = {1975},
	pages = {7},
	file = {Phong - 1975 - Illumination for Computer Generated Pictures.pdf:/home/zwerg/Zotero/storage/WVMSRF7X/Phong - 1975 - Illumination for Computer Generated Pictures.pdf:application/pdf},
}

@misc{noauthor_playlist_nodate,
	title = {Playlist {\textbar} {GTC} - {Developer} - {Biopharma} {\textbar} {NVIDIA} {On}-{Demand}},
	url = {https://www.nvidia.com/en-us/on-demand/playlist/playList-a8fd78f5-b455-4add-984b-958b8b55b6b0/},
	abstract = {Playlist},
	language = {en-us},
	urldate = {2022-06-08},
	journal = {NVIDIA},
	file = {Snapshot:/home/zwerg/Zotero/storage/6A84KJGS/playList-a8fd78f5-b455-4add-984b-958b8b55b6b0.html:text/html},
}

@article{mund_deep_2022,
	title = {Deep {Visual} {Proteomics} defines single-cell identity and heterogeneity},
	copyright = {2022 The Author(s)},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-022-01302-5},
	doi = {10.1038/s41587-022-01302-5},
	abstract = {Despite the availabilty of imaging-based and mass-spectrometry-based methods for spatial proteomics, a key challenge remains connecting images with single-cell-resolution protein abundance measurements. Here, we introduce Deep Visual Proteomics (DVP), which combines artificial-intelligence-driven image analysis of cellular phenotypes with automated single-cell or single-nucleus laser microdissection and ultra-high-sensitivity mass spectrometry. DVP links protein abundance to complex cellular or subcellular phenotypes while preserving spatial context. By individually excising nuclei from cell culture, we classified distinct cell states with proteomic profiles defined by known and uncharacterized proteins. In an archived primary melanoma tissue, DVP identified spatially resolved proteome changes as normal melanocytes transition to fully invasive melanoma, revealing pathways that change in a spatial manner as cancer progresses, such as mRNA splicing dysregulation in metastatic vertical growth that coincides with reduced interferon signaling and antigen presentation. The ability of DVP to retain precise spatial proteomic information in the tissue context has implications for the molecular profiling of clinical samples.},
	language = {en},
	urldate = {2022-06-01},
	journal = {Nature Biotechnology},
	author = {Mund, Andreas and Coscia, Fabian and Kriston, András and Hollandi, Réka and Kovács, Ferenc and Brunner, Andreas-David and Migh, Ede and Schweizer, Lisa and Santos, Alberto and Bzorek, Michael and Naimy, Soraya and Rahbek-Gjerdrum, Lise Mette and Dyring-Andersen, Beatrice and Bulkescher, Jutta and Lukas, Claudia and Eckert, Mark Adam and Lengyel, Ernst and Gnann, Christian and Lundberg, Emma and Horvath, Peter and Mann, Matthias},
	month = may,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Proteomics, Mechanisms of disease, Single-cell imaging, Tumour heterogeneity},
	pages = {1--10},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/CV5QJ7M5/Mund et al. - 2022 - Deep Visual Proteomics defines single-cell identit.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/BI59BW5W/s41587-022-01302-5.html:text/html},
}

@techreport{bello_revisiting_2021,
	title = {Revisiting {ResNets}: {Improved} {Training} and {Scaling} {Strategies}},
	shorttitle = {Revisiting {ResNets}},
	url = {http://arxiv.org/abs/2103.07579},
	abstract = {Novel computer vision architectures monopolize the spotlight, but the impact of the model architecture is often conflated with simultaneous changes to training methodology and scaling strategies. Our work revisits the canonical ResNet (He et al., 2015) and studies these three aspects in an effort to disentangle them. Perhaps surprisingly, we find that training and scaling strategies may matter more than architectural changes, and further, that the resulting ResNets match recent state-of-the-art models. We show that the best performing scaling strategy depends on the training regime and offer two new scaling strategies: (1) scale model depth in regimes where overfitting can occur (width scaling is preferable otherwise); (2) increase image resolution more slowly than previously recommended (Tan \& Le, 2019). Using improved training and scaling strategies, we design a family of ResNet architectures, ResNet-RS, which are 1.7x - 2.7x faster than EfficientNets on TPUs, while achieving similar accuracies on ImageNet. In a large-scale semi-supervised learning setup, ResNet-RS achieves 86.2\% top-1 ImageNet accuracy, while being 4.7x faster than EfficientNet NoisyStudent. The training techniques improve transfer performance on a suite of downstream tasks (rivaling state-of-the-art self-supervised algorithms) and extend to video classification on Kinetics-400. We recommend practitioners use these simple revised ResNets as baselines for future research.},
	number = {arXiv:2103.07579},
	urldate = {2022-05-31},
	institution = {arXiv},
	author = {Bello, Irwan and Fedus, William and Du, Xianzhi and Cubuk, Ekin D. and Srinivas, Aravind and Lin, Tsung-Yi and Shlens, Jonathon and Zoph, Barret},
	month = mar,
	year = {2021},
	doi = {10.48550/arXiv.2103.07579},
	note = {arXiv:2103.07579 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/6PJ5SF5T/Bello et al. - 2021 - Revisiting ResNets Improved Training and Scaling .pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/LI3JS6SU/2103.html:text/html},
}

@misc{noauthor_tactical_nodate,
	title = {Tactical tmux: {The} 10 {Most} {Important} {Commands}},
	shorttitle = {Tactical tmux},
	url = {https://danielmiessler.com/study/tmux/},
	abstract = {Shows you how to install and use Tmux in the shortest time possible, from installation to creating and managing sessions…},
	language = {en-US},
	urldate = {2022-05-25},
	journal = {Daniel Miessler},
	file = {Snapshot:/home/zwerg/Zotero/storage/BDCI4P5Z/tmux.html:text/html},
}

@misc{noauthor_detailed_nodate,
	title = {A detailed example of data generators with {Keras}},
	url = {https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly},
	urldate = {2022-05-24},
}

@article{noauthor_remembering_2020,
	title = {Remembering {Bill} {Lorensen}: {The} {Man}, the {Myth}, and {Marching} {Cubes}},
	volume = {40},
	issn = {0272-1716, 1558-1756},
	shorttitle = {Remembering {Bill} {Lorensen}},
	url = {https://ieeexplore.ieee.org/document/9020249/},
	doi = {10.1109/MCG.2020.2971168},
	language = {en},
	number = {2},
	urldate = {2022-06-11},
	journal = {IEEE Computer Graphics and Applications},
	month = mar,
	year = {2020},
	pages = {112--118},
	file = {2020 - Remembering Bill Lorensen The Man, the Myth, and .pdf:/home/zwerg/Zotero/storage/BFW7X4D9/2020 - Remembering Bill Lorensen The Man, the Myth, and .pdf:application/pdf},
}

@article{nielson_marching_2003,
	title = {On marching cubes},
	volume = {9},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2003.1207437},
	abstract = {A characterization and classification of the isosurfaces of trilinear functions is presented. Based upon these results, a new algorithm for computing a triangular mesh approximation to isosurfaces for data given on a 3D rectilinear grid is presented. The original marching cubes algorithm is based upon linear interpolation along edges of the voxels. The asymptotic decider method is based upon bilinear interpolation on faces of the voxels. The algorithm of this paper carries this theme forward to using trilinear interpolation on the interior of voxels. The algorithm described here will produce a triangular mesh surface approximation to an isosurface which preserves the same connectivity/separation of vertices as given by the isosurface of trilinear interpolation.},
	number = {3},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Nielson, G.M.},
	month = jul,
	year = {2003},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Approximation algorithms, Grid computing, Interpolation, Isosurfaces},
	pages = {283--297},
	file = {IEEE Xplore Abstract Record:/home/zwerg/Zotero/storage/XJ2G33T9/1207437.html:text/html;IEEE Xplore Full Text PDF:/home/zwerg/Zotero/storage/AHYNVK6Q/Nielson - 2003 - On marching cubes.pdf:application/pdf},
}

@article{newman_survey_2006,
	title = {A survey of the marching cubes algorithm},
	volume = {30},
	issn = {0097-8493},
	url = {https://www.sciencedirect.com/science/article/pii/S0097849306001336},
	doi = {10.1016/j.cag.2006.07.021},
	abstract = {A survey of the development of the marching cubes algorithm [W. Lorensen, H. Cline, Marching cubes: a high resolution 3D surface construction algorithm. Computer Graphics 1987; 21(4):163–9], a well-known cell-by-cell method for extraction of isosurfaces from scalar volumetric data sets, is presented. The paper's primary aim is to survey the development of the algorithm and its computational properties, extensions, and limitations (including the attempts to resolve its limitations). A rich body of publications related to this aim are included. Representative applications and spin-off work are also considered and related techniques are briefly discussed.},
	language = {en},
	number = {5},
	urldate = {2022-06-12},
	journal = {Computers \& Graphics},
	author = {Newman, Timothy S. and Yi, Hong},
	month = oct,
	year = {2006},
	keywords = {Indirect volume rendering, Isosurface extraction, Marching cubes, Scientific visualization, Volume visualization},
	pages = {854--879},
	file = {Newman and Yi - 2006 - A survey of the marching cubes algorithm.pdf:/home/zwerg/Zotero/storage/JWCK5WPC/Newman and Yi - 2006 - A survey of the marching cubes algorithm.pdf:application/pdf;ScienceDirect Snapshot:/home/zwerg/Zotero/storage/SNKB2LGQ/S0097849306001336.html:text/html},
}

@article{lorensen_history_2020,
	title = {History of the {Marching} {Cubes} {Algorithm}},
	volume = {40},
	issn = {1558-1756},
	doi = {10.1109/MCG.2020.2971284},
	abstract = {The Marching Cubes paper by Bill Lorensen and Harvey Cline, “Marching Cubes: A High Resolution 3D Surface Construction Algorithm,” was published at SIGGRAPH 1987.1 According to Google Scholar, their paper has 15,667 citations (as of January 17, 2020), the most highly cited paper in computer graphics. Sadly, while writing this article Bill Lorensen passed away on December 12, 2019. Origins Department Editor Chris Johnson contributed the text in italics. EARLY},
	number = {2},
	journal = {IEEE Computer Graphics and Applications},
	author = {Lorensen, William E.},
	month = mar,
	year = {2020},
	note = {Conference Name: IEEE Computer Graphics and Applications},
	keywords = {Biological system modeling, Biomedical imaging, Brain modeling, Computational modeling, History, Solid modeling},
	pages = {8--15},
	file = {IEEE Xplore Abstract Record:/home/zwerg/Zotero/storage/5W52APKH/9020242.html:text/html;IEEE Xplore Full Text PDF:/home/zwerg/Zotero/storage/Y2ZGV4VJ/Lorensen - 2020 - History of the Marching Cubes Algorithm.pdf:application/pdf},
}

@incollection{hutchison_generating_2006,
	address = {Berlin, Heidelberg},
	title = {Generating {Raster} {DEM} from {Mass} {Points} {Via} {TIN} {Streaming}},
	volume = {4197},
	isbn = {978-3-540-44526-5 978-3-540-44528-9},
	url = {http://link.springer.com/10.1007/11863939_13},
	abstract = {It is diﬃcult to generate raster Digital Elevation Models (DEMs) from terrain mass point data sets too large to ﬁt into memory, such as those obtained by LIDAR. We describe prototype tools for streaming DEM generation that use memory and disk I/O very eﬃciently. From 500 million bare-earth LIDAR double precision points (11.2 GB) our tool can, in just over an hour on a standard laptop with two hard drives, produce a 50,394 × 30,500 raster DEM with 20 foot post spacing in 16 bit binary BIL format (3 GB), using less than 100 MB of main memory and less than 300 MB of temporary disk space.},
	language = {en},
	urldate = {2022-06-12},
	booktitle = {Geographic {Information} {Science}},
	publisher = {Springer Berlin Heidelberg},
	author = {Isenburg, Martin and Liu, Yuanxin and Shewchuk, Jonathan and Snoeyink, Jack and Thirion, Tim},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Raubal, Martin and Miller, Harvey J. and Frank, Andrew U. and Goodchild, Michael F.},
	year = {2006},
	doi = {10.1007/11863939_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {186--198},
	file = {Isenburg et al. - 2006 - Generating Raster DEM from Mass Points Via TIN Str.pdf:/home/zwerg/Zotero/storage/LU8GSY97/Isenburg et al. - 2006 - Generating Raster DEM from Mass Points Via TIN Str.pdf:application/pdf},
}

@article{jing_neural_2020,
	title = {Neural {Style} {Transfer}: {A} {Review}},
	volume = {26},
	issn = {1941-0506},
	shorttitle = {Neural {Style} {Transfer}},
	doi = {10.1109/TVCG.2019.2921336},
	abstract = {The seminal work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNNs) in creating artistic imagery by separating and recombining image content and style. This process of using CNNs to render a content image in different styles is referred to as Neural Style Transfer (NST). Since then, NST has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention and a variety of approaches are proposed to either improve or extend the original NST algorithm. In this paper, we aim to provide a comprehensive overview of the current progress towards NST. We first propose a taxonomy of current algorithms in the field of NST. Then, we present several evaluation methods and compare different NST algorithms both qualitatively and quantitatively. The review concludes with a discussion of various applications of NST and open problems for future research. A list of papers discussed in this review, corresponding codes, pre-trained models and more comparison results are publicly available at: https://osf.io/f8tu4/.},
	number = {11},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Jing, Yongcheng and Yang, Yezhou and Feng, Zunlei and Ye, Jingwen and Yu, Yizhou and Song, Mingli},
	month = nov,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Convolutional neural networks, Art, convolutional neural network (CNN), Neural style transfer (NST), Painting, Rendering (computer graphics), Shape, Taxonomy, Visualization},
	pages = {3365--3385},
	file = {IEEE Xplore Abstract Record:/home/zwerg/Zotero/storage/UNIB4DK3/8732370.html:text/html;IEEE Xplore Full Text PDF:/home/zwerg/Zotero/storage/A2M4HUS4/Jing et al. - 2020 - Neural Style Transfer A Review.pdf:application/pdf},
}

@inproceedings{nielson_dual_2004,
	title = {Dual marching cubes},
	doi = {10.1109/VISUAL.2004.28},
	abstract = {We present the definition and computational algorithms for a new class of surfaces which are dual to the isosurface produced by the widely used marching cubes (MC) algorithm. These new isosurfaces have the same separating properties as the MC surfaces but they are comprised of quad patches that tend to eliminate the common negative aspect of poorly shaped triangles of the MC isosurfaces. Based upon the concept of this new dual operator, we describe a simple, but rather effective iterative scheme for producing smooth separating surfaces for binary, enumerated volumes which are often produced by segmentation algorithms. Both the dual surface algorithm and the iterative smoothing scheme are easily implemented.},
	booktitle = {{IEEE} {Visualization} 2004},
	author = {Nielson, G.M.},
	month = oct,
	year = {2004},
	keywords = {Isosurfaces, Solid modeling, Visualization, Chromium, Computational geometry, Computer graphics, dual graph, isosurfaces, Iterative algorithms, Lattices, Marching Cubes, segmented data, smoothing, Smoothing methods, triangular mesh, USA Councils},
	pages = {489--496},
	file = {IEEE Xplore Abstract Record:/home/zwerg/Zotero/storage/PDZF6NTZ/1372234.html:text/html;IEEE Xplore Full Text PDF:/home/zwerg/Zotero/storage/9DGQYVYJ/Nielson - 2004 - Dual marching cubes.pdf:application/pdf},
}

@article{ju_dual_nodate,
	title = {Dual {Contouring} of {Hermite} {Data}},
	language = {en},
	author = {Ju, Tao and Losasso, Frank and Schaefer, Scott and Warren, Joe},
	pages = {8},
	file = {Ju et al. - Dual Contouring of Hermite Data.pdf:/home/zwerg/Zotero/storage/B3FD4SLF/Ju et al. - Dual Contouring of Hermite Data.pdf:application/pdf},
}

@article{schaefer_dual_nodate,
	title = {Dual {Marching} {Cubes}: {Primal} {Contouring} of {Dual} {Grids}},
	abstract = {We present a method for contouring an implicit function using a grid topologically dual to structured grids such as octrees. By aligning the vertices of the dual grid with the features of the implicit function, we are able to reproduce thin features of the extracted surface without excessive subdivision required by methods such as Marching Cubes or Dual Contouring. Dual Marching Cubes produces a crackfree, adaptive polygonalization of the surface that reproduces sharp features. Our approach maintains the advantage of using structured grids for operations such as CSG while being able to conform to the relevant features of the implicit function yielding much sparser polygonalizations than has been possible using structured grids.},
	language = {en},
	author = {Schaefer, Scott and Warren, Joe},
	pages = {7},
	file = {Schaefer and Warren - Dual Marching Cubes Primal Contouring of Dual Gri.pdf:/home/zwerg/Zotero/storage/Z5FWN6QS/Schaefer and Warren - Dual Marching Cubes Primal Contouring of Dual Gri.pdf:application/pdf},
}

@article{schaefer_dual_2004,
	title = {Dual marching cubes: primal contouring of dual grids},
	shorttitle = {Dual marching cubes},
	doi = {10.1109/PCCGA.2004.1348336},
	abstract = {This work presents a method for contouring an implicit function using a grid topologically dual to structured grids such as octrees, yielding much sparser polygonalizations than has been possible using structured grids. We present a method for contouring an implicit function using a grid topologically dual to structured grids such as octrees. By aligning the vertices of the dual grid with the features of the implicit function, we are able to reproduce thin features of the extracted surface without excessive subdivision required by methods such as marching cubes or dual contouring. Dual marching cubes produces a crack-free, adaptive polygonalization of the surface that reproduces sharp features. Our approach maintains the advantage of using structured grids for operations such as CSG while being able to conform to the relevant features of the implicit function yielding much sparser polygonalizations than has been possible using structured grids.},
	journal = {12th Pacific Conference on Computer Graphics and Applications, 2004. PG 2004. Proceedings.},
	author = {Schaefer, S. and Warren, J.},
	year = {2004},
	file = {Schaefer and Warren - 2004 - Dual Marching Cubes Primal Contouring of Dual Gri.pdf:/home/zwerg/Zotero/storage/ADPEZUH3/Schaefer and Warren - 2004 - Dual Marching Cubes Primal Contouring of Dual Gri.pdf:application/pdf},
}

@misc{noauthor_free_nodate,
	title = {Free {Word} {Cloud} {Generator}},
	url = {https://www.freewordcloudgenerator.com/},
	abstract = {Transform data into insights with our Free Word Cloud Generator. Analyze customer and employee feedback. Identify SEO terms and keywords. Uncover trends and patterns to gain insights from your text data.},
	language = {en},
	urldate = {2022-06-22},
	journal = {Free Word Cloud Generator},
	file = {Snapshot:/home/zwerg/Zotero/storage/R3AUIJY2/generatewordcloud.html:text/html},
}

@misc{noauthor_opencv_2021,
	title = {{OpenCV} {Panorama} {Stitching}},
	url = {https://www.geeksforgeeks.org/opencv-panorama-stitching/},
	abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
	language = {en-us},
	urldate = {2022-06-22},
	journal = {GeeksforGeeks},
	month = oct,
	year = {2021},
	note = {Section: Python},
}

@misc{noauthor_image_2018,
	title = {Image {Stitching} with {OpenCV} and {Python}},
	url = {https://www.pyimagesearch.com/2018/12/17/image-stitching-with-opencv-and-python/},
	abstract = {In this tutorial you will learn how to perform multiple image stitching using Python, OpenCV, and the cv2.createSticher and cv2.Stitcher\_create functions.},
	language = {en-US},
	urldate = {2022-06-22},
	journal = {PyImageSearch},
	month = dec,
	year = {2018},
}

@misc{noauthor_automate_2022,
	title = {Automate and {Scale} your {Machine} {Learning} {\textbar} {Pachyderm}},
	url = {https://www.pachyderm.com/},
	abstract = {The leader in data versioning and automated pipelines for MLOps. Pachyderm helps you scale machine learning with better data orchestration.},
	language = {en-US},
	urldate = {2022-06-22},
	month = feb,
	year = {2022},
	file = {Snapshot:/home/zwerg/Zotero/storage/Z78ZJTKK/www.pachyderm.com.html:text/html},
}

@misc{iguazio_nuclio_nodate,
	title = {Nuclio {Documentation}},
	url = {https://nuclio.io/latest/},
	abstract = {Want to learn how to use Nuclio to create serverless functions for real-time and data-driven applications? Read more on the Nuclio documentation site.},
	language = {en-us},
	urldate = {2022-06-22},
	author = {Iguazio},
}

@misc{noauthor_kubeflow_nodate,
	title = {Kubeflow},
	url = {https://www.kubeflow.org/},
	abstract = {Kubeflow makes deployment of ML Workflows on Kubernetes straightforward and automated},
	language = {en},
	urldate = {2022-06-22},
	journal = {Kubeflow},
}

@misc{noauthor_best_2021,
	title = {The {Best} {MLOps} {Tools} and {How} to {Evaluate} {Them}},
	url = {https://neptune.ai/blog/best-mlops-tools},
	abstract = {In one of our articles—The Best Tools, Libraries, Frameworks and Methodologies that Machine Learning Teams Actually Use – Things We Learned from 41 ML Startups—Jean-Christophe Petkovich, CTO at Acerta, explained how their ML team approaches MLOps. According to him, there are several ingredients for a complete MLOps system: You need to be able to build […]},
	language = {en-US},
	urldate = {2022-06-22},
	journal = {neptune.ai},
	month = aug,
	year = {2021},
	file = {Snapshot:/home/zwerg/Zotero/storage/ICPRQQT3/best-mlops-tools.html:text/html},
}

@misc{geirhos_comparing_2018,
	title = {Comparing deep neural networks against humans: object recognition when the signal gets weaker},
	shorttitle = {Comparing deep neural networks against humans},
	url = {http://arxiv.org/abs/1706.06969},
	abstract = {Human visual object recognition is typically rapid and seemingly effortless, as well as largely independent of viewpoint and object orientation. Until very recently, animate visual systems were the only ones capable of this remarkable computational feat. This has changed with the rise of a class of computer vision algorithms called deep neural networks (DNNs) that achieve human-level classification performance on object recognition tasks. Furthermore, a growing number of studies report similarities in the way DNNs and the human visual system process objects, suggesting that current DNNs may be good models of human visual object recognition. Yet there clearly exist important architectural and processing differences between state-of-the-art DNNs and the primate visual system. The potential behavioural consequences of these differences are not well understood. We aim to address this issue by comparing human and DNN generalisation abilities towards image degradations. We find the human visual system to be more robust to image manipulations like contrast reduction, additive noise or novel eidolon-distortions. In addition, we find progressively diverging classification error-patterns between humans and DNNs when the signal gets weaker, indicating that there may still be marked differences in the way humans and current DNNs perform visual object recognition. We envision that our findings as well as our carefully measured and freely available behavioural datasets provide a new useful benchmark for the computer vision community to improve the robustness of DNNs and a motivation for neuroscientists to search for mechanisms in the brain that could facilitate this robustness.},
	urldate = {2022-06-20},
	publisher = {arXiv},
	author = {Geirhos, Robert and Janssen, David H. J. and Schütt, Heiko H. and Rauber, Jonas and Bethge, Matthias and Wichmann, Felix A.},
	month = dec,
	year = {2018},
	note = {arXiv:1706.06969 [cs, q-bio, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: updated article with reference to resulting publication (Geirhos et al, NeurIPS 2018)},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/TI9HTP82/Geirhos et al. - 2018 - Comparing deep neural networks against humans obj.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/AX9QZ6ZD/1706.html:text/html},
}

@techreport{hua_cytoimagenet_2021-1,
	title = {{CytoImageNet}: {A} large-scale pretraining dataset for bioimage transfer learning},
	shorttitle = {{CytoImageNet}},
	url = {http://arxiv.org/abs/2111.11646},
	abstract = {Motivation: In recent years, image-based biological assays have steadily become high-throughput, sparking a need for fast automated methods to extract biologically-meaningful information from hundreds of thousands of images. Taking inspiration from the success of ImageNet, we curate CytoImageNet, a large-scale dataset of openly-sourced and weakly-labeled microscopy images (890K images, 894 classes). Pretraining on CytoImageNet yields features that are competitive to ImageNet features on downstream microscopy classification tasks. We show evidence that CytoImageNet features capture information not available in ImageNet-trained features. The dataset is made available at https://www.kaggle.com/stanleyhua/cytoimagenet.},
	number = {arXiv:2111.11646},
	urldate = {2022-06-20},
	institution = {arXiv},
	author = {Hua, Stanley Bryan Z. and Lu, Alex X. and Moses, Alan M.},
	month = nov,
	year = {2021},
	doi = {10.48550/arXiv.2111.11646},
	note = {arXiv:2111.11646 [cs, q-bio]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Quantitative Methods},
	annote = {Comment: Accepted paper at NeurIPS 2021 Learning Meaningful Representations for Life (LMRL) Workshop},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/F3KW6TBD/Hua et al. - 2021 - CytoImageNet A large-scale pretraining dataset fo.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/SFSE4QRE/2111.html:text/html},
}

@misc{noauthor_building_nodate,
	title = {Building a data pipeline},
	url = {https://cs230.stanford.edu/blog/datapipeline/},
	urldate = {2022-06-25},
	file = {Building a data pipeline:/home/zwerg/Zotero/storage/I727YVKW/datapipeline.html:text/html},
}

@misc{noauthor_vtk_nodate,
	title = {{VTK} - {The} {Visualization} {Toolkit}},
	url = {https://vtk.org/},
	language = {en-US},
	urldate = {2022-06-25},
	file = {Snapshot:/home/zwerg/Zotero/storage/HHJRM6KW/vtk.org.html:text/html},
}

@article{sullivan_pyvista_2019,
	title = {{PyVista}: {3D} plotting and mesh analysis through a streamlined interface for the {Visualization} {Toolkit} ({VTK})},
	volume = {4},
	issn = {2475-9066},
	shorttitle = {{PyVista}},
	url = {http://joss.theoj.org/papers/10.21105/joss.01450},
	doi = {10.21105/joss.01450},
	number = {37},
	urldate = {2022-06-25},
	journal = {Journal of Open Source Software},
	author = {Sullivan, C. and Kaszynski, Alexander},
	month = may,
	year = {2019},
	pages = {1450},
	file = {Full Text:/home/zwerg/Zotero/storage/7AAFJDNG/Sullivan and Kaszynski - 2019 - PyVista 3D plotting and mesh analysis through a s.pdf:application/pdf},
}

@article{ljung_state_2016,
	title = {State of the {Art} in {Transfer} {Functions} for {Direct} {Volume} {Rendering}},
	volume = {35},
	issn = {01677055},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.12934},
	doi = {10.1111/cgf.12934},
	abstract = {A central topic in scientiﬁc visualization is the transfer function (TF) for volume rendering. The TF serves a fundamental role in translating scalar and multivariate data into color and opacity to express and reveal the relevant features present in the data studied. Beyond this core functionality, TFs also serve as a tool for encoding and utilizing domain knowledge and as an expression for visual design of material appearances. TFs also enable interactive volumetric exploration of complex data. The purpose of this state-of-the-art report (STAR) is to provide an overview of research into the various aspects of TFs, which lead to interpretation of the underlying data through the use of meaningful visual representations. The STAR classiﬁes TF research into the following aspects: dimensionality, derived attributes, aggregated attributes, rendering aspects, automation, and user interfaces. The STAR concludes with some interesting research challenges that form the basis of an agenda for the development of next generation TF tools and methodologies.},
	language = {en},
	number = {3},
	urldate = {2022-07-03},
	journal = {Computer Graphics Forum},
	author = {Ljung, Patric and Krüger, Jens and Groller, Eduard and Hadwiger, Markus and Hansen, Charles D. and Ynnerman, Anders},
	month = jun,
	year = {2016},
	pages = {669--691},
	file = {Ljung et al. - 2016 - State of the Art in Transfer Functions for Direct .pdf:/home/zwerg/Zotero/storage/3VUKH4H9/Ljung et al. - 2016 - State of the Art in Transfer Functions for Direct .pdf:application/pdf},
}

@article{pfister_transfer_2001,
	title = {The transfer function bake-off},
	volume = {21},
	issn = {02721716},
	url = {http://ieeexplore.ieee.org/document/920623/},
	doi = {10.1109/38.920623},
	language = {en},
	number = {1},
	urldate = {2022-07-03},
	journal = {IEEE Computer Graphics and Applications},
	author = {Pfister, H. and Lorensen, B. and Bajaj, C. and Kindlmann, G. and Schroeder, W. and Avila, L.S. and Raghu, K.M. and Machiraju, R. and {Jinho Lee}},
	month = feb,
	year = {2001},
	pages = {16--22},
	file = {Pfister et al. - 2001 - The transfer function bake-off.pdf:/home/zwerg/Zotero/storage/RXQC6LWD/Pfister et al. - 2001 - The transfer function bake-off.pdf:application/pdf},
}

@article{kass_snakes_1988,
	title = {Snakes: {Active} contour models},
	volume = {1},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Snakes},
	url = {http://link.springer.com/10.1007/BF00133570},
	doi = {10.1007/BF00133570},
	abstract = {A snake is an energy-minimizing spline guided by external constraint forces and influenced by image forces that pull it toward features such as lines and edges. Snakes are active contour models: they lock onto nearby edges, localizing them accurately. Scale-space continuation can be used to enlarge the capture region surrounding a feature. Snakes provide a unified account of a number of visual problems, including detection of edges, lines, and subjective contours; motion tracking; and stereo matching. We have used snakes successfully for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest.},
	language = {en},
	number = {4},
	urldate = {2022-07-03},
	journal = {International Journal of Computer Vision},
	author = {Kass, Michael and Witkin, Andrew and Terzopoulos, Demetri},
	month = jan,
	year = {1988},
	pages = {321--331},
	file = {Kass et al. - 1988 - Snakes Active contour models.pdf:/home/zwerg/Zotero/storage/GA2RKWPK/Kass et al. - 1988 - Snakes Active contour models.pdf:application/pdf},
}

@misc{noauthor_ploomberploomber_2022,
	title = {ploomber/ploomber},
	copyright = {Apache-2.0},
	url = {https://github.com/ploomber/ploomber},
	abstract = {The fastest ⚡️ way to build data pipelines. Develop iteratively, deploy anywhere. ☁️},
	urldate = {2022-07-03},
	publisher = {Ploomber},
	month = jul,
	year = {2022},
	note = {original-date: 2020-01-20T20:13:06Z},
	keywords = {machine-learning, data-engineering, data-science, jupyter, jupyter-notebooks, mlops, papermill, pipelines, pycharm, vscode, workflow},
}

@article{taleb_3d_2020,
	title = {{3D} {Self}-{Supervised} {Methods} for {Medical} {Imaging}},
	abstract = {This work proposes 3D versions for five different self-supervised methods, in the form of proxy tasks, to facilitate neural network feature learning from unlabeled 3D images, aiming to reduce the required cost for expert annotation. Self-supervised learning methods have witnessed a recent surge of interest after proving successful in multiple application fields. In this work, we leverage these techniques, and we propose 3D versions for five different self-supervised methods, in the form of proxy tasks. Our methods facilitate neural network feature learning from unlabeled 3D images, aiming to reduce the required cost for expert annotation. The developed algorithms are 3D Contrastive Predictive Coding, 3D Rotation prediction, 3D Jigsaw puzzles, Relative 3D patch location, and 3D Exemplar networks. Our experiments show that pretraining models with our 3D tasks yields more powerful semantic representations, and enables solving downstream tasks more accurately and efficiently, compared to training the models from scratch and to pretraining them on 2D slices. We demonstrate the effectiveness of our methods on three downstream tasks from the medical imaging domain: i) Brain Tumor Segmentation from 3D MRI, ii) Pancreas Tumor Segmentation from 3D CT, and iii) Diabetic Retinopathy Detection from 2D Fundus images. In each task, we assess the gains in data-efficiency, performance, and speed of convergence. Interestingly, we also find gains when transferring the learned representations, by our methods, from a large unlabeled 3D corpus to a small downstream-specific dataset. We achieve results competitive to state-of-the-art solutions at a fraction of the computational expense. We publish our implementations for the developed algorithms (both 3D and 2D versions) as an open-source library, in an effort to allow other researchers to apply and extend our methods on their datasets.},
	journal = {NeurIPS},
	author = {Taleb, Aiham and Loetzsch, W. and Danz, Noel and Severin, Julius and Gaertner, T. and Bergner, Benjamin and Lippert, C.},
	year = {2020},
	file = {Taleb et al. - 2020 - 3D Self-Supervised Methods for Medical Imaging.pdf:/home/zwerg/Zotero/storage/X65D8LCQ/Taleb et al. - 2020 - 3D Self-Supervised Methods for Medical Imaging.pdf:application/pdf},
}

@article{zhao_closer_2022,
	title = {A {Closer} {Look} at {Few}-shot {Image} {Generation}},
	doi = {10.48550/arXiv.2205.03805},
	abstract = {This work proposes a framework to analyze existing methods during the adaptation of pretrained GANs on small target data, and discovers that while some methods succeed, others fail. Modern GANs excel at generating high quality and diverse images. However, when transferring the pretrained GANs on small target data ( e.g ., 10-shot), the generator tends to replicate the training samples. Several methods have been proposed to address this few-shot image generation task, but there is a lack of effort to analyze them under a uniﬁed framework. As our ﬁrst contribution, we propose a framework to analyze existing methods during the adaptation. Our analysis discovers that while some},
	journal = {ArXiv},
	author = {Zhao, Yunqing and Ding, Henghui and Huang, Houjing and Cheung, Ngai-Man},
	year = {2022},
	file = {Zhao et al. - 2022 - A Closer Look at Few-shot Image Generation.pdf:/home/zwerg/Zotero/storage/V9Z4H27S/Zhao et al. - 2022 - A Closer Look at Few-shot Image Generation.pdf:application/pdf},
}

@article{zhou_comprehensive_2022,
	title = {A {Comprehensive} {Survey} on {Deep} {Clustering}: {Taxonomy}, {Challenges}, and {Future} {Directions}},
	shorttitle = {A {Comprehensive} {Survey} on {Deep} {Clustering}},
	doi = {10.48550/arXiv.2206.07579},
	abstract = {A comprehensive survey on deep clustering is conducted by proposing a new taxonomy of different state-of-theart approaches and summarize the essential components of deep clustered and categorize existing methods by the ways they design interactions between deep representation learning and clustering. Clustering is a fundamental machine learning task which has been widely studied in the literature. Classic clustering methods follow the assumption that data are represented as features in a vectorized form through various representation learning techniques. As the data become increasingly complicated and complex, the shallow (traditional) clustering methods can no longer handle the high-dimensional data type. With the huge success of deep learning, especially the deep unsupervised learning, many representation learning techniques with deep architectures have been proposed in the past decade. One straightforward way to incorporate the benefit of deep learning is to first learn the deep representation before feeding it into shallow clustering methods. However, this is suboptimal due to: i) the representation is not directly learned for clustering which limits the clustering performance; ii) the clustering relies on the relationship among instances which is complicated rather than linear; iii) the clustering and representation learning is dependent on each other which should be mutually enhanced. To tackle the above challenges, the concept of Deep Clustering, i.e., jointly optimizing the representation learning and clustering, has been proposed and hence attracted growing attention in the community. Motivated by the tremendous success of deep learning in clustering, one of the most fundamental machine learning tasks, and the large number of recent advances in this direction, in this paper we conduct a comprehensive survey on deep clustering by proposing a new taxonomy of different state-of-theart approaches. We summarize the essential components of deep clustering and categorize existing methods by the ways they design interactions between deep representation learning and clustering. Moreover, this survey also provides the popular benchmark datasets, evaluation metrics and open-source implementations to clearly illustrate various experimental settings. Last but not least, we discuss the practical applications of deep clustering and suggest challenging topics deserving further investigations as future directions.},
	journal = {ArXiv},
	author = {Zhou, Sheng and Xu, Hongjia and Zheng, Zhuonan and Chen, Jiawei and Li, Zhao and Bu, Jiajun and Wu, Jia and Wang, Xin and Zhu, Wenwu and Ester, M.},
	year = {2022},
	file = {Zhou et al. - 2022 - A Comprehensive Survey on Deep Clustering Taxonomy, Challenges, and Future Directions.pdf:/home/zwerg/Zotero/storage/G6CG2FKR/Zhou et al. - 2022 - A Comprehensive Survey on Deep Clustering Taxonomy, Challenges, and Future Directions.pdf:application/pdf},
}

@techreport{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2022-07-03},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv:1807.03748 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Oord et al. - 2019 - Representation Learning with Contrastive Predictive Coding.pdf:/home/zwerg/Zotero/storage/C5H2AGA5/Oord et al. - 2019 - Representation Learning with Contrastive Predictive Coding.pdf:application/pdf},
}

@article{lu_dying_2020,
	title = {Dying {ReLU} and {Initialization}: {Theory} and {Numerical} {Examples}},
	volume = {28},
	issn = {1815-2406, 1991-7120},
	shorttitle = {Dying {ReLU} and {Initialization}},
	url = {http://arxiv.org/abs/1903.06733},
	doi = {10.4208/cicp.OA-2020-0165},
	abstract = {The dying ReLU refers to the problem when ReLU neurons become inactive and only output 0 for any input. There are many empirical and heuristic explanations of why ReLU neurons die. However, little is known about its theoretical analysis. In this paper, we rigorously prove that a deep ReLU network will eventually die in probability as the depth goes to infinite. Several methods have been proposed to alleviate the dying ReLU. Perhaps, one of the simplest treatments is to modify the initialization procedure. One common way of initializing weights and biases uses symmetric probability distributions, which suffers from the dying ReLU. We thus propose a new initialization procedure, namely, a randomized asymmetric initialization. We prove that the new initialization can effectively prevent the dying ReLU. All parameters required for the new initialization are theoretically designed. Numerical examples are provided to demonstrate the effectiveness of the new initialization procedure.},
	number = {5},
	urldate = {2022-07-01},
	journal = {Communications in Computational Physics},
	author = {Lu, Lu and Shin, Yeonjong and Su, Yanhui and Karniadakis, George Em},
	month = jun,
	year = {2020},
	note = {arXiv:1903.06733 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Probability},
	pages = {1671--1706},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/IBN7225L/Lu et al. - 2020 - Dying ReLU and Initialization Theory and Numerica.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/QW5BGTPT/1903.html:text/html},
}

@misc{noauthor_ai_nodate,
	title = {{AI} {Consulting} \& {Services} {\textbar} {QuantumBlack} {\textbar} {McKinsey} \& {Company} {\textbar} {McKinsey} \& {Company}},
	url = {https://www.mckinsey.com/business-functions/quantumblack/how-we-help-clients},
	abstract = {Reinvent your organization and accelerate sustainable and inclusive growth with AI consulting from QuantumBlack, the AI-arm of McKinsey \& Company.},
	language = {en},
	urldate = {2022-06-28},
	file = {Snapshot:/home/zwerg/Zotero/storage/TNI6DDV8/how-we-help-clients.html:text/html},
}

@misc{darkonaut_answer_2019,
	title = {Answer to "multiprocessing: {Understanding} logic behind `chunksize`"},
	shorttitle = {Answer to "multiprocessing},
	url = {https://stackoverflow.com/a/54032744/7915766},
	urldate = {2022-06-28},
	journal = {Stack Overflow},
	author = {Darkonaut},
	month = jan,
	year = {2019},
	file = {Snapshot:/home/zwerg/Zotero/storage/AZTDGMG4/54032744.html:text/html},
}

@misc{turner-trauring_why_2018,
	title = {Why your multiprocessing {Pool} is stuck (it’s full of sharks!)},
	url = {https://pythonspeed.com/articles/python-multiprocessing/},
	abstract = {On Linux, the default configuration of Python’s multiprocessing library can lead to deadlocks and brokenness. Learn why, and how to fix it.},
	language = {en-us},
	urldate = {2022-06-27},
	journal = {Python⇒Speed},
	author = {Turner-Trauring, Itamar},
	month = sep,
	year = {2018},
}

@misc{lin_focal_2018,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://arxiv.org/abs/1708.02002},
	doi = {10.48550/arXiv.1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	urldate = {2022-07-05},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = feb,
	year = {2018},
	note = {arXiv:1708.02002 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/FC5GQKMM/Lin et al. - 2018 - Focal Loss for Dense Object Detection.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/P2T22BY5/1708.html:text/html},
}

@techreport{soelistyo_learning_2021,
	type = {preprint},
	title = {Learning the {Rules} of {Cell} {Competition} without {Prior} {Scientific} {Knowledge}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2021.11.24.469554},
	abstract = {A
            bstract
          
          
            Deep learning is now a powerful tool in microscopy data analysis, and is routinely used for image processing applications such as segmentation and denoising. However, it has rarely been used to directly learn mechanistic models of a biological system, owing to the complexity of the internal representations. Here, we develop an end-to-end machine learning model capable of learning the rules of a complex biological phenomenon, cell competition, directly from a large corpus of time-lapse microscopy data. Cell competition is a quality control mechanism that eliminates unfit cells from a tissue and during which cell fate is thought to be determined by the local cellular neighborhood over time. To investigate this, we developed a new approach (
            τ
            -VAE) by coupling a probabilistic encoder to a temporal convolution network to predict the fate of each cell in an epithelium. Using the
            τ
            -VAE’s latent representation of the local tissue organization and the flow of information in the network, we decode the physical parameters responsible for correct prediction of fate in cell competition. Remarkably, the model autonomously learns that cell density is the single most important factor in predicting cell fate – a conclusion that is in agreement with our current understanding from over a decade of scientific research. Finally, to test the learned internal representation, we challenge the network with experiments performed in the presence of drugs that block signalling pathways involved in competition. We present a novel discriminator network that, using the predictions of the
            τ
            -VAE, can identify conditions which deviate from the normal behaviour, paving the way for automated, mechanism-aware drug screening.},
	language = {en},
	urldate = {2022-07-04},
	institution = {Cell Biology},
	author = {Soelistyo, Christopher J. and Vallardi, Giulia and Charras, Guillaume and Lowe, Alan R.},
	month = nov,
	year = {2021},
	doi = {10.1101/2021.11.24.469554},
	file = {media-9.pdf:/home/zwerg/Zotero/storage/MICN7SK7/media-9.pdf:application/pdf;media-10.pdf:/home/zwerg/Zotero/storage/NEZ9VC94/media-10.pdf:application/pdf;Soelistyo et al. - 2021 - Learning the Rules of Cell Competition without Pri.pdf:/home/zwerg/Zotero/storage/3INZ3EBY/Soelistyo et al. - 2021 - Learning the Rules of Cell Competition without Pri.pdf:application/pdf},
}

@article{soelistyo_learning_2022,
	title = {Learning biophysical determinants of cell fate with deep neural networks},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-022-00503-6},
	doi = {10.1038/s42256-022-00503-6},
	abstract = {Deep learning is now a powerful tool in microscopy data analysis, and is routinely used for image processing applications such as segmentation and denoising. However, it has rarely been used to directly learn mechanistic models of a biological system, owing to the complexity of the internal representations. Here, we develop an end-to-end machine learning approach capable of learning an explainable model of a complex biological phenomenon, cell competition, directly from a large corpus of time-lapse microscopy data. Cell competition is a quality control mechanism that eliminates unfit cells from a tissue, during which cell fate is thought to be determined by the local cellular neighbourhood over time. To investigate this, we developed a new approach (τ-VAE) by coupling a probabilistic encoder to a temporal convolution network to predict the fate of each cell in an epithelium. Using the τ-VAE’s latent representation of the local tissue organization and the flow of information in the network, we decode the physical parameters responsible for correct prediction of fate in cell competition. Remarkably, the model autonomously learns that cell density is the single most important factor in predicting cell fate—a conclusion that is in agreement with our current understanding from over a decade of scientific research. Finally, to test the learned internal representation, we challenge the network with experiments performed in the presence of drugs that block signalling pathways involved in competition. We present a novel discriminator network, which using the predictions of the τ-VAE can identify conditions that deviate from the normal behaviour, paving the way for automated, mechanism-aware drug screening.},
	language = {en},
	urldate = {2022-07-04},
	journal = {Nature Machine Intelligence},
	author = {Soelistyo, Christopher J. and Vallardi, Giulia and Charras, Guillaume and Lowe, Alan R.},
	month = jun,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cellular imaging, Machine learning, Computer science, Drug screening},
	pages = {1--9},
	file = {Snapshot:/home/zwerg/Zotero/storage/DI8CHS8L/s42256-022-00503-6.html:text/html},
}

@inproceedings{yan_lighttrack_2021,
	address = {Nashville, TN, USA},
	title = {{LightTrack}: {Finding} {Lightweight} {Neural} {Networks} for {Object} {Tracking} via {One}-{Shot} {Architecture} {Search}},
	isbn = {978-1-66544-509-2},
	shorttitle = {{LightTrack}},
	url = {https://ieeexplore.ieee.org/document/9578709/},
	doi = {10.1109/CVPR46437.2021.01493},
	abstract = {Object tracking has achieved signiﬁcant progress over the past few years. However, state-of-the-art trackers become increasingly heavy and expensive, which limits their deployments in resource-constrained applications. In this work, we present LightTrack, which uses neural architecture search (NAS) to design more lightweight and efﬁcient object trackers. Comprehensive experiments show that our LightTrack is effective. It can ﬁnd trackers that achieve superior performance compared to handcrafted SOTA trackers, such as SiamRPN++ [30] and Ocean [56], while using much fewer model Flops and parameters. Moreover, when deployed on resource-constrained mobile chipsets, the discovered trackers run much faster. For example, on Snapdragon 845 Adreno GPU, LightTrack runs 12× faster than Ocean, while using 13× fewer parameters and 38× fewer Flops. Such improvements might narrow the gap between academic models and industrial deployments in object tracking task. LightTrack is released at here.},
	language = {en},
	urldate = {2022-07-05},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yan, Bin and Peng, Houwen and Wu, Kan and Wang, Dong and Fu, Jianlong and Lu, Huchuan},
	month = jun,
	year = {2021},
	pages = {15175--15184},
	file = {Yan et al. - 2021 - LightTrack Finding Lightweight Neural Networks fo.pdf:/home/zwerg/Zotero/storage/ZI6FGWT3/Yan et al. - 2021 - LightTrack Finding Lightweight Neural Networks fo.pdf:application/pdf},
}

@misc{noauthor_accelerated_2022,
	title = {Accelerated {Computing} {Holds} the {Key} to {Democratized} {Drug} {Discovery}},
	url = {https://developer.nvidia.com/blog/ai-research-holds-the-key-to-affordable-and-accessible-drug-development/},
	abstract = {Published in Nature Machine Intelligence, a panel of experts shares a vision for the future of biopharma featuring collaboration between ML and drug discovery powered by GPUs.},
	language = {en-US},
	urldate = {2022-07-04},
	journal = {NVIDIA Technical Blog},
	month = jun,
	year = {2022},
	file = {Snapshot:/home/zwerg/Zotero/storage/YNKEGAMA/ai-research-holds-the-key-to-affordable-and-accessible-drug-development.html:text/html},
}

@misc{amthor_impatient_2016,
	title = {Impatient {DNNs} - {Deep} {Neural} {Networks} with {Dynamic} {Time} {Budgets}},
	url = {http://arxiv.org/abs/1610.02850},
	abstract = {We propose Impatient Deep Neural Networks (DNNs) which deal with dynamic time budgets during application. They allow for individual budgets given a priori for each test example and for anytime prediction, i.e. a possible interruption at multiple stages during inference while still providing output estimates. Our approach can therefore tackle the computational costs and energy demands of DNNs in an adaptive manner, a property essential for real-time applications.},
	language = {en},
	urldate = {2022-07-06},
	publisher = {arXiv},
	author = {Amthor, Manuel and Rodner, Erik and Denzler, Joachim},
	month = oct,
	year = {2016},
	note = {arXiv:1610.02850 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: British Machine Vision Conference (BMVC) 2016},
	file = {Amthor et al. - 2016 - Impatient DNNs - Deep Neural Networks with Dynamic.pdf:/home/zwerg/Zotero/storage/MZ4KWX2G/Amthor et al. - 2016 - Impatient DNNs - Deep Neural Networks with Dynamic.pdf:application/pdf},
}

@misc{noauthor_rethinking_2020,
	title = {Rethinking {Depthwise} {Separable} {Convolutions}: {How} {Intra}-{Kernel} {Correlations} {Lead} to {Improved} {MobileNets}},
	shorttitle = {Rethinking {Depthwise} {Separable} {Convolutions}},
	url = {https://deepai.org/publication/rethinking-depthwise-separable-convolutions-how-intra-kernel-correlations-lead-to-improved-mobilenets},
	abstract = {03/30/20 - We introduce blueprint separable convolutions (BSConv) as highly efficient
building blocks for CNNs. They are motivated by quantit...},
	urldate = {2022-07-06},
	journal = {DeepAI},
	month = mar,
	year = {2020},
	file = {Snapshot:/home/zwerg/Zotero/storage/ITM3G6HN/rethinking-depthwise-separable-convolutions-how-intra-kernel-correlations-lead-to-improved-mobi.html:text/html},
}

@article{feng_deep_2021,
	title = {Deep graph cut network for weakly-supervised semantic segmentation},
	volume = {64},
	issn = {1869-1919},
	url = {https://doi.org/10.1007/s11432-020-3065-4},
	doi = {10.1007/s11432-020-3065-4},
	abstract = {The scarcity of fully-annotated data becomes the biggest obstacle that prevents many deep learning approaches from widely applied. Weakly-supervised visual learning which can utilize inexact annotations is developed rapidly to remedy such a situation. In this paper, we study the weakly-supervised task achieving pixel-level semantic segmentation only with image-level labels as supervision. Different from other methods, our approach tries to transform the weakly-supervised visual learning problem into a semi-supervised visual learning problem and then utilizes semi-supervised learning methods to solve it. Utilizing this transformation, we can adopt effective semi-supervised methods to perform transductive learning with context information. In the semi-supervised learning module, we propose to use the graph cut algorithm to label more supervision from the activation seeds generated from a classification network. The generated labels can provide the segmentation model with effective supervision information; moreover, the graph cut module can benefit from features extracted by the segmentation model. Then, each of them updates and optimizes the other iteratively until convergence. Experiment results on PASCAL VOC and COCO benchmarks demonstrate the effectiveness of the proposed deep graph cut algorithm for weakly-supervised semantic segmentation.},
	language = {en},
	number = {3},
	urldate = {2022-07-06},
	journal = {Science China Information Sciences},
	author = {Feng, Jiapei and Wang, Xinggang and Liu, Wenyu},
	month = feb,
	year = {2021},
	keywords = {semantic segmentation, graph cut, semi-supervised learning, weakly-supervised learning},
	pages = {130105},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/FQAP6559/Feng et al. - 2021 - Deep graph cut network for weakly-supervised seman.pdf:application/pdf},
}

@misc{on_cut-based_2020,
	title = {Cut-{Based} {Graph} {Learning} {Networks} to {Discover} {Compositional} {Structure} of {Sequential} {Video} {Data}},
	url = {http://arxiv.org/abs/2001.07613},
	abstract = {Conventional sequential learning methods such as Recurrent Neural Networks (RNNs) focus on interactions between consecutive inputs, i.e. ﬁrst-order Markovian dependency. However, most of sequential data, as seen with videos, have complex dependency structures that imply variable-length semantic ﬂows and their compositions, and those are hard to be captured by conventional methods. Here, we propose CutBased Graph Learning Networks (CB-GLNs) for learning video data by discovering these complex structures of the video. The CB-GLNs represent video data as a graph, with nodes and edges corresponding to frames of the video and their dependencies respectively. The CB-GLNs ﬁnd compositional dependencies of the data in multilevel graph forms via a parameterized kernel with graph-cut and a message passing framework. We evaluate the proposed method on the two different tasks for video understanding: Video theme classiﬁcation (Youtube-8M dataset (Abu-El-Haija et al. 2016)) and Video Question and Answering (TVQA dataset (Lei et al. 2018)). The experimental results show that our model efﬁciently learns the semantic compositional structure of video data. Furthermore, our model achieves the highest performance in comparison to other baseline methods.},
	language = {en},
	urldate = {2022-07-06},
	publisher = {arXiv},
	author = {On, Kyoung-Woon and Kim, Eun-Sol and Heo, Yu-Jung and Zhang, Byoung-Tak},
	month = jan,
	year = {2020},
	note = {arXiv:2001.07613 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: 8 pages, 3 figures, Association for the Advancement of Artificial Intelligence (AAAI2020). arXiv admin note: substantial text overlap with arXiv:1907.01709},
	file = {On et al. - 2020 - Cut-Based Graph Learning Networks to Discover Comp.pdf:/home/zwerg/Zotero/storage/2SKW5JZZ/On et al. - 2020 - Cut-Based Graph Learning Networks to Discover Comp.pdf:application/pdf},
}

@misc{gordon_build_2022,
	title = {Build a {Real}-{Time} {Streaming} {ETL} {Pipeline} in 3 {Steps}},
	url = {https://www.upsolver.com/blog/build-real-time-streaming-etl-pipeline},
	abstract = {In this article, we cover the steps required for a real-time streaming ETL pipeline and address the challenges of real-time streaming data.},
	language = {en-US},
	urldate = {2022-07-08},
	journal = {Upsolver},
	author = {Gordon, Shawn},
	month = mar,
	year = {2022},
}

@misc{brebner_building_2020,
	title = {Building a {Real}-{Time} {Tide} {Data} {Processing} {Pipeline}: {Using} {Apache} {Kafka}®, {Kafka} {Connect}, {Elasticsearch}™, and {Kibana}™—{Part} 1},
	shorttitle = {Building a {Real}-{Time} {Tide} {Data} {Processing} {Pipeline}},
	url = {https://www.instaclustr.com/blog/data-processing-pipeline/},
	abstract = {As part of the presentation for ApacheCon, 2020, Paul Brebner, Technology Evangelist, Instaclustr built a real-time tide data processing pipeline, using Apache Kafka, Kafka Connect, Elasticsearch, and Kibana. This is part 1 of the blog.},
	language = {en},
	urldate = {2022-07-08},
	journal = {Instaclustr},
	author = {Brebner, Paul},
	month = nov,
	year = {2020},
	file = {Snapshot:/home/zwerg/Zotero/storage/HSFATZFZ/data-processing-pipeline.html:text/html},
}

@article{kniss_multidimensional_2002,
	title = {Multidimensional {Transfer} {Functions} for {Volume} {Rendering}},
	volume = {8},
	doi = {10.1109/TVCG.2002.1021579},
	abstract = {Most direct volume renderings produced today employ 1D transfer functions which assign color and opacity to the volume based solely on the single scalar quantity which comprises the data set. Though they have not received widespread attention, multi-dimensional transfer functions are a very effective way to extract materials and their boundaries for both scalar and multivariate data. However, identifying good transfer functions is difficult enough in 1D, let alone 2D or 3D. This paper demonstrates an important class of 3D transfer functions for scalar data, and describes the application of multi-dimensional transfer functions to multivariate data. We present a set of direct manipulation widgets that make specifying such transfer functions intuitive and convenient. We also describe how to use modern graphics hardware to both interactively render with multidimensional transfer functions and to provide interactive shadows for volumes. The transfer functions, widgets and hardware combine to form a powerful system for interactive volume exploration.},
	journal = {Visualization and Computer Graphics, IEEE Transactions on},
	author = {Kniss, Joe and Kindlmann, Gordon and Hansen, Charles},
	month = aug,
	year = {2002},
	pages = {270--285},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/KUG39Z6M/Kniss et al. - 2002 - Multidimensional Transfer Functions for Volume Ren.pdf:application/pdf},
}

@inproceedings{engel_real-time_2004,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '04},
	title = {Real-time volume graphics},
	isbn = {978-1-4503-7801-7},
	url = {http://doi.org/10.1145/1103900.1103929},
	doi = {10.1145/1103900.1103929},
	abstract = {The tremendous evolution of programmable graphics hardware has made high-quality real-time volume graphics a reality. In addition to the traditional application of rendering volume data in scientific visualization, the interest in applying these techniques for real-time rendering of atmospheric phenomena and participating media such as fire, smoke, and clouds is growing rapidly. This course covers both applications in scientific visualization, e.g., medical volume data, and real-time rendering, such as advanced effects and illumination in computer games, in detail. Course participants will learn techniques for harnessing the power of consumer graphics hardware and high-level shading languages for real-time rendering of volumetric data and effects. Beginning with basic texture-based approaches including hardware ray casting, the algorithms are improved and expanded incrementally, covering local and global illumination, scattering, pre-integration, implicit surfaces and non-polygonal isosurfaces, transfer function design, volume animation and deformation, dealing with large volumes, high-quality volume clipping, rendering segmented volumes, higher-order filtering, and non-photorealistic volume rendering. Course participants are provided with documented source code covering details usually omitted in publications.},
	urldate = {2022-07-10},
	booktitle = {{ACM} {SIGGRAPH} 2004 {Course} {Notes}},
	publisher = {Association for Computing Machinery},
	author = {Engel, Klaus and Hadwiger, Markus and Kniss, Joe M. and Lefohn, Aaron E. and Salama, Christof Rezk and Weiskopf, Daniel},
	month = aug,
	year = {2004},
	pages = {29--es},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/57LZBQCS/Engel et al. - 2004 - Real-time volume graphics.pdf:application/pdf},
}

@misc{noauthor_top_2019,
	title = {Top 40 {Raspberry} {Pi} 4 {Projects} {That} {You} {Must} {Try} in 2022},
	url = {https://www.seeedstudio.com/blog/2019/09/29/top-20-best-raspberry-pi-4-projects-that-you-must-try-now/},
	abstract = {Just some Raspberry Pi 4 projects for you to get your brain juices flowing! Check more details about reTerminal with the Raspberry Pi CM4.},
	language = {en-US},
	urldate = {2022-07-18},
	journal = {Latest Open Tech From Seeed},
	month = sep,
	year = {2019},
	file = {Snapshot:/home/zwerg/Zotero/storage/6N2LISAC/top-20-best-raspberry-pi-4-projects-that-you-must-try-now.html:text/html},
}

@misc{brookes_how_nodate,
	title = {How to {Turn} {Your} {iPad} {Into} a {Drawing} {Tablet}},
	url = {https://www.howtogeek.com/744743/how-to-turn-your-ipad-into-a-drawing-tablet/},
	abstract = {The iPad is great for drawing when paired with a stylus like the Apple Pencil, but the best software for artists is usually found on the desktop. That’s where the iPad’s abilities as a graphics tablet come in, allowing you to use your tablet to draw with your favorite apps on macOS or Windows.},
	language = {en-US},
	urldate = {2022-07-19},
	journal = {How-To Geek},
	author = {Brookes, Tim},
}

@inproceedings{cabral_imaging_1993,
	address = {Not Known},
	title = {Imaging vector fields using line integral convolution},
	isbn = {978-0-89791-601-1},
	url = {http://portal.acm.org/citation.cfm?doid=166117.166151},
	doi = {10.1145/166117.166151},
	abstract = {Imaging vector ﬁelds has applications in science, art, image processing and special effects. An effective new approach is to use linear and curvilinear ﬁltering techniques to locally blur textures along a vector ﬁeld. This approach builds on several previous texture generation and ﬁltering techniques[8, 9, 11, 14, 15, 17, 23]. It is, however, unique because it is local, one-dimensional and independent of any predeﬁned geometry or texture. The technique is general and capable of imaging arbitrary two- and three-dimensional vector ﬁelds. The local one-dimensional nature of the algorithm lends itself to highly parallel and efﬁcient implementations. Furthermore, the curvilinear ﬁlter is capable of rendering detail on very intricate vector ﬁelds. Combining this technique with other rendering and image processing techniques — like periodic motion ﬁltering — results in richly informative and striking images. The technique can also produce novel special effects.},
	language = {en},
	urldate = {2022-07-19},
	booktitle = {Proceedings of the 20th annual conference on {Computer} graphics and interactive techniques  - {SIGGRAPH} '93},
	publisher = {ACM Press},
	author = {Cabral, Brian and Leedom, Leith Casey},
	year = {1993},
	pages = {263--270},
	file = {Cabral and Leedom - 1993 - Imaging vector fields using line integral convolut.pdf:/home/zwerg/Zotero/storage/CWUD7KE7/Cabral and Leedom - 1993 - Imaging vector fields using line integral convolut.pdf:application/pdf},
}

@misc{hull_towards_2021,
	title = {Towards {Automatic} {Grading} of {D3}.js {Visualizations}},
	url = {http://arxiv.org/abs/2110.11227},
	abstract = {Manually grading D3 data visualizations is a challenging endeavor, and is especially difﬁcult for large classes with hundreds of students. Grading an interactive visualization requires a combination of interactive, quantitative, and qualitative evaluation that are conventionally done manually and are difﬁcult to scale up as the visualization complexity, data size, and number of students increase. We present a ﬁrst-of-its kind automatic grading method for D3 visualizations that scalably and precisely evaluates the data bindings, visual encodings, interactions, and design speciﬁcations used in a visualization. Our method has shown potential to enhance students’ learning experience, enabling them to submit their code frequently and receive rapid feedback to better inform iteration and improvement to their code and visualization design. Our method promotes consistent grading and enables instructors to dedicate more focus to assist students in gaining visualization knowledge and experience. We have successfully deployed our method and auto-graded D3 submissions from more than 1000 undergraduate and graduate students in Georgia Tech’s CSE6242 Data and Visual Analytics course, and received positive feedback and encouragement for expanding its adoption.},
	language = {en},
	urldate = {2022-07-23},
	publisher = {arXiv},
	author = {Hull, Matthew and Guerin, Connor and Chen, Justin and Routray, Susanta and Chau, Duen Horng},
	month = oct,
	year = {2021},
	note = {arXiv:2110.11227 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, H.5},
	annote = {Comment: Accepted to IEEE VIS'21. For a demo video, see https://youtu.be/hA2I36Gm0YM},
	file = {Hull et al. - 2021 - Towards Automatic Grading of D3.js Visualizations.pdf:/home/zwerg/Zotero/storage/KNUL5KKZ/Hull et al. - 2021 - Towards Automatic Grading of D3.js Visualizations.pdf:application/pdf},
}

@article{vajiac_trafficvis_nodate,
	title = {{TRAFFICVIS}: {Fighting} {Human} {Trafﬁcking} through {Visualization}},
	abstract = {Law enforcement can detect human trafﬁcking (HT) in online escort websites by analyzing suspicious clusters of connected ads. Given such clusters, how can we interactively visualize potential evidence for law enforcement and domain experts? We present TRAFFICVIS, which, to our knowledge, is the ﬁrst interface for cluster-level HT detection and labeling. It builds on state-of-the-art HT clustering algorithms by incorporating metadata as a signal of organized and potentially suspicious activity. Also, domain experts can label clusters as HT, spam, and more, efﬁciently creating labeled datasets to enable further HT research. TRAFFICVIS has been built in close collaboration with domain experts, who estimate that TRAFFICVIS provides a median 36x speedup over manual labeling.},
	language = {en},
	author = {Vajiac, Catalina and Olligschlaeger, Andreas and Li, Yifei and Nair, Pratheeksha and Lee, Meng-Chieh and Park, Namyong and Rabbany, Reihaneh and Chau, Duen Horng and Faloutsos, Christos},
	pages = {2},
	file = {Vajiac et al. - TRAFFICVIS Fighting Human Trafﬁcking through Visu.pdf:/home/zwerg/Zotero/storage/MAYBVC9L/Vajiac et al. - TRAFFICVIS Fighting Human Trafﬁcking through Visu.pdf:application/pdf},
}

@misc{setlur_semantic_2021,
	title = {Semantic {Resizing} of {Charts} {Through} {Generalization}:{A} {Case} {Study} with {Line} {Charts}},
	shorttitle = {Semantic {Resizing} of {Charts} {Through} {Generalization}},
	url = {http://arxiv.org/abs/2110.12601},
	abstract = {Inspired by cartographic generalization principles, we present a generalization technique for rendering line charts at different sizes, preserving the important semantics of the data at that display size. The algorithm automatically determines the generalization operators to be applied at that size based on spatial density, distance, and the semantic importance of the various visualization elements in the line chart. A qualitative evaluation of the prototype that implemented the algorithm indicates that the generalized line charts preserved the general data shape, while minimizing visual clutter. We identify future opportunities where generalization can be extended and applied to other chart types and visual analysis authoring tools.},
	language = {en},
	urldate = {2022-07-23},
	publisher = {arXiv},
	author = {Setlur, Vidya and Chung, Haeyong},
	month = oct,
	year = {2021},
	note = {arXiv:2110.12601 [cs]},
	keywords = {Computer Science - Graphics, Computer Science - Human-Computer Interaction},
	annote = {Comment: 5 pages (4 + 1 page references), 4 figures},
	file = {Setlur and Chung - 2021 - Semantic Resizing of Charts Through Generalization.pdf:/home/zwerg/Zotero/storage/8IEJV4GY/Setlur and Chung - 2021 - Semantic Resizing of Charts Through Generalization.pdf:application/pdf},
}

@article{oppermann_vizsnippets_2022,
	title = {{VizSnippets}: {Compressing} {Visualization} {Bundles} {Into} {Representative} {Previews} for {Browsing} {Visualization} {Collections}},
	volume = {28},
	issn = {1077-2626, 1941-0506, 2160-9306},
	shorttitle = {{VizSnippets}},
	url = {https://ieeexplore.ieee.org/document/9555620/},
	doi = {10.1109/TVCG.2021.3114841},
	language = {en},
	number = {1},
	urldate = {2022-07-23},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Oppermann, Michael and Munzner, Tamara},
	month = jan,
	year = {2022},
	pages = {747--757},
	file = {Oppermann and Munzner - 2022 - VizSnippets Compressing Visualization Bundles Int.pdf:/home/zwerg/Zotero/storage/UL9SYY8W/Oppermann and Munzner - 2022 - VizSnippets Compressing Visualization Bundles Int.pdf:application/pdf},
}

@article{broido_scale-free_2019,
	title = {Scale-free networks are rare {\textbar} {Nature} {Communications}},
	volume = {10},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-08746-5},
	doi = {10.1038/s41467-019-08746-5},
	abstract = {Real-world networks are often claimed to be scale free, meaning that the fraction of nodes with degree k follows a power law k−α, a pattern with broad implications for the structure and dynamics of complex systems. However, the universality of scale-free networks remains controversial. Here, we organize different definitions of scale-free networks and construct a severe test of their empirical prevalence using state-of-the-art statistical tools applied to nearly 1000 social, biological, technological, transportation, and information networks. Across these networks, we find robust evidence that strongly scale-free structure is empirically rare, while for most networks, log-normal distributions fit the data as well or better than power laws. Furthermore, social networks are at best weakly scale free, while a handful of technological and biological networks appear strongly scale free. These findings highlight the structural diversity of real-world networks and the need for new theoretical explanations of these non-scale-free patterns.},
	number = {1},
	urldate = {2022-07-25},
	journal = {Nature Communications},
	author = {Broido, Anna D. and Clauset, Aaron},
	month = mar,
	year = {2019},
	pages = {1017},
	file = {Broido and Clauset - 2019 - Scale-free networks are rare  Nature Communications.pdf:/home/zwerg/Zotero/storage/N4U8NT5G/Broido and Clauset - 2019 - Scale-free networks are rare  Nature Communications.pdf:application/pdf},
}

@article{friedman_regularization_2010,
	title = {Regularization {Paths} for {Generalized} {Linear} {Models} via {Coordinate} {Descent}},
	volume = {33},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v33/i01/},
	doi = {10.18637/jss.v033.i01},
	abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, twoclass logistic regression, and multinomial regression problems while the penalties include 1 (the lasso), 2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal eﬃciently with sparse features. In comparative timings we ﬁnd that the new algorithms are considerably faster than competing methods.},
	language = {en},
	number = {1},
	urldate = {2022-07-25},
	journal = {Journal of Statistical Software},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	year = {2010},
	file = {Friedman et al. - 2010 - Regularization Paths for Generalized Linear Models.pdf:/home/zwerg/Zotero/storage/X44WW6ZY/Friedman et al. - 2010 - Regularization Paths for Generalized Linear Models.pdf:application/pdf},
}

@misc{zhou_objects_2019,
	title = {Objects as {Points}},
	url = {http://arxiv.org/abs/1904.07850},
	doi = {10.48550/arXiv.1904.07850},
	abstract = {Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point --- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1\% AP at 142 FPS, 37.4\% AP at 52 FPS, and 45.1\% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.},
	urldate = {2022-07-21},
	publisher = {arXiv},
	author = {Zhou, Xingyi and Wang, Dequan and Krähenbühl, Philipp},
	month = apr,
	year = {2019},
	note = {arXiv:1904.07850 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 12 pages, 5 figures},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/LSZLNHNN/Zhou et al. - 2019 - Objects as Points.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/HARXN2YS/1904.html:text/html},
}

@article{zhou_objects_2019-1,
	title = {Objects as {Points}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1904.07850},
	doi = {10.48550/ARXIV.1904.07850},
	abstract = {Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point --- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1\% AP at 142 FPS, 37.4\% AP at 52 FPS, and 45.1\% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.},
	urldate = {2022-07-21},
	author = {Zhou, Xingyi and Wang, Dequan and Krähenbühl, Philipp},
	year = {2019},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	annote = {Other
12 pages, 5 figures},
}

@article{zhou_objects_2019-2,
	title = {Objects as {Points}},
	url = {https://arxiv.org/abs/1904.07850v2},
	doi = {10.48550/arXiv.1904.07850},
	abstract = {Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point --- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1\% AP at 142 FPS, 37.4\% AP at 52 FPS, and 45.1\% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.},
	language = {en},
	urldate = {2022-07-21},
	author = {Zhou, Xingyi and Wang, Dequan and Krähenbühl, Philipp},
	month = apr,
	year = {2019},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/B2UHN8PL/Zhou et al. - 2019 - Objects as Points.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/A9AHJKSP/1904.html:text/html},
}

@misc{cheng_masked-attention_2022,
	title = {Masked-attention {Mask} {Transformer} for {Universal} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/2112.01527},
	abstract = {Image segmentation is about grouping pixels with different semantics, e.g., category or instance membership, where each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Masked-attention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K).},
	urldate = {2022-07-27},
	publisher = {arXiv},
	author = {Cheng, Bowen and Misra, Ishan and Schwing, Alexander G. and Kirillov, Alexander and Girdhar, Rohit},
	month = jun,
	year = {2022},
	note = {arXiv:2112.01527 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2022. Project page/code/models: https://bowenc0221.github.io/mask2former},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/C3RB54S8/Cheng et al. - 2022 - Masked-attention Mask Transformer for Universal Im.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/J6QSKNU9/2112.html:text/html},
}

@misc{kirillov_panoptic_2019-1,
	title = {Panoptic {Segmentation}},
	url = {http://arxiv.org/abs/1801.00868},
	doi = {10.48550/arXiv.1801.00868},
	abstract = {We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation.},
	urldate = {2022-07-27},
	publisher = {arXiv},
	author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollár, Piotr},
	month = apr,
	year = {2019},
	note = {arXiv:1801.00868 [cs]
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: accepted to CVPR 2019},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/U3PKIIHJ/Kirillov et al. - 2019 - Panoptic Segmentation.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/D9GH5DV7/1801.html:text/html},
}

@misc{elharrouss_panoptic_2021,
	title = {Panoptic {Segmentation}: {A} {Review}},
	shorttitle = {Panoptic {Segmentation}},
	url = {http://arxiv.org/abs/2111.10250},
	doi = {10.48550/arXiv.2111.10250},
	abstract = {Image segmentation for video analysis plays an essential role in different research fields such as smart city, healthcare, computer vision and geoscience, and remote sensing applications. In this regard, a significant effort has been devoted recently to developing novel segmentation strategies; one of the latest outstanding achievements is panoptic segmentation. The latter has resulted from the fusion of semantic and instance segmentation. Explicitly, panoptic segmentation is currently under study to help gain a more nuanced knowledge of the image scenes for video surveillance, crowd counting, self-autonomous driving, medical image analysis, and a deeper understanding of the scenes in general. To that end, we present in this paper the first comprehensive review of existing panoptic segmentation methods to the best of the authors' knowledge. Accordingly, a well-defined taxonomy of existing panoptic techniques is performed based on the nature of the adopted algorithms, application scenarios, and primary objectives. Moreover, the use of panoptic segmentation for annotating new datasets by pseudo-labeling is discussed. Moving on, ablation studies are carried out to understand the panoptic methods from different perspectives. Moreover, evaluation metrics suitable for panoptic segmentation are discussed, and a comparison of the performance of existing solutions is provided to inform the state-of-the-art and identify their limitations and strengths. Lastly, the current challenges the subject technology faces and the future trends attracting considerable interest in the near future are elaborated, which can be a starting point for the upcoming research studies. The papers provided with code are available at: https://github.com/elharroussomar/Awesome-Panoptic-Segmentation},
	urldate = {2022-07-27},
	publisher = {arXiv},
	author = {Elharrouss, Omar and Al-Maadeed, Somaya and Subramanian, Nandhini and Ottakath, Najmath and Almaadeed, Noor and Himeur, Yassine},
	month = nov,
	year = {2021},
	note = {arXiv:2111.10250 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/PYJG8IS2/Elharrouss et al. - 2021 - Panoptic Segmentation A Review.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/57KMI8YY/2111.html:text/html},
}

@misc{kobayashi_self-supervised_2021,
	title = {Self-{Supervised} {Deep}-{Learning} {Encodes} {High}-{Resolution} {Features} of {Protein} {Subcellular} {Localization}},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.03.29.437595v1},
	doi = {10.1101/2021.03.29.437595},
	abstract = {Elucidating the diversity and complexity of protein localization is essential to fully understand cellular architecture. Here, we present cytoself, a deep learning-based approach for fully self-supervised protein localization profiling and clustering. cytoself leverages a self-supervised training scheme that does not require pre-existing knowledge, categories, or annotations. Applying cytoself to images of 1311 endogenously labeled proteins from the recently released OpenCell database creates a highly resolved protein localization atlas. We show that the representations derived from cytoself encapsulate highly specific features that can be used to derive functional insights for proteins on the sole basis of their localization. Finally, to better understand the inner workings of our model, we dissect the emergent features from which our clustering is derived, interpret these features in the context of the fluorescence images, and analyze the performance contributions of the different components of our approach.},
	language = {en},
	urldate = {2022-07-26},
	publisher = {bioRxiv},
	author = {Kobayashi, Hirofumi and Cheveralls, Keith C. and Leonetti, Manuel D. and Royer, Loic A.},
	month = mar,
	year = {2021},
	note = {Pages: 2021.03.29.437595
Section: New Results},
	file = {Kobayashi et al. - 2021 - Self-Supervised Deep-Learning Encodes High-Resolut.pdf:/home/zwerg/Zotero/storage/E7P2QY2L/Kobayashi et al. - 2021 - Self-Supervised Deep-Learning Encodes High-Resolut.pdf:application/pdf;Kobayashi et al. - 2022 - Self-supervised deep learning encodes high-resolut.pdf:/home/zwerg/Zotero/storage/S9F7HUKM/Kobayashi et al. - 2022 - Self-supervised deep learning encodes high-resolut.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/3LXBY9US/2021.03.29.html:text/html},
}

@inproceedings{garland_surface_1997,
	address = {Not Known},
	title = {Surface simplification using quadric error metrics},
	isbn = {978-0-89791-896-1},
	url = {http://portal.acm.org/citation.cfm?doid=258734.258849},
	doi = {10.1145/258734.258849},
	abstract = {Many applications in computer graphics require complex, highly detailed models. However, the level of detail actually necessary may vary considerably. To control processing time, it is often desirable to use approximations in place of excessively detailed models. We have developed a surface simpliﬁcation algorithm which can rapidly produce high quality approximations of polygonal models. The algorithm uses iterative contractions of vertex pairs to simplify models and maintains surface error approximations using quadric matrices. By contracting arbitrary vertex pairs (not just edges), our algorithm is able to join unconnected regions of models. This can facilitate much better approximations, both visually and with respect to geometric error. In order to allow topological joining, our system also supports non-manifold surface models.},
	language = {en},
	urldate = {2022-07-27},
	booktitle = {Proceedings of the 24th annual conference on {Computer} graphics and interactive techniques  - {SIGGRAPH} '97},
	publisher = {ACM Press},
	author = {Garland, Michael and Heckbert, Paul S.},
	year = {1997},
	pages = {209--216},
	file = {Garland and Heckbert - 1997 - Surface simplification using quadric error metrics.pdf:/home/zwerg/Zotero/storage/YYZRBXJL/Garland and Heckbert - 1997 - Surface simplification using quadric error metrics.pdf:application/pdf},
}

@article{joseph_programmable_2020,
	title = {A {Programmable} {Approach} to {Neural} {Network} {Compression}},
	volume = {40},
	issn = {1937-4143},
	doi = {10.1109/MM.2020.3012391},
	abstract = {Deep neural networks (DNNs) frequently contain far more weights, represented at a higher precision, than are required for the specific task, which they are trained to perform. Consequently, they can often be compressed using techniques such as weight pruning and quantization that reduce both the model size and inference time without appreciable loss in accuracy. However, finding the best compression strategy and corresponding target sparsity for a given DNN, hardware platform, and optimization objective currently requires expensive, frequently manual, trial-and-error experimentation. In this article, we introduce a programmable system for model compression called CONDENSA. Users programmatically compose simple operators, in Python, to build more complex and practically interesting compression strategies. Given a strategy and user-provided objective (such as minimization of running time), CONDENSA uses a novel Bayesian optimization-based algorithm to automatically infer desirable sparsities. Our experiments on four real-world DNNs demonstrate memory footprint and hardware runtime throughput improvements of 188× and 2.59×, respectively, using at most ten samples per search.},
	number = {5},
	journal = {IEEE Micro},
	author = {Joseph, Vinu and Gopalakrishnan, Ganesh L. and Muralidharan, Saurav and Garland, Michael and Garg, Animesh},
	month = sep,
	year = {2020},
	note = {Conference Name: IEEE Micro},
	keywords = {Neural networks, Task analysis, Bayes methods, Hardware, Libraries, Optimization, Quantization (signal)},
	pages = {17--25},
	file = {Submitted Version:/home/zwerg/Zotero/storage/FT9HFLLD/Joseph et al. - 2020 - A Programmable Approach to Neural Network Compress.pdf:application/pdf},
}

@misc{noauthor_supercomputing_nodate,
	title = {Supercomputing in {Python} {With} {Legate}},
	url = {https://www.computer.org/csdl/magazine/cs/2021/04/09500090/1vBD1VOiGOc},
	urldate = {2022-07-27},
	file = {Supercomputing in Python With Legate:/home/zwerg/Zotero/storage/9HSCWF8W/1vBD1VOiGOc.html:text/html},
}

@inproceedings{garland_multiphase_2002,
	title = {A multiphase approach to efficient surface simplification},
	doi = {10.1109/VISUAL.2002.1183765},
	abstract = {We present a new multiphase method for efficiently simplifying polygonal surface models of arbitrary size. It operates by combining an initial out-of-core uniform clustering phase with a subsequent in-core iterative edge contraction phase. These two phases are both driven by quadric error metrics, and quadrics are used to pass information about the original surface between phases. The result is a method that produces approximations of a quality comparable to quadric-based iterative edge contraction, but at a fraction of the cost in terms of running time and memory consumption.},
	booktitle = {{IEEE} {Visualization}, 2002. {VIS} 2002.},
	author = {Garland, M. and Shaffer, E.},
	month = oct,
	year = {2002},
	keywords = {Solid modeling, Chromium, Computational geometry, Computer graphics, Iterative algorithms, Clustering algorithms, Clustering methods, Costs, Iterative methods, Partitioning algorithms},
	pages = {117--124},
	file = {IEEE Xplore Abstract Record:/home/zwerg/Zotero/storage/K4QAV6LQ/1183765.html:text/html;Submitted Version:/home/zwerg/Zotero/storage/KNPZJJBA/Garland and Shaffer - 2002 - A multiphase approach to efficient surface simplif.pdf:application/pdf},
}

@article{bauer_supercomputing_2021,
	title = {Supercomputing in {Python} {With} {Legate}},
	volume = {23},
	issn = {1558-366X},
	doi = {10.1109/MCSE.2021.3088239},
	abstract = {Legate is a recently developed software system for constructing scalable simulation and data analysis programs using convenient, familiar notation. It demonstrates how coupling Python, with a runtime system originally designed for high-performance computing, can enable the creation of libraries that mimic the familiar interface of NumPy and Pandas for execution on both desktops and supercomputers.},
	number = {4},
	journal = {Computing in Science \& Engineering},
	author = {Bauer, Michael and Lee, Wonchan and Papadakis, Manolis and Zalewski, Marcin and Garland, Michael},
	month = jul,
	year = {2021},
	note = {Conference Name: Computing in Science \& Engineering},
	keywords = {Data analysis, Computational modeling, Analytical models, Couplings, Runtime, Software systems, Supercomputers},
	pages = {73--79},
}

@misc{dey_interactive_2018,
	title = {Interactive {Image} {Segmentation} with {Graph}-{Cut} in {Python}},
	url = {https://sandipanweb.wordpress.com/2018/02/11/interactive-image-segmentation-with-graph-cut/},
	abstract = {In this article, interactive image segmentation with graph-cut is going to be discussed. and it will be used to segment the source object from the background in an image. This segmentation techniqu…},
	language = {en},
	urldate = {2022-07-29},
	journal = {sandipanweb},
	author = {Dey, Sandipan},
	month = feb,
	year = {2018},
	file = {Snapshot:/home/zwerg/Zotero/storage/64V8J25T/interactive-image-segmentation-with-graph-cut.html:text/html},
}

@techreport{moore_ome-ngff_2021,
	type = {preprint},
	title = {{OME}-{NGFF}: scalable format strategies for interoperable bioimaging data},
	shorttitle = {{OME}-{NGFF}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2021.03.31.437929},
	abstract = {Biological imaging is one of the most innovative fields in the modern biological sciences. New imaging modalities, probes, and analysis tools appear every few months and often prove decisive for enabling new directions in scientific discovery. One feature of this dynamic field is the need to capture new types of data and data structures. While there is a strong drive to make scientific data Findable, Accessible, Interoperable and Reproducible (FAIR 1), the rapid rate of innovation in imaging impedes the unification and adoption of standardized data formats. Despite this, the opportunities for sharing and integrating bioimaging data and, in particular, linking these data to other "omics" datasets have never been greater. Therefore, to every extent possible, increasing "FAIRness" of bioimaging data is critical for maximizing scientific value, as well as for promoting openness and integrity.},
	language = {en},
	urldate = {2022-07-29},
	institution = {Bioinformatics},
	author = {Moore, Josh and Allan, Chris and Besson, Sebastien and Burel, Jean-Marie and Diel, Erin and Gault, David and Kozlowski, Kevin and Lindner, Dominik and Linkert, Melissa and Manz, Trevor and Moore, Will and Pape, Constantin and Tischer, Christian and Swedlow, Jason R.},
	month = mar,
	year = {2021},
	doi = {10.1101/2021.03.31.437929},
	file = {Moore et al. - 2021 - OME-NGFF scalable format strategies for interoper.pdf:/home/zwerg/Zotero/storage/I7SNH8KS/Moore et al. - 2021 - OME-NGFF scalable format strategies for interoper.pdf:application/pdf},
}

@misc{harley_deep_2019,
	title = {Deep {Discriminative} {Fine}-{Tuning} for {Cancer} {Type} {Classification}},
	url = {http://arxiv.org/abs/1911.07654},
	abstract = {Determining the primary site of origin for metastatic tumors is one of the open problems in cancer care because the efficacy of treatment often depends on the cancer tissue of origin. Classification methods that can leverage tumor genomic data and predict the site of origin are therefore of great value. Because tumor DNA point mutation data is very sparse, only limited accuracy (64.5\% for 12 tumor classes) was previously demonstrated by methods that rely on point mutations as features (1). Tumor classification accuracy can be greatly improved (to over 90\% for 33 classes) by relying on gene expression data (2). However, this additional data is often not readily available in clinical setting, because point mutations are better profiled and targeted by clinical mutational profiling. Here we sought to develop an accurate deep transfer learning and fine-tuning method for tumor sub-type classification, where predicted class is indicative of the primary site of origin. Our method significantly outperforms the state-of-the-art for tumor classification using DNA point mutations, reducing the error by more than 30\% at the same time discriminating over many more classes on The Cancer Genome Atlas (TCGA) dataset. Using our method, we achieve state-of-the-art tumor type classification accuracy of 78.3\% for 29 tumor classes relying on DNA point mutations in the tumor only.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Harley, Alena},
	month = nov,
	year = {2019},
	note = {arXiv:1911.07654 [cs, q-bio, stat]
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantitative Biology - Genomics},
	annote = {Comment: 4 pages, 1 figure, ML4H NeurIPS Workshop},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/DXS9DM44/Harley - 2019 - Deep Discriminative Fine-Tuning for Cancer Type Cl.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/7P8HU6UR/1911.html:text/html},
}

@misc{noauthor_deep_2019,
	title = {Deep {Discriminative} {Fine}-{Tuning} for {Cancer} {Type} {Classification}},
	url = {https://deepai.org/publication/deep-discriminative-fine-tuning-for-cancer-type-classification},
	abstract = {11/15/19 - Determining the primary site of origin for metastatic tumors is one of the
open problems in cancer care because the efficacy of tr...},
	urldate = {2022-08-03},
	journal = {DeepAI},
	month = nov,
	year = {2019},
	file = {Snapshot:/home/zwerg/Zotero/storage/AS9E4PM7/deep-discriminative-fine-tuning-for-cancer-type-classification.html:text/html},
}

@inproceedings{zhukov_oriented_2002,
	address = {Boston, MA, USA},
	title = {Oriented tensor reconstruction: tracing neural pathways from diffusion tensor {MRI}},
	isbn = {978-0-7803-7498-0},
	shorttitle = {Oriented tensor reconstruction},
	url = {http://ieeexplore.ieee.org/document/1183799/},
	doi = {10.1109/VISUAL.2002.1183799},
	abstract = {In this paper we develop a new technique for tracing anatomical ﬁbers from 3D tensor ﬁelds. The technique extracts salient tensor features using a local regularization technique that allows the algorithm to cross noisy regions and bridge gaps in the data. We applied the method to human brain DT-MRI data and recovered identiﬁable anatomical structures that correspond to the white matter brain-ﬁber pathways. The images in this paper are derived from a dataset having 121x88x60 resolution. We were able to recover ﬁbers with less than the voxel size resolution by applying the regularization technique, i.e., using a priori assumptions about ﬁber smoothness. The regularization procedure is done through a moving least squares ﬁlter directly incorporated in the tracing algorithm.},
	language = {en},
	urldate = {2022-08-05},
	booktitle = {{IEEE} {Visualization}, 2002. {VIS} 2002.},
	publisher = {IEEE},
	author = {Zhukov, L. and Barr, A.H.},
	year = {2002},
	pages = {387--394},
	file = {Zhukov and Barr - 2002 - Oriented tensor reconstruction tracing neural pat.pdf:/home/zwerg/Zotero/storage/7C5JRT2N/Zhukov and Barr - 2002 - Oriented tensor reconstruction tracing neural pat.pdf:application/pdf},
}

@inproceedings{garland_surface_1997-1,
	address = {Not Known},
	title = {Surface simplification using quadric error metrics},
	isbn = {978-0-89791-896-1},
	url = {http://portal.acm.org/citation.cfm?doid=258734.258849},
	doi = {10.1145/258734.258849},
	abstract = {Many applications in computer graphics require complex, highly detailed models. However, the level of detail actually necessary may vary considerably. To control processing time, it is often desirable to use approximations in place of excessively detailed models. We have developed a surface simpliﬁcation algorithm which can rapidly produce high quality approximations of polygonal models. The algorithm uses iterative contractions of vertex pairs to simplify models and maintains surface error approximations using quadric matrices. By contracting arbitrary vertex pairs (not just edges), our algorithm is able to join unconnected regions of models. This can facilitate much better approximations, both visually and with respect to geometric error. In order to allow topological joining, our system also supports non-manifold surface models.},
	language = {en},
	urldate = {2022-08-05},
	booktitle = {Proceedings of the 24th annual conference on {Computer} graphics and interactive techniques  - {SIGGRAPH} '97},
	publisher = {ACM Press},
	author = {Garland, Michael and Heckbert, Paul S.},
	year = {1997},
	pages = {209--216},
	file = {Garland and Heckbert - 1997 - Surface simplification using quadric error metrics.pdf:/home/zwerg/Zotero/storage/3H5GWPJB/Garland and Heckbert - 1997 - Surface simplification using quadric error metrics.pdf:application/pdf},
}

@misc{sorscher_beyond_2022,
	title = {Beyond neural scaling laws: beating power law scaling via data pruning},
	shorttitle = {Beyond neural scaling laws},
	url = {http://arxiv.org/abs/2206.14486},
	doi = {10.48550/arXiv.2206.14486},
	abstract = {Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how both in theory and practice we can break beyond power law scaling and reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this new exponential scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling performance on ResNets trained on CIFAR-10, SVHN, and ImageNet. Given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.},
	urldate = {2022-09-11},
	publisher = {arXiv},
	author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},
	month = jun,
	year = {2022},
	note = {arXiv:2206.14486 [cs, stat]
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/QR5QXNIT/Sorscher et al. - 2022 - Beyond neural scaling laws beating power law scal.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/A6HAPFLK/2206.html:text/html},
}

@misc{merity_pointer_2016,
	title = {Pointer {Sentinel} {Mixture} {Models}},
	url = {http://arxiv.org/abs/1609.07843},
	abstract = {Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.},
	urldate = {2022-09-07},
	publisher = {arXiv},
	author = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
	month = sep,
	year = {2016},
	note = {arXiv:1609.07843 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/IUWYRIYF/Merity et al. - 2016 - Pointer Sentinel Mixture Models.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/YBPNX6ZJ/1609.html:text/html},
}

@misc{noauthor_dreambooth_nodate,
	title = {{DreamBooth}: {Fine} {Tuning} {Text}-to-{Image} {Diffusion} {Models}... - {Google} {Scholar}},
	url = {https://scholar.google.com/scholar?q=DreamBooth:+Fine+Tuning+Text-to-Image+Diffusion+Models+for+Subject-Driven+Generation+Ruiz+ArXiv+2022&hl=de&as_sdt=0,5},
	urldate = {2022-09-02},
}

@article{dhariwal_diffusion_2021,
	title = {Diffusion models beat gans on image synthesis},
	volume = {34},
	journal = {Advances in Neural Information Processing Systems},
	author = {Dhariwal, Prafulla and Nichol, Alexander},
	year = {2021},
	pages = {8780--8794},
	file = {Dhariwal and Nichol - 2021 - Diffusion models beat gans on image synthesis.pdf:/home/zwerg/Zotero/storage/PU4PYJ7A/Dhariwal and Nichol - 2021 - Diffusion models beat gans on image synthesis.pdf:application/pdf},
}

@techreport{gal_image_2022,
	title = {An {Image} is {Worth} {One} {Word}: {Personalizing} {Text}-to-{Image} {Generation} using {Textual} {Inversion}},
	shorttitle = {An {Image} is {Worth} {One} {Word}},
	url = {http://arxiv.org/abs/2208.01618},
	abstract = {Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new "words" in the embedding space of a frozen text-to-image model. These "words" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available at: https://textual-inversion.github.io},
	urldate = {2022-09-02},
	author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},
	month = aug,
	year = {2022},
	note = {arXiv:2208.01618 [cs]
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Graphics},
	file = {Gal et al. - 2022 - An Image is Worth One Word Personalizing Text-to-Image Generation using Textual Inversion.pdf:/home/zwerg/Zotero/storage/KJ44VT3L/Gal et al. - 2022 - An Image is Worth One Word Personalizing Text-to-Image Generation using Textual Inversion.pdf:application/pdf},
}

@misc{noauthor_gram-hd_nodate,
	title = {{GRAM}-{HD}},
	url = {https://jeffreyxiang.github.io/GRAM-HD/},
	urldate = {2022-08-29},
}

@techreport{wang_yolov7_2022,
	title = {{YOLOv7}: {Trainable} bag-of-freebies sets new state-of-the-art for real-time object detectors},
	shorttitle = {{YOLOv7}},
	url = {http://arxiv.org/abs/2207.02696},
	abstract = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8\% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9\% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9\% AP) by 509\% in speed and 2\% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2\% AP) by 551\% in speed and 0.7\% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https://github.com/WongKinYiu/yolov7.},
	urldate = {2022-08-20},
	author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
	month = jul,
	year = {2022},
	note = {arXiv:2207.02696 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Wang et al. - 2022 - YOLOv7 Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors.pdf:/home/zwerg/Zotero/storage/9F8MCTUJ/Wang et al. - 2022 - YOLOv7 Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors.pdf:application/pdf},
}

@misc{tolmachev_real-time_2021,
	title = {Real-time image registration on {GPU} with {VkFFT} library},
	url = {https://towardsdatascience.com/real-time-image-registration-on-gpu-with-vkfft-library-c4e47f8050a0},
	abstract = {How to detect translation, rotation and scale of an image with Fast Fourier Transform},
	language = {en},
	urldate = {2022-08-10},
	journal = {Medium},
	author = {Tolmachev, Dmitrii},
	month = jan,
	year = {2021},
	file = {Snapshot:/home/zwerg/Zotero/storage/DBY9W2MI/real-time-image-registration-on-gpu-with-vkfft-library-c4e47f8050a0.html:text/html},
}

@techreport{zhao_generative_2022,
	title = {Generative {Multiplane} {Images}: {Making} a {2D} {GAN} {3D}-{Aware}},
	shorttitle = {Generative {Multiplane} {Images}},
	url = {http://arxiv.org/abs/2207.10642},
	abstract = {What is really needed to make an existing 2D GAN 3D-aware? To answer this question, we modify a classical GAN, i.e., StyleGANv2, as little as possible. We find that only two modifications are absolutely necessary: 1) a multiplane image style generator branch which produces a set of alpha maps conditioned on their depth; 2) a pose-conditioned discriminator. We refer to the generated output as a 'generative multiplane image' (GMPI) and emphasize that its renderings are not only high-quality but also guaranteed to be view-consistent, which makes GMPIs different from many prior works. Importantly, the number of alpha maps can be dynamically adjusted and can differ between training and inference, alleviating memory concerns and enabling fast training of GMPIs in less than half a day at a resolution of \$1024{\textasciicircum}2\$. Our findings are consistent across three challenging and common high-resolution datasets, including FFHQ, AFHQv2, and MetFaces.},
	urldate = {2022-08-08},
	author = {Zhao, Xiaoming and Ma, Fangchang and Güera, David and Ren, Zhile and Schwing, Alexander G. and Colburn, Alex},
	month = jul,
	year = {2022},
	note = {arXiv:2207.10642 [cs]
version: 1
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhao et al. - 2022 - Generative Multiplane Images Making a 2D GAN 3D-Aware.pdf:/home/zwerg/Zotero/storage/P9TQGIY7/Zhao et al. - 2022 - Generative Multiplane Images Making a 2D GAN 3D-Aware.pdf:application/pdf},
}

@techreport{kumar_deviant_2022,
	title = {{DEVIANT}: {Depth} {EquiVarIAnt} {NeTwork} for {Monocular} {3D} {Object} {Detection}},
	shorttitle = {{DEVIANT}},
	url = {http://arxiv.org/abs/2207.10758},
	abstract = {Modern neural networks use building blocks such as convolutions that are equivariant to arbitrary 2D translations. However, these vanilla blocks are not equivariant to arbitrary 3D translations in the projective manifold. Even then, all monocular 3D detectors use vanilla blocks to obtain the 3D coordinates, a task for which the vanilla blocks are not designed for. This paper takes the first step towards convolutions equivariant to arbitrary 3D translations in the projective manifold. Since the depth is the hardest to estimate for monocular detection, this paper proposes Depth EquiVarIAnt NeTwork (DEVIANT) built with existing scale equivariant steerable blocks. As a result, DEVIANT is equivariant to the depth translations in the projective manifold whereas vanilla networks are not. The additional depth equivariance forces the DEVIANT to learn consistent depth estimates, and therefore, DEVIANT achieves state-of-the-art monocular 3D detection results on KITTI and Waymo datasets in the image-only category and performs competitively to methods using extra information. Moreover, DEVIANT works better than vanilla networks in cross-dataset evaluation. Code and models at https://github.com/abhi1kumar/DEVIANT},
	urldate = {2022-08-08},
	author = {Kumar, Abhinav and Brazil, Garrick and Corona, Enrique and Parchami, Armin and Liu, Xiaoming},
	month = jul,
	year = {2022},
	note = {arXiv:2207.10758 [cs]
version: 1
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Kumar et al. - 2022 - DEVIANT Depth EquiVarIAnt NeTwork for Monocular 3D Object Detection.pdf:/home/zwerg/Zotero/storage/2DB3VG8J/Kumar et al. - 2022 - DEVIANT Depth EquiVarIAnt NeTwork for Monocular 3D Object Detection.pdf:application/pdf},
}

@misc{noauthor_deploying_nodate,
	title = {Deploying {TensorFlow} {Vision} {Models} in {Hugging} {Face} with {TF} {Serving}},
	url = {https://huggingface.co/blog/tf-serving-vision},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2022-08-08},
}

@misc{chowdhery_palm_2022,
	title = {{PaLM}: {Scaling} {Language} {Modeling} with {Pathways}},
	shorttitle = {{PaLM}},
	url = {http://arxiv.org/abs/2204.02311},
	abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-speciﬁc training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).},
	language = {en},
	urldate = {2022-09-13},
	publisher = {arXiv},
	author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	month = apr,
	year = {2022},
	note = {arXiv:2204.02311 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Chowdhery et al. - 2022 - PaLM Scaling Language Modeling with Pathways.pdf:/home/zwerg/Zotero/storage/BKV9XGNG/Chowdhery et al. - 2022 - PaLM Scaling Language Modeling with Pathways.pdf:application/pdf},
}

@misc{ouyang_bioimage_2022,
	title = {{BioImage} {Model} {Zoo}: {A} {Community}-{Driven} {Resource} for {Accessible} {Deep} {Learning} in {BioImage} {Analysis}},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	shorttitle = {{BioImage} {Model} {Zoo}},
	url = {https://www.biorxiv.org/content/10.1101/2022.06.07.495102v1},
	doi = {10.1101/2022.06.07.495102},
	abstract = {Deep learning-based approaches are revolutionizing imaging-driven scientific research. However, the accessibility and reproducibility of deep learning-based workflows for imaging scientists remain far from sufficient. Several tools have recently risen to the challenge of democratizing deep learning by providing user-friendly interfaces to analyze new data with pre-trained or fine-tuned models. Still, few of the existing pre-trained models are interoperable between these tools, critically restricting a model’s overall utility and the possibility of validating and reproducing scientific analyses. Here, we present the BioImage Model Zoo (https://bioimage.io): a community-driven, fully open resource where standardized pre-trained models can be shared, explored, tested, and downloaded for further adaptation or direct deployment in multiple end user-facing tools (e.g., ilastik, deepImageJ, QuPath, StarDist, ImJoy, ZeroCostDL4Mic, CSBDeep). To enable everyone to contribute and consume the Zoo resources, we provide a model standard to enable cross-compatibility, a rich list of example models and practical use-cases, developer tools, documentation, and the accompanying infrastructure for model upload, download and testing. Our contribution aims to lay the groundwork to make deep learning methods for microscopy imaging findable, accessible, interoperable, and reusable (FAIR) across software tools and platforms.},
	language = {en},
	urldate = {2022-09-15},
	publisher = {bioRxiv},
	author = {Ouyang, Wei and Beuttenmueller, Fynn and Gómez-de-Mariscal, Estibaliz and Pape, Constantin and Burke, Tom and Garcia-López-de-Haro, Carlos and Russell, Craig and Moya-Sans, Lucía and de-la-Torre-Gutiérrez, Cristina and Schmidt, Deborah and Kutra, Dominik and Novikov, Maksim and Weigert, Martin and Schmidt, Uwe and Bankhead, Peter and Jacquemet, Guillaume and Sage, Daniel and Henriques, Ricardo and Muñoz-Barrutia, Arrate and Lundberg, Emma and Jug, Florian and Kreshuk, Anna},
	month = jun,
	year = {2022},
	note = {Pages: 2022.06.07.495102
Section: New Results},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/JSAZ87LK/Ouyang et al. - 2022 - BioImage Model Zoo A Community-Driven Resource fo.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/QZPDB447/2022.06.07.html:text/html},
}

@misc{jeong_perfception_2022,
	title = {{PeRFception}: {Perception} using {Radiance} {Fields}},
	shorttitle = {{PeRFception}},
	url = {http://arxiv.org/abs/2208.11537},
	doi = {10.48550/arXiv.2208.11537},
	abstract = {The recent progress in implicit 3D representation, i.e., Neural Radiance Fields (NeRFs), has made accurate and photorealistic 3D reconstruction possible in a differentiable manner. This new representation can effectively convey the information of hundreds of high-resolution images in one compact format and allows photorealistic synthesis of novel views. In this work, using the variant of NeRF called Plenoxels, we create the first large-scale implicit representation datasets for perception tasks, called the PeRFception, which consists of two parts that incorporate both object-centric and scene-centric scans for classification and segmentation. It shows a significant memory compression rate (96.4{\textbackslash}\%) from the original dataset, while containing both 2D and 3D information in a unified form. We construct the classification and segmentation models that directly take as input this implicit format and also propose a novel augmentation technique to avoid overfitting on backgrounds of images. The code and data are publicly available in https://postech-cvlab.github.io/PeRFception .},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Jeong, Yoonwoo and Shin, Seungjoo and Lee, Junha and Choy, Christopher and Anandkumar, Animashree and Cho, Minsu and Park, Jaesik},
	month = aug,
	year = {2022},
	note = {arXiv:2208.11537 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Project Page: https://postech-cvlab.github.io/PeRFception/},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/YB5ZDAEI/Jeong et al. - 2022 - PeRFception Perception using Radiance Fields.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/MBVXEB7F/2208.html:text/html},
}

@misc{huba_papaya_2022,
	title = {Papaya: {Practical}, {Private}, and {Scalable} {Federated} {Learning}},
	shorttitle = {Papaya},
	url = {http://arxiv.org/abs/2111.04877},
	abstract = {Cross-device Federated Learning (FL) is a distributed learning paradigm with several challenges that differentiate it from traditional distributed learning, variability in the system characteristics on each device, and millions of clients coordinating with a central server being primary ones. Most FL systems described in the literature are synchronous – they perform a synchronized aggregation of model updates from individual clients. Scaling synchronous FL is challenging since increasing the number of clients training in parallel leads to diminishing returns in training speed, analogous to large-batch training. Moreover, stragglers hinder synchronous FL training. In this work, we outline a production asynchronous FL system design. Our work tackles the aforementioned issues, sketches of some of the system design challenges and their solutions, and touches upon principles that emerged from building a production FL system for millions of clients. Empirically, we demonstrate that asynchronous FL converges faster than synchronous FL when training across nearly one hundred million devices. In particular, in high concurrency settings, asynchronous FL is 5× faster and has nearly 8× less communication overhead than synchronous FL.},
	language = {en},
	urldate = {2022-09-17},
	publisher = {arXiv},
	author = {Huba, Dzmitry and Nguyen, John and Malik, Kshitiz and Zhu, Ruiyu and Rabbat, Mike and Yousefpour, Ashkan and Wu, Carole-Jean and Zhan, Hongyuan and Ustinov, Pavel and Srinivas, Harish and Wang, Kaikai and Shoumikhin, Anthony and Min, Jesik and Malek, Mani},
	month = apr,
	year = {2022},
	note = {arXiv:2111.04877 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Huba et al. - 2022 - Papaya Practical, Private, and Scalable Federated.pdf:/home/zwerg/Zotero/storage/ZW2BP4IJ/Huba et al. - 2022 - Papaya Practical, Private, and Scalable Federated.pdf:application/pdf},
}

@misc{zhang_opt_2022,
	title = {{OPT}: {Open} {Pre}-trained {Transformer} {Language} {Models}},
	shorttitle = {{OPT}},
	url = {http://arxiv.org/abs/2205.01068},
	abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difﬁcult to replicate without signiﬁcant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difﬁcult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
	language = {en},
	urldate = {2022-09-17},
	publisher = {arXiv},
	author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
	month = jun,
	year = {2022},
	note = {arXiv:2205.01068 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Zhang et al. - 2022 - OPT Open Pre-trained Transformer Language Models.pdf:/home/zwerg/Zotero/storage/VJJPFBYQ/Zhang et al. - 2022 - OPT Open Pre-trained Transformer Language Models.pdf:application/pdf},
}

@misc{reed_generalist_2022,
	title = {A {Generalist} {Agent}},
	url = {http://arxiv.org/abs/2205.06175},
	abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
	language = {en},
	urldate = {2022-09-17},
	publisher = {arXiv},
	author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and de Freitas, Nando},
	month = may,
	year = {2022},
	note = {arXiv:2205.06175 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Robotics},
	file = {Reed et al. - 2022 - A Generalist Agent.pdf:/home/zwerg/Zotero/storage/USF7SFFK/Reed et al. - 2022 - A Generalist Agent.pdf:application/pdf},
}

@article{zhang_focusing_2021,
	title = {Focusing {Algorithm} of {Automatic} {Control} {Microscope} {Based} on {Digital} {Image} {Processing}},
	volume = {2021},
	issn = {1687-725X},
	url = {https://www.hindawi.com/journals/js/2021/5643054/},
	doi = {10.1155/2021/5643054},
	abstract = {When performing digital image processing, the most critical technology that affects its use effect is the autofocus technology. With the advancement of science and the development of computer technology, autofocus technology has become more and more widely used in various fields. Autofocus technology is a key technology in robot vision and digital video systems. In order to allow digital image processing technology to better serve humans, it is necessary to further improve the focus evaluation function algorithm. This article focuses on the imaging principle of defocused images, using different evaluation functions to analyze and process the experimental images to observe the changes in image clarity. Through the introduction and analysis of the existing evaluation function, it can be known that the focus evaluation function will directly affect the quality of digital image processing. Therefore, it is best to choose unimodality, unbiasedness, low noise sensitivity, wide coverage, and a small amount of calculation. For the evaluation function, the Laplacian gradient function is an ideal choice. However, because the current digital image processing technology is not perfect enough, the focus function is still prone to multiextreme problems when the image is severely defocused and the high-frequency components in the image are missing; the balance between image processing speed and focus accuracy also still needs to be improved. Therefore, this paper studies the autocontrol microscope focus algorithm based on digital image processing, analyzes the principle of visual image imaging, and makes some improvements to the microscope focus algorithm. Through experiments, it can be seen that the real-time data of the original Laplace function in the edge-obvious target is 76.9, and it reaches 77.6 after improvement. The improved algorithm can better maintain the single-peak state during the focusing process, which improves the image processing efficiency while ensuring the measurement accuracy.},
	language = {en},
	urldate = {2022-09-23},
	journal = {Journal of Sensors},
	author = {Zhang, Jing and Zhang, Tao},
	month = sep,
	year = {2021},
	note = {Publisher: Hindawi},
	pages = {e5643054},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/IW54JGGX/Zhang and Zhang - 2021 - Focusing Algorithm of Automatic Control Microscope.pdf:application/pdf},
}

@article{xu_robust_2011,
	title = {Robust {Automatic} {Focus} {Algorithm} for {Low} {Contrast} {Images} {Using} a {New} {Contrast} {Measure}},
	volume = {11},
	issn = {1424-8220},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3231510/},
	doi = {10.3390/s110908281},
	abstract = {Low contrast images, suffering from a lack of sharpness, are easily influenced by noise. As a result, many local false peaks may be generated in contrast measurements, making it difficult for the camera’s passive auto-focus system to perform its function of locating the focused peak. In this paper, a new passive auto-focus algorithm is proposed to address this problem. First, a noise reduction preprocessing is introduced to make our algorithm robust to both additive noise and multiplicative noise. Then, a new contrast measure is presented to bring in local false peaks, ensuring the presence of a well defined focused peak. In order to gauge the performance of our algorithm, a modified peak search algorithm is used in the experiments. The experimental results from an actual digital camera validate the effectiveness of our proposed algorithm.},
	number = {9},
	urldate = {2022-09-23},
	journal = {Sensors (Basel, Switzerland)},
	author = {Xu, Xin and Wang, Yinglin and Tang, Jinshan and Zhang, Xiaolong and Liu, Xiaoming},
	month = aug,
	year = {2011},
	pmid = {22164075},
	pmcid = {PMC3231510},
	pages = {8281--8294},
	file = {PubMed Central Full Text PDF:/home/zwerg/Zotero/storage/BGI7HPLD/Xu et al. - 2011 - Robust Automatic Focus Algorithm for Low Contrast .pdf:application/pdf},
}

@inproceedings{jacovi_understanding_2018,
	address = {Brussels, Belgium},
	title = {Understanding {Convolutional} {Neural} {Networks} for {Text} {Classification}},
	url = {https://aclanthology.org/W18-5408},
	doi = {10.18653/v1/W18-5408},
	abstract = {We present an analysis into the inner workings of Convolutional Neural Networks (CNNs) for processing text. CNNs used for computer vision can be interpreted by projecting filters into image space, but for discrete sequence inputs CNNs remain a mystery. We aim to understand the method by which the networks process and classify text. We examine common hypotheses to this problem: that filters, accompanied by global max-pooling, serve as ngram detectors. We show that filters may capture several different semantic classes of ngrams by using different activation patterns, and that global max-pooling induces behavior which separates important ngrams from the rest. Finally, we show practical use cases derived from our findings in the form of model interpretability (explaining a trained model by deriving a concrete identity for each filter, bridging the gap between visualization tools in vision tasks and NLP) and prediction interpretability (explaining predictions).},
	urldate = {2022-09-26},
	booktitle = {Proceedings of the 2018 {EMNLP} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Jacovi, Alon and Sar Shalom, Oren and Goldberg, Yoav},
	month = nov,
	year = {2018},
	pages = {56--65},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/FRHUR9W3/Jacovi et al. - 2018 - Understanding Convolutional Neural Networks for Te.pdf:application/pdf},
}

@misc{hoffmann_training_2022,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over {\textbackslash}nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, {\textbackslash}chinchilla, that uses the same compute budget as {\textbackslash}gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. {\textbackslash}chinchilla uniformly and significantly outperforms {\textbackslash}Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that {\textbackslash}chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, {\textbackslash}chinchilla reaches a state-of-the-art average accuracy of 67.5{\textbackslash}\% on the MMLU benchmark, greater than a 7{\textbackslash}\% improvement over {\textbackslash}gopher.},
	urldate = {2022-09-26},
	publisher = {arXiv},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	month = mar,
	year = {2022},
	note = {arXiv:2203.15556 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/VEPAYZW4/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/XYZYKCJI/2203.html:text/html},
}

@misc{vdumoulin_convolution_2022,
	title = {Convolution arithmetic},
	copyright = {MIT},
	url = {https://github.com/vdumoulin/conv_arithmetic/blob/af6f818b0bb396c26da79899554682a8a499101d/README.md},
	abstract = {A technical report on convolution arithmetic in the context of deep learning},
	urldate = {2022-09-26},
	author = {vdumoulin},
	month = sep,
	year = {2022},
	note = {original-date: 2016-02-24T15:18:33Z},
}

@misc{saha_comprehensive_2018,
	title = {A {Comprehensive} {Guide} to {Convolutional} {Neural} {Networks} — the {ELI5} way},
	url = {https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53},
	abstract = {Artificial Intelligence has been witnessing a monumental growth in bridging the gap between the capabilities of humans and machines…},
	language = {en},
	urldate = {2022-09-26},
	journal = {Medium},
	author = {Saha, Sumit},
	month = dec,
	year = {2018},
	file = {Snapshot:/home/zwerg/Zotero/storage/88L2KYWJ/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53.html:text/html},
}

@article{handfield_unsupervised_2013,
	title = {Unsupervised {Clustering} of {Subcellular} {Protein} {Expression} {Patterns} in {High}-{Throughput} {Microscopy} {Images} {Reveals} {Protein} {Complexes} and {Functional} {Relationships} between {Proteins}},
	volume = {9},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1003085},
	doi = {10.1371/journal.pcbi.1003085},
	abstract = {Protein subcellular localization has been systematically characterized in budding yeast using fluorescently tagged proteins. Based on the fluorescence microscopy images, subcellular localization of many proteins can be classified automatically using supervised machine learning approaches that have been trained to recognize predefined image classes based on statistical features. Here, we present an unsupervised analysis of protein expression patterns in a set of high-resolution, high-throughput microscope images. Our analysis is based on 7 biologically interpretable features which are evaluated on automatically identified cells, and whose cell-stage dependency is captured by a continuous model for cell growth. We show that it is possible to identify most previously identified localization patterns in a cluster analysis based on these features and that similarities between the inferred expression patterns contain more information about protein function than can be explained by a previous manual categorization of subcellular localization. Furthermore, the inferred cell-stage associated to each fluorescence measurement allows us to visualize large groups of proteins entering the bud at specific stages of bud growth. These correspond to proteins localized to organelles, revealing that the organelles must be entering the bud in a stereotypical order. We also identify and organize a smaller group of proteins that show subtle differences in the way they move around the bud during growth. Our results suggest that biologically interpretable features based on explicit models of cell morphology will yield unprecedented power for pattern discovery in high-resolution, highthroughput microscopy images.},
	language = {en},
	number = {6},
	urldate = {2022-09-27},
	journal = {PLoS Computational Biology},
	author = {Handfield, Louis-François and Chong, Yolanda T. and Simmons, Jibril and Andrews, Brenda J. and Moses, Alan M.},
	editor = {Murphy, Robert F.},
	month = jun,
	year = {2013},
	pages = {e1003085},
	file = {Handfield et al. - 2013 - Unsupervised Clustering of Subcellular Protein Exp.pdf:/home/zwerg/Zotero/storage/495IMR64/Handfield et al. - 2013 - Unsupervised Clustering of Subcellular Protein Exp.pdf:application/pdf},
}

@article{lu_unsupervised_2016,
	title = {An {Unsupervised} {kNN} {Method} to {Systematically} {Detect} {Changes} in {Protein} {Localization} in {High}-{Throughput} {Microscopy} {Images}},
	volume = {11},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0158712},
	doi = {10.1371/journal.pone.0158712},
	abstract = {Despite the importance of characterizing genes that exhibit subcellular localization changes between conditions in proteome-wide imaging experiments, many recent studies still rely upon manual evaluation to assess the results of high-throughput imaging experiments. We describe and demonstrate an unsupervised k-nearest neighbours method for the detection of localization changes. Compared to previous classification-based supervised change detection methods, our method is much simpler and faster, and operates directly on the feature space to overcome limitations in needing to manually curate training sets that may not generalize well between screens. In addition, the output of our method is flexible in its utility, generating both a quantitatively ranked list of localization changes that permit user-defined cut-offs, and a vector for each gene describing feature-wise direction and magnitude of localization changes. We demonstrate that our method is effective at the detection of localization changes using the Δrpd3 perturbation in Saccharomyces cerevisiae, where we capture 71.4\% of previously known changes within the top 10\% of ranked genes, and find at least four new localization changes within the top 1\% of ranked genes. The results of our analysis indicate that simple unsupervised methods may be able to identify localization changes in images without laborious manual image labelling steps.},
	language = {en},
	number = {7},
	urldate = {2022-09-27},
	journal = {PLOS ONE},
	author = {Lu, Alex Xijie and Moses, Alan M.},
	month = jul,
	year = {2016},
	note = {Publisher: Public Library of Science},
	keywords = {Green fluorescent protein, Cell cycle and cell division, Subcellular localization, Data visualization, Genetic screens, Image analysis, Imaging techniques, Saccharomyces cerevisiae},
	pages = {e0158712},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/VSLGZAE4/Lu and Moses - 2016 - An Unsupervised kNN Method to Systematically Detec.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/UPRSNVQ5/article.html:text/html},
}

@article{koh_cyclops_2015,
	title = {{CYCLoPs}: {A} {Comprehensive} {Database} {Constructed} from {Automated} {Analysis} of {Protein} {Abundance} and {Subcellular} {Localization} {Patterns} in {Saccharomyces} cerevisiae},
	volume = {5},
	issn = {2160-1836},
	shorttitle = {{CYCLoPs}},
	url = {https://doi.org/10.1534/g3.115.017830},
	doi = {10.1534/g3.115.017830},
	abstract = {Changes in protein subcellular localization and abundance are central to biological regulation in eukaryotic cells. Quantitative measures of protein dynamics in vivo are therefore highly useful for elucidating specific regulatory pathways. Using a combinatorial approach of yeast synthetic genetic array technology, high-content screening, and machine learning classifiers, we developed an automated platform to characterize protein localization and abundance patterns from images of log phase cells from the open-reading frame−green fluorescent protein collection in the budding yeast, Saccharomyces cerevisiae. For each protein, we produced quantitative profiles of localization scores for 16 subcellular compartments at single-cell resolution to trace proteome-wide relocalization in conditions over time. We generated a collection of ∼300,000 micrographs, comprising more than 20 million cells and ∼9 billion quantitative measurements. The images depict the localization and abundance dynamics of more than 4000 proteins under two chemical treatments and in a selected mutant background. Here, we describe CYCLoPs (Collection of Yeast Cells Localization Patterns), a web database resource that provides a central platform for housing and analyzing our yeast proteome dynamics datasets at the single cell level. CYCLoPs version 1.0 is available at http://cyclops.ccbr.utoronto.ca. CYCLoPs will provide a valuable resource for the yeast and eukaryotic cell biology communities and will be updated as new experiments become available.},
	number = {6},
	urldate = {2022-09-27},
	journal = {G3 Genes{\textbar}Genomes{\textbar}Genetics},
	author = {Koh, Judice L Y and Chong, Yolanda T and Friesen, Helena and Moses, Alan and Boone, Charles and Andrews, Brenda J and Moffat, Jason},
	month = jun,
	year = {2015},
	pages = {1223--1232},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/K8RL6PK6/Koh et al. - 2015 - CYCLoPs A Comprehensive Database Constructed from.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/CHIW36Q3/6025272.html:text/html},
}

@article{zhu_nanodroplet_2018,
	title = {Nanodroplet processing platform for deep and quantitative proteome profiling of 10–100 mammalian cells},
	volume = {9},
	copyright = {2018 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-018-03367-w},
	doi = {10.1038/s41467-018-03367-w},
	abstract = {Nanoscale or single-cell technologies are critical for biomedical applications. However, current mass spectrometry (MS)-based proteomic approaches require samples comprising a minimum of thousands of cells to provide in-depth profiling. Here, we report the development of a nanoPOTS (nanodroplet processing in one pot for trace samples) platform for small cell population proteomics analysis. NanoPOTS enhances the efficiency and recovery of sample processing by downscaling processing volumes to {\textless}200 nL to minimize surface losses. When combined with ultrasensitive liquid chromatography-MS, nanoPOTS allows identification of {\textasciitilde}1500 to {\textasciitilde}3000 proteins from {\textasciitilde}10 to {\textasciitilde}140 cells, respectively. By incorporating the Match Between Runs algorithm of MaxQuant, {\textgreater}3000 proteins are consistently identified from as few as 10 cells. Furthermore, we demonstrate quantification of {\textasciitilde}2400 proteins from single human pancreatic islet thin sections from type 1 diabetic and control donors, illustrating the application of nanoPOTS for spatially resolved proteome measurements from clinical tissues.},
	language = {en},
	number = {1},
	urldate = {2022-09-27},
	journal = {Nature Communications},
	author = {Zhu, Ying and Piehowski, Paul D. and Zhao, Rui and Chen, Jing and Shen, Yufeng and Moore, Ronald J. and Shukla, Anil K. and Petyuk, Vladislav A. and Campbell-Thompson, Martha and Mathews, Clayton E. and Smith, Richard D. and Qian, Wei-Jun and Kelly, Ryan T.},
	month = feb,
	year = {2018},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Proteomic analysis, Mass spectrometry},
	pages = {882},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/2N9L2Z5P/Zhu et al. - 2018 - Nanodroplet processing platform for deep and quant.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/5YSFZER2/s41467-018-03367-w.html:text/html},
}

@article{torres_high-throughput_2016,
	title = {High-throughput fluorescence microscopic analysis of protein abundance and localization in budding yeast},
	volume = {51},
	issn = {1040-9238},
	url = {https://doi.org/10.3109/10409238.2016.1145185},
	doi = {10.3109/10409238.2016.1145185},
	abstract = {Proteins directly carry out and regulate cellular functions. As a result, changes in protein levels within a cell directly influence cellular processes. Similarly, it is intuitive that the intracellular localization of proteins is a key component of their functionality. Optimal activity is achieved by a combination of protein concentration, co-compartmentalization with substrates, co-factors and regulators and sequestration from deleterious locales. The proteome within a cell is highly dynamic and changes in response to different environmental conditions. High-throughput microscopic analysis in the budding yeast Saccharomyces cerevisiae has afforded proteome-wide views of protein organization in living cells, and of how protein abundance and location is regulated and remodeled in response to stress.},
	number = {2},
	urldate = {2022-09-27},
	journal = {Critical Reviews in Biochemistry and Molecular Biology},
	author = {Torres, Nikko P. and Ho, Brandon and Brown, Grant W.},
	month = mar,
	year = {2016},
	pmid = {26893079},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.3109/10409238.2016.1145185},
	keywords = {Fluorescence microscopy, genomics, protein abundance, protein localization, quantitative cell biology, yeast},
	pages = {110--119},
}

@article{cai_experimental_2018,
	title = {Experimental and computational framework for a dynamic protein atlas of human cell division},
	volume = {561},
	copyright = {2018 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-018-0518-z},
	doi = {10.1038/s41586-018-0518-z},
	abstract = {Essential biological functions, such as mitosis, require tight coordination of hundreds of proteins in space and time. Localization, the timing of interactions and changes in cellular structure are all crucial to ensure the correct assembly, function and regulation of protein complexes1–4. Imaging of live cells can reveal protein distributions and dynamics but experimental and theoretical challenges have prevented the collection of quantitative data, which are necessary for the formulation of a model of mitosis that comprehensively integrates information and enables the analysis of the dynamic interactions between the molecular parts of the mitotic machinery within changing cellular boundaries. Here we generate a canonical model of the morphological changes during the mitotic progression of human cells on the basis of four-dimensional image data. We use this model to integrate dynamic three-dimensional concentration data of many fluorescently knocked-in mitotic proteins, imaged by fluorescence correlation spectroscopy-calibrated microscopy5. The approach taken here to generate a dynamic protein atlas of human cell division is generic; it can be applied to systematically map and mine dynamic protein localization networks that drive cell division in different cell types, and can be conceptually transferred to other cellular functions.},
	language = {en},
	number = {7723},
	urldate = {2022-09-27},
	journal = {Nature},
	author = {Cai, Yin and Hossain, M. Julius and Hériché, Jean-Karim and Politi, Antonio Z. and Walther, Nike and Koch, Birgit and Wachsmuth, Malte and Nijmeijer, Bianca and Kueblbeck, Moritz and Martinic-Kavur, Marina and Ladurner, Rene and Alexander, Stephanie and Peters, Jan-Michael and Ellenberg, Jan},
	month = sep,
	year = {2018},
	note = {Number: 7723
Publisher: Nature Publishing Group},
	keywords = {Computational models, Mitosis, Protein function predictions},
	pages = {411--415},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/5PGJT3YK/Cai et al. - 2018 - Experimental and computational framework for a dyn.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/WRZ8THUF/s41586-018-0518-z.html:text/html},
}

@article{kraus_automated_2017-1,
	title = {Automated analysis of high-content microscopy data with deep learning},
	volume = {13},
	issn = {1744-4292},
	doi = {10.15252/msb.20177551},
	abstract = {Existing computational pipelines for quantitative analysis of high-content microscopy data rely on traditional machine learning approaches that fail to accurately classify more than a single dataset without substantial tuning and training, requiring extensive analysis. Here, we demonstrate that the application of deep learning to biological image data can overcome the pitfalls associated with conventional machine learning classifiers. Using a deep convolutional neural network (DeepLoc) to analyze yeast cell images, we show improved performance over traditional approaches in the automated classification of protein subcellular localization. We also demonstrate the ability of DeepLoc to classify highly divergent image sets, including images of pheromone-arrested cells with abnormal cellular morphology, as well as images generated in different genetic backgrounds and in different laboratories. We offer an open-source implementation that enables updating DeepLoc on new microscopy datasets. This study highlights deep learning as an important tool for the expedited analysis of high-content microscopy data.},
	language = {eng},
	number = {4},
	journal = {Molecular Systems Biology},
	author = {Kraus, Oren Z. and Grys, Ben T. and Ba, Jimmy and Chong, Yolanda and Frey, Brendan J. and Boone, Charles and Andrews, Brenda J.},
	month = apr,
	year = {2017},
	pmid = {28420678},
	pmcid = {PMC5408780},
	keywords = {machine learning, Microscopy, deep learning, image analysis, Machine Learning, Neural Networks, Computer, Saccharomyces cerevisiae, high‐content screening, Saccharomyces cerevisiae Proteins, Systems Biology},
	pages = {924},
	file = {Full Text:/home/zwerg/Zotero/storage/5GDD2A8M/Kraus et al. - 2017 - Automated analysis of high-content microscopy data.pdf:application/pdf},
}

@article{parnamaa_accurate_2017,
	title = {Accurate {Classification} of {Protein} {Subcellular} {Localization} from {High}-{Throughput} {Microscopy} {Images} {Using} {Deep} {Learning}},
	volume = {7},
	issn = {2160-1836},
	url = {https://doi.org/10.1534/g3.116.033654},
	doi = {10.1534/g3.116.033654},
	abstract = {High-throughput microscopy of many single cells generates high-dimensional data that are far from straightforward to analyze. One important problem is automatically detecting the cellular compartment where a fluorescently-tagged protein resides, a task relatively simple for an experienced human, but difficult to automate on a computer. Here, we train an 11-layer neural network on data from mapping thousands of yeast proteins, achieving per cell localization classification accuracy of 91\%, and per protein accuracy of 99\% on held-out images. We confirm that low-level network features correspond to basic image characteristics, while deeper layers separate localization classes. Using this network as a feature calculator, we train standard classifiers that assign proteins to previously unseen compartments after observing only a small number of training examples. Our results are the most accurate subcellular localization classifications to date, and demonstrate the usefulness of deep learning for high-throughput microscopy.},
	number = {5},
	urldate = {2022-09-27},
	journal = {G3 Genes{\textbar}Genomes{\textbar}Genetics},
	author = {Pärnamaa, Tanel and Parts, Leopold},
	month = may,
	year = {2017},
	pages = {1385--1392},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/7JSUQBLG/Pärnamaa and Parts - 2017 - Accurate Classification of Protein Subcellular Loc.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/GFLWKTBK/6028261.html:text/html},
}

@misc{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {http://arxiv.org/abs/1503.03585},
	doi = {10.48550/arXiv.1503.03585},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	urldate = {2022-09-28},
	publisher = {arXiv},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
	month = nov,
	year = {2015},
	note = {arXiv:1503.03585 [cond-mat, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/3TYQRT3W/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/SSS5FF5N/1503.html:text/html},
}

@misc{ho_denoising_2020-1,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2022-09-28},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/2GRWL8HX/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/WXR3NST5/2006.html:text/html},
}

@misc{singer_make--video_2022,
	title = {Make-{A}-{Video}: {Text}-to-{Video} {Generation} without {Text}-{Video} {Data}},
	shorttitle = {Make-{A}-{Video}},
	url = {http://arxiv.org/abs/2209.14792},
	doi = {10.48550/arXiv.2209.14792},
	abstract = {We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.},
	urldate = {2022-09-30},
	publisher = {arXiv},
	author = {Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and Parikh, Devi and Gupta, Sonal and Taigman, Yaniv},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14792 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/RTTP8YZQ/Singer et al. - 2022 - Make-A-Video Text-to-Video Generation without Tex.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/THHL2I37/2209.html:text/html},
}

@inproceedings{klein_opennmt_2017-1,
	address = {Vancouver, Canada},
	title = {{OpenNMT}: {Open}-{Source} {Toolkit} for {Neural} {Machine} {Translation}},
	shorttitle = {{OpenNMT}},
	url = {http://aclweb.org/anthology/P17-4012},
	doi = {10.18653/v1/P17-4012},
	language = {en},
	urldate = {2022-10-03},
	booktitle = {Proceedings of {ACL} 2017, {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Klein, Guillaume and Kim, Yoon and Deng, Yuntian and Senellart, Jean and Rush, Alexander},
	year = {2017},
	pages = {67--72},
	file = {Full Text:/home/zwerg/Zotero/storage/KH669HS2/Klein et al. - 2017 - OpenNMT Open-Source Toolkit for Neural Machine Tr.pdf:application/pdf},
}

@misc{noauthor_annotated_nodate-1,
	title = {The {Annotated} {Transformer}},
	url = {http://nlp.seas.harvard.edu/annotated-transformer/},
	urldate = {2022-10-03},
	file = {The Annotated Transformer:/home/zwerg/Zotero/storage/AYJKK2CJ/annotated-transformer.html:text/html;The Annotated Transformer.pdf:/home/zwerg/Zotero/storage/4Q5ZUSMU/The Annotated Transformer.pdf:application/pdf},
}

@misc{alammar_illustrated_nodate-1,
	title = {The {Illustrated} {BERT}, {ELMo}, and co. ({How} {NLP} {Cracked} {Transfer} {Learning})},
	url = {http://jalammar.github.io/illustrated-bert/},
	abstract = {Discussions:
Hacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)


Translations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish

2021 Update: I created this brief and highly accessible video intro to BERT





The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).},
	urldate = {2022-10-03},
	author = {Alammar, Jay},
	file = {Alammar - The Illustrated BERT, ELMo, and co. (How NLP Crack.pdf:/home/zwerg/Zotero/storage/P9ISCH3Y/Alammar - The Illustrated BERT, ELMo, and co. (How NLP Crack.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/C42WTT9V/illustrated-bert.html:text/html},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	doi = {10.48550/arXiv.1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2022-10-03},
	publisher = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/KC2KYDIG/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/UV7EDHIT/1907.html:text/html},
}

@misc{sun_ernie_2019,
	title = {{ERNIE} 2.0: {A} {Continual} {Pre}-training {Framework} for {Language} {Understanding}},
	shorttitle = {{ERNIE} 2.0},
	url = {http://arxiv.org/abs/1907.12412},
	doi = {10.48550/arXiv.1907.12412},
	abstract = {Recently, pre-trained models have achieved state-of-the-art results in various language understanding tasks, which indicates that pre-training on large-scale corpora may play a crucial role in natural language processing. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entity, semantic closeness and discourse relations. In order to extract to the fullest extent, the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which builds and learns incrementally pre-training tasks through constant multi-task learning. Experimental results demonstrate that ERNIE 2.0 outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several common tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.},
	urldate = {2022-10-03},
	publisher = {arXiv},
	author = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},
	month = nov,
	year = {2019},
	note = {arXiv:1907.12412 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 11 pages, 3 figures and 7 tables; Accepted by AAAI 2020},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/UGDG9I7N/Sun et al. - 2019 - ERNIE 2.0 A Continual Pre-training Framework for .pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/IYY8RUYP/1907.html:text/html},
}

@misc{sun_ernie_2019-1,
	title = {{ERNIE}: {Enhanced} {Representation} through {Knowledge} {Integration}},
	shorttitle = {{ERNIE}},
	url = {http://arxiv.org/abs/1904.09223},
	doi = {10.48550/arXiv.1904.09223},
	abstract = {We present a novel language representation model enhanced by knowledge called ERNIE (Enhanced Representation through kNowledge IntEgration). Inspired by the masking strategy of BERT, ERNIE is designed to learn language representation enhanced by knowledge masking strategies, which includes entity-level masking and phrase-level masking. Entity-level strategy masks entities which are usually composed of multiple words.Phrase-level strategy masks the whole phrase which is composed of several words standing together as a conceptual unit.Experimental results show that ERNIE outperforms other baseline methods, achieving new state-of-the-art results on five Chinese natural language processing tasks including natural language inference, semantic similarity, named entity recognition, sentiment analysis and question answering. We also demonstrate that ERNIE has more powerful knowledge inference capacity on a cloze test.},
	urldate = {2022-10-03},
	publisher = {arXiv},
	author = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Chen, Xuyi and Zhang, Han and Tian, Xin and Zhu, Danxiang and Tian, Hao and Wu, Hua},
	month = apr,
	year = {2019},
	note = {arXiv:1904.09223 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 8 pages},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/9IIAPM7G/Sun et al. - 2019 - ERNIE Enhanced Representation through Knowledge I.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/HPUW9ZZZ/1904.html:text/html},
}

@misc{alammar_illustrated_nodate-2,
	title = {The {Illustrated} {Stable} {Diffusion}},
	url = {https://jalammar.github.io/illustrated-stable-diffusion/},
	abstract = {AI image generation is the most recent AI capability blowing people’s minds (mine included). The ability to create striking visuals from text descriptions has a magical quality to it and points clearly to a shift in how humans create art. The release of Stable Diffusion is a clear milestone in this development because it made a high-performance model available to the masses (performance in terms of image quality, as well as speed and relatively low resource/memory requirements).

After experimenting with AI image generation, you may start to wonder how it works.

This is a gentle introduction to how Stable Diffusion works.


  
  



Stable Diffusion is versatile in that it can be used in a number of different ways. Let’s focus at first on image generation from text only (text2img). The image above shows an example text input and the resulting generated image (The actual complete prompt is here). Aside from text to image, another main way of using it is by making it alter images (so inputs are text + image).},
	urldate = {2022-10-04},
	author = {Alammar, Jay},
	file = {Snapshot:/home/zwerg/Zotero/storage/ZIZK4BPG/illustrated-stable-diffusion.html:text/html},
}

@misc{sanchez_whats_2022,
	title = {What's all this hype about the {Stable} {Diffusion} model?},
	url = {https://www.narrativa.com/whats-all-this-hype-about-the-stable-diffusion-model/},
	abstract = {Everything that has to do with generating images through AI causes a lot of interest. What's all this hype with the Stable Diffusion model?},
	language = {en-US},
	urldate = {2022-10-04},
	journal = {Narrativa},
	author = {Sánchez, Sofía},
	month = aug,
	year = {2022},
	file = {Snapshot:/home/zwerg/Zotero/storage/XTHQAPFR/whats-all-this-hype-about-the-stable-diffusion-model.html:text/html},
}

@article{sutskever_sequence_nodate,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difﬁcult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a ﬁxed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difﬁculty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	language = {en},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	pages = {9},
	file = {Sutskever et al. - Sequence to Sequence Learning with Neural Networks.pdf:/home/zwerg/Zotero/storage/Z64IBJ6H/Sutskever et al. - Sequence to Sequence Learning with Neural Networks.pdf:application/pdf},
}

@misc{noauthor_nlp_nodate,
	title = {{NLP} {From} {Scratch}: {Translation} with a {Sequence} to {Sequence} {Network} and {Attention} — {PyTorch} {Tutorials} 1.12.1+cu102 documentation},
	url = {https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html},
	urldate = {2022-10-10},
}

@misc{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a ﬁxed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a ﬁxed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	language = {en},
	urldate = {2022-10-10},
	publisher = {arXiv},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv:1409.0473 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Accepted at ICLR 2015 as oral presentation},
	file = {Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:/home/zwerg/Zotero/storage/X9AYAV5G/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf},
}

@misc{bahdanau_neural_2016-1,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a ﬁxed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a ﬁxed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	language = {en},
	urldate = {2022-10-10},
	publisher = {arXiv},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv:1409.0473 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Accepted at ICLR 2015 as oral presentation},
	file = {Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:/home/zwerg/Zotero/storage/A9ZCEDMY/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf},
}

@misc{phi_illustrated_2020,
	title = {Illustrated {Guide} to {LSTM}’s and {GRU}’s: {A} step by step explanation},
	shorttitle = {Illustrated {Guide} to {LSTM}’s and {GRU}’s},
	url = {https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21},
	abstract = {Hi and welcome to an Illustrated Guide to Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU). I’m Michael, and I’m a Machine…},
	language = {en},
	urldate = {2022-10-10},
	journal = {Medium},
	author = {Phi, Michael},
	month = jun,
	year = {2020},
	file = {Snapshot:/home/zwerg/Zotero/storage/ZTH3VGF8/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21.html:text/html},
}

@misc{venkatachalam_attention_2019,
	title = {Attention in {Neural} {Networks}},
	url = {https://towardsdatascience.com/attention-in-neural-networks-e66920838742},
	abstract = {Some variations of attention architectures},
	language = {en},
	urldate = {2022-10-10},
	journal = {Medium},
	author = {Venkatachalam, Mahendran},
	month = jul,
	year = {2019},
	file = {Snapshot:/home/zwerg/Zotero/storage/9EDFSKD9/attention-in-neural-networks-e66920838742.html:text/html},
}

@misc{lihala_attention_2019,
	title = {Attention and its {Different} {Forms}},
	url = {https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc},
	abstract = {An overview of generalised attention with its different types and uses.},
	language = {en},
	urldate = {2022-10-10},
	journal = {Medium},
	author = {Lihala, Anusha},
	month = mar,
	year = {2019},
	file = {Snapshot:/home/zwerg/Zotero/storage/CKWK5JTH/attention-and-its-different-forms-7fc3674d14dc.html:text/html},
}

@article{chan_res2-unext_2022,
	title = {Res2-{UNeXt}: a novel deep learning framework for few-shot cell image segmentation},
	volume = {81},
	issn = {1573-7721},
	shorttitle = {Res2-{UNeXt}},
	url = {https://doi.org/10.1007/s11042-021-10536-5},
	doi = {10.1007/s11042-021-10536-5},
	abstract = {Recently, developing more accurate and more efficient deep learning algorithms for medical images segmentation attracts more and more attentions of researchers. Most of methods increase the depth of the network to replace with acquiring multi-information. The costs of training images annotation are too expensive to label by hand. In this paper, we propose a multi-scale and better performance deep architecture for medical image segmentation, named Res2-UNeXt. Our architecture is an encoder-decoder network with Res2XBlocks. The Res2XBlocks aim at acquiring multi-scale information better in images. To cooperate with Res2-UNeXt, we put forward a simple and efficient method of data augmentation. The data augmentation method, based on the process of cell movement and deformation, has biological implications in away. We evaluate Res2-UNeXt in comparison with recent variants of U-Net: UNet++, CE-Net and LadderNet and the method that different from U-Net architecture: FCN and DFANet on the dataset of ISBI cell tracking challenge 2019 via four different cell images. The experimental results demonstrate that Res2-UNeXt can achieve better performance than both recent variants of U-Net and non-U-Net architecture methods. Besides, the proposed architecture and the data augmentation method have been proven efficiently by the ablation experiments.},
	language = {en},
	number = {10},
	urldate = {2022-10-11},
	journal = {Multimedia Tools and Applications},
	author = {Chan, Sixian and Huang, Cheng and Bai, Cong and Ding, Weilong and Chen, Shengyong},
	month = apr,
	year = {2022},
	keywords = {Image processing, Deep learning, Data augmentation, Segmentation of medical images},
	pages = {13275--13288},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/8MDGSKZ4/Chan et al. - 2022 - Res2-UNeXt a novel deep learning framework for fe.pdf:application/pdf},
}

@misc{ha_hypernetworks_2016-1,
	title = {{HyperNetworks}},
	url = {http://arxiv.org/abs/1609.09106},
	doi = {10.48550/arXiv.1609.09106},
	abstract = {This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Ha, David and Dai, Andrew and Le, Quoc V.},
	month = dec,
	year = {2016},
	note = {arXiv:1609.09106 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/NGVDRYBA/Ha et al. - 2016 - HyperNetworks.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/HZ7XE3FJ/1609.html:text/html},
}

@misc{wang_computing_2022,
	title = {Computing {Multiple} {Image} {Reconstructions} with a {Single} {Hypernetwork}},
	url = {http://arxiv.org/abs/2202.11009},
	abstract = {Deep learning based techniques achieve state-of-the-art results in a wide range of image reconstruction tasks like compressed sensing. These methods almost always have hyperparameters, such as the weight coefﬁcients that balance the different terms in the optimized loss function. The typical approach is to train the model for a hyperparameter setting determined with some empirical or theoretical justiﬁcation. Thus, at inference time, the model can only compute reconstructions corresponding to the pre-determined hyperparameter values. In this work, we present a hypernetwork based approach, called HyperRecon, to train reconstruction models that are agnostic to hyperparameter settings. At inference time, HyperRecon can efﬁciently produce diverse reconstructions, which would each correspond to different hyperparameter values. In this framework, the user is empowered to select the most useful output(s) based on their own judgement. We demonstrate our method in compressed sensing, super-resolution and denoising tasks, using two largescale and publicly-available MRI datasets. Our code is available at https://github.com/alanqrwang/hyperrecon.},
	language = {en},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Wang, Alan Q. and Dalca, Adrian V. and Sabuncu, Mert R.},
	month = jun,
	year = {2022},
	note = {arXiv:2202.11009 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted for publication at the Journal of Machine Learning for Biomedical Imaging (MELBA) https://www.melba-journal.org/papers/2022:017.html},
	file = {Wang et al. - 2022 - Computing Multiple Image Reconstructions with a Si.pdf:/home/zwerg/Zotero/storage/PDC4D3GT/Wang et al. - 2022 - Computing Multiple Image Reconstructions with a Si.pdf:application/pdf},
}

@article{gou_knowledge_2021,
	title = {Knowledge {Distillation}: {A} {Survey}},
	volume = {129},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Knowledge {Distillation}},
	url = {http://arxiv.org/abs/2006.05525},
	doi = {10.1007/s11263-021-01453-z},
	abstract = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher-student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.},
	number = {6},
	urldate = {2022-10-12},
	journal = {International Journal of Computer Vision},
	author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen John and Tao, Dacheng},
	month = jun,
	year = {2021},
	note = {arXiv:2006.05525 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1789--1819},
	annote = {Comment: It has been accepted for publication in International Journal of Computer Vision (2021)},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/JEIC5QTL/Gou et al. - 2021 - Knowledge Distillation A Survey.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/N594E4Z9/2006.html:text/html},
}

@book{chen_online_2019,
	title = {Online {Knowledge} {Distillation} with {Diverse} {Peers}},
	abstract = {Distillation is an effective knowledge-transfer technique that uses predicted distributions of a powerful teacher model as soft targets to train a less-parameterized student model. A pre-trained high capacity teacher, however, is not always available. Recently proposed online variants use the aggre-gated intermediate predictions of multiple student models as targets to train each student model. Although group-derived targets give a good recipe for teacher-free distillation, group members are homogenized quickly with simple aggrega-tion functions, leading to early saturated solutions. In this work, we propose Online Knowledge Distillation with Diverse peers (OKDDip), which performs two-level distillation during training with multiple auxiliary peers and one group leader. In the first-level distillation, each auxiliary peer holds an individual set of aggregation weights generated with an attention-based mechanism to derive its own targets from predictions of other auxiliary peers. Learning from distinct target distributions helps to boost peer diversity for effectiveness of group-based distillation. The second-level distillation is performed to transfer the knowledge in the ensemble of auxiliary peers further to the group leader, i.e., the model used for inference. Experimental results show that the proposed framework consistently gives better performance than state-of-the-art approaches without sacrificing training or inference complexity, demonstrating the effectiveness of the proposed two-level distillation framework.},
	author = {Chen, Defang and Mei, Jian-Ping and Wang, Can and Feng, Yan and Chen, Chun},
	month = nov,
	year = {2019},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/QT9ZCFI2/Chen et al. - 2019 - Online Knowledge Distillation with Diverse Peers.pdf:application/pdf},
}

@misc{valanarasu_unext_2022,
	title = {{UNeXt}: {MLP}-based {Rapid} {Medical} {Image} {Segmentation} {Network}},
	shorttitle = {{UNeXt}},
	url = {http://arxiv.org/abs/2203.04967},
	abstract = {UNet and its latest extensions like TransUNet have been the leading medical image segmentation methods in recent years. However, these networks cannot be effectively adopted for rapid image segmentation in point-of-care applications as they are parameter-heavy, computationally complex and slow to use. To this end, we propose UNeXt which is a Convolutional multilayer perceptron (MLP) based network for image segmentation. We design UNeXt in an effective way with an early convolutional stage and a MLP stage in the latent stage. We propose a tokenized MLP block where we efficiently tokenize and project the convolutional features and use MLPs to model the representation. To further boost the performance, we propose shifting the channels of the inputs while feeding in to MLPs so as to focus on learning local dependencies. Using tokenized MLPs in latent space reduces the number of parameters and computational complexity while being able to result in a better representation to help segmentation. The network also consists of skip connections between various levels of encoder and decoder. We test UNeXt on multiple medical image segmentation datasets and show that we reduce the number of parameters by 72x, decrease the computational complexity by 68x, and improve the inference speed by 10x while also obtaining better segmentation performance over the state-of-the-art medical image segmentation architectures. Code is available at https://github.com/jeya-maria-jose/UNeXt-pytorch},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Valanarasu, Jeya Maria Jose and Patel, Vishal M.},
	month = mar,
	year = {2022},
	note = {arXiv:2203.04967 [cs, eess]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: Tech Report},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/H8GQGZIJ/Valanarasu and Patel - 2022 - UNeXt MLP-based Rapid Medical Image Segmentation .pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/TXBPIIGY/2203.html:text/html},
}

@misc{noauthor_hubmap_nodate,
	title = {{HuBMAP} {Pytorch}/fast.ai starter},
	url = {https://kaggle.com/code/iafoss/hubmap-pytorch-fast-ai-starter},
	abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbar} Using data from multiple data sources},
	language = {en},
	urldate = {2022-10-12},
	file = {Snapshot:/home/zwerg/Zotero/storage/IYG8NAPB/hubmap-pytorch-fast-ai-starter.html:text/html},
}

@article{le_analysis_2022,
	title = {Analysis of the {Human} {Protein} {Atlas} {Weakly} {Supervised} {Single}-{Cell} {Classification} competition},
	volume = {19},
	copyright = {2022 The Author(s)},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-022-01606-z},
	doi = {10.1038/s41592-022-01606-z},
	abstract = {While spatial proteomics by fluorescence imaging has quickly become an essential discovery tool for researchers, fast and scalable methods to classify and embed single-cell protein distributions in such images are lacking. Here, we present the design and analysis of the results from the competition Human Protein Atlas – Single-Cell Classification hosted on the Kaggle platform. This represents a crowd-sourced competition to develop machine learning models trained on limited annotations to label single-cell protein patterns in fluorescent images. The particular challenges of this competition include class imbalance, weak labels and multi-label classification, prompting competitors to apply a wide range of approaches in their solutions. The winning models serve as the first subcellular omics tools that can annotate single-cell locations, extract single-cell features and capture cellular dynamics.},
	language = {en},
	number = {10},
	urldate = {2022-10-13},
	journal = {Nature Methods},
	author = {Le, Trang and Winsnes, Casper F. and Axelsson, Ulrika and Xu, Hao and Mohanakrishnan Kaimal, Jayasankar and Mahdessian, Diana and Dai, Shubin and Makarov, Ilya S. and Ostankovich, Vladislav and Xu, Yang and Benhamou, Eric and Henkel, Christof and Solovyev, Roman A. and Banić, Nikola and Bošnjak, Vito and Bošnjak, Ana and Miličević, Andrija and Ouyang, Wei and Lundberg, Emma},
	month = oct,
	year = {2022},
	note = {Number: 10
Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning, Organelles},
	pages = {1221--1229},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/QS7BEU85/Le et al. - 2022 - Analysis of the Human Protein Atlas Weakly Supervi.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/ABWVVGP3/s41592-022-01606-z.html:text/html},
}

@article{fischer_modeling_2022,
	title = {Modeling intercellular communication in tissues using spatial graphs of cells},
	copyright = {2022 The Author(s)},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-022-01467-z},
	doi = {10.1038/s41587-022-01467-z},
	abstract = {Models of intercellular communication in tissues are based on molecular profiles of dissociated cells, are limited to receptor–ligand signaling and ignore spatial proximity in situ. We present node-centric expression modeling, a method based on graph neural networks that estimates the effects of niche composition on gene expression in an unbiased manner from spatial molecular profiling data. We recover signatures of molecular processes known to underlie cell communication.},
	language = {en},
	urldate = {2022-10-28},
	journal = {Nature Biotechnology},
	author = {Fischer, David S. and Schaar, Anna C. and Theis, Fabian J.},
	month = oct,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biotechnology, Sequencing},
	pages = {1--5},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/PRPBZ6MF/Fischer et al. - 2022 - Modeling intercellular communication in tissues us.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/CJ7PH3XX/s41587-022-01467-z.html:text/html},
}

@misc{noauthor_large_nodate,
	title = {Large {Motion} {Frame} {Interpolation}},
	url = {https://ai.googleblog.com/2022/10/large-motion-frame-interpolation.html},
	language = {en},
	urldate = {2022-10-31},
	file = {Snapshot:/home/zwerg/Zotero/storage/TA4QK88D/large-motion-frame-interpolation.html:text/html},
}

@article{feng_deep_2021-1,
	title = {Deep graph cut network for weakly-supervised semantic segmentation},
	volume = {64},
	issn = {1869-1919},
	url = {https://doi.org/10.1007/s11432-020-3065-4},
	doi = {10.1007/s11432-020-3065-4},
	abstract = {The scarcity of fully-annotated data becomes the biggest obstacle that prevents many deep learning approaches from widely applied. Weakly-supervised visual learning which can utilize inexact annotations is developed rapidly to remedy such a situation. In this paper, we study the weakly-supervised task achieving pixel-level semantic segmentation only with image-level labels as supervision. Different from other methods, our approach tries to transform the weakly-supervised visual learning problem into a semi-supervised visual learning problem and then utilizes semi-supervised learning methods to solve it. Utilizing this transformation, we can adopt effective semi-supervised methods to perform transductive learning with context information. In the semi-supervised learning module, we propose to use the graph cut algorithm to label more supervision from the activation seeds generated from a classification network. The generated labels can provide the segmentation model with effective supervision information; moreover, the graph cut module can benefit from features extracted by the segmentation model. Then, each of them updates and optimizes the other iteratively until convergence. Experiment results on PASCAL VOC and COCO benchmarks demonstrate the effectiveness of the proposed deep graph cut algorithm for weakly-supervised semantic segmentation.},
	language = {en},
	number = {3},
	urldate = {2022-11-02},
	journal = {Science China Information Sciences},
	author = {Feng, Jiapei and Wang, Xinggang and Liu, Wenyu},
	month = feb,
	year = {2021},
	keywords = {semantic segmentation, graph cut, semi-supervised learning, weakly-supervised learning},
	pages = {130105},
	file = {Feng et al. - 2021 - Deep graph cut network for weakly-supervised semantic segmentation.pdf:/home/zwerg/Zotero/storage/92KXX8MK/Feng et al. - 2021 - Deep graph cut network for weakly-supervised semantic segmentation.pdf:application/pdf},
}

@misc{klambauer_self-normalizing_2017,
	title = {Self-{Normalizing} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.02515},
	doi = {10.48550/arXiv.1706.02515},
	abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
	urldate = {2022-11-02},
	publisher = {arXiv},
	author = {Klambauer, Günter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
	month = sep,
	year = {2017},
	note = {arXiv:1706.02515 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 9 pages (+ 93 pages appendix)},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/526ETKH2/Klambauer et al. - 2017 - Self-Normalizing Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/KB7MBJ85/1706.html:text/html},
}

@inproceedings{chen_fast_2014,
	address = {Doha, Qatar},
	title = {A {Fast} and {Accurate} {Dependency} {Parser} using {Neural} {Networks}},
	url = {http://aclweb.org/anthology/D14-1082},
	doi = {10.3115/v1/D14-1082},
	abstract = {Almost all current dependency parsers classify based on millions of sparse indicator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed significantly. In this work, we propose a novel way of learning a neural network classiﬁer for use in a greedy, transition-based dependency parser. Because this classiﬁer learns and uses just a small number of dense features, it can work very fast, while achieving an about 2\% improvement in unlabeled and labeled attachment scores on both English and Chinese datasets. Concretely, our parser is able to parse more than 1000 sentences per second at 92.2\% unlabeled attachment score on the English Penn Treebank.},
	language = {en},
	urldate = {2022-11-05},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Danqi and Manning, Christopher},
	year = {2014},
	pages = {740--750},
	file = {Chen and Manning - 2014 - A Fast and Accurate Dependency Parser using Neural.pdf:/home/zwerg/Zotero/storage/ZUJR8UVI/Chen and Manning - 2014 - A Fast and Accurate Dependency Parser using Neural.pdf:application/pdf},
}

@article{li_real-time_2022,
	title = {Real-time denoising enables high-sensitivity fluorescence time-lapse imaging beyond the shot-noise limit},
	copyright = {2022 The Author(s)},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-022-01450-8},
	doi = {10.1038/s41587-022-01450-8},
	abstract = {A fundamental challenge in fluorescence microscopy is the photon shot noise arising from the inevitable stochasticity of photon detection. Noise increases measurement uncertainty and limits imaging resolution, speed and sensitivity. To achieve high-sensitivity fluorescence imaging beyond the shot-noise limit, we present DeepCAD-RT, a self-supervised deep learning method for real-time noise suppression. Based on our previous framework DeepCAD, we reduced the number of network parameters by 94\%, memory consumption by 27-fold and processing time by a factor of 20, allowing real-time processing on a two-photon microscope. A high imaging signal-to-noise ratio can be acquired with tenfold fewer photons than in standard imaging approaches. We demonstrate the utility of DeepCAD-RT in a series of photon-limited experiments, including in vivo calcium imaging of mice, zebrafish larva and fruit flies, recording of three-dimensional (3D) migration of neutrophils after acute brain injury and imaging of 3D dynamics of cortical ATP release. DeepCAD-RT will facilitate the morphological and functional interrogation of biological dynamics with a minimal photon budget.},
	language = {en},
	urldate = {2022-11-09},
	journal = {Nature Biotechnology},
	author = {Li, Xinyang and Li, Yixin and Zhou, Yiliang and Wu, Jiamin and Zhao, Zhifeng and Fan, Jiaqi and Deng, Fei and Wu, Zhaofa and Xiao, Guihua and He, Jing and Zhang, Yuanlong and Zhang, Guoxun and Hu, Xiaowan and Chen, Xingye and Zhang, Yi and Qiao, Hui and Xie, Hao and Li, Yulong and Wang, Haoqian and Fang, Lu and Dai, Qionghai},
	month = sep,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Image processing, Microscopy, Fluorescence imaging, Software},
	pages = {1--11},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/PJHGQBBB/Li et al. - 2022 - Real-time denoising enables high-sensitivity fluor.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/NZMA4ZIJ/s41587-022-01450-8.html:text/html},
}

@article{laine_imaging_2021-1,
	title = {Imaging in focus: {An} introduction to denoising bioimages in the era of deep learning},
	volume = {140},
	issn = {1357-2725},
	shorttitle = {Imaging in focus},
	url = {https://www.sciencedirect.com/science/article/pii/S1357272521001588},
	doi = {10.1016/j.biocel.2021.106077},
	abstract = {Fluorescence microscopy enables the direct observation of previously hidden dynamic processes of life, allowing profound insights into mechanisms of health and disease. However, imaging of live samples is fundamentally limited by the toxicity of the illuminating light and images are often acquired using low light conditions. As a consequence, images can become very noisy which severely complicates their interpretation. In recent years, deep learning (DL) has emerged as a very successful approach to remove this noise while retaining the useful signal. Unlike classical algorithms which use well-defined mathematical functions to remove noise, DL methods learn to denoise from example data, providing a powerful content-aware approach. In this review, we first describe the different types of noise that typically corrupt fluorescence microscopy images and introduce the denoising task. We then present the main DL-based denoising methods and their relative advantages and disadvantages. We aim to provide insights into how DL-based denoising methods operate and help users choose the most appropriate tools for their applications.},
	language = {en},
	urldate = {2022-11-09},
	journal = {The International Journal of Biochemistry \& Cell Biology},
	author = {Laine, Romain F. and Jacquemet, Guillaume and Krull, Alexander},
	month = nov,
	year = {2021},
	keywords = {Deep learning, Microscopy, Denoising, Live-cell imaging, Noise},
	pages = {106077},
	file = {ScienceDirect Full Text PDF:/home/zwerg/Zotero/storage/D5TWZRN8/Laine et al. - 2021 - Imaging in focus An introduction to denoising bio.pdf:application/pdf;ScienceDirect Snapshot:/home/zwerg/Zotero/storage/ZM3RTUQG/S1357272521001588.html:text/html},
}

@misc{dorkenwald_multi-layered_2022,
	title = {Multi-{Layered} {Maps} of {Neuropil} with {Segmentation}-{Guided} {Contrastive} {Learning}},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.03.29.486320v2},
	doi = {10.1101/2022.03.29.486320},
	abstract = {Maps of the nervous system that identify individual cells along with their type, subcellular components, and connectivity have the potential to reveal fundamental organizational principles of neural circuits. Volumetric nanometer-resolution imaging of brain tissue provides the raw data needed to build such maps, but inferring all the relevant cellular and subcellular annotation layers is challenging. Here, we present Segmentation-Guided Contrastive Learning of Representations (“SegCLR”), a self-supervised machine learning technique that produces highly informative representations of cells directly from 3d electron microscope imagery and segmentations. When applied to volumes of human and mouse cerebral cortex, SegCLR enabled the classification of cellular subcompartments (axon, dendrite, soma, astrocytic process) with 4,000-fold less labeled data compared to fully supervised approaches. Surprisingly, SegCLR also enabled inference of cell types (neurons, glia, and subtypes of each) from fragments with lengths as small as 10 micrometers, a task that can be difficult for humans to perform and whose feasibility greatly enhances the utility of imaging portions of brains in which many neuron fragments terminate at a volume boundary. These predictions were further augmented via Gaussian process uncertainty estimation to enable analyses restricted to high confidence subsets of the data. Finally, SegCLR enabled detailed exploration of layer-5 pyramidal cell subtypes and automated large-scale statistical analysis of upstream and downstream synaptic partners in mouse visual cortex.},
	language = {en},
	urldate = {2022-11-11},
	publisher = {bioRxiv},
	author = {Dorkenwald, Sven and Li, Peter H. and Januszewski, Michał and Berger, Daniel R. and Maitin-Shepard, Jeremy and Bodor, Agnes L. and Collman, Forrest and Schneider-Mizell, Casey M. and Costa, Nuno Maçarico da and Lichtman, Jeff W. and Jain, Viren},
	month = nov,
	year = {2022},
	note = {Pages: 2022.03.29.486320
Section: New Results},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/9KUQ9NLN/Dorkenwald et al. - 2022 - Multi-Layered Maps of Neuropil with Segmentation-G.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/33NIVEDY/2022.03.29.html:text/html},
}

@misc{noauthor_multi-layered_nodate,
	title = {Multi-layered {Mapping} of {Brain} {Tissue} via {Segmentation} {Guided} {Contrastive} {Learning}},
	url = {https://ai.googleblog.com/2022/11/multi-layered-mapping-of-brain-tissue.html},
	language = {en},
	urldate = {2022-11-11},
	file = {Snapshot:/home/zwerg/Zotero/storage/JJMGVBBG/multi-layered-mapping-of-brain-tissue.html:text/html},
}

@article{pennington_survos_2022,
	title = {{SuRVoS} 2: {Accelerating} {Annotation} and {Segmentation} for {Large} {Volumetric} {Bioimage} {Workflows} {Across} {Modalities} and {Scales}},
	volume = {10},
	issn = {2296-634X},
	shorttitle = {{SuRVoS} 2},
	url = {https://www.frontiersin.org/articles/10.3389/fcell.2022.842342},
	abstract = {As sample preparation and imaging techniques have expanded and improved to include a variety of options for larger sized and numbers of samples, the bottleneck in volumetric imaging is now data analysis. Annotation and segmentation are both common, yet difficult, data analysis tasks which are required to bring meaning to the volumetric data. The SuRVoS application has been updated and redesigned to provide access to both manual and machine learning-based segmentation and annotation techniques, including support for crowd sourced data. Combining adjacent, similar voxels (supervoxels) provides a mechanism for speeding up segmentation both in the painting of annotation and by training a segmentation model on a small amount of annotation. The support for layers allows multiple datasets to be viewed and annotated together which, for example, enables the use of correlative data (e.g. crowd-sourced annotations or secondary imaging techniques) to guide segmentation. The ability to work with larger data on high-performance servers with GPUs has been added through a client-server architecture and the Pytorch-based image processing and segmentation server is flexible and extensible, and allows the implementation of deep learning-based segmentation modules. The client side has been built around Napari allowing integration of SuRVoS into an ecosystem for open-source image analysis while the server side has been built with cloud computing and extensibility through plugins in mind. Together these improvements to SuRVoS provide a platform for accelerating the annotation and segmentation of volumetric and correlative imaging data across modalities and scales.},
	urldate = {2022-11-15},
	journal = {Frontiers in Cell and Developmental Biology},
	author = {Pennington, Avery and King, Oliver N. F. and Tun, Win Min and Ho, Elaine M. L. and Luengo, Imanol and Darrow, Michele C. and Basham, Mark},
	year = {2022},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/FJJHSQEN/Pennington et al. - 2022 - SuRVoS 2 Accelerating Annotation and Segmentation.pdf:application/pdf},
}

@misc{bavarian_efficient_2022,
	title = {Efficient {Training} of {Language} {Models} to {Fill} in the {Middle}},
	url = {http://arxiv.org/abs/2207.14255},
	doi = {10.48550/arXiv.2207.14255},
	abstract = {We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.},
	urldate = {2022-11-18},
	publisher = {arXiv},
	author = {Bavarian, Mohammad and Jun, Heewoo and Tezak, Nikolas and Schulman, John and McLeavey, Christine and Tworek, Jerry and Chen, Mark},
	month = jul,
	year = {2022},
	note = {arXiv:2207.14255 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/SWUGXQZH/Bavarian et al. - 2022 - Efficient Training of Language Models to Fill in t.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/IDNUSTGJ/2207.html:text/html},
}

@article{dominguez_conde_cross-tissue_2022,
	title = {Cross-tissue immune cell analysis reveals tissue-specific features in humans},
	volume = {376},
	issn = {0036-8075},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7612735/},
	doi = {10.1126/science.abl5197},
	abstract = {Despite their crucial role in health and disease, our knowledge of immune cells within human tissues remains limited. Here, we surveyed the immune compartment of 16 tissues from 12 adult donors by single-cell RNA sequencing and VDJ sequencing generating a dataset of 360,000 cells. To systematically resolve immune cell heterogeneity across tissues, we developed CellTypist, a machine learning tool for rapid and precise cell type annotation. Using this approach, combined with detailed curation, we determined the tissue distribution of finely phenotyped immune cell types, revealing hitherto unappreciated tissue-specific features and clonal architecture of T and B cells. Our multi-tissue approach lays the foundation for identifying highly resolved immune cell types by leveraging a common reference dataset, tissue-integrated expression analysis and antigen receptor sequencing.},
	number = {6594},
	urldate = {2022-11-25},
	journal = {Science (New York, N.Y.)},
	author = {Domínguez Conde, C and Xu, C and Jarvis, LB and Rainbow, DB and Wells, SB and Gomes, T and Howlett, SK and Suchanek, O and Polanski, K and King, HW and Mamanova, L and Huang, N and Szabo, PA and Richardson, L and Bolt, L and Fasouli, ES and Mahbubani, KT and Prete, M and Tuck, L and Richoz, N and Tuong, ZK and Campos, L and Mousa, HS and Needham, EJ and Pritchard, S and Li, T and Elmentaite, R and Park, J and Rahmani, E and Chen, D and Menon, DK and Bayraktar, OA and James, LK and Meyer, KB and Yosef, N and Clatworthy, MR and Sims, PA and Farber, DL and Saeb-Parsy, K and Jones, JL and Teichmann, SA},
	month = may,
	year = {2022},
	pmid = {35549406},
	pmcid = {PMC7612735},
	pages = {eabl5197},
	file = {PubMed Central Full Text PDF:/home/zwerg/Zotero/storage/4LESJZL3/Domínguez Conde et al. - 2022 - Cross-tissue immune cell analysis reveals tissue-s.pdf:application/pdf},
}

@misc{noauthor_glove_nodate,
	title = {{GloVe}: {Global} {Vectors} for {Word} {Representation}},
	url = {https://nlp.stanford.edu/projects/glove/},
	urldate = {2022-11-27},
	file = {GloVe\: Global Vectors for Word Representation:/home/zwerg/Zotero/storage/58JR87XG/glove.html:text/html},
}

@article{moen_deep_2019-1,
	title = {Deep learning for cellular image analysis},
	volume = {16},
	copyright = {2019 Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-019-0403-1},
	doi = {10.1038/s41592-019-0403-1},
	abstract = {Recent advances in computer vision and machine learning underpin a collection of algorithms with an impressive ability to decipher the content of images. These deep learning algorithms are being applied to biological images and are transforming the analysis and interpretation of imaging data. These advances are positioned to render difficult analyses routine and to enable researchers to carry out new, previously impossible experiments. Here we review the intersection between deep learning and cellular image analysis and provide an overview of both the mathematical mechanics and the programming frameworks of deep learning that are pertinent to life scientists. We survey the field’s progress in four key applications: image classification, image segmentation, object tracking, and augmented microscopy. Last, we relay our labs’ experience with three key aspects of implementing deep learning in the laboratory: annotating training data, selecting and training a range of neural network architectures, and deploying solutions. We also highlight existing datasets and implementations for each surveyed application.},
	language = {en},
	number = {12},
	urldate = {2022-11-28},
	journal = {Nature Methods},
	author = {Moen, Erick and Bannon, Dylan and Kudo, Takamasa and Graf, William and Covert, Markus and Van Valen, David},
	month = dec,
	year = {2019},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Image processing, Software},
	pages = {1233--1246},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/X2R2NP6C/Moen et al. - 2019 - Deep learning for cellular image analysis.pdf:application/pdf},
}

@misc{ko_coshnet_2022,
	title = {{CoShNet}: {A} {Hybrid} {Complex} {Valued} {Neural} {Network} using {Shearlets}},
	shorttitle = {{CoShNet}},
	url = {http://arxiv.org/abs/2208.06882},
	doi = {10.48550/arXiv.2208.06882},
	abstract = {In a hybrid neural network, the expensive convolutional layers are replaced by a non-trainable fixed transform with a great reduction in parameters. In previous works, good results were obtained by replacing the convolutions with wavelets. However, wavelet based hybrid network inherited wavelet's lack of vanishing moments along curves and its axis-bias. We propose to use Shearlets with its robust support for important image features like edges, ridges and blobs. The resulting network is called Complex Shearlets Network (CoShNet). It was tested on Fashion-MNIST against ResNet-50 and Resnet-18, obtaining 92.2\% versus 90.7\% and 91.8\% respectively. The proposed network has 49.9k parameters versus ResNet-18 with 11.18m and use 52 times fewer FLOPs. Finally, we trained in under 20 epochs versus 200 epochs required by ResNet and do not need any hyperparameter tuning nor regularization. Code: https://github.com/Ujjawal-K-Panchal/coshnet},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Ko, Manny and Panchal, Ujjawal K. and Andrade-Loarca, Héctor and Mendez-Vazquez, Andres},
	month = oct,
	year = {2022},
	note = {arXiv:2208.06882 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 16 pages, 11 figures},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/BNSUZPN6/Ko et al. - 2022 - CoShNet A Hybrid Complex Valued Neural Network us.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/AG7BZSX5/2208.html:text/html},
}

@misc{chandra_gradient_2022,
	title = {Gradient {Descent}: {The} {Ultimate} {Optimizer}},
	shorttitle = {Gradient {Descent}},
	url = {http://arxiv.org/abs/1909.13371},
	doi = {10.48550/arXiv.1909.13371},
	abstract = {Working with any gradient-based machine learning algorithm involves the tedious task of tuning the optimizer's hyperparameters, such as its step size. Recent work has shown how the step size can itself be optimized alongside the model parameters by manually deriving expressions for "hypergradients" ahead of time. We show how to automatically compute hypergradients with a simple and elegant modification to backpropagation. This allows us to easily apply the method to other optimizers and hyperparameters (e.g. momentum coefficients). We can even recursively apply the method to its own hyper-hyperparameters, and so on ad infinitum. As these towers of optimizers grow taller, they become less sensitive to the initial choice of hyperparameters. We present experiments validating this for MLPs, CNNs, and RNNs. Finally, we provide a simple PyTorch implementation of this algorithm (see people.csail.mit.edu/kach/gradient-descent-the-ultimate-optimizer).},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Chandra, Kartik and Xie, Audrey and Ragan-Kelley, Jonathan and Meijer, Erik},
	month = oct,
	year = {2022},
	note = {arXiv:1909.13371 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/F9SLBZ9K/Chandra et al. - 2022 - Gradient Descent The Ultimate Optimizer.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/IF64PJJG/1909.html:text/html},
}

@misc{lugmayr_srflow_2020,
	title = {{SRFlow}: {Learning} the {Super}-{Resolution} {Space} with {Normalizing} {Flow}},
	shorttitle = {{SRFlow}},
	url = {http://arxiv.org/abs/2006.14200},
	abstract = {Super-resolution is an ill-posed problem, since it allows for multiple predictions for a given low-resolution image. This fundamental fact is largely ignored by state-of-the-art deep learning based approaches. These methods instead train a deterministic mapping using combinations of reconstruction and adversarial losses. In this work, we therefore propose SRFlow: a normalizing flow based super-resolution method capable of learning the conditional distribution of the output given the low-resolution input. Our model is trained in a principled manner using a single loss, namely the negative log-likelihood. SRFlow therefore directly accounts for the ill-posed nature of the problem, and learns to predict diverse photo-realistic high-resolution images. Moreover, we utilize the strong image posterior learned by SRFlow to design flexible image manipulation techniques, capable of enhancing super-resolved images by, e.g., transferring content from other images. We perform extensive experiments on faces, as well as on super-resolution in general. SRFlow outperforms state-of-the-art GAN-based approaches in terms of both PSNR and perceptual quality metrics, while allowing for diversity through the exploration of the space of super-resolved solutions.},
	language = {en},
	urldate = {2022-12-02},
	publisher = {arXiv},
	author = {Lugmayr, Andreas and Danelljan, Martin and Van Gool, Luc and Timofte, Radu},
	month = jul,
	year = {2020},
	note = {arXiv:2006.14200 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: ECCV 2020 Spotlight {\textbar} git.io/SRFlow},
	file = {Lugmayr et al. - 2020 - SRFlow Learning the Super-Resolution Space with N.pdf:/home/zwerg/Zotero/storage/YEYFKQPR/Lugmayr et al. - 2020 - SRFlow Learning the Super-Resolution Space with N.pdf:application/pdf},
}

@article{kingma_glow_nodate,
	title = {Glow: {Generative} {Flow} with {Invertible} 1×1 {Convolutions}},
	abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative ﬂow using an invertible 1 × 1 convolution. Using our method we demonstrate a signiﬁcant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efﬁcient realisticlooking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow.},
	language = {en},
	author = {Kingma, Diederik P and Dhariwal, Prafulla},
	file = {Kingma and Dhariwal - Glow Generative Flow with Invertible 1×1 Convolut.pdf:/home/zwerg/Zotero/storage/JPL8C9YE/Kingma and Dhariwal - Glow Generative Flow with Invertible 1×1 Convolut.pdf:application/pdf},
}

@article{hinton_forward-forward_nodate,
	title = {The {Forward}-{Forward} {Algorithm}: {Some} {Preliminary} {Investigations}},
	abstract = {The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth serious investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes can be separated in time, the negative passes can be done ofﬂine, which makes the learning much simpler in the positive pass and allows video to be pipelined through the network without ever storing activities or stopping to propagate derivatives.},
	language = {en},
	author = {Hinton, Geoffrey},
	file = {Hinton - The Forward-Forward Algorithm Some Preliminary In.pdf:/home/zwerg/Zotero/storage/NREJSNGC/Hinton - The Forward-Forward Algorithm Some Preliminary In.pdf:application/pdf},
}

@misc{gebotys_going_2019,
	title = {Going with the {Flow}: {An} {Introduction} to {Normalizing} {Flows}},
	shorttitle = {Going with the {Flow}},
	url = {https://gebob19.github.io/normalizing-flows/},
	abstract = {This blog post/tutorial dives deep into the theory and PyTorch code for Normalizing Flows},
	urldate = {2022-12-20},
	journal = {Brennan Gebotys},
	author = {Gebotys, Brennan},
	month = jul,
	year = {2019},
	file = {Snapshot:/home/zwerg/Zotero/storage/6YRZAV6Y/normalizing-flows.html:text/html},
}

@misc{noauthor_google_nodate-1,
	title = {Google {Colaboratory}},
	url = {https://colab.research.google.com/github/papercup-open-source/tutorials/blob/master/intro_nf/nf_tutorial_torch.ipynb},
	language = {en},
	urldate = {2022-12-20},
}

@misc{noauthor_tutorial_2020,
	title = {Tutorial on normalizing flows, part 2},
	url = {https://papercup.dev/posts/normalizing-flows-part-2/},
	abstract = {In this blog post, I am going to teach you how you can make super-duper cool GIFs. Isn’t that really the point of life anyway? I will assume…},
	language = {en},
	urldate = {2022-12-20},
	journal = {Papercup Engineering Blog},
	month = may,
	year = {2020},
	file = {Snapshot:/home/zwerg/Zotero/storage/NNIXZJ2U/normalizing-flows-part-2.html:text/html},
}

@misc{noauthor_normalizing_nodate,
	title = {Normalizing {Flows}},
	url = {http://akosiorek.github.io/ml/2018/04/03/norm_flows.html},
	urldate = {2022-12-20},
	file = {Normalizing Flows:/home/zwerg/Zotero/storage/CJGM8JHM/norm_flows.html:text/html},
}

@misc{noauthor_tutorial_2020-1,
	title = {Tutorial on normalizing flows, part 1},
	url = {https://papercup.dev/posts/normalizing-flows-part-1/},
	abstract = {Before we start, I would like to mention that this blog post assumes a familiarity with generative models and modern deep learning…},
	language = {en},
	urldate = {2022-12-20},
	journal = {Papercup Engineering Blog},
	month = jan,
	year = {2020},
	file = {Snapshot:/home/zwerg/Zotero/storage/WZMZQ99P/normalizing-flows-part-1.html:text/html},
}

@misc{prasad_normalizing_2021,
	title = {Normalizing {Flows}},
	url = {https://grishmaprs.medium.com/normalizing-flows-5b5a713e45e2},
	abstract = {I have been learning about Normalizing flows since last few days. It is one of those famous Generative models in Machine Learning and Deep…},
	language = {en},
	urldate = {2022-12-20},
	journal = {Medium},
	author = {Prasad, Grishma},
	month = jul,
	year = {2021},
	file = {Snapshot:/home/zwerg/Zotero/storage/22RKHRS7/normalizing-flows-5b5a713e45e2.html:text/html},
}

@misc{noauthor_protein_2022,
	title = {Protein targeting},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Protein_targeting&oldid=1115199152},
	abstract = {This article deals with protein targeting in eukaryotes unless specified otherwise.Protein targeting or protein sorting is the biological mechanism by which proteins are transported to their appropriate destinations within or outside the cell. Proteins can be targeted to the inner space of an organelle, different intracellular membranes, the plasma membrane, or to the exterior of the cell via secretion. Information contained in the protein itself directs this delivery process. Correct sorting is crucial for the cell; errors or dysfunction in sorting have been linked to multiple diseases.},
	language = {en},
	urldate = {2022-12-19},
	journal = {Wikipedia},
	month = oct,
	year = {2022},
	note = {Page Version ID: 1115199152},
	file = {Snapshot:/home/zwerg/Zotero/storage/EDI7TM7B/Protein_targeting.html:text/html},
}

@misc{rezende_variational_2016,
	title = {Variational {Inference} with {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/1505.05770},
	doi = {10.48550/arXiv.1505.05770},
	abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
	urldate = {2022-12-16},
	publisher = {arXiv},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	month = jun,
	year = {2016},
	note = {arXiv:1505.05770 [cs, stat]
version: 5},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Methodology, Statistics - Computation},
	annote = {Comment: Proceedings of the 32nd International Conference on Machine Learning},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/K428U86V/Rezende and Mohamed - 2016 - Variational Inference with Normalizing Flows.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/6JHZEBVP/1505.html:text/html},
}

@article{wang_controllable_2022,
	title = {Controllable {Data} {Generation} by {Deep} {Learning}: {A} {Review}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Controllable {Data} {Generation} by {Deep} {Learning}},
	url = {https://arxiv.org/abs/2207.09542},
	doi = {10.48550/ARXIV.2207.09542},
	abstract = {Designing and generating new data under targeted properties has been attracting various critical applications such as molecule design, image editing and speech synthesis. Traditional hand-crafted approaches heavily rely on expertise experience and intensive human efforts, yet still suffer from the insufficiency of scientific knowledge and low throughput to support effective and efficient data generation. Recently, the advancement of deep learning induces expressive methods that can learn the underlying representation and properties of data. Such capability provides new opportunities in figuring out the mutual relationship between the structural patterns and functional properties of the data and leveraging such relationship to generate structural data given the desired properties. This article provides a systematic review of this promising research area, commonly known as controllable deep data generation. Firstly, the potential challenges are raised and preliminaries are provided. Then the controllable deep data generation is formally defined, a taxonomy on various techniques is proposed and the evaluation metrics in this specific domain are summarized. After that, exciting applications of controllable deep data generation are introduced and existing works are experimentally analyzed and compared. Finally, the promising future directions of controllable deep data generation are highlighted and five potential challenges are identified.},
	urldate = {2022-12-14},
	author = {Wang, Shiyu and Du, Yuanqi and Guo, Xiaojie and Pan, Bo and Qin, Zhaohui and Zhao, Liang},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 5},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Artificial Intelligence (cs.AI)},
	annote = {[TLDR] A systematic review that explains this promising research area, commonly known as controllable deep data generation, and formally defines and proposes a taxonomy on various techniques and summarizes the evaluation metrics in this specific domain.},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/MLKFKTWD/Wang et al. - 2022 - Controllable Data Generation by Deep Learning A R.pdf:application/pdf},
}

@misc{ardizzone_guided_2019,
	title = {Guided {Image} {Generation} with {Conditional} {Invertible} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1907.02392},
	doi = {10.48550/arXiv.1907.02392},
	abstract = {In this work, we address the task of natural image generation guided by a conditioning input. We introduce a new architecture called conditional invertible neural network (cINN). The cINN combines the purely generative INN model with an unconstrained feed-forward network, which efficiently preprocesses the conditioning input into useful features. All parameters of the cINN are jointly optimized with a stable, maximum likelihood-based training procedure. By construction, the cINN does not experience mode collapse and generates diverse samples, in contrast to e.g. cGANs. At the same time our model produces sharp images since no reconstruction loss is required, in contrast to e.g. VAEs. We demonstrate these properties for the tasks of MNIST digit generation and image colorization. Furthermore, we take advantage of our bi-directional cINN architecture to explore and manipulate emergent properties of the latent space, such as changing the image style in an intuitive way.},
	urldate = {2022-12-14},
	publisher = {arXiv},
	author = {Ardizzone, Lynton and Lüth, Carsten and Kruse, Jakob and Rother, Carsten and Köthe, Ullrich},
	month = jul,
	year = {2019},
	note = {arXiv:1907.02392 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, 68T01},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/CUGX3LJM/Ardizzone et al. - 2019 - Guided Image Generation with Conditional Invertibl.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/ST5XARUV/1907.html:text/html},
}

@article{verma_modular_2022,
	title = {Modular {Flows}: {Differential} {Molecular} {Generation}},
	shorttitle = {Modular {Flows}},
	url = {https://www.semanticscholar.org/reader/6caf97d6134fd4e78d9ea90d5bdc971654ab18b8},
	doi = {10.48550/arXiv.2210.06032},
	abstract = {An academic search engine that utilizes artificial intelligence methods to provide highly relevant results and novel tools to filter them with ease.},
	language = {en},
	urldate = {2022-12-14},
	journal = {ArXiv},
	author = {Verma, Yogesh and Kaski, S. and Heinonen, Markus and Garg, Vikas K.},
	year = {2022},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/DPVV7HCB/Verma et al. - 2022 - Modular Flows Differential Molecular Generation.pdf:application/pdf},
}

@misc{zhen_flow-based_2021,
	title = {Flow-based {Generative} {Models} for {Learning} {Manifold} to {Manifold} {Mappings}},
	url = {http://arxiv.org/abs/2012.10013},
	doi = {10.48550/arXiv.2012.10013},
	abstract = {Many measurements or observations in computer vision and machine learning manifest as non-Euclidean data. While recent proposals (like spherical CNN) have extended a number of deep neural network architectures to manifold-valued data, and this has often provided strong improvements in performance, the literature on generative models for manifold data is quite sparse. Partly due to this gap, there are also no modality transfer/translation models for manifold-valued data whereas numerous such methods based on generative models are available for natural images. This paper addresses this gap, motivated by a need in brain imaging -- in doing so, we expand the operating range of certain generative models (as well as generative models for modality transfer) from natural images to images with manifold-valued measurements. Our main result is the design of a two-stream version of GLOW (flow-based invertible generative models) that can synthesize information of a field of one type of manifold-valued measurements given another. On the theoretical side, we introduce three kinds of invertible layers for manifold-valued data, which are not only analogous to their functionality in flow-based generative models (e.g., GLOW) but also preserve the key benefits (determinants of the Jacobian are easy to calculate). For experiments, on a large dataset from the Human Connectome Project (HCP), we show promising results where we can reliably and accurately reconstruct brain images of a field of orientation distribution functions (ODF) from diffusion tensor images (DTI), where the latter has a \$5{\textbackslash}times\$ faster acquisition time but at the expense of worse angular resolution.},
	urldate = {2022-12-14},
	publisher = {arXiv},
	author = {Zhen, Xingjian and Chakraborty, Rudrasis and Yang, Liu and Singh, Vikas},
	month = mar,
	year = {2021},
	note = {arXiv:2012.10013 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: This paper has been accepted by AAAI 2021. A video introduction is on YouTube: https://youtu.be/0r96U0vXsCM The official GitHub repo is: https://github.com/zhenxingjian/Dual\_Manifold\_GLOW},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/9P3UMLB3/Zhen et al. - 2021 - Flow-based Generative Models for Learning Manifold.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/SI2QJETD/2012.html:text/html},
}

@misc{xiao_method_2019,
	title = {A {Method} to {Model} {Conditional} {Distributions} with {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/1911.02052},
	abstract = {In this work, we investigate the use of normalizing flows to model conditional distributions. In particular, we use our proposed method to analyze inverse problems with invertible neural networks by maximizing the posterior likelihood. Our method uses only a single loss and is easy to train. This is an improvement on the previous method that solves similar inverse problems with invertible neural networks but which involves a combination of several loss terms with ad-hoc weighting. In addition, our method provides a natural framework to incorporate conditioning in normalizing flows, and therefore, we can train an invertible network to perform conditional generation. We analyze our method and perform a careful comparison with previous approaches. Simple experiments show the effectiveness of our method, and more comprehensive experimental evaluations are undergoing.},
	urldate = {2022-12-14},
	publisher = {arXiv},
	author = {Xiao, Zhisheng and Yan, Qing and Amit, Yali},
	month = nov,
	year = {2019},
	note = {arXiv:1911.02052 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 10 pages. Work in progress},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/JS9JX6PN/Xiao et al. - 2019 - A Method to Model Conditional Distributions with N.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/66Z3HI6K/1911.html:text/html},
}

@misc{dinh_density_2017,
	title = {Density estimation using {Real} {NVP}},
	url = {http://arxiv.org/abs/1605.08803},
	doi = {10.48550/arXiv.1605.08803},
	abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
	urldate = {2022-12-14},
	publisher = {arXiv},
	author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
	month = feb,
	year = {2017},
	note = {arXiv:1605.08803 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 10 pages of main content, 3 pages of bibliography, 18 pages of appendix. Accepted at ICLR 2017},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/MXKQEHSI/Dinh et al. - 2017 - Density estimation using Real NVP.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/2S76ZBLA/1605.html:text/html},
}

@misc{dinh_nice_2015,
	title = {{NICE}: {Non}-linear {Independent} {Components} {Estimation}},
	shorttitle = {{NICE}},
	url = {http://arxiv.org/abs/1410.8516},
	doi = {10.48550/arXiv.1410.8516},
	abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
	urldate = {2022-12-14},
	publisher = {arXiv},
	author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
	month = apr,
	year = {2015},
	note = {arXiv:1410.8516 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 11 pages and 2 pages Appendix, workshop paper at ICLR 2015},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/9SVSAED4/Dinh et al. - 2015 - NICE Non-linear Independent Components Estimation.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/Q96D6Y55/1410.html:text/html},
}

@misc{bassey_survey_2021,
	title = {A {Survey} of {Complex}-{Valued} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2101.12249},
	doi = {10.48550/arXiv.2101.12249},
	abstract = {Artificial neural networks (ANNs) based machine learning models and especially deep learning models have been widely applied in computer vision, signal processing, wireless communications, and many other domains, where complex numbers occur either naturally or by design. However, most of the current implementations of ANNs and machine learning frameworks are using real numbers rather than complex numbers. There are growing interests in building ANNs using complex numbers, and exploring the potential advantages of the so-called complex-valued neural networks (CVNNs) over their real-valued counterparts. In this paper, we discuss the recent development of CVNNs by performing a survey of the works on CVNNs in the literature. Specifically, a detailed review of various CVNNs in terms of activation function, learning and optimization, input and output representations, and their applications in tasks such as signal processing and computer vision are provided, followed by a discussion on some pertinent challenges and future research directions.},
	urldate = {2022-12-29},
	publisher = {arXiv},
	author = {Bassey, Joshua and Qian, Lijun and Li, Xianfang},
	month = jan,
	year = {2021},
	note = {arXiv:2101.12249 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 15 pages},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/CTB2KRZR/Bassey et al. - 2021 - A Survey of Complex-Valued Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/XNLRD52X/2101.html:text/html},
}

@misc{barrachina_complex-valued_2021,
	title = {Complex-{Valued} vs. {Real}-{Valued} {Neural} {Networks} for {Classification} {Perspectives}: {An} {Example} on {Non}-{Circular} {Data}},
	shorttitle = {Complex-{Valued} vs. {Real}-{Valued} {Neural} {Networks} for {Classification} {Perspectives}},
	url = {http://arxiv.org/abs/2009.08340},
	doi = {10.48550/arXiv.2009.08340},
	abstract = {The contributions of this paper are twofold. First, we show the potential interest of Complex-Valued Neural Network (CVNN) on classification tasks for complex-valued datasets. To highlight this assertion, we investigate an example of complex-valued data in which the real and imaginary parts are statistically dependent through the property of non-circularity. In this context, the performance of fully connected feed-forward CVNNs is compared against a real-valued equivalent model. The results show that CVNN performs better for a wide variety of architectures and data structures. CVNN accuracy presents a statistically higher mean and median and lower variance than Real-Valued Neural Network (RVNN). Furthermore, if no regularization technique is used, CVNN exhibits lower overfitting. The second contribution is the release of a Python library (Barrachina 2019) using Tensorflow as back-end that enables the implementation and training of CVNNs in the hopes of motivating further research on this area.},
	urldate = {2022-12-29},
	publisher = {arXiv},
	author = {Barrachina, Jose Agustin and Ren, Chenfang and Morisseau, Christele and Vieillard, Gilles and Ovarlez, Jean-Philippe},
	month = apr,
	year = {2021},
	note = {arXiv:2009.08340 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 6 pages, 5 figures, conference, preprint},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/3JHA95R5/Barrachina et al. - 2021 - Complex-Valued vs. Real-Valued Neural Networks for.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/QIY3HSN2/2009.html:text/html},
}

@misc{stewart_torchgeo_2022,
	title = {{TorchGeo}: {Deep} {Learning} {With} {Geospatial} {Data}},
	shorttitle = {{TorchGeo}},
	url = {http://arxiv.org/abs/2111.08872},
	abstract = {Remotely sensed geospatial data are critical for applications including precision agriculture, urban planning, disaster monitoring and response, and climate change research, among others. Deep learning methods are particularly promising for modeling many remote sensing tasks given the success of deep neural networks in similar computer vision tasks and the sheer volume of remotely sensed imagery available. However, the variance in data collection methods and handling of geospatial metadata make the application of deep learning methodology to remotely sensed data nontrivial. For example, satellite imagery often includes additional spectral bands beyond red, green, and blue and must be joined to other geospatial data sources that can have differing coordinate systems, bounds, and resolutions. To help realize the potential of deep learning for remote sensing applications, we introduce TorchGeo, a Python library for integrating geospatial data into the PyTorch deep learning ecosystem. TorchGeo provides data loaders for a variety of benchmark datasets, composable datasets for generic geospatial data sources, samplers for geospatial data, and transforms that work with multispectral imagery. TorchGeo is also the first library to provide pre-trained models for multispectral satellite imagery (e.g., models that use all bands from the Sentinel-2 satellites), allowing for advances in transfer learning on downstream remote sensing tasks with limited labeled data. We use TorchGeo to create reproducible benchmark results on existing datasets and benchmark our proposed method for preprocessing geospatial imagery on the fly. TorchGeo is open source and available on GitHub: https://github.com/microsoft/torchgeo.},
	language = {en},
	urldate = {2022-12-30},
	publisher = {arXiv},
	author = {Stewart, Adam J. and Robinson, Caleb and Corley, Isaac A. and Ortiz, Anthony and Ferres, Juan M. Lavista and Banerjee, Arindam},
	month = sep,
	year = {2022},
	note = {arXiv:2111.08872 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Stewart et al. - 2022 - TorchGeo Deep Learning With Geospatial Data.pdf:/home/zwerg/Zotero/storage/HQ864TDT/Stewart et al. - 2022 - TorchGeo Deep Learning With Geospatial Data.pdf:application/pdf},
}

@article{sheridan_local_2022,
	title = {Local shape descriptors for neuron segmentation},
	copyright = {2022 The Author(s)},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-022-01711-z},
	doi = {10.1038/s41592-022-01711-z},
	abstract = {We present an auxiliary learning task for the problem of neuron segmentation in electron microscopy volumes. The auxiliary task consists of the prediction of local shape descriptors (LSDs), which we combine with conventional voxel-wise direct neighbor affinities for neuron boundary detection. The shape descriptors capture local statistics about the neuron to be segmented, such as diameter, elongation, and direction. On a study comparing several existing methods across various specimen, imaging techniques, and resolutions, auxiliary learning of LSDs consistently increases segmentation accuracy of affinity-based methods over a range of metrics. Furthermore, the addition of LSDs promotes affinity-based segmentation methods to be on par with the current state of the art for neuron segmentation (flood-filling networks), while being two orders of magnitudes more efficient—a critical requirement for the processing of future petabyte-sized datasets.},
	language = {en},
	urldate = {2023-01-04},
	journal = {Nature Methods},
	author = {Sheridan, Arlo and Nguyen, Tri M. and Deb, Diptodip and Lee, Wei-Chung Allen and Saalfeld, Stephan and Turaga, Srinivas C. and Manor, Uri and Funke, Jan},
	month = dec,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning, Computational neuroscience},
	pages = {1--9},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/PUZQWXFW/Sheridan et al. - 2022 - Local shape descriptors for neuron segmentation.pdf:application/pdf},
}

@misc{azizi_big_2021,
	title = {Big {Self}-{Supervised} {Models} {Advance} {Medical} {Image} {Classification}},
	url = {http://arxiv.org/abs/2101.05224},
	abstract = {Self-supervised pretraining followed by supervised fine-tuning has seen success in image recognition, especially when labeled examples are scarce, but has received limited attention in medical image analysis. This paper studies the effectiveness of self-supervised learning as a pretraining strategy for medical image classification. We conduct experiments on two distinct tasks: dermatology skin condition classification from digital camera images and multi-label chest X-ray classification, and demonstrate that self-supervised learning on ImageNet, followed by additional self-supervised learning on unlabeled domain-specific medical images significantly improves the accuracy of medical image classifiers. We introduce a novel Multi-Instance Contrastive Learning (MICLe) method that uses multiple images of the underlying pathology per patient case, when available, to construct more informative positive pairs for self-supervised learning. Combining our contributions, we achieve an improvement of 6.7\% in top-1 accuracy and an improvement of 1.1\% in mean AUC on dermatology and chest X-ray classification respectively, outperforming strong supervised baselines pretrained on ImageNet. In addition, we show that big self-supervised models are robust to distribution shift and can learn efficiently with a small number of labeled medical images.},
	urldate = {2023-01-04},
	publisher = {arXiv},
	author = {Azizi, Shekoofeh and Mustafa, Basil and Ryan, Fiona and Beaver, Zachary and Freyberg, Jan and Deaton, Jonathan and Loh, Aaron and Karthikesalingam, Alan and Kornblith, Simon and Chen, Ting and Natarajan, Vivek and Norouzi, Mohammad},
	month = apr,
	year = {2021},
	note = {arXiv:2101.05224 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/WH7HIVS7/Azizi et al. - 2021 - Big Self-Supervised Models Advance Medical Image C.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/QV3VWFS4/2101.html:text/html},
}

@misc{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	doi = {10.48550/arXiv.2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2023-01-04},
	publisher = {arXiv},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv:2002.05709 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICML'2020. Code and pretrained models at https://github.com/google-research/simclr},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/LFAHL9VZ/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/DVX8W2JE/2002.html:text/html},
}

@misc{khosla_supervised_2021,
	title = {Supervised {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2004.11362},
	doi = {10.48550/arXiv.2004.11362},
	abstract = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the ImageNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement, and reference TensorFlow code is released at https://t.ly/supcon.},
	urldate = {2023-01-04},
	publisher = {arXiv},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	month = mar,
	year = {2021},
	note = {arXiv:2004.11362 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/3U9K3GDC/Khosla et al. - 2021 - Supervised Contrastive Learning.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/AAGXQBKI/2004.html:text/html},
}

@misc{noauthor_extending_nodate,
	title = {Extending {Contrastive} {Learning} to the {Supervised} {Setting}},
	url = {https://ai.googleblog.com/2021/06/extending-contrastive-learning-to.html},
	language = {en},
	urldate = {2023-01-04},
}

@article{wang_deep_2021-1,
	title = {Deep {Face} {Recognition}: {A} {Survey}},
	volume = {429},
	issn = {09252312},
	shorttitle = {Deep {Face} {Recognition}},
	url = {http://arxiv.org/abs/1804.06655},
	doi = {10.1016/j.neucom.2020.10.081},
	abstract = {Deep learning applies multiple processing layers to learn representations of data with multiple levels of feature extraction. This emerging technique has reshaped the research landscape of face recognition (FR) since 2014, launched by the breakthroughs of DeepFace and DeepID. Since then, deep learning technique, characterized by the hierarchical architecture to stitch together pixels into invariant face representation, has dramatically improved the state-of-the-art performance and fostered successful real-world applications. In this survey, we provide a comprehensive review of the recent developments on deep FR, covering broad topics on algorithm designs, databases, protocols, and application scenes. First, we summarize different network architectures and loss functions proposed in the rapid evolution of the deep FR methods. Second, the related face processing methods are categorized into two classes: "one-to-many augmentation" and "many-to-one normalization". Then, we summarize and compare the commonly used databases for both model training and evaluation. Third, we review miscellaneous scenes in deep FR, such as cross-factor, heterogenous, multiple-media and industrial scenes. Finally, the technical challenges and several promising directions are highlighted.},
	urldate = {2023-01-04},
	journal = {Neurocomputing},
	author = {Wang, Mei and Deng, Weihong},
	month = mar,
	year = {2021},
	note = {arXiv:1804.06655 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {215--244},
	annote = {Comment: Neurocomputing},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/EZPVS5QQ/Wang and Deng - 2021 - Deep Face Recognition A Survey.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/UI2RCXWT/1804.html:text/html},
}

@article{deng_arcface_2021,
	title = {{ArcFace}: {Additive} {Angular} {Margin} {Loss} for {Deep} {Face} {Recognition}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {{ArcFace}},
	url = {http://arxiv.org/abs/1801.07698},
	doi = {10.1109/TPAMI.2021.3087709},
	abstract = {Recently, a popular line of research in face recognition is adopting margins in the well-established softmax loss function to maximize class separability. In this paper, we first introduce an Additive Angular Margin Loss (ArcFace), which not only has a clear geometric interpretation but also significantly enhances the discriminative power. Since ArcFace is susceptible to the massive label noise, we further propose sub-center ArcFace, in which each class contains \$K\$ sub-centers and training samples only need to be close to any of the \$K\$ positive sub-centers. Sub-center ArcFace encourages one dominant sub-class that contains the majority of clean faces and non-dominant sub-classes that include hard or noisy faces. Based on this self-propelled isolation, we boost the performance through automatically purifying raw web faces under massive real-world noise. Besides discriminative feature embedding, we also explore the inverse problem, mapping feature vectors to face images. Without training any additional generator or discriminator, the pre-trained ArcFace model can generate identity-preserved face images for both subjects inside and outside the training data only by using the network gradient and Batch Normalization (BN) priors. Extensive experiments demonstrate that ArcFace can enhance the discriminative feature embedding as well as strengthen the generative face synthesis.},
	urldate = {2023-01-04},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Deng, Jiankang and Guo, Jia and Yang, Jing and Xue, Niannan and Kotsia, Irene and Zafeiriou, Stefanos},
	year = {2021},
	note = {arXiv:1801.07698 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1--1},
	annote = {Comment: ArcFace TPAMI version},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/VQAD7DZU/Deng et al. - 2021 - ArcFace Additive Angular Margin Loss for Deep Fac.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/5WFSLG5B/1801.html:text/html},
}

@misc{andrews_year_2021,
	title = {The {Year} in {Math} and {Computer} {Science}},
	url = {https://www.quantamagazine.org/the-year-in-math-and-computer-science-20211223/},
	abstract = {Mathematicians and computer scientists answered major questions in topology, set theory and even physics, even as computers continued to grow more capable.},
	language = {en},
	urldate = {2023-01-09},
	journal = {Quanta Magazine},
	author = {Andrews, Bill},
	month = dec,
	year = {2021},
}

@article{qiao_informing_2022,
	title = {Informing {Geometric} {Deep} {Learning} with {Electronic} {Interactions} to {Accelerate} {Quantum} {Chemistry}},
	volume = {119},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/2105.14655},
	doi = {10.1073/pnas.2205221119},
	abstract = {Predicting electronic energies, densities, and related chemical properties can facilitate the discovery of novel catalysts, medicines, and battery materials. By developing a physics-inspired equivariant neural network, we introduce a method to learn molecular representations based on the electronic interactions among atomic orbitals. Our method, OrbNet-Equi, leverages efficient tight-binding simulations and learned mappings to recover high fidelity quantum chemical properties. OrbNet-Equi models a wide spectrum of target properties with an accuracy consistently better than standard machine learning methods and a speed orders of magnitude greater than density functional theory. Despite only using training samples collected from readily available small-molecule libraries, OrbNet-Equi outperforms traditional methods on comprehensive downstream benchmarks that encompass diverse main-group chemical processes. Our method also describes interactions in challenging charge-transfer complexes and open-shell systems. We anticipate that the strategy presented here will help to expand opportunities for studies in chemistry and materials science, where the acquisition of experimental or reference training data is costly.},
	number = {31},
	urldate = {2023-01-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Qiao, Zhuoran and Christensen, Anders S. and Welborn, Matthew and Manby, Frederick R. and Anandkumar, Anima and Miller III, Thomas F.},
	month = aug,
	year = {2022},
	note = {arXiv:2105.14655 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Chemical Physics},
	pages = {e2205221119},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/HNBB2KQX/Qiao et al. - 2022 - Informing Geometric Deep Learning with Electronic .pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/Z99U4VJ9/2105.html:text/html},
}

@misc{qiao_dynamic-backbone_2022,
	title = {Dynamic-{Backbone} {Protein}-{Ligand} {Structure} {Prediction} with {Multiscale} {Generative} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2209.15171},
	doi = {10.48550/arXiv.2209.15171},
	abstract = {Molecular complexes formed by proteins and small-molecule ligands are ubiquitous, and predicting their 3D structures can facilitate both biological discoveries and the design of novel enzymes or drug molecules. Here we propose NeuralPLexer, a deep generative model framework to rapidly predict protein-ligand complex structures and their fluctuations using protein backbone template and molecular graph inputs. NeuralPLexer jointly samples protein and small-molecule 3D coordinates at an atomistic resolution through a generative model that incorporates biophysical constraints and inferred proximity information into a time-truncated diffusion process. The reverse-time generative diffusion process is learned by a novel stereochemistry-aware equivariant graph transformer that enables efficient, concurrent gradient field prediction for all heavy atoms in the protein-ligand complex. NeuralPLexer outperforms existing physics-based and learning-based methods on benchmarking problems including fixed-backbone blind protein-ligand docking and ligand-coupled binding site repacking. Moreover, we identify preliminary evidence that NeuralPLexer enriches bound-state-like protein structures when applied to systems where protein folding landscapes are significantly altered by the presence of ligands. Our results reveal that a data-driven approach can capture the structural cooperativity among protein and small-molecule entities, showing promise for the computational identification of novel drug targets and the end-to-end differentiable design of functional small-molecules and ligand-binding proteins.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Qiao, Zhuoran and Nie, Weili and Vahdat, Arash and Miller III, Thomas F. and Anandkumar, Anima},
	month = sep,
	year = {2022},
	note = {arXiv:2209.15171 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules, Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/KKJCQ7YY/Qiao et al. - 2022 - Dynamic-Backbone Protein-Ligand Structure Predicti.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/KA8WGV6U/2209.html:text/html},
}

@misc{li_fourier_2021,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2010.08895},
	doi = {10.48550/arXiv.2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	month = may,
	year = {2021},
	note = {arXiv:2010.08895 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/L8FQEBIY/Li et al. - 2021 - Fourier Neural Operator for Parametric Partial Dif.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/HC9SFBV2/2010.html:text/html},
}

@misc{noauthor_minedojo_nodate,
	title = {{MineDojo} {\textbar} {Building} {Open}-{Ended} {Embodied} {Agents} with {Internet}-{Scale} {Knowledge}},
	url = {https://minedojo.org/},
	urldate = {2023-01-09},
	file = {MineDojo | Building Open-Ended Embodied Agents with Internet-Scale Knowledge:/home/zwerg/Zotero/storage/SIM3KK82/minedojo.org.html:text/html},
}

@misc{noauthor_vima_nodate,
	title = {{VIMA} {\textbar} {General} {Robot} {Manipulation} with {Multimodal} {Prompts}},
	url = {https://vimalabs.github.io/},
	urldate = {2023-01-09},
}

@misc{zhou_understanding_2022,
	title = {Understanding {The} {Robustness} in {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2204.12451},
	doi = {10.48550/arXiv.2204.12451},
	abstract = {Recent studies show that Vision Transformers(ViTs) exhibit strong robustness against various corruptions. Although this property is partly attributed to the self-attention mechanism, there is still a lack of systematic understanding. In this paper, we examine the role of self-attention in learning robust representations. Our study is motivated by the intriguing properties of the emerging visual grouping in Vision Transformers, which indicates that self-attention may promote robustness through improved mid-level representations. We further propose a family of fully attentional networks (FANs) that strengthen this capability by incorporating an attentional channel processing design. We validate the design comprehensively on various hierarchical backbones. Our model achieves a state-of-the-art 87.1\% accuracy and 35.8\% mCE on ImageNet-1k and ImageNet-C with 76.8M parameters. We also demonstrate state-of-the-art accuracy and robustness in two downstream tasks: semantic segmentation and object detection. Code is available at: https://github.com/NVlabs/FAN.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Zhou, Daquan and Yu, Zhiding and Xie, Enze and Xiao, Chaowei and Anandkumar, Anima and Feng, Jiashi and Alvarez, Jose M.},
	month = nov,
	year = {2022},
	note = {arXiv:2204.12451 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/YHKXW8Y5/Zhou et al. - 2022 - Understanding The Robustness in Vision Transformer.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/PPSXNQ3N/2204.html:text/html},
}

@misc{xiao_1st_2022,
	title = {1st {Place} {Solution} of {The} {Robust} {Vision} {Challenge} 2022 {Semantic} {Segmentation} {Track}},
	url = {http://arxiv.org/abs/2210.12852},
	doi = {10.48550/arXiv.2210.12852},
	abstract = {This report describes the winning solution to the Robust Vision Challenge (RVC) semantic segmentation track at ECCV 2022. Our method adopts the FAN-B-Hybrid model as the encoder and uses SegFormer as the segmentation framework. The model is trained on a composite dataset consisting of images from 9 datasets (ADE20K, Cityscapes, Mapillary Vistas, ScanNet, VIPER, WildDash 2, IDD, BDD, and COCO) with a simple dataset balancing strategy. All the original labels are projected to a 256-class unified label space, and the model is trained using a cross-entropy loss. Without significant hyperparameter tuning or any specific loss weighting, our solution ranks the first place on all the testing semantic segmentation benchmarks from multiple domains (ADE20K, Cityscapes, Mapillary Vistas, ScanNet, VIPER, and WildDash 2). The proposed method can serve as a strong baseline for the multi-domain segmentation task and benefit future works. Code will be available at https://github.com/lambert-x/RVC\_Segmentation.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Xiao, Junfei and Xu, Zhichao and Lan, Shiyi and Yu, Zhiding and Yuille, Alan and Anandkumar, Anima},
	month = nov,
	year = {2022},
	note = {arXiv:2210.12852 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: The Winning Solution to The Robust Vision Challenge 2022 Semantic Segmentation Track},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/2FG5ZMF8/Xiao et al. - 2022 - 1st Place Solution of The Robust Vision Challenge .pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/KZYNBBWH/2210.html:text/html},
}

@misc{zhao_zero_2022,
	title = {{ZerO} {Initialization}: {Initializing} {Neural} {Networks} with only {Zeros} and {Ones}},
	shorttitle = {{ZerO} {Initialization}},
	url = {http://arxiv.org/abs/2110.12661},
	doi = {10.48550/arXiv.2110.12661},
	abstract = {Deep neural networks are usually initialized with random weights, with adequately selected initial variance to ensure stable signal propagation during training. However, selecting the appropriate variance becomes challenging especially as the number of layers grows. In this work, we replace random weight initialization with a fully deterministic initialization scheme, viz., ZerO, which initializes the weights of networks with only zeros and ones (up to a normalization factor), based on identity and Hadamard transforms. Through both theoretical and empirical studies, we demonstrate that ZerO is able to train networks without damaging their expressivity. Applying ZerO on ResNet achieves state-of-the-art performance on various datasets, including ImageNet, which suggests random weights may be unnecessary for network initialization. In addition, ZerO has many benefits, such as training ultra deep networks (without batch-normalization), exhibiting low-rank learning trajectories that result in low-rank and sparse solutions, and improving training reproducibility.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Zhao, Jiawei and Schäfer, Florian and Anandkumar, Anima},
	month = nov,
	year = {2022},
	note = {arXiv:2110.12661 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/XA5T6QGX/Zhao et al. - 2022 - ZerO Initialization Initializing Neural Networks .pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/GE6IRIJP/2110.html:text/html},
}

@article{le_analysis_2022-1,
	title = {Analysis of the {Human} {Protein} {Atlas} {Weakly} {Supervised} {Single}-{Cell} {Classification} competition},
	volume = {19},
	copyright = {2022 The Author(s)},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-022-01606-z},
	doi = {10.1038/s41592-022-01606-z},
	abstract = {While spatial proteomics by fluorescence imaging has quickly become an essential discovery tool for researchers, fast and scalable methods to classify and embed single-cell protein distributions in such images are lacking. Here, we present the design and analysis of the results from the competition Human Protein Atlas – Single-Cell Classification hosted on the Kaggle platform. This represents a crowd-sourced competition to develop machine learning models trained on limited annotations to label single-cell protein patterns in fluorescent images. The particular challenges of this competition include class imbalance, weak labels and multi-label classification, prompting competitors to apply a wide range of approaches in their solutions. The winning models serve as the first subcellular omics tools that can annotate single-cell locations, extract single-cell features and capture cellular dynamics.},
	language = {en},
	number = {10},
	urldate = {2023-01-05},
	journal = {Nature Methods},
	author = {Le, Trang and Winsnes, Casper F. and Axelsson, Ulrika and Xu, Hao and Mohanakrishnan Kaimal, Jayasankar and Mahdessian, Diana and Dai, Shubin and Makarov, Ilya S. and Ostankovich, Vladislav and Xu, Yang and Benhamou, Eric and Henkel, Christof and Solovyev, Roman A. and Banić, Nikola and Bošnjak, Vito and Bošnjak, Ana and Miličević, Andrija and Ouyang, Wei and Lundberg, Emma},
	month = oct,
	year = {2022},
	note = {Number: 10
Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning, Organelles},
	pages = {1221--1229},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/PU63U9CU/Le et al. - 2022 - Analysis of the Human Protein Atlas Weakly Supervi.pdf:application/pdf;Le et al. - 2022 - Analysis of the Human Protein Atlas Weakly Supervi.pdf:/home/zwerg/Zotero/storage/HY9YJ8D3/Le et al. - 2022 - Analysis of the Human Protein Atlas Weakly Supervi.pdf:application/pdf},
}

@misc{fan_minedojo_2022,
	title = {{MineDojo}: {Building} {Open}-{Ended} {Embodied} {Agents} with {Internet}-{Scale} {Knowledge}},
	shorttitle = {{MineDojo}},
	url = {http://arxiv.org/abs/2206.08853},
	abstract = {Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities. Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a ﬂexible and scalable agent architecture. We introduce MINEDOJO, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions. Using MINEDOJO’s data, we propose a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function. Our agent is able to solve a variety of openended tasks speciﬁed in free-form language without any manually designed dense shaping reward. We open-source the simulation suite, knowledge bases, algorithm implementation, and pretrained models (https://minedojo.org) to promote research towards the goal of generally capable embodied agents.},
	language = {en},
	urldate = {2023-01-11},
	publisher = {arXiv},
	author = {Fan, Linxi and Wang, Guanzhi and Jiang, Yunfan and Mandlekar, Ajay and Yang, Yuncong and Zhu, Haoyi and Tang, Andrew and Huang, De-An and Zhu, Yuke and Anandkumar, Anima},
	month = nov,
	year = {2022},
	note = {arXiv:2206.08853 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	annote = {Comment: Outstanding Paper Award at NeurIPS 2022. Project website: https://minedojo.org},
	file = {Fan et al. - 2022 - MineDojo Building Open-Ended Embodied Agents with.pdf:/home/zwerg/Zotero/storage/H4CNPN8A/Fan et al. - 2022 - MineDojo Building Open-Ended Embodied Agents with.pdf:application/pdf},
}

@misc{stringer_cellpose_2022,
	title = {Cellpose 2.0: how to train your own model},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	shorttitle = {Cellpose 2.0},
	url = {https://www.biorxiv.org/content/10.1101/2022.04.01.486764v1},
	doi = {10.1101/2022.04.01.486764},
	abstract = {Generalist models for cellular segmentation, like Cellpose, provide good out-of-the-box results for many types of images. However, such models do not allow users to adapt the segmentation style to their specific needs and may perform sub-optimally for test images that are very different from the training images. Here we introduce Cellpose 2.0, a new package which includes an ensemble of diverse pretrained models as well as a human-in-the-loop pipeline for quickly prototyping new specialist models. We show that specialist models pretrained on the Cellpose dataset can achieve state-of-the-art segmentation on new image categories with very little user-provided training data. Models trained on 500-1000 segmented regions-of-interest (ROIs) performed nearly as well as models trained on entire datasets with up to 200,000 ROIs. A human-in-the-loop approach further reduced the required user annotations to 100-200 ROIs, while maintaining state-of-the-art segmentation performance. This approach enables a new generation of specialist segmentation models that can be trained on new image types with only 1-2 hours of user effort. We provide software tools including an annotation GUI, a model zoo and a human-in-the-loop pipeline to facilitate the adoption of Cellpose 2.0.},
	language = {en},
	urldate = {2023-01-12},
	publisher = {bioRxiv},
	author = {Stringer, Carsen and Pachitariu, Marius},
	month = apr,
	year = {2022},
	note = {Pages: 2022.04.01.486764
Section: New Results},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/GA3JSRFT/Stringer and Pachitariu - 2022 - Cellpose 2.0 how to train your own model.pdf:application/pdf},
}

@misc{hamilton_unsupervised_2022,
	title = {Unsupervised {Semantic} {Segmentation} by {Distilling} {Feature} {Correspondences}},
	url = {https://arxiv.org/abs/2203.08414},
	doi = {70},
	abstract = {Unsupervised semantic segmentation aims to discover and localize semantically meaningful categories within image corpora without any form of annotation. To solve this task, algorithms must produce features for every pixel that are both semantically meaningful and compact enough to form distinct clusters. Unlike previous works which achieve this with a single end-to-end framework, we propose to separate feature learning from cluster compactification. Empirically, we show that current unsupervised feature learning frameworks already generate dense features whose correlations are semantically consistent. This observation motivates us to design STEGO (\${\textbackslash}textbf\{S\}\$elf-supervised \${\textbackslash}textbf\{T\}\$ransformer with \${\textbackslash}textbf\{E\}\$nergy-based \${\textbackslash}textbf\{G\}\$raph \${\textbackslash}textbf\{O\}\$ptimization), a novel framework that distills unsupervised features into high-quality discrete semantic labels. At the core of STEGO is a novel contrastive loss function that encourages features to form compact clusters while preserving their relationships across the corpora. STEGO yields a significant improvement over the prior state of the art, on both the CocoStuff (\${\textbackslash}textbf\{+14 mIoU\}\$) and Cityscapes (\${\textbackslash}textbf\{+9 mIoU\}\$) semantic segmentation challenges.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Hamilton, Mark and Zhang, Zhoutong and Hariharan, Bharath and Snavely, Noah and Freeman, William T.},
	month = mar,
	year = {2022},
	note = {arXiv:2203.08414 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/2YJDJZS5/Hamilton et al. - 2022 - Unsupervised Semantic Segmentation by Distilling F.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/Z2N5FRAE/2203.html:text/html},
}

@article{thumuluri_deeploc_2022,
	title = {{DeepLoc} 2.0: multi-label subcellular localization prediction using protein language models},
	volume = {50},
	issn = {0305-1048},
	shorttitle = {{DeepLoc} 2.0},
	url = {https://doi.org/10.1093/nar/gkac278},
	doi = {10.1093/nar/gkac278},
	abstract = {The prediction of protein subcellular localization is of great relevance for proteomics research. Here, we propose an update to the popular tool DeepLoc with multi-localization prediction and improvements in both performance and interpretability. For training and validation, we curate eukaryotic and human multi-location protein datasets with stringent homology partitioning and enriched with sorting signal information compiled from the literature. We achieve state-of-the-art performance in DeepLoc 2.0 by using a pre-trained protein language model. It has the further advantage that it uses sequence input rather than relying on slower protein profiles. We provide two means of better interpretability: an attention output along the sequence and highly accurate prediction of nine different types of protein sorting signals. We find that the attention output correlates well with the position of sorting signals. The webserver is available at services.healthtech.dtu.dk/service.php?DeepLoc-2.0.},
	number = {W1},
	urldate = {2023-01-17},
	journal = {Nucleic Acids Research},
	author = {Thumuluri, Vineet and Almagro Armenteros, José Juan and Johansen, Alexander Rosenberg and Nielsen, Henrik and Winther, Ole},
	month = jul,
	year = {2022},
	pages = {W228--W234},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/LUVYITKS/Thumuluri et al. - 2022 - DeepLoc 2.0 multi-label subcellular localization .pdf:application/pdf;gkac278_supplemental_file.pdf:/home/zwerg/Zotero/storage/R73JAIQW/gkac278_supplemental_file.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/VUHYHUH7/6576357.html:text/html},
}

@misc{hamilton_stego_2023,
	title = {{STEGO}: {Unsupervised} {Semantic} {Segmentation} by {Distilling} {Feature} {Correspondences}},
	copyright = {MIT},
	shorttitle = {{STEGO}},
	url = {https://github.com/mhamilton723/STEGO},
	abstract = {Unsupervised Semantic Segmentation by Distilling Feature Correspondences},
	urldate = {2023-01-13},
	author = {Hamilton, Mark},
	month = jan,
	year = {2023},
	note = {original-date: 2022-03-07T15:06:43Z},
	keywords = {computer-vision, deep-learning, pytorch, semantic-segmentation, iclr2022, unsupervised-learning},
}

@article{schmidt_welches_2021,
	title = {Welches {Heißgetränk} gegen {Nephrolithiasis}?},
	volume = {25},
	issn = {2196-5676},
	url = {https://doi.org/10.1007/s00092-021-4681-4},
	doi = {10.1007/s00092-021-4681-4},
	language = {de},
	number = {9},
	urldate = {2023-01-26},
	journal = {Uro-News},
	author = {Schmidt, Joana},
	month = sep,
	year = {2021},
	pages = {9--9},
	file = {Schmidt - 2021 - Welches Heißgetränk gegen Nephrolithiasis.pdf:/home/zwerg/Zotero/storage/FPWM6UC9/Schmidt - 2021 - Welches Heißgetränk gegen Nephrolithiasis.pdf:application/pdf},
}

@misc{caron_emerging_2021-1,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2104.14294},
	doi = {10.48550/arXiv.2104.14294},
	abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	urldate = {2023-01-23},
	publisher = {arXiv},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = may,
	year = {2021},
	note = {arXiv:2104.14294 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 21 pages},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/ZCES3W9V/Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/U2IDT5IQ/2104.html:text/html},
}

@misc{rocca_understanding_2021,
	title = {Understanding {Variational} {Autoencoders} ({VAEs})},
	url = {https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73},
	abstract = {Building, step by step, the reasoning that leads to VAEs.},
	language = {en},
	urldate = {2023-02-02},
	journal = {Medium},
	author = {Rocca, Joseph},
	month = mar,
	year = {2021},
	file = {Snapshot:/home/zwerg/Zotero/storage/X2F27Y9X/understanding-variational-autoencoders-vaes-f70510919f73.html:text/html},
}

@misc{razavi_generating_2019,
	title = {Generating {Diverse} {High}-{Fidelity} {Images} with {VQ}-{VAE}-2},
	url = {http://arxiv.org/abs/1906.00446},
	doi = {10.48550/arXiv.1906.00446},
	abstract = {We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Razavi, Ali and Oord, Aaron van den and Vinyals, Oriol},
	month = jun,
	year = {2019},
	note = {arXiv:1906.00446 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/34W9JHKX/Razavi et al. - 2019 - Generating Diverse High-Fidelity Images with VQ-VA.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/BURV8BQC/1906.html:text/html},
}

@misc{guo_zero-reference_2020,
	title = {Zero-{Reference} {Deep} {Curve} {Estimation} for {Low}-{Light} {Image} {Enhancement}},
	url = {http://arxiv.org/abs/2001.06826},
	doi = {10.48550/arXiv.2001.06826},
	abstract = {The paper presents a novel method, Zero-Reference Deep Curve Estimation (Zero-DCE), which formulates light enhancement as a task of image-specific curve estimation with a deep network. Our method trains a lightweight deep network, DCE-Net, to estimate pixel-wise and high-order curves for dynamic range adjustment of a given image. The curve estimation is specially designed, considering pixel value range, monotonicity, and differentiability. Zero-DCE is appealing in its relaxed assumption on reference images, i.e., it does not require any paired or unpaired data during training. This is achieved through a set of carefully formulated non-reference loss functions, which implicitly measure the enhancement quality and drive the learning of the network. Our method is efficient as image enhancement can be achieved by an intuitive and simple nonlinear curve mapping. Despite its simplicity, we show that it generalizes well to diverse lighting conditions. Extensive experiments on various benchmarks demonstrate the advantages of our method over state-of-the-art methods qualitatively and quantitatively. Furthermore, the potential benefits of our Zero-DCE to face detection in the dark are discussed. Code and model will be available at https://github.com/Li-Chongyi/Zero-DCE.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Guo, Chunle and Li, Chongyi and Guo, Jichang and Loy, Chen Change and Hou, Junhui and Kwong, Sam and Cong, Runmin},
	month = mar,
	year = {2020},
	note = {arXiv:2001.06826 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 10 pages},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/U3ASCSMH/Guo et al. - 2020 - Zero-Reference Deep Curve Estimation for Low-Light.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/R492X76V/2001.html:text/html},
}

@misc{caron_emerging_2021-2,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2104.14294},
	doi = {10.48550/arXiv.2104.14294},
	abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	urldate = {2023-02-08},
	publisher = {arXiv},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = may,
	year = {2021},
	note = {arXiv:2104.14294 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 21 pages},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/IH2BYD4E/Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/67WXNBTP/2104.html:text/html},
}

@misc{noauthor_vid2avatar_nodate,
	title = {{Vid2Avatar}: {3D} {Avatar} {Reconstruction} from {Videos} in the {Wild} via {Self}-supervised {Scene} {Decomposition}},
	url = {https://moygcc.github.io/vid2avatar/},
	urldate = {2023-02-28},
	file = {Vid2Avatar\: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition:/home/zwerg/Zotero/storage/SQXLNSC2/vid2avatar.html:text/html},
}

@misc{noauthor_documentation_nodate,
	title = {Documentation {\textbar} {Electron}},
	url = {http://man.hubwiz.com/docset/electron.docset/Contents/Resources/Documents/docs/index.html},
	urldate = {2023-03-03},
}

@article{takagi_high-resolution_nodate,
	title = {High-resolution image reconstruction with latent diffusion models from human brain activity},
	language = {en},
	author = {Takagi, Yu and Nishimoto, Shinji},
	file = {Takagi and Nishimoto - High-resolution image reconstruction with latent d.pdf:/home/zwerg/Zotero/storage/4BL7SM9C/Takagi and Nishimoto - High-resolution image reconstruction with latent d.pdf:application/pdf},
}

@article{eberwine_subcellular_2023,
	title = {Subcellular omics: a new frontier pushing the limits of resolution, complexity and throughput},
	volume = {20},
	copyright = {2023 Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {Subcellular omics},
	url = {https://www.nature.com/articles/s41592-023-01788-0},
	doi = {10.1038/s41592-023-01788-0},
	abstract = {We argue that the study of single-cell subcellular organelle omics is needed to understand and regulate cell function. This requires and is being enabled by new technology development.},
	language = {en},
	number = {3},
	urldate = {2023-03-15},
	journal = {Nature Methods},
	author = {Eberwine, James and Kim, Junhyong and Anafi, Ron C. and Brem, Steven and Bucan, Maja and Fisher, Stephen A. and Grady, M. Sean and Herr, Amy E. and Issadore, David and Jeong, Hyejoong and Kim, HyunBum and Lee, Daeyeon and Rubakhin, Stanislav and Sul, Jai-Yoon and Sweedler, Jonathan V. and Wolf, John A. and Zaret, Kenneth S. and Zou, James},
	month = mar,
	year = {2023},
	note = {Number: 3
Publisher: Nature Publishing Group},
	keywords = {Organelles, Analytical biochemistry, Molecular biology},
	pages = {331--335},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/AXYDBXQW/Eberwine et al. - 2023 - Subcellular omics a new frontier pushing the limi.pdf:application/pdf},
}

@misc{noauthor_papers_nodate-1,
	title = {Papers with {Code} - {GPT}-4 {Technical} {Report}},
	url = {https://paperswithcode.com/paper/gpt-4-technical-report},
	abstract = {🏆 SOTA for Arithmetic Reasoning on GSM8K (Accuracy metric)},
	language = {en},
	urldate = {2023-03-15},
	file = {Snapshot:/home/zwerg/Zotero/storage/7DC38ZWC/gpt-4-technical-report.html:text/html},
}

@article{winding_connectome_2023,
	title = {The connectome of an insect brain},
	volume = {379},
	url = {https://www.science.org/doi/10.1126/science.add9330},
	doi = {10.1126/science.add9330},
	abstract = {Brains contain networks of interconnected neurons and so knowing the network architecture is essential for understanding brain function. We therefore mapped the synaptic-resolution connectome of an entire insect brain (Drosophila larva) with rich behavior, including learning, value computation, and action selection, comprising 3016 neurons and 548,000 synapses. We characterized neuron types, hubs, feedforward and feedback pathways, as well as cross-hemisphere and brain-nerve cord interactions. We found pervasive multisensory and interhemispheric integration, highly recurrent architecture, abundant feedback from descending neurons, and multiple novel circuit motifs. The brain’s most recurrent circuits comprised the input and output neurons of the learning center. Some structural features, including multilayer shortcuts and nested recurrent loops, resembled state-of-the-art deep learning architectures. The identified brain architecture provides a basis for future experimental and theoretical studies of neural circuits.},
	number = {6636},
	urldate = {2023-03-16},
	journal = {Science},
	author = {Winding, Michael and Pedigo, Benjamin D. and Barnes, Christopher L. and Patsolic, Heather G. and Park, Youngser and Kazimiers, Tom and Fushiki, Akira and Andrade, Ingrid V. and Khandelwal, Avinash and Valdes-Aleman, Javier and Li, Feng and Randel, Nadine and Barsotti, Elizabeth and Correia, Ana and Fetter, Richard D. and Hartenstein, Volker and Priebe, Carey E. and Vogelstein, Joshua T. and Cardona, Albert and Zlatic, Marta},
	month = mar,
	year = {2023},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eadd9330},
}

@inproceedings{shajii_codon_2023,
	address = {New York, NY, USA},
	series = {{CC} 2023},
	title = {Codon: {A} {Compiler} for {High}-{Performance} {Pythonic} {Applications} and {DSLs}},
	isbn = {9798400700880},
	shorttitle = {Codon},
	url = {https://dl.acm.org/doi/10.1145/3578360.3580275},
	doi = {10.1145/3578360.3580275},
	abstract = {Domain-specific languages (DSLs) are able to provide intuitive high-level abstractions that are easy to work with while attaining better performance than general-purpose languages. Yet, implementing new DSLs is a burdensome task. As a result, new DSLs are usually embedded in general-purpose languages. While low-level languages like C or C++ often provide better performance as a host than high-level languages like Python, high-level languages are becoming more prevalent in many domains due to their ease and flexibility. Here, we present Codon, a domain-extensible compiler and DSL framework for high-performance DSLs with Python's syntax and semantics. Codon builds on previous work on ahead-of-time type checking and compilation of Python programs and leverages a novel intermediate representation to easily incorporate domain-specific optimizations and analyses. We showcase and evaluate several compiler extensions and DSLs for Codon targeting various domains, including bioinformatics, secure multi-party computation, block-based data compression and parallel programming, showing that Codon DSLs can provide benefits of familiar high-level languages and achieve performance typically only seen with low-level languages, thus bridging the gap between performance and usability.},
	urldate = {2023-03-18},
	booktitle = {Proceedings of the 32nd {ACM} {SIGPLAN} {International} {Conference} on {Compiler} {Construction}},
	publisher = {Association for Computing Machinery},
	author = {Shajii, Ariya and Ramirez, Gabriel and Smajlović, Haris and Ray, Jessica and Berger, Bonnie and Amarasinghe, Saman and Numanagić, Ibrahim},
	month = feb,
	year = {2023},
	keywords = {domain-specific languages, intermediate representation, optimization, Python, type checking},
	pages = {191--202},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/THNB8PWP/Shajii et al. - 2023 - Codon A Compiler for High-Performance Pythonic Ap.pdf:application/pdf},
}

@misc{noauthor_llama_nodate,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models} - {Meta} {Research}},
	shorttitle = {{LLaMA}},
	url = {https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to...},
	language = {en},
	urldate = {2023-03-28},
	journal = {Meta Research},
}

@article{touvron_llama_nodate,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla70B and PaLM-540B. We release all our models to the research community1.},
	language = {en},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothee and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	file = {Touvron et al. - LLaMA Open and Efficient Foundation Language Mode.pdf:/home/zwerg/Zotero/storage/RR2HNW5K/Touvron et al. - LLaMA Open and Efficient Foundation Language Mode.pdf:application/pdf},
}

@article{bommasani_opportunities_nodate,
	title = {On the {Opportunities} and {Risks} of {Foundation} {Models}},
	language = {en},
	author = {Bommasani, Rishi and Hudson, Drew A and Altman, Ehsan Adeli Russ and Arora, Simran},
	file = {Bommasani et al. - On the Opportunities and Risks of Foundation Model.pdf:/home/zwerg/Zotero/storage/VFN6B2MI/Bommasani et al. - On the Opportunities and Risks of Foundation Model.pdf:application/pdf},
}

@misc{wang_self-instruct_2022,
	title = {Self-{Instruct}: {Aligning} {Language} {Model} with {Self} {Generated} {Instructions}},
	shorttitle = {Self-{Instruct}},
	url = {http://arxiv.org/abs/2212.10560},
	doi = {10.48550/arXiv.2212.10560},
	abstract = {Large "instruction-tuned" language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model. Applying our method to vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT\_001, which is trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT\_001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
	urldate = {2023-03-28},
	publisher = {arXiv},
	author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10560 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: work in progress},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/4TTFMWKN/Wang et al. - 2022 - Self-Instruct Aligning Language Model with Self G.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/5PPMDBCI/2212.html:text/html},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full ﬁne-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁnetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	language = {en},
	urldate = {2023-03-28},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency},
	file = {Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:/home/zwerg/Zotero/storage/7AVIQF4G/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 100 pages},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/G3PFP956/OpenAI - 2023 - GPT-4 Technical Report.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/LCF3H8T4/2303.html:text/html},
}

@misc{bubeck_sparks_2023,
	title = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
	url = {http://arxiv.org/abs/2303.12712},
	doi = {10.48550/arXiv.2303.12712},
	abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	month = mar,
	year = {2023},
	note = {arXiv:2303.12712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/F27WASWE/Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early e.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/4RVXNC6Q/2303.html:text/html},
}

@misc{bubeck_sparks_2023-1,
	title = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
	url = {http://arxiv.org/abs/2303.12712},
	doi = {10.48550/arXiv.2303.12712},
	abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	month = mar,
	year = {2023},
	note = {arXiv:2303.12712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/YBSBPGBN/Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early e.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/BMSMJEDP/2303.html:text/html},
}

@article{von_chamier_democratising_2021,
	title = {Democratising deep learning for microscopy with {ZeroCostDL4Mic}},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-22518-0},
	doi = {10.1038/s41467-021-22518-0},
	abstract = {Deep Learning (DL) methods are powerful analytical tools for microscopy and can outperform conventional image processing pipelines. Despite the enthusiasm and innovations fuelled by DL technology, the need to access powerful and compatible resources to train DL networks leads to an accessibility barrier that novice users often find difficult to overcome. Here, we present ZeroCostDL4Mic, an entry-level platform simplifying DL access by leveraging the free, cloud-based computational resources of Google Colab. ZeroCostDL4Mic allows researchers with no coding expertise to train and apply key DL networks to perform tasks including segmentation (using U-Net and StarDist), object detection (using YOLOv2), denoising (using CARE and Noise2Void), super-resolution microscopy (using Deep-STORM), and image-to-image translation (using Label-free prediction - fnet, pix2pix and CycleGAN). Importantly, we provide suitable quantitative tools for each network to evaluate model performance, allowing model optimisation. We demonstrate the application of the platform to study multiple biological processes.},
	language = {en},
	number = {1},
	urldate = {2023-04-03},
	journal = {Nature Communications},
	author = {von Chamier, Lucas and Laine, Romain F. and Jukkala, Johanna and Spahn, Christoph and Krentzel, Daniel and Nehme, Elias and Lerche, Martina and Hernández-Pérez, Sara and Mattila, Pieta K. and Karinou, Eleni and Holden, Séamus and Solak, Ahmet Can and Krull, Alexander and Buchholz, Tim-Oliver and Jones, Martin L. and Royer, Loïc A. and Leterrier, Christophe and Shechtman, Yoav and Jug, Florian and Heilemann, Mike and Jacquemet, Guillaume and Henriques, Ricardo},
	month = apr,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Image processing, Cellular imaging, Machine learning},
	pages = {2276},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/2TJUNIB6/von Chamier et al. - 2021 - Democratising deep learning for microscopy with Ze.pdf:application/pdf},
}

@article{boiko_emergent_nodate,
	title = {Emergent autonomous scientific research capabilities of large language models},
	abstract = {Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent’s scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.},
	language = {en},
	author = {Boiko, Daniil A and MacKnight, Robert and Gomes, Gabe},
	file = {Boiko et al. - Emergent autonomous scientific research capabiliti.pdf:/home/zwerg/Zotero/storage/Y54DDEZC/Boiko et al. - Emergent autonomous scientific research capabiliti.pdf:application/pdf},
}

@misc{singh_effectiveness_2023,
	title = {The effectiveness of {MAE} pre-pretraining for billion-scale pretraining},
	url = {http://arxiv.org/abs/2303.13496},
	abstract = {This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video recognition, object detection, low-shot classification and zero-shot recognition. Our largest model achieves new state-of-the-art results on iNaturalist-18 (91.3\%), 1-shot ImageNet-1k (62.1\%), and zero-shot transfer on Food-101 (96.0\%). Our study reveals that model initialization plays a significant role, even for web-scale pretraining with billions of images.},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Singh, Mannat and Duval, Quentin and Alwala, Kalyan Vasudev and Fan, Haoqi and Aggarwal, Vaibhav and Adcock, Aaron and Joulin, Armand and Dollár, Piotr and Feichtenhofer, Christoph and Girshick, Ross and Girdhar, Rohit and Misra, Ishan},
	month = mar,
	year = {2023},
	note = {arXiv:2303.13496 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/S7WIHVJJ/Singh et al. - 2023 - The effectiveness of MAE pre-pretraining for billi.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/FYZJTC2H/2303.html:text/html},
}

@misc{ji_segment_2023,
	title = {Segment {Anything} {Is} {Not} {Always} {Perfect}: {An} {Investigation} of {SAM} on {Different} {Real}-world {Applications}},
	shorttitle = {Segment {Anything} {Is} {Not} {Always} {Perfect}},
	url = {http://arxiv.org/abs/2304.05750},
	doi = {10.48550/arXiv.2304.05750},
	abstract = {Recently, Meta AI Research approaches a general, promptable Segment Anything Model (SAM) pre-trained on an unprecedentedly large segmentation dataset (SA-1B). Without a doubt, the emergence of SAM will yield significant benefits for a wide array of practical image segmentation applications. In this study, we conduct a series of intriguing investigations into the performance of SAM across various applications, particularly in the fields of natural images, agriculture, manufacturing, remote sensing, and healthcare. We analyze and discuss the benefits and limitations of SAM and provide an outlook on future development of segmentation tasks. Note that our work does not intend to propose new algorithms or theories, but rather provide a comprehensive view of SAM in practice. This work is expected to provide insights that facilitate future research activities toward generic segmentation.},
	urldate = {2023-04-17},
	publisher = {arXiv},
	author = {Ji, Wei and Li, Jingjing and Bi, Qi and Li, Wenbo and Cheng, Li},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05750 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech Report},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/VP4J727E/Ji et al. - 2023 - Segment Anything Is Not Always Perfect An Investi.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/7XB643LF/2304.html:text/html},
}

@misc{kirillov_segment_2023,
	title = {Segment {Anything}},
	url = {http://arxiv.org/abs/2304.02643},
	doi = {10.48550/arXiv.2304.02643},
	abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
	urldate = {2023-04-17},
	publisher = {arXiv},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02643 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Project web-page: https://segment-anything.com},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/IJIXBSQD/Kirillov et al. - 2023 - Segment Anything.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/7WMNI27M/2304.html:text/html},
}

@misc{noauthor_segment_nodate,
	title = {Segment {Anything} {\textbar} {Meta} {AI}},
	url = {https://segment-anything.com/},
	language = {en},
	urldate = {2023-04-06},
	file = {Snapshot:/home/zwerg/Zotero/storage/TYTA873I/segment-anything.com.html:text/html},
}

@misc{dosovitskiy_image_2021-1,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2023-04-29},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf:/home/zwerg/Zotero/storage/22Y2EC4F/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf:application/pdf},
}

@misc{he_masked_2021,
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	url = {http://arxiv.org/abs/2111.06377},
	doi = {10.48550/arXiv.2111.06377},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
	urldate = {2023-04-29},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
	month = dec,
	year = {2021},
	note = {arXiv:2111.06377 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf:/home/zwerg/Zotero/storage/F6BXRQKF/He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf:application/pdf},
}

@misc{noauthor_visual_2023,
	title = {Visual {Blocks} for {ML}: {Accelerating} machine learning prototyping with interactive tools},
	shorttitle = {Visual {Blocks} for {ML}},
	url = {https://ai.googleblog.com/2023/04/visual-blocks-for-ml-accelerating.html?m=1},
	urldate = {2023-05-05},
	month = apr,
	year = {2023},
}

@misc{noauthor_fourthbraingenaiai-superstar-dataset_nodate,
	title = {{FourthBrainGenAI}/{AI}-{Superstar}-{Dataset} · {Datasets} at {Hugging} {Face}},
	url = {https://huggingface.co/datasets/FourthBrainGenAI/AI-Superstar-Dataset},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-05-02},
	file = {Snapshot:/home/zwerg/Zotero/storage/ZWMQIQMC/AI-Superstar-Dataset.html:text/html},
}

@misc{zhao_survey_2023,
	title = {A {Survey} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2303.18223},
	doi = {10.48550/arXiv.2303.18223},
	abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	month = apr,
	year = {2023},
	note = {arXiv:2303.18223 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: ongoing work; 58 pages},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/WJTZI56G/Zhao et al. - 2023 - A Survey of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/ED5ZUY77/2303.html:text/html},
}

@misc{henderson_pile_2022,
	title = {Pile of {Law}: {Learning} {Responsible} {Data} {Filtering} from the {Law} and a {256GB} {Open}-{Source} {Legal} {Dataset}},
	shorttitle = {Pile of {Law}},
	url = {http://arxiv.org/abs/2207.00220},
	doi = {10.48550/arXiv.2207.00220},
	abstract = {One concern with the rise of large language models lies with their potential for significant harm, particularly from pretraining on biased, obscene, copyrighted, and private information. Emerging ethical approaches have attempted to filter pretraining material, but such approaches have been ad hoc and failed to take context into account. We offer an approach to filtering grounded in law, which has directly addressed the tradeoffs in filtering material. First, we gather and make available the Pile of Law, a 256GB (and growing) dataset of open-source English-language legal and administrative data, covering court opinions, contracts, administrative rules, and legislative records. Pretraining on the Pile of Law may help with legal tasks that have the promise to improve access to justice. Second, we distill the legal norms that governments have developed to constrain the inclusion of toxic or private content into actionable lessons for researchers and discuss how our dataset reflects these norms. Third, we show how the Pile of Law offers researchers the opportunity to learn such filtering rules directly from the data, providing an exciting new research direction in model-based processing.},
	urldate = {2023-05-02},
	author = {Henderson, Peter and Krass, Mark S. and Zheng, Lucia and Guha, Neel and Manning, Christopher D. and Jurafsky, Dan and Ho, Daniel E.},
	month = nov,
	year = {2022},
	note = {arXiv:2207.00220 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Henderson et al. - 2022 - Pile of Law Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset.pdf:/home/zwerg/Zotero/storage/ES4Z4BL8/Henderson et al. - 2022 - Pile of Law Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset.pdf:application/pdf},
}

@misc{boiko_emergent_2023,
	title = {Emergent autonomous scientific research capabilities of large language models},
	url = {http://arxiv.org/abs/2304.05332},
	doi = {10.48550/arXiv.2304.05332},
	abstract = {Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.},
	urldate = {2023-05-05},
	publisher = {arXiv},
	author = {Boiko, Daniil A. and MacKnight, Robert and Gomes, Gabe},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05332 [physics]},
	keywords = {Computer Science - Computation and Language, Physics - Chemical Physics},
	annote = {Comment: Version 1, April 11, 2023. 48 pages},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/XR9QKFVE/Boiko et al. - 2023 - Emergent autonomous scientific research capabiliti.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/URA56U44/2304.html:text/html},
}

@misc{ambatipudi_comparison_2023,
	title = {A {Comparison} of {HDF5}, {Zarr}, and {netCDF4} in {Performing} {Common} {I}/{O} {Operations}},
	url = {http://arxiv.org/abs/2207.09503},
	doi = {10.48550/arXiv.2207.09503},
	abstract = {Scientific data is often stored in files because of the simplicity they provide in managing, transferring, and sharing data. These files are typically structured in a specific arrangement and contain metadata to understand the structure the data is stored in. There are numerous file formats in use in various scientific domains that provide abstractions for storing and retrieving data. With the abundance of file formats aiming to store large amounts of scientific data quickly and easily, a question that arises is, "Which scientific file format is best suited for a general use case?" In this study, we compiled a set of benchmarks for common file operations, i.e., create, open, read, write, and close, and used the results of these benchmarks to compare three popular formats: HDF5, netCDF4, and Zarr.},
	urldate = {2023-05-05},
	publisher = {arXiv},
	author = {Ambatipudi, Sriniket and Byna, Suren},
	month = feb,
	year = {2023},
	note = {arXiv:2207.09503 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	annote = {Comment: 6 pages, 12 figures},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/IQZFYU48/Ambatipudi and Byna - 2023 - A Comparison of HDF5, Zarr, and netCDF4 in Perform.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/6TSFRV85/2207.html:text/html},
}

@misc{zelikman_parsel_2023,
	title = {Parsel: {A} ({De}-)compositional {Framework} for {Algorithmic} {Reasoning} with {Language} {Models}},
	shorttitle = {Parsel},
	url = {http://arxiv.org/abs/2212.10561},
	abstract = {Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs, taking hierarchical function descriptions in natural language as input. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis, robotic planning, and theorem proving. We show that LLMs generating Parsel solve more competition-level problems in the APPS dataset, resulting in pass rates that are over 75\% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. We also find that LLM-generated robotic plans using Parsel as an intermediate language are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers.},
	urldate = {2023-05-05},
	publisher = {arXiv},
	author = {Zelikman, Eric and Huang, Qian and Poesia, Gabriel and Goodman, Noah D. and Haber, Nick},
	month = jan,
	year = {2023},
	note = {arXiv:2212.10561 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: new quantitative details},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/PUMFZLBQ/2212.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/NA2AG7UL/Zelikman et al. - 2023 - Parsel A (De-)compositional Framework for Algorit.pdf:application/pdf},
}

@inproceedings{du_rapsai_2023,
	address = {Hamburg Germany},
	title = {Rapsai: {Accelerating} {Machine} {Learning} {Prototyping} of {Multimedia} {Applications} through {Visual} {Programming}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {Rapsai},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581338},
	doi = {10.1145/3544548.3581338},
	abstract = {In recent years, there has been a proliferation of multimedia applications that leverage machine learning (ML) for interactive experiences. Prototyping ML-based applications is, however, still challenging, given complex workflows that are not ideal for design and experimentation. To better understand these challenges, we conducted a formative study with seven ML practitioners to gather insights about common ML evaluation workflows.},
	language = {en},
	urldate = {2023-05-05},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Du, Ruofei and Li, Na and Jin, Jing and Carney, Michelle and Miles, Scott and Kleiner, Maria and Yuan, Xiuxiu and Zhang, Yinda and Kulkarni, Anuva and Liu, Xingyu and Sabie, Ahmed and Orts-Escolano, Sergio and Kar, Abhishek and Yu, Ping and Iyengar, Ram and Kowdle, Adarsh and Olwal, Alex},
	month = apr,
	year = {2023},
	pages = {1--23},
	file = {Du_Rapsai-AcceleratingMachineLearningPrototypingOfMultimediaApplicationsThroughVisualProgramming_CHI2023.pdf:/home/zwerg/Zotero/storage/L69M6AAA/Du_Rapsai-AcceleratingMachineLearningPrototypingOfMultimediaApplicationsThroughVisualProgramming_CHI2023.pdf:application/pdf},
}

@misc{noauthor_visual_2023-1,
	title = {Visual {Blocks}},
	copyright = {Apache-2.0},
	url = {https://github.com/google/visualblocks},
	urldate = {2023-05-05},
	publisher = {Google},
	month = may,
	year = {2023},
	note = {original-date: 2023-03-29T21:33:01Z},
}

@misc{noauthor_end--end_2022,
	title = {End-to-{End} {AI} for {NVIDIA}-{Based} {PCs}: {Transitioning} {AI} {Models} with {ONNX}},
	shorttitle = {End-to-{End} {AI} for {NVIDIA}-{Based} {PCs}},
	url = {https://developer.nvidia.com/blog/end-to-end-ai-for-pcs-transitioning-ai-models-with-onnx/},
	abstract = {This post is the second in a series about optimizing end-to-end AI. In this post, I discuss how to use ONNX to transition your AI models from research to production while avoiding common mistakes.},
	language = {en-US},
	urldate = {2023-05-05},
	journal = {NVIDIA Technical Blog},
	month = dec,
	year = {2022},
	file = {Snapshot:/home/zwerg/Zotero/storage/RAX2CHX6/end-to-end-ai-for-pcs-transitioning-ai-models-with-onnx.html:text/html},
}

@misc{jun_shap-e_2023,
	title = {Shap-{E}: {Generating} {Conditional} {3D} {Implicit} {Functions}},
	shorttitle = {Shap-{E}},
	url = {http://arxiv.org/abs/2305.02463},
	doi = {10.48550/arXiv.2305.02463},
	abstract = {We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space. We release model weights, inference code, and samples at https://github.com/openai/shap-e.},
	urldate = {2023-05-10},
	publisher = {arXiv},
	author = {Jun, Heewoo and Nichol, Alex},
	month = may,
	year = {2023},
	note = {arXiv:2305.02463 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 23 pages, 13 figures},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/AFYPIQQF/Jun and Nichol - 2023 - Shap-E Generating Conditional 3D Implicit Functio.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/A5J54MCA/2305.html:text/html},
}

@misc{hudsona_stanford_2023,
	title = {Stanford {AI} {Lab} {Papers} and {Talks} at {ICLR} 2023},
	url = {http://ai.stanford.edu/blog/iclr-2023/},
	abstract = {The official Stanford AI Lab blog},
	urldate = {2023-05-08},
	journal = {SAIL Blog},
	author = {Hudson{\textless}/a{\textgreater}, Compiled by {\textless}a href='https://cs stanford edu/{\textasciitilde}dorarad/'{\textgreater}Drew A.},
	month = may,
	year = {2023},
	file = {Snapshot:/home/zwerg/Zotero/storage/KMUT7BJU/iclr-2023.html:text/html},
}

@misc{ainslie_colt5_2023,
	title = {{CoLT5}: {Faster} {Long}-{Range} {Transformers} with {Conditional} {Computation}},
	shorttitle = {{CoLT5}},
	url = {http://arxiv.org/abs/2303.09752},
	doi = {10.48550/arXiv.2303.09752},
	abstract = {Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.},
	urldate = {2023-05-15},
	publisher = {arXiv},
	author = {Ainslie, Joshua and Lei, Tao and de Jong, Michiel and Ontañón, Santiago and Brahma, Siddhartha and Zemlyanskiy, Yury and Uthus, David and Guo, Mandy and Lee-Thorp, James and Tay, Yi and Sung, Yun-Hsuan and Sanghai, Sumit},
	month = apr,
	year = {2023},
	note = {arXiv:2303.09752 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Added CoDA reference and minor edits to clarify routing},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/DJU8SRX5/Ainslie et al. - 2023 - CoLT5 Faster Long-Range Transformers with Conditi.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/NP3M7WTV/2303.html:text/html},
}

@misc{noauthor_document_nodate,
	title = {Document analysis platform {\textbar} {Petal}},
	url = {https://www.petal.org/},
	abstract = {Petal is an AI-powered document analysis platform that enables you to chat with your documents. Petal’s context-aware generative AI provides you with accurate and reliable answers sourced directly from documents you trust. Understand complex and technical topics quickly and painlessly. Summarize, translate, and even draft new content using out built-in Notebook. Collaborate with your team and share documents, annotations, and comments. Use our multi-document AI table to compare documents and set filtering criteria using conversational natural language. Work smarter, not harder!},
	urldate = {2023-05-12},
	file = {Snapshot:/home/zwerg/Zotero/storage/PR9MJLZW/www.petal.org.html:text/html},
}

@misc{noauthor_parallel_nodate,
	title = {Parallel {Mean} \& {Variance} {Calculation} — {Parallel} {Statistics} in {Python} documentation},
	url = {https://parallel-statistics.readthedocs.io/en/stable/mean_variance.html#parallel_statistics.ParallelMeanVariance},
	urldate = {2023-05-10},
	file = {Parallel Mean & Variance Calculation — Parallel Statistics in Python documentation:/home/zwerg/Zotero/storage/TES6GRVW/mean_variance.html:text/html},
}

@inproceedings{schubert_numerically_2018,
	address = {Bozen-Bolzano Italy},
	title = {Numerically stable parallel computation of (co-)variance},
	isbn = {978-1-4503-6505-5},
	url = {https://dl.acm.org/doi/10.1145/3221269.3223036},
	doi = {10.1145/3221269.3223036},
	language = {en},
	urldate = {2023-05-10},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Scientific} and {Statistical} {Database} {Management}},
	publisher = {ACM},
	author = {Schubert, Erich and Gertz, Michael},
	month = jul,
	year = {2018},
	pages = {1--12},
	file = {Schubert and Gertz - 2018 - Numerically stable parallel computation of (co-)va.pdf:/home/zwerg/Zotero/storage/X2HVZ3LP/Schubert and Gertz - 2018 - Numerically stable parallel computation of (co-)va.pdf:application/pdf},
}

@inproceedings{schubert_numerically_2018-1,
	address = {New York, NY, USA},
	series = {{SSDBM} '18},
	title = {Numerically stable parallel computation of (co-)variance},
	isbn = {978-1-4503-6505-5},
	url = {https://doi.org/10.1145/3221269.3223036},
	doi = {10.1145/3221269.3223036},
	abstract = {With the advent of big data, we see an increasing interest in computing correlations in huge data sets with both many instances and many variables. Essential descriptive statistics such as the variance, standard deviation, covariance, and correlation can suffer from a numerical instability known as "catastrophic cancellation" that can lead to problems when naively computing these statistics with a popular textbook equation. While this instability has been discussed in the literature already 50 years ago, we found that even today, some high-profile tools still employ the instable version. In this paper, we study a popular incremental technique originally proposed by Welford, which we extend to weighted covariance and correlation. We also discuss strategies for further improving numerical precision, how to compute such statistics online on a data stream, with exponential aging, with missing data, and a batch parallelization for both high performance and numerical precision. We demonstrate when the numerical instability arises, and the performance of different approaches under these conditions. We showcase applications from the classic computation of variance as well as advanced applications such as stock market analysis with exponentially weighted moving models and Gaussian mixture modeling for cluster analysis that all benefit from this approach.},
	urldate = {2023-05-10},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Scientific} and {Statistical} {Database} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Schubert, Erich and Gertz, Michael},
	month = jul,
	year = {2018},
	pages = {1--12},
}

@article{patrick_spatial_nodate,
	title = {Spatial analysis for highly multiplexed imaging data to identify tissue microenvironments},
	volume = {n/a},
	issn = {1552-4930},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cyto.a.24729},
	doi = {10.1002/cyto.a.24729},
	abstract = {Highly multiplexed in situ imaging cytometry assays have made it possible to study the spatial organization of numerous cell types simultaneously. We have addressed the challenge of quantifying complex multi-cellular relationships by proposing a statistical method which clusters local indicators of spatial association. Our approach successfully identifies distinct tissue architectures in datasets generated from three state-of-the-art high-parameter assays demonstrating its value in summarizing the information-rich data generated from these technologies.},
	language = {en},
	number = {n/a},
	urldate = {2023-05-23},
	journal = {Cytometry Part A},
	author = {Patrick, Ellis and Canete, Nicolas P. and Iyengar, Sourish S. and Harman, Andrew N. and Sutherland, Greg T. and Yang, Pengyi},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cyto.a.24729},
	keywords = {imaging, R, spatial analysis, statistics},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/3GXYXUUK/Patrick et al. - Spatial analysis for highly multiplexed imaging da.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/35LXPWT2/cyto.a.html:text/html},
}

@article{pitman_lecture_nodate,
	title = {Lecture 26 : {Poisson} {Point} {Processes}},
	language = {en},
	journal = {Poisson Point Processes},
	author = {Pitman, Jim and Hough, Ben},
	file = {Pitman and Hough - Lecture 26  Poisson Point Processes.pdf:/home/zwerg/Zotero/storage/XHKN43BD/Pitman and Hough - Lecture 26  Poisson Point Processes.pdf:application/pdf},
}

@misc{wu_bloomberggpt_2023,
	title = {{BloombergGPT}: {A} {Large} {Language} {Model} for {Finance}},
	shorttitle = {{BloombergGPT}},
	url = {http://arxiv.org/abs/2303.17564},
	doi = {10.48550/arXiv.2303.17564},
	abstract = {The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.},
	urldate = {2023-06-03},
	author = {Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
	month = may,
	year = {2023},
	note = {arXiv:2303.17564 [cs, q-fin]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Quantitative Finance - General Finance},
	file = {Wu et al. - 2023 - BloombergGPT A Large Language Model for Finance.pdf:/home/zwerg/Zotero/storage/8IAUEYQ6/Wu et al. - 2023 - BloombergGPT A Large Language Model for Finance.pdf:application/pdf},
}

@article{zhang_toward_2023,
	title = {Toward the third generation artificial intelligence},
	volume = {66},
	issn = {1869-1919},
	url = {https://doi.org/10.1007/s11432-021-3449-x},
	doi = {10.1007/s11432-021-3449-x},
	abstract = {There have been two competing paradigms in artificial intelligence (AI) development ever since its birth in 1956, i.e., symbolism and connectionism (or sub-symbolism). While symbolism dominated AI research by the end of 1980s, connectionism gained momentum in the 1990s and is gradually displacing symbolism. This paper considers symbolism as the first generation of AI and connectionism as the second generation. However, each of these two paradigms simulates the human mind from only one perspective. AI cannot achieve true human behaviors by relying on only one paradigm. In order to develop novel AI technologies that are safe, reliable, and extensible, it is necessary to establish a new explainable and robust AI theory. To this end, this paper looks toward developing a third generation artificial intelligence by combining the current paradigms.},
	language = {en},
	number = {2},
	urldate = {2023-06-10},
	journal = {Science China Information Sciences},
	author = {Zhang, Bo and Zhu, Jun and Su, Hang},
	month = jan,
	year = {2023},
	keywords = {artificial intelligence, connectionism, dual-space model, single-space model, symbolism, triple-space model},
	pages = {121101},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/HDCPALJ8/Zhang et al. - 2023 - Toward the third generation artificial intelligenc.pdf:application/pdf},
}

@misc{assran_self-supervised_2023,
	title = {Self-{Supervised} {Learning} from {Images} with a {Joint}-{Embedding} {Predictive} {Architecture}},
	url = {http://arxiv.org/abs/2301.08243},
	doi = {10.48550/arXiv.2301.08243},
	abstract = {This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.},
	urldate = {2023-06-16},
	author = {Assran, Mahmoud and Duval, Quentin and Misra, Ishan and Bojanowski, Piotr and Vincent, Pascal and Rabbat, Michael and LeCun, Yann and Ballas, Nicolas},
	month = apr,
	year = {2023},
	note = {arXiv:2301.08243 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Assran et al. - 2023 - Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.pdf:/home/zwerg/Zotero/storage/B64GYJW8/Assran et al. - 2023 - Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.pdf:application/pdf},
}

@misc{mukherjee_orca_2023,
	title = {Orca: {Progressive} {Learning} from {Complex} {Explanation} {Traces} of {GPT}-4},
	shorttitle = {Orca},
	url = {http://arxiv.org/abs/2306.02707},
	abstract = {Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model’s capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca, a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100\% in complex zero-shot reasoning benchmarks like BigBench Hard (BBH) and 42\% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.},
	language = {en},
	urldate = {2023-06-17},
	publisher = {arXiv},
	author = {Mukherjee, Subhabrata and Mitra, Arindam and Jawahar, Ganesh and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02707 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Mukherjee et al. - 2023 - Orca Progressive Learning from Complex Explanatio.pdf:/home/zwerg/Zotero/storage/8V69NXZT/Mukherjee et al. - 2023 - Orca Progressive Learning from Complex Explanatio.pdf:application/pdf},
}

@misc{gudibande_false_2023,
	title = {The {False} {Promise} of {Imitating} {Proprietary} {LLMs}},
	url = {http://arxiv.org/abs/2305.15717},
	doi = {10.48550/arXiv.2305.15717},
	abstract = {An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT's style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.},
	urldate = {2023-06-17},
	publisher = {arXiv},
	author = {Gudibande, Arnav and Wallace, Eric and Snell, Charlie and Geng, Xinyang and Liu, Hao and Abbeel, Pieter and Levine, Sergey and Song, Dawn},
	month = may,
	year = {2023},
	note = {arXiv:2305.15717 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/GI4SYRHR/Gudibande et al. - 2023 - The False Promise of Imitating Proprietary LLMs.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/DFV32YVD/2305.html:text/html},
}

@misc{noauthor_vicuna_nodate,
	title = {Vicuna: {An} {Open}-{Source} {Chatbot} {Impressing} {GPT}-4 with 90\%* {ChatGPT} {Quality} {\textbar} {LMSYS} {Org}},
	shorttitle = {Vicuna},
	url = {https://lmsys.org/blog/2023-03-30-vicuna},
	abstract = {{\textless}p{\textgreater}We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation ...},
	language = {en},
	urldate = {2023-06-18},
	file = {Snapshot:/home/zwerg/Zotero/storage/FGLJS8MG/2023-03-30-vicuna.html:text/html},
}

@misc{noauthor_chat_nodate,
	title = {Chat with {Open} {Large} {Language} {Models}},
	url = {https://gradio.app/},
	language = {en-US},
	urldate = {2023-06-18},
	file = {Snapshot:/home/zwerg/Zotero/storage/2PARYGTN/chat.lmsys.org.html:text/html},
}

@misc{zhong_agieval_2023,
	title = {{AGIEval}: {A} {Human}-{Centric} {Benchmark} for {Evaluating} {Foundation} {Models}},
	shorttitle = {{AGIEval}},
	url = {http://arxiv.org/abs/2304.06364},
	abstract = {Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artiﬁcial General Intelligence (AGI). Traditional benchmarks, which rely on artiﬁcial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark speciﬁcally designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualiﬁcation tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95\% accuracy rate on the SAT Math test and a 92.5\% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary foundation models. In contrast, we also ﬁnd that GPT-4 is less proﬁcient in tasks that require complex reasoning or speciﬁc domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal these models’ strengths and limitations, providing valuable insights into future directions for enhancing their general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a more meaningful and robust evaluation of foundation models’ performance in real-world scenarios2.},
	language = {en},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06364 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 19 pages},
	file = {Zhong et al. - 2023 - AGIEval A Human-Centric Benchmark for Evaluating .pdf:/home/zwerg/Zotero/storage/586X89LD/Zhong et al. - 2023 - AGIEval A Human-Centric Benchmark for Evaluating .pdf:application/pdf},
}

@misc{suzgun_challenging_2022,
	title = {Challenging {BIG}-{Bench} {Tasks} and {Whether} {Chain}-of-{Thought} {Can} {Solve} {Them}},
	url = {http://arxiv.org/abs/2210.09261},
	abstract = {BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65\% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.},
	language = {en},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Suzgun, Mirac and Scales, Nathan and Schärli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V. and Chi, Ed H. and Zhou, Denny and Wei, Jason},
	month = oct,
	year = {2022},
	note = {arXiv:2210.09261 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: GitHub repository: https://github.com/suzgunmirac/BIG-Bench-Hard},
	file = {Suzgun et al. - 2022 - Challenging BIG-Bench Tasks and Whether Chain-of-T.pdf:/home/zwerg/Zotero/storage/S6YPP7QF/Suzgun et al. - 2022 - Challenging BIG-Bench Tasks and Whether Chain-of-T.pdf:application/pdf},
}

@misc{cai_large_2023,
	title = {Large {Language} {Models} as {Tool} {Makers}},
	url = {http://arxiv.org/abs/2305.17126},
	abstract = {Recent research shows the potential of enhancing the problem-solving ability of large language models (LLMs) through the use of external tools. However, prior work along this line depends on the availability of existing tools. In this work, we take an initial step towards removing this dependency by proposing a closed-loop framework, referred to as LLMs As Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two key phases: 1) tool making: an LLM acts as the tool maker that crafts tools for given tasks, where a tool is implemented as a Python utility function. 2) tool using: an LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. The tool user can be either the same or a different LLM from the tool maker. Tool-making enables an LLM to continually generate tools that can be applied to different requests so that future requests can call the corresponding APIs when beneficial for solving the tasks. Furthermore, the division of labor among LLMs for tool-making and tool-using phases introduces the opportunity to achieve cost effectiveness without degrading the quality of generated tools and problem solutions. For example, recognizing that tool-making demands more sophisticated capabilities than tool-using, we can apply a powerful yet resource-intensive model as the tool maker, and a lightweight while cost-effective model as the tool user. We validate the effectiveness of our approach across a variety of complex reasoning tasks, including Big-Bench tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool user, LATM can achieve performance that is on par with using GPT-4 for both tool making and tool using, while the inference cost is significantly reduced.},
	language = {en},
	urldate = {2023-06-18},
	publisher = {arXiv},
	author = {Cai, Tianle and Wang, Xuezhi and Ma, Tengyu and Chen, Xinyun and Zhou, Denny},
	month = may,
	year = {2023},
	note = {arXiv:2305.17126 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Code available at https://github.com/ctlllll/LLM-ToolMaker},
	file = {Cai et al. - 2023 - Large Language Models as Tool Makers.pdf:/home/zwerg/Zotero/storage/VU5ZE5ZY/Cai et al. - 2023 - Large Language Models as Tool Makers.pdf:application/pdf},
}

@misc{noauthor_falcon_nodate,
	title = {Falcon {LLM}},
	url = {https://falconllm.tii.ae/},
	abstract = {Falcon LLM, a foundational large language model (LLM) with 40 billion parameters},
	language = {en},
	urldate = {2023-06-20},
	file = {Snapshot:/home/zwerg/Zotero/storage/TL4E6R98/falconllm.tii.ae.html:text/html},
}

@misc{tiu_understanding_2021,
	title = {Understanding {Contrastive} {Learning}},
	url = {https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607},
	abstract = {Learn how to learn without labels.},
	language = {en},
	urldate = {2023-06-23},
	journal = {Medium},
	author = {Tiu, Ekin},
	month = jan,
	year = {2021},
	file = {Snapshot:/home/zwerg/Zotero/storage/4AU7K6V9/understanding-contrastive-learning-d5b19fd96607.html:text/html},
}

@misc{tiu_understanding_2021-1,
	title = {Understanding {Zero}-{Shot} {Learning} — {Making} {ML} {More} {Human}},
	url = {https://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccab},
	abstract = {An intuitive overview of how a model can recognize what it hasn’t seen.},
	language = {en},
	urldate = {2023-06-23},
	journal = {Medium},
	author = {Tiu, Ekin},
	month = jul,
	year = {2021},
	file = {Snapshot:/home/zwerg/Zotero/storage/KSC3G2LE/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccab.html:text/html},
}

@misc{hayasaka_how_2022,
	title = {How {Many} {Clusters}?},
	url = {https://towardsdatascience.com/how-many-clusters-6b3f220f0ef5},
	abstract = {Methods for choosing the right number of clusters},
	language = {en},
	urldate = {2023-06-23},
	journal = {Medium},
	author = {Hayasaka, Satoru},
	month = feb,
	year = {2022},
	file = {Snapshot:/home/zwerg/Zotero/storage/NLQQHDRW/how-many-clusters-6b3f220f0ef5.html:text/html},
}

@article{chen_speed_2023,
	title = {Speed {Is} {All} {You} {Need}: {On}-{Device} {Acceleration} of {Large} {Diffusion} {Models} via {GPU}-{Aware} {Optimizations}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Speed {Is} {All} {You} {Need}},
	url = {https://arxiv.org/abs/2304.11267},
	doi = {10.48550/ARXIV.2304.11267},
	abstract = {The rapid development and application of foundation models have revolutionized the field of artificial intelligence. Large diffusion models have gained significant attention for their ability to generate photorealistic images and support various tasks. On-device deployment of these models provides benefits such as lower server costs, offline functionality, and improved user privacy. However, common large diffusion models have over 1 billion parameters and pose challenges due to restricted computational and memory resources on devices. We present a series of implementation optimizations for large diffusion models that achieve the fastest reported inference latency to-date (under 12 seconds for Stable Diffusion 1.4 without int8 quantization on Samsung S23 Ultra for a 512x512 image with 20 iterations) on GPU-equipped mobile devices. These enhancements broaden the applicability of generative AI and improve the overall user experience across a wide range of devices.},
	urldate = {2023-06-29},
	author = {Chen, Yu-Hui and Sarokin, Raman and Lee, Juhyun and Tang, Jiuqiang and Chang, Chuo-Ling and Kulik, Andrei and Grundmann, Matthias},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG), FOS: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.IV)},
	annote = {Other
4 pages (not including references), 2 figures, 2 tables. Accepted to Efficient Deep Learning for Computer Vision workshop 2023},
}

@misc{chen_speed_2023-1,
	title = {Speed {Is} {All} {You} {Need}: {On}-{Device} {Acceleration} of {Large} {Diffusion} {Models} via {GPU}-{Aware} {Optimizations}},
	shorttitle = {Speed {Is} {All} {You} {Need}},
	url = {https://arxiv.org/abs/2304.11267v2},
	abstract = {The rapid development and application of foundation models have revolutionized the field of artificial intelligence. Large diffusion models have gained significant attention for their ability to generate photorealistic images and support various tasks. On-device deployment of these models provides benefits such as lower server costs, offline functionality, and improved user privacy. However, common large diffusion models have over 1 billion parameters and pose challenges due to restricted computational and memory resources on devices. We present a series of implementation optimizations for large diffusion models that achieve the fastest reported inference latency to-date (under 12 seconds for Stable Diffusion 1.4 without int8 quantization on Samsung S23 Ultra for a 512x512 image with 20 iterations) on GPU-equipped mobile devices. These enhancements broaden the applicability of generative AI and improve the overall user experience across a wide range of devices.},
	language = {en},
	urldate = {2023-06-29},
	journal = {arXiv.org},
	author = {Chen, Yu-Hui and Sarokin, Raman and Lee, Juhyun and Tang, Jiuqiang and Chang, Chuo-Ling and Kulik, Andrei and Grundmann, Matthias},
	month = apr,
	year = {2023},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/X89WESD3/Chen et al. - 2023 - Speed Is All You Need On-Device Acceleration of L.pdf:application/pdf},
}

@article{pineda_geometric_2023,
	title = {Geometric deep learning reveals the spatiotemporal features of microscopic motion},
	volume = {5},
	copyright = {2023 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-022-00595-0},
	doi = {10.1038/s42256-022-00595-0},
	abstract = {The characterization of dynamical processes in living systems provides important clues for their mechanistic interpretation and link to biological functions. Owing to recent advances in microscopy techniques, it is now possible to routinely record the motion of cells, organelles and individual molecules at multiple spatiotemporal scales in physiological conditions. However, the automated analysis of dynamics occurring in crowded and complex environments still lags behind the acquisition of microscopic image sequences. Here we present a framework based on geometric deep learning that achieves the accurate estimation of dynamical properties in various biologically relevant scenarios. This deep-learning approach relies on a graph neural network enhanced by attention-based components. By processing object features with geometric priors, the network is capable of performing multiple tasks, from linking coordinates into trajectories to inferring local and global dynamic properties. We demonstrate the flexibility and reliability of this approach by applying it to real and simulated data corresponding to a broad range of biological experiments.},
	language = {en},
	number = {1},
	urldate = {2023-06-28},
	journal = {Nature Machine Intelligence},
	author = {Pineda, Jesús and Midtvedt, Benjamin and Bachimanchi, Harshith and Noé, Sergio and Midtvedt, Daniel and Volpe, Giovanni and Manzo, Carlo},
	month = jan,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Cellular imaging, Cell migration, Super-resolution microscopy, Single-molecule biophysics},
	pages = {71--82},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/C2SJD473/Pineda et al. - 2023 - Geometric deep learning reveals the spatiotemporal.pdf:application/pdf},
}

@article{xu_cross-modality_2023,
	title = {Cross-modality supervised image restoration enables nanoscale tracking of synaptic plasticity in living mice},
	volume = {20},
	copyright = {2023 The Author(s)},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-023-01871-6},
	doi = {10.1038/s41592-023-01871-6},
	abstract = {Learning is thought to involve changes in glutamate receptors at synapses, submicron structures that mediate communication between neurons in the central nervous system. Due to their small size and high density, synapses are difficult to resolve in vivo, limiting our ability to directly relate receptor dynamics to animal behavior. Here we developed a combination of computational and biological methods to overcome these challenges. First, we trained a deep-learning image-restoration algorithm that combines the advantages of ex vivo super-resolution and in vivo imaging modalities to overcome limitations specific to each optical system. When applied to in vivo images from transgenic mice expressing fluorescently labeled glutamate receptors, this restoration algorithm super-resolved synapses, enabling the tracking of behavior-associated synaptic plasticity with high spatial resolution. This method demonstrates the capabilities of image enhancement to learn from ex vivo data and imaging techniques to improve in vivo imaging resolution.},
	language = {en},
	number = {6},
	urldate = {2023-06-28},
	journal = {Nature Methods},
	author = {Xu, Yu Kang T. and Graves, Austin R. and Coste, Gabrielle I. and Huganir, Richard L. and Bergles, Dwight E. and Charles, Adam S. and Sulam, Jeremias},
	month = jun,
	year = {2023},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning, Optical imaging, Mouse, Synaptic plasticity},
	pages = {935--944},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/TJ2EACUZ/Xu et al. - 2023 - Cross-modality supervised image restoration enable.pdf:application/pdf},
}

@inproceedings{jain_oneformer_2023,
	title = {{OneFormer}: {One} {Transformer} {To} {Rule} {Universal} {Image} {Segmentation}},
	shorttitle = {{OneFormer}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Jain_OneFormer_One_Transformer_To_Rule_Universal_Image_Segmentation_CVPR_2023_paper.html?ref=blog.roboflow.com},
	language = {en},
	urldate = {2023-06-28},
	author = {Jain, Jitesh and Li, Jiachen and Chiu, Mang Tik and Hassani, Ali and Orlov, Nikita and Shi, Humphrey},
	year = {2023},
	pages = {2989--2998},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/6M4BHF2D/Jain et al. - 2023 - OneFormer One Transformer To Rule Universal Image.pdf:application/pdf},
}

@misc{mehrizi_multi-omics_2023,
	title = {Multi-omics {Prediction} from {High}-content {Cellular} {Imaging} with {Deep} {Learning}},
	url = {http://arxiv.org/abs/2306.09391},
	abstract = {High-content cellular imaging, transcriptomics, and proteomics data provide rich and complementary views on the molecular layers of biology that influence cellular states and function. However, the biological determinants through which changes in multi-omics measurements influence cellular morphology have not yet been systematically explored, and the degree to which cell imaging could potentially enable the prediction of multi-omics directly from cell imaging data is therefore currently unclear. Here, we address the question of whether it is possible to predict bulk multi-omics measurements directly from cell images using Image2Omics -- a deep learning approach that predicts multi-omics in a cell population directly from high-content images stained with multiplexed fluorescent dyes. We perform an experimental evaluation in gene-edited macrophages derived from human induced pluripotent stem cell (hiPSC) under multiple stimulation conditions and demonstrate that Image2Omics achieves significantly better performance in predicting transcriptomics and proteomics measurements directly from cell images than predictors based on the mean observed training set abundance. We observed significant predictability of abundances for 5903 (22.43\%; 95\% CI: 8.77\%, 38.88\%) and 5819 (22.11\%; 95\% CI: 10.40\%, 38.08\%) transcripts out of 26137 in M1 and M2-stimulated macrophages respectively and for 1933 (38.77\%; 95\% CI: 36.94\%, 39.85\%) and 2055 (41.22\%; 95\% CI: 39.31\%, 42.42\%) proteins out of 4986 in M1 and M2-stimulated macrophages respectively. Our results show that some transcript and protein abundances are predictable from cell imaging and that cell imaging may potentially, in some settings and depending on the mechanisms of interest and desired performance threshold, even be a scalable and resource-efficient substitute for multi-omics measurements.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Mehrizi, Rahil and Mehrjou, Arash and Alegro, Maryana and Zhao, Yi and Carbone, Benedetta and Fishwick, Carl and Vappiani, Johanna and Bi, Jing and Sanford, Siobhan and Keles, Hakan and Bantscheff, Marcus and Nguyen, Cuong and Schwab, Patrick},
	month = jun,
	year = {2023},
	note = {arXiv:2306.09391 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Quantitative Methods, Quantitative Biology - Genomics},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/U2CLRK95/2306.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/G8IDZYF5/Mehrizi et al. - 2023 - Multi-omics Prediction from High-content Cellular .pdf:application/pdf},
}

@misc{gal_image_2022-1,
	title = {An {Image} is {Worth} {One} {Word}: {Personalizing} {Text}-to-{Image} {Generation} using {Textual} {Inversion}},
	shorttitle = {An {Image} is {Worth} {One} {Word}},
	url = {http://arxiv.org/abs/2208.01618},
	doi = {10.48550/arXiv.2208.01618},
	abstract = {Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new "words" in the embedding space of a frozen text-to-image model. These "words" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available at: https://textual-inversion.github.io},
	urldate = {2023-07-04},
	publisher = {arXiv},
	author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},
	month = aug,
	year = {2022},
	note = {arXiv:2208.01618 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Graphics},
	annote = {Comment: Project page: https://textual-inversion.github.io},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/HWI433KI/Gal et al. - 2022 - An Image is Worth One Word Personalizing Text-to-.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/UGKF3248/2208.html:text/html},
}

@article{gal_image_2022-2,
	title = {An {Image} is {Worth} {One} {Word}: {Personalizing} {Text}-to-{Image} {Generation} using {Textual} {Inversion}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {An {Image} is {Worth} {One} {Word}},
	url = {https://arxiv.org/abs/2208.01618},
	doi = {10.48550/ARXIV.2208.01618},
	abstract = {Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new "words" in the embedding space of a frozen text-to-image model. These "words" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available at: https://textual-inversion.github.io},
	urldate = {2023-07-04},
	author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG), Computation and Language (cs.CL), Graphics (cs.GR)},
	annote = {Other
Project page: https://textual-inversion.github.io},
}

@misc{bousmalis_robocat_2023,
	title = {{RoboCat}: {A} {Self}-{Improving} {Foundation} {Agent} for {Robotic} {Manipulation}},
	shorttitle = {{RoboCat}},
	url = {http://arxiv.org/abs/2306.11706},
	abstract = {The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a foundation agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming multi-embodiment action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100--1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent's capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks.},
	language = {en},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Bousmalis, Konstantinos and Vezzani, Giulia and Rao, Dushyant and Devin, Coline and Lee, Alex X. and Bauza, Maria and Davchev, Todor and Zhou, Yuxiang and Gupta, Agrim and Raju, Akhil and Laurens, Antoine and Fantacci, Claudio and Dalibard, Valentin and Zambelli, Martina and Martins, Murilo and Pevceviciute, Rugile and Blokzijl, Michiel and Denil, Misha and Batchelor, Nathan and Lampe, Thomas and Parisotto, Emilio and Żołna, Konrad and Reed, Scott and Colmenarejo, Sergio Gómez and Scholz, Jon and Abdolmaleki, Abbas and Groth, Oliver and Regli, Jean-Baptiste and Sushkov, Oleg and Rothörl, Tom and Chen, José Enrique and Aytar, Yusuf and Barker, Dave and Ortiz, Joy and Riedmiller, Martin and Springenberg, Jost Tobias and Hadsell, Raia and Nori, Francesco and Heess, Nicolas},
	month = jun,
	year = {2023},
	note = {arXiv:2306.11706 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Bousmalis et al. - 2023 - RoboCat A Self-Improving Foundation Agent for Rob.pdf:/home/zwerg/Zotero/storage/SID5NY25/Bousmalis et al. - 2023 - RoboCat A Self-Improving Foundation Agent for Rob.pdf:application/pdf},
}

@misc{cui_scgpt_2023,
	title = {{scGPT}: {Towards} {Building} a {Foundation} {Model} for {Single}-{Cell} {Multi}-omics {Using} {Generative} {AI}},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{scGPT}},
	url = {https://www.biorxiv.org/content/10.1101/2023.04.30.538439v2},
	doi = {10.1101/2023.04.30.538439},
	abstract = {Generative pre-trained models have achieved remarkable success in various domains such as natural language processing and computer vision. Specifically, the combination of large-scale diverse datasets and pre-trained transformers has emerged as a promising approach for developing foundation models. Drawing parallels between linguistic constructs and cellular biology - where texts comprise words, similarly, cells are defined by genes - our study probes the applicability of foundation models to advance cellular biology and genetics research. Utilizing the burgeoning single-cell sequencing data, we have pioneered the construction of a foundation model for single-cell biology, scGPT, which is based on generative pre-trained transformer across a repository of over 33 million cells. Our findings illustrate that scGPT, a generative pre-trained transformer, effectively distills critical biological insights concerning genes and cells. Through the further adaptation of transfer learning, scGPT can be optimized to achieve superior performance across diverse downstream applications. This includes tasks such as cell-type annotation, multi-batch integration, multi-omic integration, genetic perturbation prediction, and gene network inference. The scGPT codebase is publicly available at https://github.com/bowang-lab/scGPT.},
	language = {en},
	urldate = {2023-07-10},
	publisher = {bioRxiv},
	author = {Cui, Haotian and Wang, Chloe and Maan, Hassaan and Pang, Kuan and Luo, Fengning and Wang, Bo},
	month = jul,
	year = {2023},
	note = {Pages: 2023.04.30.538439
Section: New Results},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/ICMBACB6/Cui et al. - 2023 - scGPT Towards Building a Foundation Model for Sin.pdf:application/pdf},
}

@misc{candel_h2ogpt_2023,
	title = {{h2oGPT}: {Democratizing} {Large} {Language} {Models}},
	shorttitle = {{h2oGPT}},
	url = {http://arxiv.org/abs/2306.08161},
	doi = {10.48550/arXiv.2306.08161},
	abstract = {Applications built on top of Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their human-level capabilities in natural language processing. However, they also pose many significant risks such as the presence of biased, private, or harmful text, and the unauthorized inclusion of copyrighted material. We introduce h2oGPT, a suite of open-source code repositories for the creation and use of LLMs based on Generative Pretrained Transformers (GPTs). The goal of this project is to create the world's best truly open-source alternative to closed-source approaches. In collaboration with and as part of the incredible and unstoppable open-source community, we open-source several fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial use under fully permissive Apache 2.0 licenses. Included in our release is 100{\textbackslash}\% private document search using natural language. Open-source language models help boost AI development and make it more accessible and trustworthy. They lower entry hurdles, allowing people and groups to tailor these models to their needs. This openness increases innovation, transparency, and fairness. An open-source strategy is needed to share AI benefits fairly, and H2O.ai will continue to democratize AI and LLMs.},
	urldate = {2023-07-13},
	author = {Candel, Arno and McKinney, Jon and Singer, Philipp and Pfeiffer, Pascal and Jeblick, Maximilian and Prabhu, Prithvi and Gambera, Jeff and Landry, Mark and Bansal, Shivam and Chesler, Ryan and Lee, Chun Ming and Conde, Marcos V. and Stetsenko, Pasha and Grellier, Olivier and Ambati, SriSatish},
	month = jun,
	year = {2023},
	note = {arXiv:2306.08161 [cs]
version: 2},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Information Retrieval},
	file = {Candel et al. - 2023 - h2oGPT Democratizing Large Language Models.pdf:/home/zwerg/Zotero/storage/DAT63N78/Candel et al. - 2023 - h2oGPT Democratizing Large Language Models.pdf:application/pdf},
}

@article{marx_share_2023,
	title = {To share is to be a scientist},
	volume = {20},
	copyright = {2023 Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-023-01927-7},
	doi = {10.1038/s41592-023-01927-7},
	abstract = {Wrangling big data is now part of being a biomedical scientist, and mandates on data sharing have entered the scene. Mandates can alter behavior, but data sharing also needs incentives and shifts in science culture.},
	language = {en},
	number = {7},
	urldate = {2023-07-12},
	journal = {Nature Methods},
	author = {Marx, Vivien},
	month = jul,
	year = {2023},
	note = {Number: 7
Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Neuroscience, Scientific community},
	pages = {984--989},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/YZGP32F9/Marx - 2023 - To share is to be a scientist.pdf:application/pdf},
}

@misc{httpswwwcloudbookletcom_fast_2023,
	title = {Fast {SAM}: {The} {Easy} {Way} to {Segment} {Anything} - {Cloudbooklet}},
	shorttitle = {Fast {SAM}},
	url = {https://www.cloudbooklet.com/fast-sam-the-easy-way-to-segment-anything/},
	abstract = {Fast SAM simplifies segmentation with its advanced capabilities and intuitive interface. Achieve accurate results effortlessly, with fast inference times and efficient memory usage.},
	language = {en-US},
	urldate = {2023-07-18},
	author = {https://www.cloudbooklet.com and https://facebook.com/cloudbooklet},
	month = jun,
	year = {2023},
	note = {Section: Artificial Intelligence},
	file = {Snapshot:/home/zwerg/Zotero/storage/IGC5X9MG/fast-sam-the-easy-way-to-segment-anything.html:text/html},
}

@misc{noauthor_using_nodate-1,
	title = {Using {Meta}'s {Segment} {Anything} ({SAM}) model on video with {Labelbox}'s model-assisted labeling},
	url = {https://labelbox.com/guides/using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling/},
	urldate = {2023-07-18},
	file = {Using Meta's Segment Anything (SAM) model on video with Labelbox's model-assisted labeling:/home/zwerg/Zotero/storage/Y2C7NG9H/using-metas-segment-anything-sam-model-on-video-with-labelbox-model-assisted-labeling.html:text/html},
}

@misc{yu_segment_2023,
	title = {Segment {Anything} {Meets} {Point} {Tracking}},
	url = {https://www.vis.xyz/pub/sam-pt/},
	abstract = {Extending SAM to video segmentation with point-based tracking to demonstrate strong zero-shot performance across popular video segmentation benchmarks.},
	language = {en},
	urldate = {2023-07-18},
	author = {Yu, Fisher},
	month = jul,
	year = {2023},
	note = {Section: pub},
	file = {Snapshot:/home/zwerg/Zotero/storage/W6TSEEJF/sam-pt.html:text/html},
}

@misc{terven_comprehensive_2023,
	title = {A {Comprehensive} {Review} of {YOLO}: {From} {YOLOv1} and {Beyond}},
	shorttitle = {A {Comprehensive} {Review} of {YOLO}},
	url = {http://arxiv.org/abs/2304.00501},
	doi = {10.48550/arXiv.2304.00501},
	abstract = {YOLO has become a central real-time object detection system for robotics, driverless cars, and video monitoring applications. We present a comprehensive analysis of YOLO's evolution, examining the innovations and contributions in each iteration from the original YOLO to YOLOv8 and YOLO-NAS. We start by describing the standard metrics and postprocessing; then, we discuss the major changes in network architecture and training tricks for each model. Finally, we summarize the essential lessons from YOLO's development and provide a perspective on its future, highlighting potential research directions to enhance real-time object detection systems.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Terven, Juan and Cordova-Esparza, Diana},
	month = jun,
	year = {2023},
	note = {arXiv:2304.00501 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 33 pages, 18 figures, 4 tables, submitted to ACM Computing Surveys. This version adds detailed diagrams for YOLOv6, YOLOv7, and PP-YOLOE},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/5Y52M29Z/Terven and Cordova-Esparza - 2023 - A Comprehensive Review of YOLO From YOLOv1 and Beyond.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/IUVI4NR9/2304.html:text/html},
}

@misc{zhang_meta-transformer_2023,
	title = {Meta-{Transformer}: {A} {Unified} {Framework} for {Multimodal} {Learning}},
	shorttitle = {Meta-{Transformer}},
	url = {http://arxiv.org/abs/2307.10802},
	doi = {10.48550/arXiv.2307.10802},
	abstract = {Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities (\${\textbackslash}textit\{e.g.\}\$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a \${\textbackslash}textbf\{frozen\}\$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modalities with unpaired data. Experiments on different benchmarks reveal that Meta-Transformer can handle a wide range of tasks including fundamental perception (text, image, point cloud, audio, video), practical application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph, tabular, and time-series). Meta-Transformer indicates a promising future for developing unified multimodal intelligence with transformers. Code will be available at https://github.com/invictus717/MetaTransformer},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Zhang, Yiyuan and Gong, Kaixiong and Zhang, Kaipeng and Li, Hongsheng and Qiao, Yu and Ouyang, Wanli and Yue, Xiangyu},
	month = jul,
	year = {2023},
	note = {arXiv:2307.10802 [cs]
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Multimedia},
	annote = {Comment: Project website: https://kxgong.github.io/meta\_transformer/},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/PXQDEIVC/Zhang et al. - 2023 - Meta-Transformer A Unified Framework for Multimod.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/RHG9MRRE/2307.html:text/html},
}

@article{nyandwi_transformer_2023,
	title = {The {Transformer} {Blueprint}: {A} {Holistic} {Guide} to the {Transformer} {Neural} {Network} {Architecture}},
	shorttitle = {The {Transformer} {Blueprint}},
	url = {https://deeprevision.github.io/posts/001-transformer/},
	abstract = {A deep dive into Transformer a neural network architecture that was introduced in the famous paper “attention is all you need” in 2017, its applications, impacts, challenges and future directions},
	language = {en},
	urldate = {2023-08-13},
	journal = {Deep Learning Revision},
	author = {Nyandwi, Jean},
	month = jul,
	year = {2023},
}

@article{nyandwi_transformer_2023-1,
	title = {The {Transformer} {Blueprint}: {A} {Holistic} {Guide} to the {Transformer} {Neural} {Network} {Architecture}},
	shorttitle = {The {Transformer} {Blueprint}},
	url = {https://deeprevision.github.io/posts/001-transformer/},
	abstract = {A deep dive into Transformer a neural network architecture that was introduced in the famous paper “attention is all you need” in 2017, its applications, impacts, challenges and future directions},
	language = {en},
	urldate = {2023-09-09},
	journal = {Deep Learning Revision},
	author = {Nyandwi, Jean},
	month = jul,
	year = {2023},
}

@article{radford_improving_nodate,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	file = {Radford et al. - Improving Language Understanding by Generative Pre-Training.pdf:/home/zwerg/Zotero/storage/6B3UASS7/Radford et al. - Improving Language Understanding by Generative Pre-Training.pdf:application/pdf},
}

@misc{alammar_illustrated_nodate-3,
	title = {The {Illustrated} {Transformer}},
	url = {https://jalammar.github.io/illustrated-transformer/},
	abstract = {Discussions:
Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)


Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese

Watch: MIT’s Deep Learning State of the Art lecture referencing this post

In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.

The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.

2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:




A High-Level Look
Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.},
	urldate = {2023-09-08},
	author = {Alammar, Jay},
	file = {Snapshot:/home/zwerg/Zotero/storage/TI28QT6B/illustrated-transformer.html:text/html},
}

@misc{noauthor_gpt-who_nodate,
	title = {{GPT}-{Who}? {Exploring} the history of {GPT}},
	url = {https://bpben.github.io/2020/10/28/gpt_explore/},
	urldate = {2023-09-08},
	file = {GPT-Who? Exploring the history of GPT:/home/zwerg/Zotero/storage/XKXCVL82/gpt_explore.html:text/html},
}

@misc{sun_videobert_2019,
	title = {{VideoBERT}: {A} {Joint} {Model} for {Video} and {Language} {Representation} {Learning}},
	shorttitle = {{VideoBERT}},
	url = {http://arxiv.org/abs/1904.01766},
	doi = {10.48550/arXiv.1904.01766},
	abstract = {Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.},
	urldate = {2023-09-08},
	author = {Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia},
	month = sep,
	year = {2019},
	note = {arXiv:1904.01766 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Sun et al. - 2019 - VideoBERT A Joint Model for Video and Language Representation Learning.pdf:/home/zwerg/Zotero/storage/D3DRX724/Sun et al. - 2019 - VideoBERT A Joint Model for Video and Language Representation Learning.pdf:application/pdf},
}

@misc{archit_probabilistic_2023,
	title = {Probabilistic {Domain} {Adaptation} for {Biomedical} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/2303.11790},
	doi = {10.48550/arXiv.2303.11790},
	abstract = {Segmentation is a key analysis tasks in biomedical imaging. Given the many different experimental settings in this field, the lack of generalization limits the use of deep learning in practice. Domain adaptation is a promising remedy: it trains a model for a given task on a source dataset with labels and adapts it to a target dataset without additional labels. We introduce a probabilistic domain adaptation method, building on self-training approaches and the Probabilistic UNet. We use the latter to sample multiple segmentation hypothesis to implement better pseudo-label filtering. We further study joint and separate source-target training strategies and evaluate our method on three challenging domain adaptation tasks for biomedical segmentation.},
	urldate = {2023-09-14},
	publisher = {arXiv},
	author = {Archit, Anwai and Pape, Constantin},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11790 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/SZIXCCH9/Archit and Pape - 2023 - Probabilistic Domain Adaptation for Biomedical Image Segmentation.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/RABLEDCX/2303.html:text/html},
}

@misc{lester_power_2021,
	title = {The {Power} of {Scale} for {Parameter}-{Efficient} {Prompt} {Tuning}},
	url = {http://arxiv.org/abs/2104.08691},
	abstract = {In this work, we explore "prompt tuning", a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's "few-shot" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed "prefix tuning" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
	month = sep,
	year = {2021},
	note = {arXiv:2104.08691 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to EMNLP 2021},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/C5A7EVG8/2104.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/MZI4J4NT/Lester et al. - 2021 - The Power of Scale for Parameter-Efficient Prompt Tuning.pdf:application/pdf},
}

@misc{dettmers_qlora_2023,
	title = {{QLoRA}: {Efficient} {Finetuning} of {Quantized} {LLMs}},
	shorttitle = {{QLoRA}},
	url = {http://arxiv.org/abs/2305.14314},
	abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	month = may,
	year = {2023},
	note = {arXiv:2305.14314 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Extended NeurIPS submission},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/N9PKI48A/2305.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/XJW8WFN2/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf:application/pdf},
}

@misc{hu_lora_2021-1,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/FW3TBLLV/2106.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/LWX5MHEJ/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf},
}

@misc{fu_effectiveness_2022,
	title = {On the {Effectiveness} of {Parameter}-{Efficient} {Fine}-{Tuning}},
	url = {http://arxiv.org/abs/2211.15583},
	abstract = {Fine-tuning pre-trained models has been ubiquitously proven to be effective in a wide range of NLP tasks. However, fine-tuning the whole model is parameter inefficient as it always yields an entirely new model for each task. Currently, many research works propose to only fine-tune a small portion of the parameters while keeping most of the parameters shared across different tasks. These methods achieve surprisingly good performance and are shown to be more stable than their corresponding fully fine-tuned counterparts. However, such kind of methods is still not well understood. Some natural questions arise: How does the parameter sparsity lead to promising performance? Why is the model more stable than the fully fine-tuned models? How to choose the tunable parameters? In this paper, we first categorize the existing methods into random approaches, rule-based approaches, and projection-based approaches based on how they choose which parameters to tune. Then, we show that all of the methods are actually sparse fine-tuned models and conduct a novel theoretical analysis of them. We indicate that the sparsity is actually imposing a regularization on the original model by controlling the upper bound of the stability. Such stability leads to better generalization capability which has been empirically observed in a lot of recent research works. Despite the effectiveness of sparsity grounded by our theory, it still remains an open problem of how to choose the tunable parameters. To better choose the tunable parameters, we propose a novel Second-order Approximation Method (SAM) which approximates the original problem with an analytically solvable optimization function. The tunable parameters are determined by directly optimizing the approximation function. The experimental results show that our proposed SAM model outperforms many strong baseline models and it also verifies our theoretical analysis.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Fu, Zihao and Yang, Haoran and So, Anthony Man-Cho and Lam, Wai and Bing, Lidong and Collier, Nigel},
	month = nov,
	year = {2022},
	note = {arXiv:2211.15583 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/FEVZMAK7/2211.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/SRJ7FZEY/Fu et al. - 2022 - On the Effectiveness of Parameter-Efficient Fine-Tuning.pdf:application/pdf},
}

@misc{lialin_scaling_2023,
	title = {Scaling {Down} to {Scale} {Up}: {A} {Guide} to {Parameter}-{Efficient} {Fine}-{Tuning}},
	shorttitle = {Scaling {Down} to {Scale} {Up}},
	url = {http://arxiv.org/abs/2303.15647},
	abstract = {This paper presents a systematic overview and comparison of parameter-efficient fine-tuning methods covering over 40 papers published between February 2019 and February 2023. These methods aim to resolve the infeasibility and impracticality of fine-tuning large language models by only training a small set of parameters. We provide a taxonomy that covers a broad range of methods and present a detailed method comparison with a specific focus on real-life efficiency and fine-tuning multibillion-scale language models.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
	month = mar,
	year = {2023},
	note = {arXiv:2303.15647 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/7CHGSHB6/2303.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/2H8IDICJ/Lialin et al. - 2023 - Scaling Down to Scale Up A Guide to Parameter-Efficient Fine-Tuning.pdf:application/pdf},
}

@misc{srivastava_beyond_2023,
	title = {Beyond the {Imitation} {Game}: {Quantifying} and extrapolating the capabilities of language models},
	shorttitle = {Beyond the {Imitation} {Game}},
	url = {http://arxiv.org/abs/2206.04615},
	abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlmüller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karakaş, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bartłomiej and Özyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Orinion, Bryan and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ramírez, César Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and González, Daniel Moseguí and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Schrader, Dylan and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Martínez-Plumed, Fernando and Happé, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and de Melo, Gerard and Kruszewski, Germán and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and Jaimovitch-López, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Schütze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fernández and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Kocoń, Jan and Thompson, Jana and Wingfield, Janelle and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Batchelder, Jonathan and Berant, Jonathan and Frohberg, Jörg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Guerr, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Colón, Luis Oliveros and Metz, Luke and Şenel, Lütfi Kerem and Bosma, Maarten and Sap, Maarten and ter Hoeve, Maartje and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ramírez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, Mátyás and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Swędrowski, Michał and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Walker, Mitch and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan A. and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Martinez, Nicole and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Miłkowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Risco, Ramon and Millière, Raphaël and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Théo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
	month = jun,
	year = {2023},
	note = {arXiv:2206.04615 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
	annote = {Comment: 27 pages, 17 figures + references and appendices, repo: https://github.com/google/BIG-bench},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/WXPZZKKM/2206.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/YUUNQR2W/Srivastava et al. - 2023 - Beyond the Imitation Game Quantifying and extrapolating the capabilities of language models.pdf:application/pdf},
}

@misc{hendrycks_measuring_2021,
	title = {Measuring {Massive} {Multitask} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2009.03300},
	abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	month = jan,
	year = {2021},
	note = {arXiv:2009.03300 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
	annote = {Comment: ICLR 2021; the test and code is available at https://github.com/hendrycks/test},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/26N9LJKS/2009.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/SR6JC8Q9/Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf:application/pdf},
}

@inproceedings{lin_rouge_2004,
	address = {Barcelona, Spain},
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	shorttitle = {{ROUGE}},
	url = {https://aclanthology.org/W04-1013},
	urldate = {2023-09-19},
	booktitle = {Text {Summarization} {Branches} {Out}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew},
	month = jul,
	year = {2004},
	pages = {74--81},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/7K6SLLMK/Lin - 2004 - ROUGE A Package for Automatic Evaluation of Summaries.pdf:application/pdf},
}

@misc{noauthor_superglue_nodate,
	title = {{SuperGLUE} {Benchmark}},
	url = {https://super.gluebenchmark.com/},
	abstract = {SuperGLUE is a new benchmark styled after original GLUE benchmark with a set of more difficult language understanding tasks, improved resources, and a new public leaderboard.},
	language = {en},
	urldate = {2023-09-19},
	journal = {SuperGLUE Benchmark},
	file = {Snapshot:/home/zwerg/Zotero/storage/VUTMGCFL/super.gluebenchmark.com.html:text/html},
}

@misc{liang_holistic_2022,
	title = {Holistic {Evaluation} of {Language} {Models}},
	url = {http://arxiv.org/abs/2211.09110},
	abstract = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5\% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9\% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and Ré, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
	month = nov,
	year = {2022},
	note = {arXiv:2211.09110 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Authored by the Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered Artificial Intelligence (HAI). Project page: https://crfm.stanford.edu/helm/v1.0},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/BZPGX4U2/2211.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/BWGLFS6F/Liang et al. - 2022 - Holistic Evaluation of Language Models.pdf:application/pdf},
}

@misc{noauthor_holistic_nodate,
	title = {Holistic {Evaluation} of {Language} {Models} ({HELM})},
	url = {https://crfm.stanford.edu/helm/latest/},
	urldate = {2023-09-19},
}

@misc{noauthor_holistic_nodate-1,
	title = {Holistic {Evaluation} of {Language} {Models} ({HELM})},
	url = {https://crfm.stanford.edu/helm/latest/},
	urldate = {2023-09-19},
}

@misc{chung_scaling_2022,
	title = {Scaling {Instruction}-{Finetuned} {Language} {Models}},
	url = {http://arxiv.org/abs/2210.11416},
	abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
	month = dec,
	year = {2022},
	note = {arXiv:2210.11416 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Public checkpoints: https://huggingface.co/docs/transformers/model\_doc/flan-t5},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/8329NKXP/2210.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/YKZ8MSKG/Chung et al. - 2022 - Scaling Instruction-Finetuned Language Models.pdf:application/pdf},
}

@misc{wang_what_2022,
	title = {What {Language} {Model} {Architecture} and {Pretraining} {Objective} {Work} {Best} for {Zero}-{Shot} {Generalization}?},
	url = {http://arxiv.org/abs/2204.05832},
	abstract = {Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Scao, Teven Le and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin},
	month = apr,
	year = {2022},
	note = {arXiv:2204.05832 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/8QS9WX6H/2204.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/HWNH3CZB/Wang et al. - 2022 - What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization.pdf:application/pdf},
}

@misc{workshop_bloom_2023,
	title = {{BLOOM}: {A} {176B}-{Parameter} {Open}-{Access} {Multilingual} {Language} {Model}},
	shorttitle = {{BLOOM}},
	url = {http://arxiv.org/abs/2211.05100},
	doi = {10.48550/arXiv.2211.05100},
	abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ilić, Suzana and Hesslow, Daniel and Castagné, Roman and Luccioni, Alexandra Sasha and Yvon, François and Gallé, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Benoît and Muennighoff, Niklas and del Moral, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and McMillan-Major, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Laurençon, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and van Strien, Daniel and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo González and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, Gérard and Kruszewski, Germán and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and Gonzalez-Dios, Itziar and de la Rosa, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, Jörg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and allal, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Muñoz, Manuel Romero and Masoud, Maraim and Grandury, María and Šaško, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and de Gibert, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and López, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Taşar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M. Saiful and Al-shaibani, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and Ben-David, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and von Platen, Patrick and Cornette, Pierre and Lavallée, Pierre François and Lacroix, Rémi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, Stéphane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and Névéol, Aurélie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and van der Wal, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zdeněk and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Muñoz and McDuff, Daniel and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and Miranda-Escalada, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Clémentine and Periñán, Daniel León and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and de Bykhovetz, Madeleine Hahn and Takeuchi, Maiko and Pàmies, Marc and Castillo, Maria A. and Nezhurina, Marianna and Sänger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S. and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and Sang-aroonsiri, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Théo and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},
	month = jun,
	year = {2023},
	note = {arXiv:2211.05100 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/E4KR3CII/Workshop et al. - 2023 - BLOOM A 176B-Parameter Open-Access Multilingual Language Model.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/TE7TSHXH/2211.html:text/html},
}

@misc{wu_bloomberggpt_2023-1,
	title = {{BloombergGPT}: {A} {Large} {Language} {Model} for {Finance}},
	shorttitle = {{BloombergGPT}},
	url = {http://arxiv.org/abs/2303.17564},
	doi = {10.48550/arXiv.2303.17564},
	abstract = {The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
	month = may,
	year = {2023},
	note = {arXiv:2303.17564 [cs, q-fin]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Quantitative Finance - General Finance},
	annote = {Comment: Updated to include Training Chronicles (Appendix C)},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/A33Z38YT/Wu et al. - 2023 - BloombergGPT A Large Language Model for Finance.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/V2RCNXUF/2303.html:text/html},
}

@misc{taylor_galactica_2022,
	title = {Galactica: {A} {Large} {Language} {Model} for {Science}},
	shorttitle = {Galactica},
	url = {http://arxiv.org/abs/2211.09085},
	doi = {10.48550/arXiv.2211.09085},
	abstract = {Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2\% versus 49.0\%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3\% to 35.7\%, and PaLM 540B on MATH with a score of 20.4\% versus 8.8\%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6\% and 52.9\%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
	month = nov,
	year = {2022},
	note = {arXiv:2211.09085 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/ZMCFAC6H/Taylor et al. - 2022 - Galactica A Large Language Model for Science.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/HMRR36P4/2211.html:text/html},
}

@misc{bahri_explaining_2021,
	title = {Explaining {Neural} {Scaling} {Laws}},
	url = {http://arxiv.org/abs/2102.06701},
	doi = {10.48550/arXiv.2102.06701},
	abstract = {The test loss of well-trained neural networks often follows precise power-law scaling relations with either the size of the training dataset or the number of parameters in the network. We propose a theory that explains and connects these scaling laws. We identify variance-limited and resolution-limited scaling behavior for both dataset and model size, for a total of four scaling regimes. The variance-limited scaling follows simply from the existence of a well-behaved infinite data or infinite width limit, while the resolution-limited regime can be explained by positing that models are effectively resolving a smooth data manifold. In the large width limit, this can be equivalently obtained from the spectrum of certain kernels, and we present evidence that large width and large dataset resolution-limited scaling exponents are related by a duality. We exhibit all four scaling regimes in the controlled setting of large random feature and pretrained models and test the predictions empirically on a range of standard architectures and datasets. We also observe several empirical relationships between datasets and scaling exponents: super-classing image tasks does not change exponents, while changing input distribution (via changing datasets or adding noise) has a strong effect. We further explore the effect of architecture aspect ratio on scaling exponents.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
	month = feb,
	year = {2021},
	note = {arXiv:2102.06701 [cond-mat, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks},
	annote = {Comment: 11 pages, 5 figures + Supplement},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/FGD53SNF/Bahri et al. - 2021 - Explaining Neural Scaling Laws.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/4SIP6IUK/2102.html:text/html},
}

@misc{hernandez_scaling_2021,
	title = {Scaling {Laws} for {Transfer}},
	url = {http://arxiv.org/abs/2102.01293},
	doi = {10.48550/arXiv.2102.01293},
	abstract = {We study empirical scaling laws for transfer learning between distributions in an unsupervised, fine-tuning setting. When we train increasingly large neural networks from-scratch on a fixed-size dataset, they eventually become data-limited and stop improving in performance (cross-entropy loss). When we do the same for models pre-trained on a large language dataset, the slope in performance gains is merely reduced rather than going to zero. We calculate the effective data "transferred" from pre-training by determining how much data a transformer of the same size would have required to achieve the same loss when training from scratch. In other words, we focus on units of data while holding everything else fixed. We find that the effective data transferred is described well in the low data regime by a power-law of parameter count and fine-tuning dataset size. We believe the exponents in these power-laws correspond to measures of the generality of a model and proximity of distributions (in a directed rather than symmetric sense). We find that pre-training effectively multiplies the fine-tuning dataset size. Transfer, like overall performance, scales predictably in terms of parameters, data, and compute.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Hernandez, Danny and Kaplan, Jared and Henighan, Tom and McCandlish, Sam},
	month = feb,
	year = {2021},
	note = {arXiv:2102.01293 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 19 pages, 15 figures},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/IYLVR4ZV/Hernandez et al. - 2021 - Scaling Laws for Transfer.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/K4RR34DV/2102.html:text/html},
}

@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	doi = {10.48550/arXiv.2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08361 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 19 pages, 15 figures},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/L2V5L5DU/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/D2K9XVXL/2001.html:text/html},
}

@misc{hoffmann_training_2022-1,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	doi = {10.48550/arXiv.2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	month = mar,
	year = {2022},
	note = {arXiv:2203.15556 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/8VPGSIYF/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/ZXNM4VGL/2203.html:text/html},
}

@misc{zhao_pytorch_2023,
	title = {{PyTorch} {FSDP}: {Experiences} on {Scaling} {Fully} {Sharded} {Data} {Parallel}},
	shorttitle = {{PyTorch} {FSDP}},
	url = {https://arxiv.org/abs/2304.11277v2},
	abstract = {It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.},
	language = {en},
	urldate = {2023-09-19},
	journal = {arXiv.org},
	author = {Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and Desmaison, Alban and Balioglu, Can and Damania, Pritam and Nguyen, Bernard and Chauhan, Geeta and Hao, Yuchen and Mathews, Ajit and Li, Shen},
	month = apr,
	year = {2023},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/WS925R73/Zhao et al. - 2023 - PyTorch FSDP Experiences on Scaling Fully Sharded Data Parallel.pdf:application/pdf},
}

@misc{chase_langchain_2022,
	title = {{LangChain}},
	copyright = {MIT},
	url = {https://github.com/hwchase17/langchain},
	abstract = {⚡ Building applications with LLMs through composability ⚡},
	urldate = {2023-09-20},
	author = {Chase, Harrison},
	month = oct,
	year = {2022},
	note = {original-date: 2022-10-17T02:58:36Z},
}

@misc{yao_react_2023,
	title = {{ReAct}: {Synergizing} {Reasoning} and {Acting} in {Language} {Models}},
	shorttitle = {{ReAct}},
	url = {http://arxiv.org/abs/2210.03629},
	doi = {10.48550/arXiv.2210.03629},
	abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
	month = mar,
	year = {2023},
	note = {arXiv:2210.03629 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: v3 is the ICLR camera ready version with some typos fixed. Project site with code: https://react-lm.github.io},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/LIMNSML7/Yao et al. - 2023 - ReAct Synergizing Reasoning and Acting in Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/9Y9CL574/2210.html:text/html},
}

@misc{casado_who_2023,
	title = {Who {Owns} the {Generative} {AI} {Platform}?},
	url = {https://a16z.com/who-owns-the-generative-ai-platform/},
	abstract = {We’re starting to see the very early stages of a tech stack emerge in generative artificial intelligence (AI). Hundreds of new startups are rushing into the market to develop foundation models, build AI-native apps, and stand up infrastructure/tooling. Many hot technology trends get over-hyped far before the market catches up. But the generative AI boom...},
	language = {en},
	urldate = {2023-09-20},
	journal = {Andreessen Horowitz},
	author = {Casado, Guido Appenzeller, Martin, Matt Bornstein},
	month = jan,
	year = {2023},
}

@misc{gao_pal_2023,
	title = {{PAL}: {Program}-aided {Language} {Models}},
	shorttitle = {{PAL}},
	url = {http://arxiv.org/abs/2211.10435},
	doi = {10.48550/arXiv.2211.10435},
	abstract = {Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("few-shot prompting"). Much of this success can be attributed to prompting methods such as "chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15\% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
	month = jan,
	year = {2023},
	note = {arXiv:2211.10435 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: The first three authors contributed equally. Our code and data are publicly available at http://reasonwithpal.com/},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/P535FTCN/Gao et al. - 2023 - PAL Program-aided Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/CWJR88HF/2211.html:text/html},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/E4KV8V3Q/2201.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/3QLQUCHN/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf:application/pdf},
}

@misc{bai_constitutional_2022,
	title = {Constitutional {AI}: {Harmlessness} from {AI} {Feedback}},
	shorttitle = {Constitutional {AI}},
	url = {http://arxiv.org/abs/2212.08073},
	abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
	month = dec,
	year = {2022},
	note = {arXiv:2212.08073 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/U35EVVYS/2212.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/5SRZK9M4/Bai et al. - 2022 - Constitutional AI Harmlessness from AI Feedback.pdf:application/pdf},
}

@misc{rafailov_direct_2023,
	title = {Direct {Preference} {Optimization}: {Your} {Language} {Model} is {Secretly} a {Reward} {Model}},
	shorttitle = {Direct {Preference} {Optimization}},
	url = {http://arxiv.org/abs/2305.18290},
	abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds RLHF's ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
	month = may,
	year = {2023},
	note = {arXiv:2305.18290 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/UEY4G4F4/2305.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/AEFQUJ7D/Rafailov et al. - 2023 - Direct Preference Optimization Your Language Model is Secretly a Reward Model.pdf:application/pdf},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/CVPY34CA/1707.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/L5YQBL6S/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf},
}

@misc{stiennon_learning_2022,
	title = {Learning to summarize from human feedback},
	url = {http://arxiv.org/abs/2009.01325},
	abstract = {As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M. and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul},
	month = feb,
	year = {2022},
	note = {arXiv:2009.01325 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: NeurIPS 2020},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/MBZ5Y3AK/2009.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/J5PXYE9T/Stiennon et al. - 2022 - Learning to summarize from human feedback.pdf:application/pdf},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/RWFL8KDG/2203.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/LW4BGL9I/Ouyang et al. - 2022 - Training language models to follow instructions with human feedback.pdf:application/pdf},
}

@misc{yao_react_2023-1,
	title = {{ReAct}: {Synergizing} {Reasoning} and {Acting} in {Language} {Models}},
	shorttitle = {{ReAct}},
	url = {http://arxiv.org/abs/2210.03629},
	doi = {10.48550/arXiv.2210.03629},
	abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
	month = mar,
	year = {2023},
	note = {arXiv:2210.03629 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: v3 is the ICLR camera ready version with some typos fixed. Project site with code: https://react-lm.github.io},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/RKCGMNAE/Yao et al. - 2023 - ReAct Synergizing Reasoning and Acting in Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/T6VZE6FS/2210.html:text/html},
}

@misc{noauthor_fine-tuning_nodate,
	title = {Fine-tuning {20B} {LLMs} with {RLHF} on a {24GB} consumer {GPU}},
	url = {https://huggingface.co/blog/trl-peft},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-09-20},
	file = {Snapshot:/home/zwerg/Zotero/storage/5XGIGQQR/trl-peft.html:text/html},
}

@misc{noauthor_synergistic_nodate,
	title = {A synergistic future for {AI} and ecology {\textbar} {PNAS}},
	url = {https://www.pnas.org/doi/10.1073/pnas.2220283120},
	urldate = {2023-09-22},
	file = {A synergistic future for AI and ecology | PNAS:/home/zwerg/Zotero/storage/2WVCJ534/pnas.html:text/html},
}

@article{belkin_reconciling_2019,
	title = {Reconciling modern machine-learning practice and the classical bias–variance trade-off},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1903070116},
	doi = {10.1073/pnas.1903070116},
	abstract = {Significance
            While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms.
          , 
            Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
	language = {en},
	number = {32},
	urldate = {2023-09-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	month = aug,
	year = {2019},
	pages = {15849--15854},
	file = {Belkin et al. - 2019 - Reconciling modern machine-learning practice and t.pdf:/home/zwerg/Zotero/storage/MNJZSD8E/Belkin et al. - 2019 - Reconciling modern machine-learning practice and t.pdf:application/pdf;downloadSupplement:/home/zwerg/Zotero/storage/8M3ITT2Y/downloadSupplement:application/pdf},
}

@article{hastie_ridge_2020,
	title = {Ridge {Regularization}: {An} {Essential} {Concept} in {Data} {Science}},
	volume = {62},
	issn = {0040-1706, 1537-2723},
	shorttitle = {Ridge {Regularization}},
	url = {https://www.tandfonline.com/doi/full/10.1080/00401706.2020.1791959},
	doi = {10.1080/00401706.2020.1791959},
	abstract = {Ridge or more formally 2 regularization shows up in many areas of statistics and machine learning. It is one of those essential devices that any good data scientist needs to master for their craft. In this brief ridge fest, I have collected together some of the magic and beauty of ridge that my colleagues and I have encountered over the past 40 years in applied statistics.},
	language = {en},
	number = {4},
	urldate = {2023-09-29},
	journal = {Technometrics},
	author = {Hastie, Trevor},
	month = oct,
	year = {2020},
	pages = {426--433},
	file = {Hastie - 2020 - Ridge Regularization An Essential Concept in Data.pdf:/home/zwerg/Zotero/storage/MRYMEPPI/Hastie - 2020 - Ridge Regularization An Essential Concept in Data.pdf:application/pdf},
}

@misc{schopf_exploring_2023,
	title = {Exploring the {Landscape} of {Natural} {Language} {Processing} {Research}},
	url = {http://arxiv.org/abs/2307.10652},
	doi = {10.26615/978-954-452-092-2_111},
	abstract = {As an efficient approach to understand, generate, and process natural language texts, research in natural language processing (NLP) has exhibited a rapid spread and wide adoption in recent years. Given the increasing research work in this area, several NLP-related approaches have been surveyed in the research community. However, a comprehensive study that categorizes established topics, identifies trends, and outlines areas for future research remains absent. Contributing to closing this gap, we have systematically classified and analyzed research papers in the ACL Anthology. As a result, we present a structured overview of the research landscape, provide a taxonomy of fields of study in NLP, analyze recent developments in NLP, summarize our findings, and highlight directions for future work.},
	urldate = {2023-09-30},
	author = {Schopf, Tim and Arabi, Karim and Matthes, Florian},
	month = sep,
	year = {2023},
	note = {arXiv:2307.10652 [cs]},
	keywords = {Computer Science - Computation and Language, I.2.7},
	annote = {Comment: Extended version of the paper accepted to the 14th International Conference on Recent Advances in Natural Language Processing (RANLP 2023)},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/WTT8KV35/Schopf et al. - 2023 - Exploring the Landscape of Natural Language Proces.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/A4SGM339/2307.html:text/html},
}

@article{dar_farewell_2021,
	title = {A {Farewell} to the {Bias}-{Variance} {Tradeoff}? {An} {Overview} of the {Theory} of {Overparameterized} {Machine} {Learning}},
	shorttitle = {A {Farewell} to the {Bias}-{Variance} {Tradeoff}?},
	url = {https://www.semanticscholar.org/paper/A-Farewell-to-the-Bias-Variance-Tradeoff-An-of-the-Dar-Muthukumar/83e815f7c4c6dd068ccec80d94c10a6dd21c0da9#citing-papers},
	abstract = {The rapid recent progress in machine learning (ML) has raised a number of scientific questions that challenge the longstanding dogma of the field. One of the most important riddles is the good empirical generalization of overparameterized models. Overparameterized models are excessively complex with respect to the size of the training dataset, which results in them perfectly fitting (i.e., interpolating) the training data, which is usually noisy. Such interpolation of noisy data is traditionally associated with detrimental overfitting, and yet a wide range of interpolating models -- from simple linear models to deep neural networks -- have recently been observed to generalize extremely well on fresh test data. Indeed, the recently discovered double descent phenomenon has revealed that highly overparameterized models often improve over the best underparameterized model in test performance. Understanding learning in this overparameterized regime requires new theory and foundational empirical studies, even for the simplest case of the linear model. The underpinnings of this understanding have been laid in very recent analyses of overparameterized linear regression and related statistical learning tasks, which resulted in precise analytic characterizations of double descent. This paper provides a succinct overview of this emerging theory of overparameterized ML (henceforth abbreviated as TOPML) that explains these recent findings through a statistical signal processing perspective. We emphasize the unique aspects that define the TOPML research area as a subfield of modern ML theory and outline interesting open questions that remain.},
	urldate = {2023-09-30},
	journal = {ArXiv},
	author = {Dar, Yehuda and Muthukumar, Vidya and Baraniuk, Richard},
	month = sep,
	year = {2021},
	file = {Dar et al. - 2021 - A Farewell to the Bias-Variance Tradeoff An Overview of the Theory of Overparameterized Machine Learning.pdf:/home/zwerg/Zotero/storage/YDTKSYV4/Dar et al. - 2021 - A Farewell to the Bias-Variance Tradeoff An Overview of the Theory of Overparameterized Machine Learning.pdf:application/pdf},
}

@article{neal_modern_2018,
	title = {A {Modern} {Take} on the {Bias}-{Variance} {Tradeoff} in {Neural} {Networks}},
	url = {https://www.semanticscholar.org/paper/A-Modern-Take-on-the-Bias-Variance-Tradeoff-in-Neal-Mittal/463a1779585aab66a6ef3fdc1b19e8c5d34d8070},
	abstract = {The bias-variance tradeoff tells us that as model complexity increases, bias falls and variances increases, leading to a U-shaped test error curve. However, recent empirical results with over-parameterized neural networks are marked by a striking absence of the classic U-shaped test error curve: test error keeps decreasing in wider networks. This suggests that there might not be a bias-variance tradeoff in neural networks with respect to network width, unlike was originally claimed by, e.g., Geman et al. (1992). Motivated by the shaky evidence used to support this claim in neural networks, we measure bias and variance in the modern setting. We find that both bias and variance can decrease as the number of parameters grows. To better understand this, we introduce a new decomposition of the variance to disentangle the effects of optimization and data sampling. We also provide theoretical analysis in a simplified setting that is consistent with our empirical findings.},
	urldate = {2023-09-30},
	journal = {ArXiv},
	author = {Neal, Brady and Mittal, Sarthak and Baratin, A. and Tantia, Vinayak and Scicluna, Matthew and Lacoste-Julien, S. and Mitliagkas, Ioannis},
	month = sep,
	year = {2018},
	file = {Neal et al. - 2018 - A Modern Take on the Bias-Variance Tradeoff in Neural Networks.pdf:/home/zwerg/Zotero/storage/8WSSP2UT/Neal et al. - 2018 - A Modern Take on the Bias-Variance Tradeoff in Neural Networks.pdf:application/pdf},
}

@misc{talebi_how_2023,
	title = {How to {Build} an {LLM} from {Scratch}},
	url = {https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9},
	abstract = {Data Curation, Transformers, Training at Scale, and Model Evaluation},
	language = {en},
	urldate = {2023-10-06},
	journal = {Medium},
	author = {Talebi, Shawhin},
	month = oct,
	year = {2023},
	file = {Snapshot:/home/zwerg/Zotero/storage/NSW5XDGG/how-to-build-an-llm-from-scratch-8c477768f1f9.html:text/html},
}

@misc{chandra_retentive_2023,
	title = {Retentive {Networks} ({RetNet}) {Explained}: {The} much-awaited {Transformers}-killer is here},
	shorttitle = {Retentive {Networks} ({RetNet}) {Explained}},
	url = {https://medium.com/ai-fusion-labs/retentive-networks-retnet-explained-the-much-awaited-transformers-killer-is-here-6c17e3e8add8},
	abstract = {Transformers have become the de-facto architecture for LLMs, as they efficiently overcome the sequential training issues of the recurrent…},
	language = {en},
	urldate = {2023-10-06},
	journal = {AI FUSION LABS},
	author = {Chandra, Shantanu},
	month = aug,
	year = {2023},
	file = {Snapshot:/home/zwerg/Zotero/storage/ANZ2SF5F/retentive-networks-retnet-explained-the-much-awaited-transformers-killer-is-here-6c17e3e8add8.html:text/html},
}

@misc{sun_retentive_2023,
	title = {Retentive {Network}: {A} {Successor} to {Transformer} for {Large} {Language} {Models}},
	shorttitle = {Retentive {Network}},
	url = {http://arxiv.org/abs/2307.08621},
	doi = {10.48550/arXiv.2307.08621},
	abstract = {In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost \$O(1)\$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
	month = aug,
	year = {2023},
	note = {arXiv:2307.08621 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/6JXWIBKU/Sun et al. - 2023 - Retentive Network A Successor to Transformer for Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/535M5RRN/2307.html:text/html},
}

@misc{noauthor_polymathic_nodate,
	title = {Polymathic},
	url = {https://polymathic-ai.org/blog/mpp/},
	urldate = {2023-10-14},
	file = {Polymathic:/home/zwerg/Zotero/storage/EKVVVZ5N/mpp.html:text/html},
}

@misc{noauthor_polymathic_nodate-1,
	title = {Polymathic},
	url = {https://polymathic-ai.org/blog/astroclip/},
	urldate = {2023-10-14},
}

@misc{wang_sam-med3d_2023,
	title = {{SAM}-{Med3D}},
	url = {dataset},
	abstract = {Although the Segment Anything Model (SAM) has demonstrated impressive performance in 2D natural image segmentation, its application to 3D volumetric medical images reveals significant shortcomings, namely suboptimal performance and unstable prediction, necessitating an excessive number of prompt points to attain the desired outcomes. These issues can hardly be addressed by fine-tuning SAM on medical data because the original 2D structure of SAM neglects 3D spatial information. In this paper, we introduce SAM-Med3D, the most comprehensive study to modify SAM for 3D medical images. Our approach is characterized by its comprehensiveness in two primary aspects: firstly, by comprehensively reformulating SAM to a thorough 3D architecture trained on a comprehensively processed large-scale volumetric medical dataset; and secondly, by providing a comprehensive evaluation of its performance. Specifically, we train SAM-Med3D with over 131K 3D masks and 247 categories. Our SAM-Med3D excels at capturing 3D spatial information, exhibiting competitive performance with significantly fewer prompt points than the top-performing fine-tuned SAM in the medical domain. We then evaluate its capabilities across 15 datasets and analyze it from multiple perspectives, including anatomical structures, modalities, targets, and generalization abilities. Our approach, compared with SAM, showcases pronouncedly enhanced efficiency and broad segmentation capabilities for 3D volumetric medical images. Our code is released at https://github.com/uni-medical/SAM-Med3D.},
	urldate = {2023-10-24},
	publisher = {arXiv},
	author = {Wang, Haoyu and Guo, Sizheng and Ye, Jin and Deng, Zhongying and Cheng, Junlong and Li, Tianbin and Chen, Jianpin and Su, Yanzhou and Huang, Ziyan and Shen, Yiqing and Fu, Bin and Zhang, Shaoting and He, Junjun and Qiao, Yu},
	month = oct,
	year = {2023},
	note = {arXiv:2310.15161 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/LZN6PLY9/2310.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/P3WN3VEP/Wang et al. - 2023 - SAM-Med3D.pdf:application/pdf},
}

@article{lake_human-like_2023,
	title = {Human-like systematic generalization through a meta-learning neural network},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06668-3},
	doi = {10.1038/s41586-023-06668-3},
	abstract = {The power of human language and thought arises from systematic compositionality—the algebraic ability to understand and produce novel combinations from known components. Fodor and Pylyshyn1 famously argued that artificial neural networks lack this capacity and are therefore not viable models of the mind. Neural networks have advanced considerably in the years since, yet the systematicity challenge persists. Here we successfully address Fodor and Pylyshyn’s challenge by providing evidence that neural networks can achieve human-like systematicity when optimized for their compositional skills. To do so, we introduce the meta-learning for compositionality (MLC) approach for guiding training through a dynamic stream of compositional tasks. To compare humans and machines, we conducted human behavioural experiments using an instruction learning paradigm. After considering seven different models, we found that, in contrast to perfectly systematic but rigid probabilistic symbolic models, and perfectly flexible but unsystematic neural networks, only MLC achieves both the systematicity and flexibility needed for human-like generalization. MLC also advances the compositional skills of machine learning systems in several systematic generalization benchmarks. Our results show how a standard neural network architecture, optimized for its compositional skills, can mimic human systematic generalization in a head-to-head comparison.},
	language = {en},
	urldate = {2023-10-26},
	journal = {Nature},
	author = {Lake, Brenden M. and Baroni, Marco},
	month = oct,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Human behaviour},
	pages = {1--7},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/9J77BFYQ/Lake and Baroni - 2023 - Human-like systematic generalization through a met.pdf:application/pdf},
}

@inproceedings{li_prefix-tuning_2021,
	address = {Online},
	title = {Prefix-{Tuning}: {Optimizing} {Continuous} {Prompts} for {Generation}},
	shorttitle = {Prefix-{Tuning}},
	url = {https://aclanthology.org/2021.acl-long.353},
	doi = {10.18653/v1/2021.acl-long.353},
	abstract = {Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.},
	urldate = {2023-11-02},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Xiang Lisa and Liang, Percy},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {4582--4597},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/H96QJPVG/Li and Liang - 2021 - Prefix-Tuning Optimizing Continuous Prompts for G.pdf:application/pdf},
}

@inproceedings{le_scao_how_2021,
	address = {Online},
	title = {How many data points is a prompt worth?},
	url = {https://aclanthology.org/2021.naacl-main.208},
	doi = {10.18653/v1/2021.naacl-main.208},
	abstract = {When fine-tuning pretrained models for classification, researchers either use a generic model head or a task-specific prompt for prediction. Proponents of prompting have argued that prompts provide a method for injecting task-specific guidance, which is beneficial in low-data regimes. We aim to quantify this benefit through rigorous testing of prompts in a fair setting: comparing prompted and head-based fine-tuning in equal conditions across many tasks and data sizes. By controlling for many sources of advantage, we find that prompting does indeed provide a benefit, and that this benefit can be quantified per task. Results show that prompting is often worth 100s of data points on average across classification tasks.},
	urldate = {2023-11-02},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Le Scao, Teven and Rush, Alexander},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {2627--2636},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/PDHJ6BXE/Le Scao and Rush - 2021 - How many data points is a prompt worth.pdf:application/pdf},
}

@inproceedings{ke_adapting_2023,
	title = {Adapting a {Language} {Model} {While} {Preserving} its {General} {Knowledge}},
	copyright = {Creative Commons Zero v1.0 Universal},
	url = {https://arxiv.org/abs/2301.08986},
	doi = {10.48550/ARXIV.2301.08986},
	abstract = {Domain-adaptive pre-training (or DA-training for short), also known as post-training, aims to train a pre-trained general-purpose language model (LM) using an unlabeled corpus of a particular domain to adapt the LM so that end-tasks in the domain can give improved performances. However, existing DA-training methods are in some sense blind as they do not explicitly identify what knowledge in the LM should be preserved and what should be changed by the domain corpus. This paper shows that the existing methods are suboptimal and proposes a novel method to perform a more informed adaptation of the knowledge in the LM by (1) soft-masking the attention heads based on their importance to best preserve the general knowledge in the LM and (2) contrasting the representations of the general and the full (both general and domain knowledge) to learn an integrated representation with both general and domain-specific knowledge. Experimental results will demonstrate the effectiveness of the proposed approach.},
	urldate = {2023-11-02},
	publisher = {arXiv},
	author = {Ke, Zixuan and Shao, Yijia and Lin, Haowei and Xu, Hu and Shu, Lei and Liu, Bing},
	year = {2023},
	note = {Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Neural and Evolutionary Computing (cs.NE)},
	annote = {Other
EMNLP 2022},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/66ZLM7K5/Ke et al. - 2023 - Adapting a Language Model While Preserving its Gen.pdf:application/pdf},
}

@article{chen_adaptformer_2022,
	title = {{AdaptFormer}: {Adapting} {Vision} {Transformers} for {Scalable} {Visual} {Recognition}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{AdaptFormer}},
	url = {https://arxiv.org/abs/2205.13535},
	doi = {10.48550/ARXIV.2205.13535},
	abstract = {Pretraining Vision Transformers (ViTs) has achieved great success in visual recognition. A following scenario is to adapt a ViT to various image and video recognition tasks. The adaptation is challenging because of heavy computation and memory storage. Each model needs an independent and complete finetuning process to adapt to different tasks, which limits its transferability to different visual domains. To address this challenge, we propose an effective adaptation approach for Transformer, namely AdaptFormer, which can adapt the pre-trained ViTs into many different image and video tasks efficiently. It possesses several benefits more appealing than prior arts. Firstly, AdaptFormer introduces lightweight modules that only add less than 2\% extra parameters to a ViT, while it is able to increase the ViT's transferability without updating its original pre-trained parameters, significantly outperforming the existing 100{\textbackslash}\% fully fine-tuned models on action recognition benchmarks. Secondly, it can be plug-and-play in different Transformers and scalable to many visual tasks. Thirdly, extensive experiments on five image and video datasets show that AdaptFormer largely improves ViTs in the target domains. For example, when updating just 1.5\% extra parameters, it achieves about 10\% and 19\% relative improvement compared to the fully fine-tuned models on Something-Something{\textasciitilde}v2 and HMDB51, respectively. Code is available at https://github.com/ShoufaChen/AdaptFormer.},
	urldate = {2023-11-02},
	author = {Chen, Shoufa and Ge, Chongjian and Tong, Zhan and Wang, Jiangliu and Song, Yibing and Wang, Jue and Luo, Ping},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	annote = {Other
Accepted by NeurIPS 2022. Code: https://github.com/ShoufaChen/AdaptFormer},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/E5HQXAC5/Chen et al. - 2022 - AdaptFormer Adapting Vision Transformers for Scal.pdf:application/pdf},
}

@article{gu_systematic_2023,
	title = {A {Systematic} {Survey} of {Prompt} {Engineering} on {Vision}-{Language} {Foundation} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2307.12980},
	doi = {10.48550/ARXIV.2307.12980},
	abstract = {Prompt engineering is a technique that involves augmenting a large pre-trained model with task-specific hints, known as prompts, to adapt the model to new tasks. Prompts can be created manually as natural language instructions or generated automatically as either natural language instructions or vector representations. Prompt engineering enables the ability to perform predictions based solely on prompts without updating model parameters, and the easier application of large pre-trained models in real-world tasks. In past years, Prompt engineering has been well-studied in natural language processing. Recently, it has also been intensively studied in vision-language modeling. However, there is currently a lack of a systematic overview of prompt engineering on pre-trained vision-language models. This paper aims to provide a comprehensive survey of cutting-edge research in prompt engineering on three types of vision-language models: multimodal-to-text generation models (e.g. Flamingo), image-text matching models (e.g. CLIP), and text-to-image generation models (e.g. Stable Diffusion). For each type of model, a brief model summary, prompting methods, prompting-based applications, and the corresponding responsibility and integrity issues are summarized and discussed. Furthermore, the commonalities and differences between prompting on vision-language models, language models, and vision models are also discussed. The challenges, future directions, and research opportunities are summarized to foster future research on this topic.},
	urldate = {2023-11-02},
	author = {Gu, Jindong and Han, Zhen and Chen, Shuo and Beirami, Ahmad and He, Bailan and Zhang, Gengyuan and Liao, Ruotong and Qin, Yao and Tresp, Volker and Torr, Philip},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	annote = {[TLDR] This paper aims to provide a comprehensive survey of cutting-edge research in prompt engineering on three types of vision-language models: multimodal-to-text generation models, image-text matching models, and text- to-image generation models.},
	file = {Full Text:/home/zwerg/Zotero/storage/GW2VPAAZ/Gu et al. - 2023 - A Systematic Survey of Prompt Engineering on Visio.pdf:application/pdf},
}

@inproceedings{song_communication_2023,
	title = {A {Communication} {Theory} {Perspective} on {Prompting} {Engineering} {Methods} for {Large} {Language} {Models}},
	url = {https://www.semanticscholar.org/paper/A-Communication-Theory-Perspective-on-Prompting-for-Song-He/31b092d23154d3467d4a92065d77e8b441dfa440},
	abstract = {The springing up of Large Language Models (LLMs) has shifted the community from single-task-orientated natural language processing (NLP) research to a holistic end-to-end multi-task learning paradigm. Along this line of research endeavors in the area, LLM-based prompting methods have attracted much attention, partially due to the technological advantages brought by prompt engineering (PE) as well as the underlying NLP principles disclosed by various prompting methods. Traditional supervised learning usually requires training a model based on labeled data and then making predictions. In contrast, PE methods directly use the powerful capabilities of existing LLMs (i.e., GPT-3 and GPT-4) via composing appropriate prompts, especially under few-shot or zero-shot scenarios. Facing the abundance of studies related to the prompting and the ever-evolving nature of this field, this article aims to (i) illustrate a novel perspective to review existing PE methods, within the well-established communication theory framework; (ii) facilitate a better/deeper understanding of developing trends of existing PE methods used in four typical tasks; (iii) shed light on promising research directions for future PE methods.},
	urldate = {2023-11-02},
	author = {Song, Yuanfeng and He, Yuanqin and Zhao, Xuefang and Gu, Hanlin and Jiang, Di and Yang, Haijun and Fan, Lixin and Yang, Qiang},
	month = oct,
	year = {2023},
	annote = {[TLDR] A novel perspective is illustrated to review existingPE methods, within the well-established communication theory framework to facilitate a better/deeper understanding of developing trends of existing PE methods used in four typical tasks and shed light on promising research directions for future PE methods.},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/SUL2XD9V/Song et al. - 2023 - A Communication Theory Perspective on Prompting En.pdf:application/pdf},
}

@inproceedings{wei_finetuned_2021-1,
	title = {Finetuned {Language} {Models} are {Zero}-{Shot} {Learners}},
	url = {https://openreview.net/forum?id=gEZrGCozdqR},
	abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
	language = {en},
	urldate = {2023-11-02},
	author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
	month = oct,
	year = {2021},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/EX8A4SUM/Wei et al. - 2021 - Finetuned Language Models are Zero-Shot Learners.pdf:application/pdf},
}

@article{chen_live_2008-1,
	title = {Live {Cell} {Dynamics} of {Promyelocytic} {Leukemia} {Nuclear} {Bodies} upon {Entry} into and {Exit} from {Mitosis}},
	volume = {19},
	issn = {1059-1524},
	url = {https://www.molbiolcell.org/doi/full/10.1091/mbc.E08-01-0035},
	doi = {10.1091/mbc.e08-01-0035},
	abstract = {Promyelocytic leukemia nuclear bodies (PML NBs) have been proposed to be involved in tumor suppression, viral defense, DNA repair, and/or transcriptional regulation. To study the dynamics of PML NBs during mitosis, we developed several U2OS cell lines stably coexpressing PML-enhanced cyan fluorescent protein with other individual marker proteins. Using three-dimensional time-lapse live cell imaging and four-dimensional particle tracking, we quantitatively demonstrated that PML NBs exhibit a high percentage of directed movement when cells progressed from prophase to prometaphase. The timing of this increased dynamic movement occurred just before or upon nuclear entry of cyclin B1, but before nuclear envelope breakdown. Our data suggest that entry into prophase leads to a loss of tethering between regions of chromatin and PML NBs, resulting in their increased dynamics. On exit from mitosis, Sp100 and Fas death domain-associated protein (Daxx) entered the daughter nuclei after a functional nuclear membrane was reformed. However, the recruitment of these proteins to PML NBs was delayed and correlated with the timing of de novo PML NB formation. Together, these results provide insight into the dynamic changes associated with PML NBs during mitosis.},
	number = {7},
	urldate = {2023-11-06},
	journal = {Molecular Biology of the Cell},
	author = {Chen, Yi-Chun M. and Kappel, Constantin and Beaudouin, Joel and Eils, Roland and Spector, David L.},
	month = jul,
	year = {2008},
	note = {Publisher: American Society for Cell Biology (mboc)},
	pages = {3147--3162},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/4AWHC2WY/Chen et al. - 2008 - Live Cell Dynamics of Promyelocytic Leukemia Nucle.pdf:application/pdf},
}

@article{huang_visuallanguage_2023,
	title = {A visual–language foundation model for pathology image analysis using medical {Twitter}},
	volume = {29},
	copyright = {2023 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-023-02504-3},
	doi = {10.1038/s41591-023-02504-3},
	abstract = {The lack of annotated publicly available medical images is a major barrier for computational research and education innovations. At the same time, many de-identified images and much knowledge are shared by clinicians on public forums such as medical Twitter. Here we harness these crowd platforms to curate OpenPath, a large dataset of 208,414 pathology images paired with natural language descriptions. We demonstrate the value of this resource by developing pathology language–image pretraining (PLIP), a multimodal artificial intelligence with both image and text understanding, which is trained on OpenPath. PLIP achieves state-of-the-art performances for classifying new pathology images across four external datasets: for zero-shot classification, PLIP achieves F1 scores of 0.565–0.832 compared to F1 scores of 0.030–0.481 for previous contrastive language–image pretrained model. Training a simple supervised classifier on top of PLIP embeddings also achieves 2.5\% improvement in F1 scores compared to using other supervised model embeddings. Moreover, PLIP enables users to retrieve similar cases by either image or natural language search, greatly facilitating knowledge sharing. Our approach demonstrates that publicly shared medical information is a tremendous resource that can be harnessed to develop medical artificial intelligence for enhancing diagnosis, knowledge sharing and education.},
	language = {en},
	number = {9},
	urldate = {2023-11-07},
	journal = {Nature Medicine},
	author = {Huang, Zhi and Bianchi, Federico and Yuksekgonul, Mert and Montine, Thomas J. and Zou, James},
	month = sep,
	year = {2023},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Medical research},
	pages = {2307--2316},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/AZ8SPJGN/Huang et al. - 2023 - A visual–language foundation model for pathology i.pdf:application/pdf},
}

@article{noauthor_six_nodate,
	title = {Six factors affecting reproducibility in life science research and how to handle them},
	copyright = {© 2023 Springer Nature Limited},
	url = {https://www.nature.com/articles/d42473-019-00004-y},
	abstract = {Scientific advancement depends on a strong foundation of data credibility. However, scientific findings in biomedical research are not always reproducible. Meet the organizations that are promoting best practices and helping researchers perform the highest-quality science.},
	language = {en},
	urldate = {2023-11-08},
	note = {Bandiera\_abtest: a
Cg\_type: Advertisement Feature},
}

@misc{noauthor_five_nodate,
	title = {Five ways to tackle the reproducibility crisis in biomedical research},
	url = {https://www.sciencedaily.com/releases/2016/09/160913124535.htm},
	abstract = {Scientists are discussing the chasm between biomedical scientists' astounding preclinical success and the meager clinical translatability. They also suggest ways that researchers can improve and standardize experiments so that the joy of exciting results can come with a rigorous scientific story.},
	language = {en},
	urldate = {2023-11-08},
	journal = {ScienceDaily},
}

@article{mouriaux_effects_2016,
	title = {Effects of {Long}-term {Serial} {Passaging} on the {Characteristics} and {Properties} of {Cell} {Lines} {Derived} {From} {Uveal} {Melanoma} {Primary} {Tumors}},
	volume = {57},
	issn = {1552-5783},
	url = {https://doi.org/10.1167/iovs.16-19317},
	doi = {10.1167/iovs.16-19317},
	abstract = {Development of liver metastasis remains the most common cause of mortality in uveal melanoma (UM). A few cell lines cultured from primary UM tumors have been used widely to investigate the pathobiology of UM. However, the translation of basic knowledge to the clinic for the treatment of the metastatic disease has remained incremental at best. In this study, we examined whether the properties of UM cell lines at various passages were similar to their corresponding primary tumors.    Gene expression profiling by microarray was performed on UM primary tumors and derived cell lines cultured at varying passages. Expression of UM protein markers was monitored by immunohistochemical analyses and Western blotting. The in vivo tumorigenic properties of UM cultures were evaluated using athymic nude mice.    Cell passaging severely reduced the expression of genes encoding markers typical of UM, including those of the prognostic gene signature. Marked differences between gene expression profiles of primary tumors and cell lines could be linked to the infiltrating immune and stromal cells in situ. In addition, the tumorigenic properties of UM cell lines also increased with cell passaging in culture as evaluated by their subcutaneous injection into athymic mice.    Together, these findings demonstrate that the short-term UM primary cultures exhibit molecular features that resemble the respective surgical material and, thus, represent the best model for in vitro–assessed cancer treatments.},
	number = {13},
	urldate = {2023-11-08},
	journal = {Investigative Ophthalmology \& Visual Science},
	author = {Mouriaux, Frédéric and Zaniolo, Karine and Bergeron, Marjorie-Allison and Weidmann, Cindy and De La Fouchardière, Arnaud and Fournier, Frédéric and Droit, Arnaud and Morcos, Mohib W. and Landreville, Solange and Guérin, Sylvain L.},
	month = oct,
	year = {2016},
	pages = {5288--5301},
	file = {Full Text:/home/zwerg/Zotero/storage/K7U5Y8X9/Mouriaux et al. - 2016 - Effects of Long-term Serial Passaging on the Chara.pdf:application/pdf},
}

@inproceedings{gallucci_latent_2021,
	address = {Online Only, United States},
	title = {A latent space exploration for microscopic skin lesion augmentations with {VQ}-{VAE}-2 and {PixelSNAIL}},
	isbn = {978-1-5106-4021-4 978-1-5106-4022-1},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11596/2580664/A-latent-space-exploration-for-microscopic-skin-lesion-augmentations-with/10.1117/12.2580664.full},
	doi = {10.1117/12.2580664},
	abstract = {Skin cancer affects more than 3 million people only in the US. Comprehensive microscopic databases include around 30 thousand samples, limiting the richness of patterns that can be presented to machine learning. To this end, generative models such as GANs have been proposed for creating realistic synthetic images but, despite their popularity, they are often difficult to train and control. Recently an autoregressive approach based on a quantized autoencoder showed state of the art performances while being simple to train and provide synthetic data generation opportunities. In the first part of this paper we evaluate the training of VQ-VAE-2 with different latent space configuration. In the second part, we show how to use a learned prior over the latent space with PixelSNAIL to generate and modify skin lesions. We show how this process can be used for powerful data augmentation and visualization for skin health, evaluating it on a downstream application that classifies malignant lesions},
	urldate = {2023-11-09},
	booktitle = {Medical {Imaging} 2021: {Image} {Processing}},
	publisher = {SPIE},
	author = {Gallucci, Alessio and Pezzotti, Nicola and Znamenskiy, Dmitry and Petkovic, Milan},
	editor = {Landman, Bennett A. and Išgum, Ivana},
	month = feb,
	year = {2021},
	pages = {102},
	annote = {[TLDR] This paper evaluates the training of VQ-VAE-2 with different latent space configuration and shows how to use a learned prior over the latent space with PixelSNAIL to generate and modify skin lesions.},
}

@article{nakada_study_2023,
	title = {A {Study} on {Voxel} {Shape} {Generation} and {Reconstruction} with {VQ}-{VAE}-2},
	url = {https://dl.acm.org/doi/10.1145/3606283.3606285},
	doi = {10.1145/3606283.3606285},
	abstract = {With the recent advances in deep learning, research on reconstructing and generating 3D shapes has become increasingly popular. While there are several types of representation methods for 3D shapes, our work focuses on voxel models that are arranged in a regular grid and are easy to handle on computers. In the area of 2D image generation, generative models such as GANs have been proposed that can output higher quality generative images, but are difficult to train. The recently proposed VQ-VAE-2 is easy to learn and shows performance comparable to the latest GANs. Therefore, this work proposes a method that can generate and reconstruct sharp voxel shape using VQ-VAE-2. Experiments compare the reconstruction and generation results and demonstrate that the proposed method performs better than existing methods in both experiments and is capable to output sharper shapes.},
	language = {en},
	urldate = {2023-11-09},
	journal = {Proceedings of the 2023 7th International Conference on Graphics and Signal Processing},
	author = {Nakada, Kenta and Kimata, Hideaki},
	month = jun,
	year = {2023},
	note = {Conference Name: ICGSP 2023: 2023 The 7th International Conference on Graphics and Signal Processing
ISBN: 9798400700460
Place: Fujisawa Japan
Publisher: ACM},
	pages = {9--15},
	annote = {[TLDR] This work proposes a method that can generate and reconstruct sharp voxel shape using VQ-VAE-2, and demonstrates that the proposed method performs better than existing methods in both experiments and is capable to output sharper shapes.},
}

@article{zavrtanik_cheating_2023,
	title = {Cheating {Depth}: {Enhancing} {3D} {Surface} {Anomaly} {Detection} via {Depth} {Simulation}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Cheating {Depth}},
	url = {https://arxiv.org/abs/2311.01117},
	doi = {10.48550/ARXIV.2311.01117},
	abstract = {RGB-based surface anomaly detection methods have advanced significantly. However, certain surface anomalies remain practically invisible in RGB alone, necessitating the incorporation of 3D information. Existing approaches that employ point-cloud backbones suffer from suboptimal representations and reduced applicability due to slow processing. Re-training RGB backbones, designed for faster dense input processing, on industrial depth datasets is hindered by the limited availability of sufficiently large datasets. We make several contributions to address these challenges. (i) We propose a novel Depth-Aware Discrete Autoencoder (DADA) architecture, that enables learning a general discrete latent space that jointly models RGB and 3D data for 3D surface anomaly detection. (ii) We tackle the lack of diverse industrial depth datasets by introducing a simulation process for learning informative depth features in the depth encoder. (iii) We propose a new surface anomaly detection method 3DSR, which outperforms all existing state-of-the-art on the challenging MVTec3D anomaly detection benchmark, both in terms of accuracy and processing speed. The experimental results validate the effectiveness and efficiency of our approach, highlighting the potential of utilizing depth information for improved surface anomaly detection.},
	urldate = {2023-11-09},
	author = {Zavrtanik, Vitjan and Kristan, Matej and Skočaj, Danijel},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	annote = {Other
Accepted at WACV 2024},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/NB2SGXDL/Zavrtanik et al. - 2023 - Cheating Depth Enhancing 3D Surface Anomaly Detec.pdf:application/pdf},
}

@article{lin_catch_2023,
	title = {Catch {Missing} {Details}: {Image} {Reconstruction} with {Frequency} {Augmented} {Variational} {Autoencoder}},
	shorttitle = {Catch {Missing} {Details}},
	url = {https://ieeexplore.ieee.org/document/10203987/},
	doi = {10.1109/CVPR52729.2023.00173},
	abstract = {The popular VQ-VAE models reconstruct images through learning a discrete codebook but suffer from a significant issue in the rapid quality degradation of image reconstruction as the compression rate rises. One major reason is that a higher compression rate induces more loss of visual signals on the higher frequency spectrum which reflect the details on pixel space. In this paper, a Frequency Complement Module (FCM) architecture is proposed to capture the missing frequency information for enhancing reconstruction quality. The FCM can be easily incorporated into the VQ-VAE structure, and we refer to the new model as Frequancy Augmented VAE (FAVAE). In addition, a Dynamic Spectrum Loss (DSL) is introduced to guide the FCMs to balance between various frequencies dynamically for optimal reconstruction. FA-VAE is further extended to the text-to-image synthesis task, and a Crossattention Autoregressive Transformer (CAT) is proposed to obtain more precise semantic attributes in texts. Extensive reconstruction experiments with different compression rates are conducted on several benchmark datasets, and the results demonstrate that the proposed FA-VAE is able to restore more faithfully the details compared to SOTA methods. CAT also shows improved generation quality with better image-text semantic alignment.},
	urldate = {2023-11-09},
	journal = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	author = {Lin, Xinmiao and Li, Yikang and Hsiao, Jenhao and Ho, Chiuman and Kong, Yu},
	month = jun,
	year = {2023},
	note = {Conference Name: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
ISBN: 9798350301298
Place: Vancouver, BC, Canada
Publisher: IEEE},
	pages = {1736--1745},
	annote = {[TLDR] A Frequency Complement Module (FCM) architecture is proposed to capture the missing frequency information for enhancing reconstruction quality and the proposed FA-VAE is able to restore more faithfully the details compared to SOTA methods.},
	file = {Submitted Version:/home/zwerg/Zotero/storage/PEMP7RWT/Lin et al. - 2023 - Catch Missing Details Image Reconstruction with F.pdf:application/pdf},
}

@article{ho_cascaded_2021,
	title = {Cascaded {Diffusion} {Models} for {High} {Fidelity} {Image} {Generation}},
	url = {https://www.semanticscholar.org/paper/Cascaded-Diffusion-Models-for-High-Fidelity-Image-Ho-Saharia/0f183bcfe65781c06b1a48a6f56e0f3c63e8e4a4},
	abstract = {We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation benchmark, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and classification accuracy scores of 63.02\% (top-1) and 84.06\% (top-5) at 256x256, outperforming VQ-VAE-2.},
	urldate = {2023-11-09},
	journal = {J. Mach. Learn. Res.},
	author = {Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J. and Norouzi, Mohammad and Salimans, Tim},
	month = may,
	year = {2021},
	annote = {[TLDR] It is shown that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x128 and 4.88 at 256x256 resolutions.},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/YNWSN4AE/Ho et al. - 2021 - Cascaded Diffusion Models for High Fidelity Image .pdf:application/pdf},
}

@article{gregor_deep_2013,
	title = {Deep {AutoRegressive} {Networks}},
	url = {https://www.semanticscholar.org/paper/Deep-AutoRegressive-Networks-Gregor-Danihelka/695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864},
	abstract = {We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling. We derive an efficient approximate parameter estimation method based on the minimum description length (MDL) principle, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference. We demonstrate state-of-the-art generative performance on a number of classic data sets, including several UCI data sets, MNIST and Atari 2600 games.},
	urldate = {2023-11-09},
	journal = {ArXiv},
	author = {Gregor, Karol and Danihelka, Ivo and Mnih, A. and Blundell, C. and Wierstra, Daan},
	month = oct,
	year = {2013},
	annote = {[TLDR] An efficient approximate parameter estimation method based on the minimum description length (MDL) principle is derived, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference.},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/L7Y6QH6Q/Gregor et al. - 2013 - Deep AutoRegressive Networks.pdf:application/pdf},
}

@inproceedings{salakhutdinov_deep_2009,
	title = {Deep {Boltzmann} {Machines}},
	url = {https://www.semanticscholar.org/paper/Deep-Boltzmann-Machines-Salakhutdinov-Hinton/85021c84383d18a7a4434d76dc8135fc6bdc0aa6},
	abstract = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.},
	urldate = {2023-11-09},
	author = {Salakhutdinov, R. and Hinton, Geoffrey E.},
	month = apr,
	year = {2009},
	annote = {[TLDR] A new learning algorithm for Boltzmann machines that contain many layers of hidden variables that is made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized with a single bottomup pass.},
}

@article{jabbar_survey_2022,
	title = {A {Survey} on {Generative} {Adversarial} {Networks}: {Variants}, {Applications}, and {Training}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {A {Survey} on {Generative} {Adversarial} {Networks}},
	url = {https://dl.acm.org/doi/10.1145/3463475},
	doi = {10.1145/3463475},
	abstract = {The Generative Models have gained considerable attention in unsupervised learning via a new and practical framework called Generative Adversarial Networks (GAN) due to their outstanding data generation capability. Many GAN models have been proposed, and several practical applications have emerged in various domains of computer vision and machine learning. Despite GANs excellent success, there are still obstacles to stable training. The problems are Nash equilibrium, internal covariate shift, mode collapse, vanishing gradient, and lack of proper evaluation metrics. Therefore, stable training is a crucial issue in different applications for the success of GANs. Herein, we survey several training solutions proposed by different researchers to stabilize GAN training. We discuss (I) the original GAN model and its modified versions, (II) a detailed analysis of various GAN applications in different domains, and (III) a detailed study about the various GAN training obstacles as well as training solutions. Finally, we reveal several issues as well as research outlines to the topic.},
	language = {en},
	number = {8},
	urldate = {2023-11-09},
	journal = {ACM Computing Surveys},
	author = {Jabbar, Abdul and Li, Xi and Omar, Bourahla},
	month = nov,
	year = {2022},
	pages = {1--49},
	annote = {[TLDR] This work surveys several training solutions proposed by different researchers to stabilize GAN training, and surveys the original GAN model and its modified classical versions, and detail analysis of various GAN applications in different domains.},
	file = {Submitted Version:/home/zwerg/Zotero/storage/ER5QEC82/Jabbar et al. - 2022 - A Survey on Generative Adversarial Networks Varia.pdf:application/pdf},
}

@article{zhang_survey_2023,
	title = {A {Survey} on {Audio} {Diffusion} {Models}: {Text} {To} {Speech} {Synthesis} and {Enhancement} in {Generative} {AI}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {A {Survey} on {Audio} {Diffusion} {Models}},
	url = {https://arxiv.org/abs/2303.13336},
	doi = {10.48550/ARXIV.2303.13336},
	abstract = {Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered in this survey.},
	urldate = {2023-11-09},
	author = {Zhang, Chenshuang and Zhang, Chaoning and Zheng, Sheng and Zhang, Mengchun and Qamar, Maryam and Bae, Sung-Ho and Kweon, In So},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Electrical engineering, electronic engineering, information engineering, Audio and Speech Processing (eess.AS), Multimedia (cs.MM), Sound (cs.SD)},
	annote = {Other
18 pages},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/JFXIM73F/Zhang et al. - 2023 - A Survey on Audio Diffusion Models Text To Speech.pdf:application/pdf},
}

@misc{bengio_estimating_2013,
	title = {Estimating or {Propagating} {Gradients} {Through} {Stochastic} {Neurons} for {Conditional} {Computation}},
	url = {http://arxiv.org/abs/1308.3432},
	doi = {10.48550/arXiv.1308.3432},
	abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of \{{\textbackslash}em conditional computation\}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
	month = aug,
	year = {2013},
	note = {arXiv:1308.3432 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: arXiv admin note: substantial text overlap with arXiv:1305.2982},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/3V6MFIZR/Bengio et al. - 2013 - Estimating or Propagating Gradients Through Stocha.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/J583G59Q/1308.html:text/html},
}

@article{wang_vector_2020,
	title = {A {Vector} {Quantized} {Variational} {Autoencoder} ({VQ}-{VAE}) {Autoregressive} {Neural} {F}\_0 {Model} for {Statistical} {Parametric} {Speech} {Synthesis}},
	volume = {28},
	issn = {2329-9304},
	url = {https://ieeexplore.ieee.org/document/8884734},
	doi = {10.1109/TASLP.2019.2950099},
	abstract = {Recurrent neural networks (RNNs) can predict fundamental frequency (F0) for statistical parametric speech synthesis systems, given linguistic features as input. However, these models assume conditional independence between consecutive F0 values, given the RNN state. In a previous study, we proposed autoregressive (AR) neural F0 models to capture the causal dependency of successive F0 values. In subjective evaluations, a deep AR model (DAR) outperformed an RNN. Here, we propose a Vector Quantized Variational Autoencoder (VQ-VAE) neural F0 model that is both more efficient and more interpretable than the DAR. This model has two stages: one uses the VQ-VAE framework to learn a latent code for the F0 contour of each linguistic unit, and other learns to map from linguistic features to latent codes. In contrast to the DAR and RNN, which process the input linguistic features frame-by-frame, the new model converts one linguistic feature vector into one latent code for each linguistic unit. The new model achieves better objective scores than the DAR, has a smaller memory footprint and is computationally faster. Visualization of the latent codes for phones and moras reveals that each latent code represents an F0 shape for a linguistic unit.},
	urldate = {2023-11-09},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Wang, Xin and Takaki, Shinji and Yamagishi, Junichi and King, Simon and Tokuda, Keiichi},
	year = {2020},
	note = {Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	pages = {157--170},
	file = {Accepted Version:/home/zwerg/Zotero/storage/Z3WS3N68/Wang et al. - 2020 - A Vector Quantized Variational Autoencoder (VQ-VAE.pdf:application/pdf},
}

@article{chen_blind_2022,
	title = {Blind {Image} {Super} {Resolution} with {Semantic}-{Aware} {Quantized} {Texture} {Prior}},
	url = {https://www.semanticscholar.org/paper/Blind-Image-Super-Resolution-with-Semantic-Aware-Chen-Shi/7c6f694d9879785bcaef1d412718a8cb42a254a8},
	abstract = {A key challenge of blind image super resolution is to recover realistic textures for low-resolution images with unknown degradations. Most recent works completely rely on the generative ability of GANs, which are difﬁcult to train. Other methods resort to high-resolution image references that are usually not available. In this work, we propose a novel framework, denoted as QuanTexSR , to restore realistic textures with the Quant ized Tex ture Priors encoded in Vector Quantized GAN. The QuanTexSR generates textures by aligning the textureless content features to the quantized feature vectors, i.e., a pretrained feature codebook. Speciﬁcally, QuanTexSR formulates the texture generation as a feature matching problem between textureless features and a pretrained feature codebook. The ﬁnal textures are then generated by the quantized features from the codebook. Since features in the codebook have shown the ability to generate natural textures in the pretrain stage, QuanTexSR can generate rich and realistic textures with the pretrained codebook as texture priors. Moreover, we propose a semantic regularization technique that regularizes the pre-training of the codebook using clusters of features extracted from the pretrained VGG19 network. This further improves texture generation with semantic context. Experiments demonstrate that the proposed QuanTexSR can generate competitive or better textures than previous approaches.},
	urldate = {2023-11-09},
	journal = {ArXiv},
	author = {Chen, Chaofeng and Shi, Xinyu and Qin, Yipeng and Li, Xiaoming and Han, Xiaoguang and Yang, T. and Guo, Shihui},
	year = {2022},
	annote = {[TLDR] This work proposes a novel framework, denoted as QuanTexSR, to restore realistic textures with the Quant ized Tex ture Priors encoded in Vector Quantized GAN, and proposes a semantic regularization technique that regularizes the pre-training of the codebook using clusters of features extracted from the pretrained VGG19 network.},
}

@inproceedings{gangloff_leveraging_2022,
	title = {Leveraging {Vector}-{Quantized} {Variational} {Autoencoder} {Inner} {Metrics} for {Anomaly} {Detection}},
	url = {https://ieeexplore.ieee.org/document/9956102},
	doi = {10.1109/ICPR56361.2022.9956102},
	abstract = {Anomaly Detection (AD) is an important research topic, with very diverse applications such as industrial defect detection, medical diagnosis, fraud detection, intrusion detection, etc. Within the last few years, deep learning-based methods have become the standard approach for AD. In many practical cases, the anomalies are unknown in advance. Therefore, most of challenging AD problems need to be addressed in an unsupervised or weakly supervised framework. In this context, deep generative models are widely used, in particular Variational Autoencoder (VAE) models. VAEs have been extended to Vector-Quantized VAEs (VQ-VAEs), a model increasingly popular because of its versatility enabled by the discrete latent space. We present for the first time a robust approach which takes advantage of the inner metrics of VQ-VAEs for AD. We show that the distance between the output of the encoder and the codebook vectors of a VQ-VAE provides a valuable information which can be used to localize the anomalies. In our approach, this metric complements a reconstruction-based metric to improve AD results. We compare our model with state-of-the-art AD models on three standards datasets, including the MVTec, UCSD-Ped1 and CIFAR-10 datasets. Experiments show that the proposed method yields high competitive results.},
	urldate = {2023-11-09},
	booktitle = {2022 26th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Gangloff, Hugo and Pham, Minh-Tan and Courtrai, Luc and Lefèvre, Sébastien},
	month = aug,
	year = {2022},
	note = {ISSN: 2831-7475},
	pages = {435--441},
	file = {Submitted Version:/home/zwerg/Zotero/storage/67HAMAZY/Gangloff et al. - 2022 - Leveraging Vector-Quantized Variational Autoencode.pdf:application/pdf},
}

@misc{jaderberg_spatial_2016,
	title = {Spatial {Transformer} {Networks}},
	url = {http://arxiv.org/abs/1506.02025},
	doi = {10.48550/arXiv.1506.02025},
	abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
	month = feb,
	year = {2016},
	note = {arXiv:1506.02025 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/CA9SLP3P/Jaderberg et al. - 2016 - Spatial Transformer Networks.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/BLDMZ7M3/1506.html:text/html},
}

@article{shamsolmoali_entropy_2023,
	title = {Entropy {Transformer} {Networks}: {A} {Learning} {Approach} via {Tangent} {Bundle} {Data} {Manifold}},
	shorttitle = {Entropy {Transformer} {Networks}},
	url = {https://ieeexplore.ieee.org/document/10191125/},
	doi = {10.1109/IJCNN54540.2023.10191125},
	abstract = {This paper focuses on an accurate and fast interpolation approach for image transformation employed in the design of CNN architectures. Standard Spatial Transformer Networks (STNs) use bilinear or linear interpolation as their interpolation, with unrealistic assumptions about the underlying data distributions, which leads to poor performance under scale variations. Moreover, STNs do not preserve the norm of gradients in propagation due to their dependency on sparse neighboring pixels. To address this problem, a novel Entropy STN (ESTN) is proposed that interpolates on the data manifold distributions. In particular, random samples are generated for each pixel in association with the tangent space of the data manifold, and construct a linear approximation of their intensity values with an entropy regularizer to compute the transformer parameters. A simple yet effective technique is also proposed to normalize the non-zero values of the convolution operation, to fine-tune the layers for gradients' norm-regularization during training. Experiments on challenging benchmarks show that the proposed ESTN can improve predictive accuracy over a range of computer vision tasks, including image reconstruction, and classification, while reducing the computational cost.},
	urldate = {2023-11-09},
	journal = {2023 International Joint Conference on Neural Networks (IJCNN)},
	author = {Shamsolmoali, Pourya and Zareapoor, Masoumeh},
	month = jun,
	year = {2023},
	note = {Conference Name: 2023 International Joint Conference on Neural Networks (IJCNN)
ISBN: 9781665488679
Place: Gold Coast, Australia
Publisher: IEEE},
	pages = {1--8},
	annote = {[TLDR] Experiments on challenging benchmarks show that the proposed ESTN can improve predictive accuracy over a range of computer vision tasks, including image reconstruction, and classification, while reducing the computational cost.},
	file = {Submitted Version:/home/zwerg/Zotero/storage/TVMTG9TY/Shamsolmoali and Zareapoor - 2023 - Entropy Transformer Networks A Learning Approach .pdf:application/pdf},
}

@article{liu_3d_2022,
	title = {{3D} {Brain} and {Heart} {Volume} {Generative} {Models}: {A} {Survey}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {{3D} {Brain} and {Heart} {Volume} {Generative} {Models}},
	url = {https://arxiv.org/abs/2210.05952},
	doi = {10.48550/ARXIV.2210.05952},
	abstract = {Generative models such as generative adversarial networks and autoencoders have gained a great deal of attention in the medical field due to their excellent data generation capability. This paper provides a comprehensive survey of generative models for three-dimensional (3D) volumes, focusing on the brain and heart. A new and elaborate taxonomy of unconditional and conditional generative models is proposed to cover diverse medical tasks for the brain and heart: unconditional synthesis, classification, conditional synthesis, segmentation, denoising, detection, and registration. We provide relevant background, examine each task and also suggest potential future directions. A list of the latest publications will be updated on Github to keep up with the rapid influx of papers at https://github.com/csyanbin/3D-Medical-Generative-Survey.},
	urldate = {2023-11-09},
	author = {Liu, Yanbin and Dwivedi, Girish and Boussaid, Farid and Bennamoun, Mohammed},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.IV), 92C55 (Primary), 68U10 (Secondary), I.4},
	annote = {Other
A survey on the 3D brain and heart volume generative models},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/7F4X5B4G/Liu et al. - 2022 - 3D Brain and Heart Volume Generative Models A Sur.pdf:application/pdf},
}

@article{ogretir_variational_2022,
	title = {A {Variational} {Autoencoder} for {Heterogeneous} {Temporal} and {Longitudinal} {Data}},
	url = {https://ieeexplore.ieee.org/document/10069449/},
	doi = {10.1109/ICMLA55696.2022.00239},
	abstract = {The variational autoencoder (VAE) is a popular deep latent variable model used to analyse high-dimensional datasets by learning a low-dimensional latent representation of the data. It simultaneously learns a generative model and an inference network to perform approximate posterior inference. Recently proposed extensions to VAEs that can handle temporal and longitudinal data have applications in healthcare, behavioural modelling, and predictive maintenance. However, these extensions do not account for heterogeneous data, i.e., data comprising of continuous and discrete attributes, which is common in many real-life applications. In this work, we propose the heterogeneous longitudinal VAE (HL-VAE) that extends the existing temporal and longitudinal VAEs to heterogeneous data for imputing missing values and unseen time point prediction. HL-VAE provides efficient inference for high-dimensional datasets and includes likelihood models for continuous, count, categorical, and ordinal data while accounting for missing observations. We demonstrate our model’s efficacy through simulated as well as clinical datasets, and show that our proposed model achieves competitive performance in missing value imputation and predictive accuracy.},
	urldate = {2023-11-09},
	journal = {2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)},
	author = {Öğretir, Mine and Ramchandran, Siddharth and Papatheodorou, Dimitrios and Lähdesmäki, Harri},
	month = dec,
	year = {2022},
	note = {Conference Name: 2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)
ISBN: 9781665462839
Place: Nassau, Bahamas
Publisher: IEEE},
	pages = {1522--1529},
	annote = {[TLDR] The heterogeneous longitudinal VAE (HL-VAE) is proposed that extends the existing temporal and longitudinal VAEs to heterogeneous data for imputing missing values and unseen time point prediction and provides efficient inference for high-dimensional datasets and includes likelihood models for continuous, count, categorical, and ordinal data while accounting for missing observations.},
	file = {Accepted Version:/home/zwerg/Zotero/storage/4PFUQCXE/Öğretir et al. - 2022 - A Variational Autoencoder for Heterogeneous Tempor.pdf:application/pdf},
}

@article{shamsolmoali_vtae_2023,
	title = {{VTAE}: {Variational} {Transformer} {Autoencoder} {With} {Manifolds} {Learning}},
	volume = {32},
	issn = {1057-7149, 1941-0042},
	shorttitle = {{VTAE}},
	url = {https://ieeexplore.ieee.org/document/10201469/},
	doi = {10.1109/TIP.2023.3299495},
	abstract = {Deep generative models have demonstrated successful applications in learning non-linear data distributions through a number of latent variables and these models use a non-linear function (generator) to map latent samples into the data space. On the other hand, the non-linearity of the generator implies that the latent space shows an unsatisfactory projection of the data space, which results in poor representation learning. This weak projection, however, can be addressed by a Riemannian metric, and we show that geodesics computation and accurate interpolations between data samples on the Riemannian manifold can substantially improve the performance of deep generative models. In this paper, a Variational spatial-Transformer AutoEncoder (VTAE) is proposed to minimize geodesics on a Riemannian manifold and improve representation learning. In particular, we carefully design the variational autoencoder with an encoded spatial-Transformer to explicitly expand the latent variable model to data on a Riemannian manifold, and obtain global context modelling. Moreover, to have smooth and plausible interpolations while traversing between two different objects’ latent representations, we propose a geodesic interpolation network different from the existing models that use linear interpolation with inferior performance. Experiments on benchmarks show that our proposed model can improve predictive accuracy and versatility over a range of computer vision tasks, including image interpolations, and reconstructions.},
	urldate = {2023-11-09},
	journal = {IEEE Transactions on Image Processing},
	author = {Shamsolmoali, Pourya and Zareapoor, Masoumeh and Zhou, Huiyu and Tao, Dacheng and Li, Xuelong},
	year = {2023},
	pages = {4486--4500},
	annote = {[TLDR] It is shown that geodesics computation and accurate interpolations between data samples on the Riemannian manifold can substantially improve the performance of deep generative models.},
	file = {Submitted Version:/home/zwerg/Zotero/storage/M3W2DZ5R/Shamsolmoali et al. - 2023 - VTAE Variational Transformer Autoencoder With Man.pdf:application/pdf},
}

@misc{ramchandran_learning_2022,
	title = {Learning {Conditional} {Variational} {Autoencoders} with {Missing} {Covariates}},
	url = {http://arxiv.org/abs/2203.01218},
	doi = {10.48550/arXiv.2203.01218},
	abstract = {Conditional variational autoencoders (CVAEs) are versatile deep generative models that extend the standard VAE framework by conditioning the generative model with auxiliary covariates. The original CVAE model assumes that the data samples are independent, whereas more recent conditional VAE models, such as the Gaussian process (GP) prior VAEs, can account for complex correlation structures across all data samples. While several methods have been proposed to learn standard VAEs from partially observed datasets, these methods fall short for conditional VAEs. In this work, we propose a method to learn conditional VAEs from datasets in which auxiliary covariates can contain missing values as well. The proposed method augments the conditional VAEs with a prior distribution for the missing covariates and estimates their posterior using amortised variational inference. At training time, our method marginalises the uncertainty associated with the missing covariates while simultaneously maximising the evidence lower bound. We develop computationally efficient methods to learn CVAEs and GP prior VAEs that are compatible with mini-batching. Our experiments on simulated datasets as well as on a clinical trial study show that the proposed method outperforms previous methods in learning conditional VAEs from non-temporal, temporal, and longitudinal datasets.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Ramchandran, Siddharth and Tikhonov, Gleb and Lönnroth, Otto and Tiikkainen, Pekka and Lähdesmäki, Harri},
	month = mar,
	year = {2022},
	note = {arXiv:2203.01218 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/UPTKMDAW/Ramchandran et al. - 2022 - Learning Conditional Variational Autoencoders with.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/8JPH9EM8/2203.html:text/html},
}

@misc{henderson_variational_2022,
	title = {A {Variational} {AutoEncoder} for {Transformers} with {Nonparametric} {Variational} {Information} {Bottleneck}},
	url = {http://arxiv.org/abs/2207.13529},
	doi = {10.48550/arXiv.2207.13529},
	abstract = {We propose a VAE for Transformers by developing a variational information bottleneck regulariser for Transformer embeddings. We formalise the embedding space of Transformer encoders as mixture probability distributions, and use Bayesian nonparametrics to derive a nonparametric variational information bottleneck (NVIB) for such attention-based embeddings. The variable number of mixture components supported by nonparametric methods captures the variable number of vectors supported by attention, and the exchangeability of our nonparametric distributions captures the permutation invariance of attention. This allows NVIB to regularise the number of vectors accessible with attention, as well as the amount of information in individual vectors. By regularising the cross-attention of a Transformer encoder-decoder with NVIB, we propose a nonparametric variational autoencoder (NVAE). Initial experiments on training a NVAE on natural language text show that the induced embedding space has the desired properties of a VAE for Transformers.},
	urldate = {2023-11-10},
	publisher = {arXiv},
	author = {Henderson, James and Fehr, Fabio},
	month = aug,
	year = {2022},
	note = {arXiv:2207.13529 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 33 pages, 10 figures, 3 tables. First time this work has been made public},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/SUZ5PGHA/Henderson and Fehr - 2022 - A Variational AutoEncoder for Transformers with No.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/FLYKCZQU/2207.html:text/html},
}

@article{ok_informative_2022,
	title = {Informative {Language} {Encoding} by {Variational} {Autoencoders} {Using} {Transformer}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/16/7968},
	doi = {10.3390/app12167968},
	abstract = {In natural language processing (NLP), Transformer is widely used and has reached the state-of-the-art level in numerous NLP tasks such as language modeling, summarization, and classification. Moreover, a variational autoencoder (VAE) is an efficient generative model in representation learning, combining deep learning with statistical inference in encoded representations. However, the use of VAE in natural language processing often brings forth practical difficulties such as a posterior collapse, also known as Kullback–Leibler (KL) vanishing. To mitigate this problem, while taking advantage of the parallelization of language data processing, we propose a new language representation model as the integration of two seemingly different deep learning models, which is a Transformer model solely coupled with a variational autoencoder. We compare the proposed model with previous works, such as a VAE connected with a recurrent neural network (RNN). Our experiments with four real-life datasets show that implementation with KL annealing mitigates posterior collapses. The results also show that the proposed Transformer model outperforms RNN-based models in reconstruction and representation learning, and that the encoded representations of the proposed model are more informative than other tested models.},
	language = {en},
	number = {16},
	urldate = {2023-11-10},
	journal = {Applied Sciences},
	author = {Ok, Changwon and Lee, Geonseok and Lee, Kichun},
	month = jan,
	year = {2022},
	note = {Number: 16
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {natural language processing, transformer, text mining, variational autoencoder},
	pages = {7968},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/SP7NMXWC/Ok et al. - 2022 - Informative Language Encoding by Variational Autoe.pdf:application/pdf},
}

@misc{fang_transformer-based_2021,
	title = {Transformer-based {Conditional} {Variational} {Autoencoder} for {Controllable} {Story} {Generation}},
	url = {http://arxiv.org/abs/2101.00828},
	doi = {10.48550/arXiv.2101.00828},
	abstract = {We investigate large-scale latent variable models (LVMs) for neural story generation -- an under-explored application for open-domain long text -- with objectives in two threads: generation effectiveness and controllability. LVMs, especially the variational autoencoder (VAE), have achieved both effective and controllable generation through exploiting flexible distributional latent representations. Recently, Transformers and its variants have achieved remarkable effectiveness without explicit latent representation learning, thus lack satisfying controllability in generation. In this paper, we advocate to revive latent variable modeling, essentially the power of representation learning, in the era of Transformers to enhance controllability without hurting state-of-the-art generation effectiveness. Specifically, we integrate latent representation vectors with a Transformer-based pre-trained architecture to build conditional variational autoencoder (CVAE). Model components such as encoder, decoder and the variational posterior are all built on top of pre-trained language models -- GPT2 specifically in this paper. Experiments demonstrate state-of-the-art conditional generation ability of our model, as well as its excellent representation learning capability and controllability.},
	urldate = {2023-11-10},
	publisher = {arXiv},
	author = {Fang, Le and Zeng, Tao and Liu, Chaochun and Bo, Liefeng and Dong, Wen and Chen, Changyou},
	month = jul,
	year = {2021},
	note = {arXiv:2101.00828 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/KI3GY6KE/Fang et al. - 2021 - Transformer-based Conditional Variational Autoenco.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/NTPWT3W7/2101.html:text/html},
}

@misc{noauthor_comprehensive_nodate,
	title = {Comprehensive single-cell transcriptional profiling of a multicellular organism},
	url = {https://www.science.org/doi/10.1126/science.aam8940},
	language = {en},
	urldate = {2023-11-13},
	doi = {10.1126/science.aam8940},
	file = {Full Text:/home/zwerg/Zotero/storage/FMP3694W/Comprehensive single-cell transcriptional profilin.pdf:application/pdf;Snapshot:/home/zwerg/Zotero/storage/UJX73J2T/science.html:text/html},
}

@misc{noauthor_comprehensive_nodate-1,
	title = {Comprehensive single-cell transcriptional profiling of a multicellular organism {\textbar} {Science}},
	url = {https://www.science.org/doi/10.1126/science.aam8940},
	urldate = {2023-11-13},
}

@misc{lange_zebrahub_2023,
	title = {Zebrahub – {Multimodal} {Zebrafish} {Developmental} {Atlas} {Reveals} the {State}-{Transition} {Dynamics} of {Late}-{Vertebrate} {Pluripotent} {Axial} {Progenitors}},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.03.06.531398v2},
	doi = {10.1101/2023.03.06.531398},
	abstract = {Elucidating the developmental processes of organisms requires a comprehensive understanding of cellular lineages in the spatial, temporal, and molecular domains. In this study, we introduce Zebrahub, a dynamic atlas of zebrafish embryonic development that integrates single-cell sequencing time course data with lineage reconstructions facilitated by light-sheet microscopy. This atlas offers high-resolution and in-depth molecular insights into zebrafish development, achieved through the sequencing of individual embryos across ten developmental stages, complemented by trajectory reconstructions. Zebrahub also incorporates an interactive tool to navigate the complex cellular flows and lineages derived from light-sheet microscopy data, enabling in silico fate mapping experiments. To demonstrate the versatility of our multi-modal resource, we utilize Zebrahub to provide fresh insights into the pluripotency of Neuro-Mesodermal Progenitors (NMPs). Our publicly accessible web-based platform, Zebrahub, is a foundational resource for studying developmental processes at both transcriptional and spatiotemporal levels, providing researchers with an integrated approach to exploring and analyzing the complexities of cellular lineages during zebrafish embryogenesis.},
	language = {en},
	urldate = {2023-11-13},
	publisher = {bioRxiv},
	author = {Lange, Merlin and Granados, Alejandro and VijayKumar, Shruthi and Bragantini, Jordão and Ancheta, Sarah and Santhosh, Sreejith and Borja, Michael and Kobayashi, Hirofumi and McGeever, Erin and Solak, Ahmet Can and Yang, Bin and Zhao, Xiang and Liu, Yang and Detweiler, Angela M. and Paul, Sheryl and Mekonen, Honey and Lao, Tiger and Banks, Rachel and Kim, Yang-Joon and Jacobo, Adrian and Balla, Keir and Awayan, Kyle and D’Souza, Samuel and Haase, Robert and Dizeux, Alexandre and Pourquie, Olivier and Gómez-Sjöberg, Rafael and Huber, Greg and Serra, Mattia and Neff, Norma and Pisco, Angela Oliveira and Royer, Loïc A.},
	month = jun,
	year = {2023},
	note = {Pages: 2023.03.06.531398
Section: New Results},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/RJK3HCZX/Lange et al. - 2023 - Zebrahub – Multimodal Zebrafish Developmental Atla.pdf:application/pdf},
}

@misc{kipf_semi-supervised_2017,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	doi = {10.48550/arXiv.1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2023-11-14},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv:1609.02907 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2017},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/VEBYPQPG/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/I46NUSVM/1609.html:text/html},
}

@misc{gabriel_weakly_2023,
	title = {Weakly supervised cross-modal learning in high-content screening},
	url = {http://arxiv.org/abs/2311.04678},
	doi = {10.48550/arXiv.2311.04678},
	abstract = {With the surge in available data from various modalities, there is a growing need to bridge the gap between different data types. In this work, we introduce a novel approach to learn cross-modal representations between image data and molecular representations for drug discovery. We propose EMM and IMM, two innovative loss functions built on top of CLIP that leverage weak supervision and cross sites replicates in High-Content Screening. Evaluating our model against known baseline on cross-modal retrieval, we show that our proposed approach allows to learn better representations and mitigate batch effect. In addition, we also present a preprocessing method for the JUMP-CP dataset that effectively reduce the required space from 85Tb to a mere usable 7Tb size, still retaining all perturbations and most of the information content.},
	urldate = {2023-11-14},
	publisher = {arXiv},
	author = {Gabriel, Watkinson and Ethan, Cohen and Nicolas, Bourriez and Ihab, Bendidi and Guillaume, Bollot and Auguste, Genovesio},
	month = nov,
	year = {2023},
	note = {arXiv:2311.04678 [cs, q-bio]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/9IPJTTR5/Gabriel et al. - 2023 - Weakly supervised cross-modal learning in high-con.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/W58Q85DN/2311.html:text/html},
}

@misc{archit_segment_2023,
	title = {Segment {Anything} for {Microscopy}},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.08.21.554208v1},
	doi = {10.1101/2023.08.21.554208},
	abstract = {We present Segment Anything for Microscopy, a tool for interactive and automatic segmentation and tracking of objects in multi-dimensional microscopy data. Our method is based on Segment Anything, a vision foundation model for image segmentation. We extend it by training specialized models for microscopy data that significantly improve segmentation quality for a wide range of imaging conditions. We also implement annotation tools for interactive (volumetric) segmentation and tracking, that speed up data annotation significantly compared to established tools. Our work constitutes the first application of vision foundation models to microscopy, laying the groundwork for solving image analysis problems in these domains with a small set of powerful deep learning architectures.},
	language = {en},
	urldate = {2023-11-16},
	publisher = {bioRxiv},
	author = {Archit, Anwai and Nair, Sushmita and Khalid, Nabeel and Hilt, Paul and Rajashekar, Vikas and Freitag, Marei and Gupta, Sagnik and Dengel, Andreas and Ahmed, Sheraz and Pape, Constantin},
	month = aug,
	year = {2023},
	note = {Pages: 2023.08.21.554208
Section: New Results},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/QCBU2TGU/Archit et al. - 2023 - Segment Anything for Microscopy.pdf:application/pdf},
}

@article{wu_three-dimensional_2019-1,
	title = {Three-dimensional virtual refocusing of fluorescence microscopy images using deep learning},
	volume = {16},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-019-0622-5},
	doi = {10.1038/s41592-019-0622-5},
	abstract = {We demonstrate that a deep neural network can be trained to virtually refocus a two-dimensional fluorescence image onto user-defined three-dimensional (3D) surfaces within the sample. Using this method, termed Deep-Z, we imaged the neuronal activity of a Caenorhabditis elegans worm in 3D using a time sequence of fluorescence images acquired at a single focal plane, digitally increasing the depth-of-field by 20-fold without any axial scanning, additional hardware or a trade-off of imaging resolution and speed. Furthermore, we demonstrate that this approach can correct for sample drift, tilt and other aberrations, all digitally performed after the acquisition of a single fluorescence image. This framework also cross-connects different imaging modalities to each other, enabling 3D refocusing of a single wide-field fluorescence image to match confocal microscopy images acquired at different sample planes. Deep-Z has the potential to improve volumetric imaging speed while reducing challenges relating to sample drift, aberration and defocusing that are associated with standard 3D fluorescence microscopy.},
	language = {en},
	number = {12},
	urldate = {2023-11-30},
	journal = {Nature Methods},
	author = {Wu, Yichen and Rivenson, Yair and Wang, Hongda and Luo, Yilin and Ben-David, Eyal and Bentolila, Laurent A. and Pritz, Christian and Ozcan, Aydogan},
	month = dec,
	year = {2019},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Fluorescence imaging, Software, Confocal microscopy, Wide-field fluorescence microscopy, Ca2+ imaging},
	pages = {1323--1331},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/RRY4GLR4/Wu et al. - 2019 - Three-dimensional virtual refocusing of fluorescen.pdf:application/pdf},
}

@misc{zhang_survey_2023-1,
	title = {A {Survey} on {Segment} {Anything} {Model} ({SAM}): {Vision} {Foundation} {Model} {Meets} {Prompt} {Engineering}},
	shorttitle = {A {Survey} on {Segment} {Anything} {Model} ({SAM})},
	url = {http://arxiv.org/abs/2306.06211},
	doi = {10.48550/arXiv.2306.06211},
	abstract = {Segment anything model (SAM) developed by Meta AI Research has recently attracted significant attention. Trained on a large segmentation dataset of over 1 billion masks, SAM is capable of segmenting any object on a certain image. In the original SAM work, the authors turned to zero-short transfer tasks (like edge detection) for evaluating the performance of SAM. Recently, numerous works have attempted to investigate the performance of SAM in various scenarios to recognize and segment objects. Moreover, numerous projects have emerged to show the versatility of SAM as a foundation model by combining it with other models, like Grounding DINO, Stable Diffusion, ChatGPT, etc. With the relevant papers and projects increasing exponentially, it is challenging for the readers to catch up with the development of SAM. To this end, this work conducts the first yet comprehensive survey on SAM. This is an ongoing project and we intend to update the manuscript on a regular basis. Therefore, readers are welcome to contact us if they complete new works related to SAM so that we can include them in our next version.},
	urldate = {2023-11-30},
	publisher = {arXiv},
	author = {Zhang, Chaoning and Puspitasari, Fachrina Dewi and Zheng, Sheng and Li, Chenghao and Qiao, Yu and Kang, Taegoo and Shan, Xinru and Zhang, Chenshuang and Qin, Caiyan and Rameau, Francois and Lee, Lik-Hang and Bae, Sung-Ho and Hong, Choong Seon},
	month = jul,
	year = {2023},
	note = {arXiv:2306.06211 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: First survey on Segment Anything Model (SAM), work under progress},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/M2QNN2YT/Zhang et al. - 2023 - A Survey on Segment Anything Model (SAM) Vision F.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/6755ZCP2/2306.html:text/html},
}

@article{noauthor_cellular_2022,
	title = {A cellular segmentation algorithm with fast customization},
	volume = {19},
	copyright = {2022 Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-022-01664-3},
	doi = {10.1038/s41592-022-01664-3},
	abstract = {Common cellular segmentation models based on machine learning perform suboptimally for test images that differ greatly from training images. Cellpose 2.0 allows biologists to quickly train state-of-the-art segmentation models on their own imaging data. This was previously only possible using large, annotated datasets and required expert machine learning knowledge.},
	language = {en},
	number = {12},
	urldate = {2023-11-30},
	journal = {Nature Methods},
	month = dec,
	year = {2022},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning, Software},
	pages = {1536--1537},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/JGC3EMVF/2022 - A cellular segmentation algorithm with fast custom.pdf:application/pdf},
}

@misc{noauthor_331_nodate,
	title = {331 - {Fine}-tune {Segment} {Anything} {Model} ({SAM}) using custom data - {YouTube}},
	url = {https://www.youtube.com/watch?v=83tnWs_YBRQ},
	urldate = {2023-11-30},
	file = {331 - Fine-tune Segment Anything Model (SAM) using custom data - YouTube:/home/zwerg/Zotero/storage/D37UWX8B/watch.html:text/html},
}

@article{spahn_deepbacs_2022,
	title = {{DeepBacs} for multi-task bacterial image analysis using open-source deep learning approaches},
	volume = {5},
	issn = {2399-3642},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9271087/},
	doi = {10.1038/s42003-022-03634-z},
	abstract = {This work demonstrates and guides how to use a range of state-of-the-art artificial neural-networks to analyse bacterial microscopy images using the recently developed ZeroCostDL4Mic platform. We generated a database of image datasets used to train networks for various image analysis tasks and present strategies for data acquisition and curation, as well as model training. We showcase different deep learning (DL) approaches for segmenting bright field and fluorescence images of different bacterial species, use object detection to classify different growth stages in time-lapse imaging data, and carry out DL-assisted phenotypic profiling of antibiotic-treated cells. To also demonstrate the ability of DL to enhance low-phototoxicity live-cell microscopy, we showcase how image denoising can allow researchers to attain high-fidelity data in faster and longer imaging. Finally, artificial labelling of cell membranes and predictions of super-resolution images allow for accurate mapping of cell shape and intracellular targets. Our purposefully-built database of training and testing data aids in novice users’ training, enabling them to quickly explore how to analyse their data through DL. We hope this lays a fertile ground for the efficient application of DL in microbiology and fosters the creation of tools for bacterial cell biology and antibiotic research., DeepBacs guides users without expertise in machine learning methods to leverage state-of-the-art artificial neural networks to analyse bacterial microscopy images.},
	urldate = {2023-11-29},
	journal = {Communications Biology},
	author = {Spahn, Christoph and Gómez-de-Mariscal, Estibaliz and Laine, Romain F. and Pereira, Pedro M. and von Chamier, Lucas and Conduit, Mia and Pinho, Mariana G. and Jacquemet, Guillaume and Holden, Séamus and Heilemann, Mike and Henriques, Ricardo},
	month = jul,
	year = {2022},
	pmid = {35810255},
	pmcid = {PMC9271087},
	pages = {688},
	file = {PubMed Central Full Text PDF:/home/zwerg/Zotero/storage/F97P7K4A/Spahn et al. - 2022 - DeepBacs for multi-task bacterial image analysis u.pdf:application/pdf},
}

@article{greenwald_whole-cell_2022,
	title = {Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning},
	volume = {40},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-021-01094-0},
	doi = {10.1038/s41587-021-01094-0},
	abstract = {A principal challenge in the analysis of tissue imaging data is cell segmentation—the task of identifying the precise boundary of every cell in an image. To address this problem we constructed TissueNet, a dataset for training segmentation models that contains more than 1 million manually labeled cells, an order of magnitude more than all previously published segmentation training datasets. We used TissueNet to train Mesmer, a deep-learning-enabled segmentation algorithm. We demonstrated that Mesmer is more accurate than previous methods, generalizes to the full diversity of tissue types and imaging platforms in TissueNet, and achieves human-level performance. Mesmer enabled the automated extraction of key cellular features, such as subcellular localization of protein signal, which was challenging with previous approaches. We then adapted Mesmer to harness cell lineage information in highly multiplexed datasets and used this enhanced version to quantify cell morphology changes during human gestation. All code, data and models are released as a community resource.},
	language = {en},
	number = {4},
	urldate = {2023-11-29},
	journal = {Nature Biotechnology},
	author = {Greenwald, Noah F. and Miller, Geneva and Moen, Erick and Kong, Alex and Kagel, Adam and Dougherty, Thomas and Fullaway, Christine Camacho and McIntosh, Brianna J. and Leow, Ke Xuan and Schwartz, Morgan Sarah and Pavelchek, Cole and Cui, Sunny and Camplisson, Isabella and Bar-Tal, Omer and Singh, Jaiveer and Fong, Mara and Chaudhry, Gautam and Abraham, Zion and Moseley, Jackson and Warshawsky, Shiri and Soon, Erin and Greenbaum, Shirley and Risom, Tyler and Hollmann, Travis and Bendall, Sean C. and Keren, Leeat and Graf, William and Angelo, Michael and Van Valen, David},
	month = apr,
	year = {2022},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Image processing, Imaging, Software},
	pages = {555--565},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/LMLMZ5LA/Greenwald et al. - 2022 - Whole-cell segmentation of tissue images with huma.pdf:application/pdf},
}

@article{pachitariu_cellpose_2022,
	title = {Cellpose 2.0: how to train your own model},
	volume = {19},
	copyright = {2022 The Author(s)},
	issn = {1548-7105},
	shorttitle = {Cellpose 2.0},
	url = {https://www.nature.com/articles/s41592-022-01663-4},
	doi = {10.1038/s41592-022-01663-4},
	abstract = {Pretrained neural network models for biological segmentation can provide good out-of-the-box results for many image types. However, such models do not allow users to adapt the segmentation style to their specific needs and can perform suboptimally for test images that are very different from the training images. Here we introduce Cellpose 2.0, a new package that includes an ensemble of diverse pretrained models as well as a human-in-the-loop pipeline for rapid prototyping of new custom models. We show that models pretrained on the Cellpose dataset can be fine-tuned with only 500–1,000 user-annotated regions of interest (ROI) to perform nearly as well as models trained on entire datasets with up to 200,000 ROI. A human-in-the-loop approach further reduced the required user annotation to 100–200 ROI, while maintaining high-quality segmentations. We provide software tools such as an annotation graphical user interface, a model zoo and a human-in-the-loop pipeline to facilitate the adoption of Cellpose 2.0.},
	language = {en},
	number = {12},
	urldate = {2023-11-29},
	journal = {Nature Methods},
	author = {Pachitariu, Marius and Stringer, Carsen},
	month = dec,
	year = {2022},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Image processing, Computational platforms and environments},
	pages = {1634--1641},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/6RR4QUJ5/Pachitariu and Stringer - 2022 - Cellpose 2.0 how to train your own model.pdf:application/pdf},
}

@misc{deiseroth_atman_2023,
	title = {{AtMan}: {Understanding} {Transformer} {Predictions} {Through} {Memory} {Efficient} {Attention} {Manipulation}},
	shorttitle = {{AtMan}},
	url = {http://arxiv.org/abs/2301.08110},
	doi = {10.48550/arXiv.2301.08110},
	abstract = {Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstrate that AtMan outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient. As such, AtMan is suitable for use in large model inference deployments.},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Deiseroth, Björn and Deb, Mayukh and Weinbach, Samuel and Brack, Manuel and Schramowski, Patrick and Kersting, Kristian},
	month = nov,
	year = {2023},
	note = {arXiv:2301.08110 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/IM8E5NY3/Deiseroth et al. - 2023 - AtMan Understanding Transformer Predictions Throu.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/MKTZ74RB/2301.html:text/html},
}

@misc{bellagente_multifusion_2023,
	title = {{MultiFusion}: {Fusing} {Pre}-{Trained} {Models} for {Multi}-{Lingual}, {Multi}-{Modal} {Image} {Generation}},
	shorttitle = {{MultiFusion}},
	url = {http://arxiv.org/abs/2305.15296},
	doi = {10.48550/arXiv.2305.15296},
	abstract = {The recent popularity of text-to-image diffusion models (DM) can largely be attributed to the intuitive interface they provide to users. The intended generation can be expressed in natural language, with the model producing faithful interpretations of text prompts. However, expressing complex or nuanced ideas in text alone can be difficult. To ease image generation, we propose MultiFusion that allows one to express complex and nuanced concepts with arbitrarily interleaved inputs of multiple modalities and languages. MutliFusion leverages pre-trained models and aligns them for integration into a cohesive system, thereby avoiding the need for extensive training from scratch. Our experimental results demonstrate the efficient transfer of capabilities from individual modules to the downstream model. Specifically, the fusion of all independent components allows the image generation module to utilize multilingual, interleaved multimodal inputs despite being trained solely on monomodal data in a single language.},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Bellagente, Marco and Brack, Manuel and Teufel, Hannah and Friedrich, Felix and Deiseroth, Björn and Eichenberg, Constantin and Dai, Andrew and Baldock, Robert and Nanda, Souradeep and Oostermeijer, Koen and Cruz-Salinas, Andres Felipe and Schramowski, Patrick and Kersting, Kristian and Weinbach, Samuel},
	month = nov,
	year = {2023},
	note = {arXiv:2305.15296 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Proceedings of Advances in Neural Information Processing Systems: Annual Conference on Neural Information Processing Systems (NeurIPS)},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/SULDP4R4/Bellagente et al. - 2023 - MultiFusion Fusing Pre-Trained Models for Multi-L.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/HY63LRG7/2305.html:text/html},
}

@misc{bradley_quality-diversity_2023,
	title = {Quality-{Diversity} through {AI} {Feedback}},
	url = {http://arxiv.org/abs/2310.13032},
	doi = {10.48550/arXiv.2310.13032},
	abstract = {In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-QD controls. Further, human evaluation of QDAIF-generated creative texts validates reasonable agreement between AI and human evaluation. Our results thus highlight the potential of AI feedback to guide open-ended search for creative and original solutions, providing a recipe that seemingly generalizes to many domains and modalities. In this way, QDAIF is a step towards AI systems that can independently search, diversify, evaluate, and improve, which are among the core skills underlying human society's capacity for innovation.},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Bradley, Herbie and Dai, Andrew and Teufel, Hannah and Zhang, Jenny and Oostermeijer, Koen and Bellagente, Marco and Clune, Jeff and Stanley, Kenneth and Schott, Grégory and Lehman, Joel},
	month = nov,
	year = {2023},
	note = {arXiv:2310.13032 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: minor edits, correction in appendix for improved clarity},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/KRTUSR76/Bradley et al. - 2023 - Quality-Diversity through AI Feedback.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/HNFLC5RP/2310.html:text/html},
}

@inproceedings{eichenberg_magma_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{MAGMA} – {Multimodal} {Augmentation} of {Generative} {Models} through {Adapter}-based {Finetuning}},
	url = {https://aclanthology.org/2022.findings-emnlp.179},
	doi = {10.18653/v1/2022.findings-emnlp.179},
	abstract = {Large-scale pretraining is fast becoming the norm in Vision-Language (VL) modeling. However, prevailing VL approaches are limited by the requirement for labeled data and the use of complex multi-step pretraining objectives. We present MAGMA - a simple method for augmenting generative language models with additional modalities using adapter-based finetuning. Building on Frozen, we train a series of VL models that autoregressively generate text from arbitrary combinations of visual and textual input. The pretraining is entirely end-to-end using a single language modeling objective, simplifying optimization compared to previous approaches. Importantly, the language model weights remain unchanged during training, allowing for transfer of encyclopedic knowledge and in-context learning abilities from language pretraining. MAGMA outperforms Frozen on open-ended generative tasks, achieving state of the art results on the OKVQA benchmark and competitive results on a range of other popular VL benchmarks, while pretraining on 0.2 \% of the number of samples used to train SimVLM.},
	urldate = {2023-12-04},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Eichenberg, Constantin and Black, Sidney and Weinbach, Samuel and Parcalabescu, Letitia and Frank, Anette},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {2416--2428},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/6CQCXBT2/Eichenberg et al. - 2022 - MAGMA – Multimodal Augmentation of Generative Mode.pdf:application/pdf},
}

@misc{workshop_bloom_2023-1,
	title = {{BLOOM}: {A} {176B}-{Parameter} {Open}-{Access} {Multilingual} {Language} {Model}},
	shorttitle = {{BLOOM}},
	url = {http://arxiv.org/abs/2211.05100},
	abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ilić, Suzana and Hesslow, Daniel and Castagné, Roman and Luccioni, Alexandra Sasha and Yvon, François and Gallé, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Benoît and Muennighoff, Niklas and del Moral, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and McMillan-Major, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Laurençon, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and van Strien, Daniel and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo González and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, Gérard and Kruszewski, Germán and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and Gonzalez-Dios, Itziar and de la Rosa, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, Jörg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and allal, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Muñoz, Manuel Romero and Masoud, Maraim and Grandury, María and Šaško, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and de Gibert, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and López, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Taşar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M. Saiful and Al-shaibani, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and Ben-David, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and von Platen, Patrick and Cornette, Pierre and Lavallée, Pierre François and Lacroix, Rémi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, Stéphane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and Névéol, Aurélie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and van der Wal, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zdeněk and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Muñoz and McDuff, Daniel and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and Miranda-Escalada, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Clémentine and Periñán, Daniel León and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and de Bykhovetz, Madeleine Hahn and Takeuchi, Maiko and Pàmies, Marc and Castillo, Maria A. and Nezhurina, Marianna and Sänger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S. and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and Sang-aroonsiri, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Théo and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},
	month = jun,
	year = {2023},
	note = {arXiv:2211.05100 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/DLYGEJXT/2211.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/Q22YKSRD/Workshop et al. - 2023 - BLOOM A 176B-Parameter Open-Access Multilingual L.pdf:application/pdf},
}

@misc{ho_denoising_2020-2,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2023-12-08},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/9GLXS3EQ/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/Z48PM8KI/2006.html:text/html},
}

@misc{gal_image_2022-3,
	title = {An {Image} is {Worth} {One} {Word}: {Personalizing} {Text}-to-{Image} {Generation} using {Textual} {Inversion}},
	shorttitle = {An {Image} is {Worth} {One} {Word}},
	url = {http://arxiv.org/abs/2208.01618},
	doi = {10.48550/arXiv.2208.01618},
	abstract = {Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new "words" in the embedding space of a frozen text-to-image model. These "words" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available at: https://textual-inversion.github.io},
	urldate = {2023-12-16},
	publisher = {arXiv},
	author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},
	month = aug,
	year = {2022},
	note = {arXiv:2208.01618 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Graphics},
	annote = {Comment: Project page: https://textual-inversion.github.io},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/RP3YY5YH/Gal et al. - 2022 - An Image is Worth One Word Personalizing Text-to-Image Generation using Textual Inversion.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/S5284R5S/2208.html:text/html},
}

@misc{ruiz_dreambooth_2023,
	title = {{DreamBooth}: {Fine} {Tuning} {Text}-to-{Image} {Diffusion} {Models} for {Subject}-{Driven} {Generation}},
	shorttitle = {{DreamBooth}},
	url = {http://arxiv.org/abs/2208.12242},
	doi = {10.48550/arXiv.2208.12242},
	abstract = {Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for "personalization" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/},
	urldate = {2023-12-16},
	publisher = {arXiv},
	author = {Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
	month = mar,
	year = {2023},
	note = {arXiv:2208.12242 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	annote = {Comment: Published at CVPR 2023. Project page: https://dreambooth.github.io/},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/DGWAXWVZ/Ruiz et al. - 2023 - DreamBooth Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/2KMT4Q6A/2208.html:text/html},
}

@misc{noauthor_dreambooth_nodate-1,
	title = {{DreamBooth}},
	url = {https://dreambooth.github.io/},
	urldate = {2023-12-16},
}

@misc{noauthor_generative_nodate-1,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution} {\textbar} {Yang} {Song}},
	url = {https://yang-song.net/blog/2021/score/},
	urldate = {2023-12-16},
	file = {Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song:/home/zwerg/Zotero/storage/RHTJ5KM6/score.html:text/html},
}

@misc{noauthor_guidance_2022,
	title = {Guidance: a cheat code for diffusion models},
	shorttitle = {Guidance},
	url = {https://sander.ai/2022/05/26/guidance.html},
	abstract = {A quick post with some thoughts on diffusion guidance},
	language = {en},
	urldate = {2023-12-16},
	journal = {Sander Dieleman},
	month = may,
	year = {2022},
	file = {Snapshot:/home/zwerg/Zotero/storage/UDYI8HC5/guidance.html:text/html},
}

@misc{noauthor_diffusion_2022,
	title = {Diffusion models are autoencoders},
	url = {https://sander.ai/2022/01/31/diffusion.html},
	abstract = {Diffusion models have become very popular over the last two years. There is an underappreciated link between diffusion models and autoencoders.},
	language = {en},
	urldate = {2023-12-16},
	journal = {Sander Dieleman},
	month = jan,
	year = {2022},
	file = {Snapshot:/home/zwerg/Zotero/storage/28QIJAUS/diffusion.html:text/html},
}

@misc{song_generative_2020-1,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution}},
	url = {http://arxiv.org/abs/1907.05600},
	doi = {10.48550/arXiv.1907.05600},
	abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
	urldate = {2023-12-16},
	publisher = {arXiv},
	author = {Song, Yang and Ermon, Stefano},
	month = oct,
	year = {2020},
	note = {arXiv:1907.05600 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2019 (Oral)},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/88RKNJW5/Song and Ermon - 2020 - Generative Modeling by Estimating Gradients of the Data Distribution.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/FXV928PG/1907.html:text/html},
}

@misc{noauthor_biccn_nodate,
	title = {{BICCN}: {The} first complete cell census and atlas of a mammalian brain},
	shorttitle = {{BICCN}},
	url = {https://www.nature.com/immersive/d42859-023-00069-2/index.html},
	abstract = {Generating a complete multimodal cell census and atlas of the mouse brain through collaborative data collection, tool development and analysis.},
	language = {en},
	urldate = {2023-12-14},
}

@article{johnson_building_2023,
	title = {Building the next generation of virtual cells to understand cellular biology},
	volume = {122},
	issn = {0006-3495},
	url = {https://www.sciencedirect.com/science/article/pii/S0006349523002369},
	doi = {10.1016/j.bpj.2023.04.006},
	abstract = {Cell science has made significant progress by focusing on understanding individual cellular processes through reductionist approaches. However, the sheer volume of knowledge collected presents challenges in integrating this information across different scales of space and time to comprehend cellular behaviors, as well as making the data and methods more accessible for the community to tackle complex biological questions. This perspective proposes the creation of next-generation virtual cells, which are dynamic 3D models that integrate information from diverse sources, including simulations, biophysical models, image-based models, and evidence-based knowledge graphs. These virtual cells would provide statistically accurate and holistic views of real cells, bridging the gap between theoretical concepts and experimental data, and facilitating productive new collaborations among researchers across related fields.},
	number = {18},
	urldate = {2023-12-13},
	journal = {Biophysical Journal},
	author = {Johnson, Graham T. and Agmon, Eran and Akamatsu, Matthew and Lundberg, Emma and Lyons, Blair and Ouyang, Wei and Quintero-Carmona, Omar A. and Riel-Mehan, Megan and Rafelski, Susanne and Horwitz, Rick},
	month = sep,
	year = {2023},
	keywords = {3D models, Biophysical models, Cell science, Community collaboration, Community modeling, Integrating information, Knowledge graphs, Multiscale modeling, Reproducibility, Simulations, Spatial models, Virtual cells},
	pages = {3560--3569},
	file = {Full Text:/home/zwerg/Zotero/storage/IK93KRCY/Johnson et al. - 2023 - Building the next generation of virtual cells to understand cellular biology.pdf:application/pdf;ScienceDirect Snapshot:/home/zwerg/Zotero/storage/XMQ9PESX/S0006349523002369.html:text/html},
}

@misc{noauthor_eric_nodate,
	title = {Eric {Schmidt}: {This} is how {AI} will transform the way science gets done},
	shorttitle = {Eric {Schmidt}},
	url = {https://www.technologyreview.com/2023/07/05/1075865/eric-schmidt-ai-will-transform-science/},
	abstract = {Science is about to become much more exciting—and that will affect us all, argues Google's former CEO.},
	language = {en},
	urldate = {2023-12-13},
	journal = {MIT Technology Review},
}

@misc{noauthor_how_nodate-2,
	title = {How {AI} can help us understand how cells work—and help cure diseases},
	url = {https://www.technologyreview.com/2023/09/19/1079261/czi-ai-cell-disease/},
	abstract = {A virtual cell modeling system, powered by AI, will lead to breakthroughs in our understanding of diseases, argue the cofounders of the Chan Zuckerberg Initiative.},
	language = {en},
	urldate = {2023-12-13},
	journal = {MIT Technology Review},
}

@misc{wei_magicoder_2023,
	title = {Magicoder: {Source} {Code} {Is} {All} {You} {Need}},
	shorttitle = {Magicoder},
	url = {http://arxiv.org/abs/2312.02120},
	doi = {10.48550/arXiv.2312.02120},
	abstract = {We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate high-quality instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs by empowering them with a wealth of open-source references for the production of more diverse, realistic, and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks, including Python text-to-code generation, multilingual coding, and data-science program completion. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1). Overall, OSS-Instruct opens a new direction for low-bias and high-quality instruction tuning using abundant open-source references.},
	urldate = {2023-12-13},
	author = {Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming},
	month = dec,
	year = {2023},
	note = {arXiv:2312.02120 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {Wei et al. - 2023 - Magicoder Source Code Is All You Need.pdf:/home/zwerg/Zotero/storage/6ANG8YGD/Wei et al. - 2023 - Magicoder Source Code Is All You Need.pdf:application/pdf},
}

@article{cui_dictionary_2023,
	title = {Dictionary of immune responses to cytokines at single-cell resolution},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06816-9},
	doi = {10.1038/s41586-023-06816-9},
	abstract = {Cytokines mediate cell–cell communication in the immune system and represent important therapeutic targets1–3. A myriad of studies have highlighted their central role in immune function4–13, yet we lack a global view of the cellular responses of each immune cell type to each cytokine. To address this gap, we created the Immune Dictionary, a compendium of single-cell transcriptomic profiles of more than 17 immune cell types in response to each of 86 cytokines ({\textgreater}1,400 cytokine–cell type combinations) in mouse lymph nodes in vivo. A cytokine-centric view of the dictionary revealed that most cytokines induce highly cell-type-specific responses. For example, the inflammatory cytokine interleukin-1β induces distinct gene programmes in almost every cell type. A cell-type-centric view of the dictionary identified more than 66 cytokine-driven cellular polarization states across immune cell types, including previously uncharacterized states such as an interleukin-18-induced polyfunctional natural killer cell state. Based on this dictionary, we developed companion software, Immune Response Enrichment Analysis, for assessing cytokine activities and immune cell polarization from gene expression data, and applied it to reveal cytokine networks in tumours following immune checkpoint blockade therapy. Our dictionary generates new hypotheses for cytokine functions, illuminates pleiotropic effects of cytokines, expands our knowledge of activation states of each immune cell type, and provides a framework to deduce the roles of specific cytokines and cell–cell communication networks in any immune response.},
	language = {en},
	urldate = {2023-12-12},
	journal = {Nature},
	author = {Cui, Ang and Huang, Teddy and Li, Shuqiang and Ma, Aileen and Pérez, Jorge L. and Sander, Chris and Keskin, Derin B. and Wu, Catherine J. and Fraenkel, Ernest and Hacohen, Nir},
	month = dec,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Immunology, Computational biology and bioinformatics, Systems biology},
	pages = {1--8},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/DAHF3ABI/Cui et al. - 2023 - Dictionary of immune responses to cytokines at single-cell resolution.pdf:application/pdf},
}

@misc{lu_foundational_2023,
	title = {A {Foundational} {Multimodal} {Vision} {Language} {AI} {Assistant} for {Human} {Pathology}},
	url = {http://arxiv.org/abs/2312.07814},
	doi = {10.48550/arXiv.2312.07814},
	abstract = {The field of computational pathology has witnessed remarkable progress in the development of both task-specific predictive models and task-agnostic self-supervised vision encoders. However, despite the explosive growth of generative artificial intelligence (AI), there has been limited study on building general purpose, multimodal AI assistants tailored to pathology. Here we present PathChat, a vision-language generalist AI assistant for human pathology using an in-house developed foundational vision encoder pretrained on 100 million histology images from over 100,000 patient cases and 1.18 million pathology image-caption pairs. The vision encoder is then combined with a pretrained large language model and the whole system is finetuned on over 250,000 diverse disease agnostic visual language instructions. We compare PathChat against several multimodal vision language AI assistants as well as GPT4V, which powers the commercially available multimodal general purpose AI assistant ChatGPT-4. When relevant clinical context is provided with the histology image, PathChat achieved a diagnostic accuracy of 87\% on multiple-choice questions based on publicly available cases of diverse tissue origins and disease models. Additionally, using open-ended questions and human expert evaluation, we found that overall PathChat produced more accurate and pathologist-preferable responses to diverse queries related to pathology. As an interactive and general vision language AI assistant that can flexibly handle both visual and natural language inputs, PathChat can potentially find impactful applications in pathology education, research, and human-in-the-loop clinical decision making.},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Lu, Ming Y. and Chen, Bowen and Williamson, Drew F. K. and Chen, Richard J. and Ikamura, Kenji and Gerber, Georg and Liang, Ivy and Le, Long Phi and Ding, Tong and Parwani, Anil V. and Mahmood, Faisal},
	month = dec,
	year = {2023},
	note = {arXiv:2312.07814 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/AVT2AYIP/Lu et al. - 2023 - A Foundational Multimodal Vision Language AI Assistant for Human Pathology.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/PYMZTWZ4/2312.html:text/html},
}

@misc{radford_learning_2021-1,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	doi = {10.48550/arXiv.2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2103.00020 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/YF7M4CXK/Radford et al. - 2021 - Learning Transferable Visual Models From Natural Language Supervision.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/A957FXNX/2103.html:text/html},
}

@article{huang_visuallanguage_2023-1,
	title = {A visual–language foundation model for pathology image analysis using medical {Twitter}},
	volume = {29},
	copyright = {2023 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-023-02504-3},
	doi = {10.1038/s41591-023-02504-3},
	abstract = {The lack of annotated publicly available medical images is a major barrier for computational research and education innovations. At the same time, many de-identified images and much knowledge are shared by clinicians on public forums such as medical Twitter. Here we harness these crowd platforms to curate OpenPath, a large dataset of 208,414 pathology images paired with natural language descriptions. We demonstrate the value of this resource by developing pathology language–image pretraining (PLIP), a multimodal artificial intelligence with both image and text understanding, which is trained on OpenPath. PLIP achieves state-of-the-art performances for classifying new pathology images across four external datasets: for zero-shot classification, PLIP achieves F1 scores of 0.565–0.832 compared to F1 scores of 0.030–0.481 for previous contrastive language–image pretrained model. Training a simple supervised classifier on top of PLIP embeddings also achieves 2.5\% improvement in F1 scores compared to using other supervised model embeddings. Moreover, PLIP enables users to retrieve similar cases by either image or natural language search, greatly facilitating knowledge sharing. Our approach demonstrates that publicly shared medical information is a tremendous resource that can be harnessed to develop medical artificial intelligence for enhancing diagnosis, knowledge sharing and education.},
	language = {en},
	number = {9},
	urldate = {2023-12-19},
	journal = {Nature Medicine},
	author = {Huang, Zhi and Bianchi, Federico and Yuksekgonul, Mert and Montine, Thomas J. and Zou, James},
	month = sep,
	year = {2023},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Medical research},
	pages = {2307--2316},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/8C7GGT6T/Huang et al. - 2023 - A visual–language foundation model for pathology image analysis using medical Twitter.pdf:application/pdf},
}

@misc{zhang_large-scale_2023,
	title = {Large-{Scale} {Domain}-{Specific} {Pretraining} for {Biomedical} {Vision}-{Language} {Processing}},
	url = {http://arxiv.org/abs/2303.00915},
	doi = {10.48550/arXiv.2303.00915},
	abstract = {Contrastive pretraining on parallel image-text data has attained great success in vision-language processing (VLP), as exemplified by CLIP and related methods. However, prior explorations tend to focus on general domains in the web. Biomedical images and text are rather different, but publicly available datasets are small and skew toward chest X-ray, thus severely limiting progress. In this paper, we conducted by far the largest study on biomedical VLP, using 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central. Our dataset (PMC-15M) is two orders of magnitude larger than existing biomedical image-text datasets such as MIMIC-CXR, and spans a diverse range of biomedical images. The standard CLIP method is suboptimal for the biomedical domain. We propose BiomedCLIP with domain-specific adaptations tailored to biomedical VLP. We conducted extensive experiments and ablation studies on standard biomedical imaging tasks from retrieval to classification to visual question-answering (VQA). BiomedCLIP established new state of the art in a wide range of standard datasets, substantially outperformed prior VLP approaches. Surprisingly, BiomedCLIP even outperformed radiology-specific state-of-the-art models such as BioViL on radiology-specific tasks such as RSNA pneumonia detection, thus highlighting the utility in large-scale pretraining across all biomedical image types. We will release our models at https://aka.ms/biomedclip to facilitate future research in biomedical VLP.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Zhang, Sheng and Xu, Yanbo and Usuyama, Naoto and Bagga, Jaspreet and Tinn, Robert and Preston, Sam and Rao, Rajesh and Wei, Mu and Valluri, Naveen and Wong, Cliff and Lungren, Matthew P. and Naumann, Tristan and Poon, Hoifung},
	month = mar,
	year = {2023},
	note = {arXiv:2303.00915 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	annote = {Comment: The models will be released soon at https://aka.ms/biomedclip},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/DFW3Y7VD/Zhang et al. - 2023 - Large-Scale Domain-Specific Pretraining for Biomedical Vision-Language Processing.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/3HU7GEXX/2303.html:text/html},
}

@misc{ikezogwo_quilt-1m_2023,
	title = {Quilt-{1M}: {One} {Million} {Image}-{Text} {Pairs} for {Histopathology}},
	shorttitle = {Quilt-{1M}},
	url = {http://arxiv.org/abs/2306.11207},
	doi = {10.48550/arXiv.2306.11207},
	abstract = {Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of analogous data in the medical field, specifically in histopathology, has slowed comparable progress. To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering \$1,087\$ hours of valuable educational histopathology videos from expert clinicians. From YouTube, we curate QUILT: a large-scale vision-language dataset consisting of \$802, 144\$ image and text pairs. QUILT was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition. In comparison, the most comprehensive datasets curated for histopathology amass only around \$200\$K samples. We combine QUILT with datasets from other sources, including Twitter, research papers, and the internet in general, to create an even larger dataset: QUILT-1M, with \$1\$M paired image-text samples, marking it as the largest vision-language histopathology dataset to date. We demonstrate the value of QUILT-1M by fine-tuning a pre-trained CLIP model. Our model outperforms state-of-the-art models on both zero-shot and linear probing tasks for classifying new histopathology images across \$13\$ diverse patch-level datasets of \$8\$ different sub-pathologies and cross-modal retrieval tasks.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Ikezogwo, Wisdom Oluchi and Seyfioglu, Mehmet Saygin and Ghezloo, Fatemeh and Geva, Dylan Stefan Chan and Mohammed, Fatwir Sheikh and Anand, Pavan Kumar and Krishna, Ranjay and Shapiro, Linda},
	month = oct,
	year = {2023},
	note = {arXiv:2306.11207 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/HFX5DA8A/Ikezogwo et al. - 2023 - Quilt-1M One Million Image-Text Pairs for Histopathology.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/23NLKJ9U/2306.html:text/html},
}

@misc{oquab_dinov2_2023,
	title = {{DINOv2}: {Learning} {Robust} {Visual} {Features} without {Supervision}},
	shorttitle = {{DINOv2}},
	url = {http://arxiv.org/abs/2304.07193},
	doi = {10.48550/arXiv.2304.07193},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07193 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/PIW72UDZ/Oquab et al. - 2023 - DINOv2 Learning Robust Visual Features without Supervision.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/C4FLFNMC/2304.html:text/html},
}

@misc{chen_general-purpose_2023,
	title = {A {General}-{Purpose} {Self}-{Supervised} {Model} for {Computational} {Pathology}},
	url = {http://arxiv.org/abs/2308.15474},
	doi = {10.48550/arXiv.2308.15474},
	abstract = {Tissue phenotyping is a fundamental computational pathology (CPath) task in learning objective characterizations of histopathologic biomarkers in anatomic pathology. However, whole-slide imaging (WSI) poses a complex computer vision problem in which the large-scale image resolutions of WSIs and the enormous diversity of morphological phenotypes preclude large-scale data annotation. Current efforts have proposed using pretrained image encoders with either transfer learning from natural image datasets or self-supervised pretraining on publicly-available histopathology datasets, but have not been extensively developed and evaluated across diverse tissue types at scale. We introduce UNI, a general-purpose self-supervised model for pathology, pretrained using over 100 million tissue patches from over 100,000 diagnostic haematoxylin and eosin-stained WSIs across 20 major tissue types, and evaluated on 33 representative CPath clinical tasks in CPath of varying diagnostic difficulties. In addition to outperforming previous state-of-the-art models, we demonstrate new modeling capabilities in CPath such as resolution-agnostic tissue classification, slide classification using few-shot class prototypes, and disease subtyping generalization in classifying up to 108 cancer types in the OncoTree code classification system. UNI advances unsupervised representation learning at scale in CPath in terms of both pretraining data and downstream evaluation, enabling data-efficient AI models that can generalize and transfer to a gamut of diagnostically-challenging tasks and clinical workflows in anatomic pathology.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Chen, Richard J. and Ding, Tong and Lu, Ming Y. and Williamson, Drew F. K. and Jaume, Guillaume and Chen, Bowen and Zhang, Andrew and Shao, Daniel and Song, Andrew H. and Shaban, Muhammad and Williams, Mane and Vaidya, Anurag and Sahai, Sharifa and Oldenburg, Lukas and Weishaupt, Luca L. and Wang, Judy J. and Williams, Walt and Le, Long Phi and Gerber, Georg and Mahmood, Faisal},
	month = aug,
	year = {2023},
	note = {arXiv:2308.15474 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Tissues and Organs},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/BVANQTAD/Chen et al. - 2023 - A General-Purpose Self-Supervised Model for Computational Pathology.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/2NQZSXDE/2308.html:text/html},
}

@misc{rombach_high-resolution_2022,
	title = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2112.10752},
	doi = {10.48550/arXiv.2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
	urldate = {2023-12-20},
	publisher = {arXiv},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	month = apr,
	year = {2022},
	note = {arXiv:2112.10752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2022},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/DSD9MBT8/2112.html:text/html;Full Text:/home/zwerg/Zotero/storage/J9UTIG6M/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffusion Models.pdf:application/pdf},
}

@misc{touvron_llama_2023,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	urldate = {2024-01-05},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/RQKWX56F/2307.html:text/html;Full Text PDF:/home/zwerg/Zotero/storage/SPSZDBHA/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf:application/pdf},
}

@inproceedings{huang_improving_2020,
	series = {{ICML}'20},
	title = {Improving transformer optimization through better initialization},
	volume = {119},
	abstract = {The Transformer architecture has achieved considerable success recently; the key component of the Transformer is the attention layer that enables the model to focus on important regions within an input sequence. Gradient optimization with attention layers can be notoriously difficult requiring tricks such as learning rate warmup to prevent divergence. As Transformer models are becoming larger and more expensive to train, recent research has focused on understanding and improving optimization in these architectures. In this work our contributions are two-fold: we first investigate and empirically validate the source of optimization problems in the encoder-decoder Transformer architecture; we then propose a new weight initialization scheme with theoretical justification, that enables training without warmup or layer normalization. Empirical results on public machine translation benchmarks show that our approach achieves leading accuracy, allowing to train deep Transformer models with 200 layers in both encoder and decoder (over 1000 attention/MLP blocks) without difficulty.},
	urldate = {2024-01-04},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Huang, Xiao Shi and Pérez, Felipe and Ba, Jimmy and Volkovs, Maksims},
	month = jul,
	year = {2020},
	pages = {4475--4483},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/DYR3IUTX/Huang et al. - 2020 - Improving transformer optimization through better initialization.pdf:application/pdf},
}

@misc{noauthor_structured_nodate,
	title = {Structured {State} {Spaces} for {Sequence} {Modeling} ({S4})},
	url = {https://hazyresearch.stanford.edu/blog/2022-01-14-s4-1},
	language = {en},
	urldate = {2024-01-03},
	file = {Snapshot:/home/zwerg/Zotero/storage/UPXDYIBW/2022-01-14-s4-1.html:text/html},
}

@misc{zhu_solving_2024,
	title = {Solving {Reasoning} {Problems} with {LLMs} in 2023},
	url = {https://towardsdatascience.com/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d},
	abstract = {It’s the beginning of 2024 and ChatGPT just celebrated its one-year birthday. One year is a super long time for the community of large…},
	language = {en},
	urldate = {2024-01-11},
	journal = {Medium},
	author = {Zhu, Zhaocheng},
	month = jan,
	year = {2024},
	file = {Snapshot:/home/zwerg/Zotero/storage/R6XK25LR/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d.html:text/html},
}

@misc{xiong_yformerefficientsam_2024,
	title = {yformer/{EfficientSAM}},
	copyright = {Apache-2.0},
	url = {https://github.com/yformer/EfficientSAM},
	abstract = {EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything},
	urldate = {2024-01-10},
	author = {Xiong, Yunyang},
	month = jan,
	year = {2024},
	note = {original-date: 2023-11-28T03:26:35Z},
}

@article{xiong_efficientsam_nodate,
	title = {{EfficientSAM}: {Leveraged} {Masked} {Image} {Pretraining} for {Efficient} {Segment} {Anything}},
	url = {https://arxiv.org/pdf/2312.00863.pdf},
	abstract = {Segment Anything Model (SAM) has emerged as a powerful tool for numerous vision applications. A key component that drives the impressive performance for zero-shot transfer and high versatility is a super large Transformer model trained on the extensive high-quality SA-1B dataset. While beneficial, the huge computation cost of SAM model has limited its applications to wider real-world applications. To address this limitation, we propose EfficientSAMs, lightweight SAM models that exhibits decent performance with largely reduced complexity. Our idea is based on leveraging masked image pretraining, SAMI, which learns to reconstruct features from SAM image encoder for effective visual representation learning. Further, we take SAMI-pretrained light-weight image encoders and mask decoder to build EfficientSAMs, and finetune the models on SA-1B for segment anything task. We perform evaluations on multiple vision tasks including image classification, object detection, instance segmentation, and semantic object detection, and find that our proposed pretraining method, SAMI, consistently outperforms other masked image pretraining methods. On segment anything task such as zero-shot instance segmentation, our EfficientSAMs with SAMI-pretrained lightweight image encoders perform favorably with a significant gain (e.g., ∼4 AP on COCO/LVIS) over other fast SAM models.},
	language = {en},
	author = {Xiong, Yunyang and Varadarajan, Bala and Wu, Lemeng and Xiang, Xiaoyu and Xiao, Fanyi and Zhu, Chenchen and Dai, Xiaoliang and Wang, Dilin and Sun, Fei and Iandola, Forrest and Krishnamoorthi, Raghuraman and Chandra, Vikas},
	file = {Xiong et al. - EfficientSAM Leveraged Masked Image Pretraining for Efficient Segment Anything.pdf:/home/zwerg/Zotero/storage/5A4EE4NR/Xiong et al. - EfficientSAM Leveraged Masked Image Pretraining for Efficient Segment Anything.pdf:application/pdf},
}

@misc{sun_pathasst_2023,
	title = {{PathAsst}: {Redefining} {Pathology} through {Generative} {Foundation} {AI} {Assistant} for {Pathology}},
	shorttitle = {{PathAsst}},
	url = {http://arxiv.org/abs/2305.15072},
	doi = {10.48550/arXiv.2305.15072},
	abstract = {As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, with significant applications in natural image interpretation. However, the field of pathology has largely remained untapped in this regard, despite the growing need for accurate, timely, and personalized diagnostics. To bridge the gap in pathology MLLMs, we present the PathAsst in this study, which is a generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. To develop PathAsst, we collect over 142K high-quality pathology image-text pairs from a variety of reliable sources, including PubMed, comprehensive pathology textbooks, reputable pathology websites, and private data annotated by pathologists. Leveraging the advanced capabilities of ChatGPT/GPT-4, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data, specifically tailored for the invocation of the pathology-specific models, allowing the PathAsst to effectively interact with these models based on the input image and user intent, consequently enhancing the model's diagnostic capabilities. Subsequently, our PathAsst is trained based on Vicuna-13B language model in coordination with the CLIP vision encoder. The results of PathAsst show the potential of harnessing the AI-powered generative foundation model to improve pathology diagnosis and treatment processes. We are committed to open-sourcing our meticulously curated dataset, as well as a comprehensive toolkit designed to aid researchers in the extensive collection and preprocessing of their own datasets. Resources can be obtained at https://github.com/superjamessyx/Generative-Foundation-AI-Assistant-for-Pathology.},
	urldate = {2024-01-09},
	publisher = {arXiv},
	author = {Sun, Yuxuan and Zhu, Chenglu and Zheng, Sunyi and Zhang, Kai and Shui, Zhongyi and Yu, Xiaoxuan and Zhao, Yizhi and Li, Honglin and Zhang, Yunlong and Zhao, Ruojia and Lyu, Xinheng and Yang, Lin},
	month = may,
	year = {2023},
	note = {arXiv:2305.15072 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
	annote = {Comment: 13 pages, 5 figures, conference},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/R8I2ZZSX/Sun et al. - 2023 - PathAsst Redefining Pathology through Generative Foundation AI Assistant for Pathology.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/V29U6GH3/2305.html:text/html},
}

@misc{zhang_root_2019,
	title = {Root {Mean} {Square} {Layer} {Normalization}},
	url = {http://arxiv.org/abs/1910.07467},
	doi = {10.48550/arXiv.1910.07467},
	abstract = {Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However, the computational overhead introduced by LayerNorm makes these improvements expensive and significantly slows the underlying network, e.g. RNN in particular. In this paper, we hypothesize that re-centering invariance in LayerNorm is dispensable and propose root mean square layer normalization, or RMSNorm. RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm is computationally simpler and thus more efficient than LayerNorm. We also present partial RMSNorm, or pRMSNorm where the RMS is estimated from p\% of the summed inputs without breaking the above properties. Extensive experiments on several tasks using diverse network architectures show that RMSNorm achieves comparable performance against LayerNorm but reduces the running time by 7\%{\textasciitilde}64\% on different models. Source code is available at https://github.com/bzhangGo/rmsnorm.},
	urldate = {2024-01-08},
	publisher = {arXiv},
	author = {Zhang, Biao and Sennrich, Rico},
	month = oct,
	year = {2019},
	note = {arXiv:1910.07467 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: NeurIPS 2019},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/5D3I4WLK/Zhang and Sennrich - 2019 - Root Mean Square Layer Normalization.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/D7N5EGFX/1910.html:text/html},
}

@misc{khan_building_2023,
	title = {Building a {Million}-{Parameter} {LLM} from {Scratch} {Using} {Python}},
	url = {https://levelup.gitconnected.com/building-a-million-parameter-llm-from-scratch-using-python-f612398f06c2},
	abstract = {A Step-by-Step Guide to Replicating LLaMA Architecture},
	language = {en},
	urldate = {2024-01-08},
	journal = {Medium},
	author = {Khan, Fareed},
	month = dec,
	year = {2023},
}

@article{farquhar_discussion_nodate,
	title = {Discussion: {Challenges} with {Unsupervised} {LLM} {Knowledge} {Discovery}},
	shorttitle = {Discussion},
	url = {https://www.lesswrong.com/posts/wtfvbsYjNHYYBmT3k/discussion-challenges-with-unsupervised-llm-knowledge-1},
	abstract = {TL;DR: Contrast-consistent search (CCS) seemed exciting to us and we were keen to apply it. At this point, we think it is unlikely to be directly hel…},
	language = {en},
	urldate = {2024-01-08},
	author = {Farquhar, Seb and Varma, Vikrant and zac\_kenton and gasteigerjo and Mikulik, Vlad and Shah, Rohin},
}

@misc{noauthor_recipe_nodate,
	title = {Recipe for {Serving} {Thousands} of {Concurrent} {LoRA} {Adapters} {\textbar} {LMSYS} {Org}},
	url = {https://lmsys.org/blog/2023-11-15-slora},
	abstract = {{\textless}p{\textgreater}In this blog post, we introduce {\textless}a href="https://arxiv.org/abs/2311.03285"{\textgreater}S-LoRA{\textless}/a{\textgreater} ({\textless}a href="https://github.com/S-LoRA/S-LoRA"{\textgreater}code{\textless}/a{\textgreater}), a system desi...},
	language = {en},
	urldate = {2024-01-08},
	file = {Snapshot:/home/zwerg/Zotero/storage/49ER7N6B/2023-11-15-slora.html:text/html},
}

@misc{ma_multi-modality_2023,
	title = {The {Multi}-modality {Cell} {Segmentation} {Challenge}: {Towards} {Universal} {Solutions}},
	shorttitle = {The {Multi}-modality {Cell} {Segmentation} {Challenge}},
	url = {https://arxiv.org/abs/2308.05864v1},
	abstract = {Cell segmentation is a critical step for quantitative single-cell analysis in microscopy images. Existing cell segmentation methods are often tailored to specific modalities or require manual interventions to specify hyperparameters in different experimental settings. Here, we present a multi-modality cell segmentation benchmark, comprising over 1500 labeled images derived from more than 50 diverse biological experiments. The top participants developed a Transformer-based deep-learning algorithm that not only exceeds existing methods, but can also be applied to diverse microscopy images across imaging platforms and tissue types without manual parameter adjustments. This benchmark and the improved algorithm offer promising avenues for more accurate and versatile cell analysis in microscopy imaging.},
	language = {en},
	urldate = {2024-01-12},
	journal = {arXiv.org},
	author = {Ma, Jun and Xie, Ronald and Ayyadhury, Shamini and Ge, Cheng and Gupta, Anubha and Gupta, Ritu and Gu, Song and Zhang, Yao and Lee, Gihun and Kim, Joonkee and Lou, Wei and Li, Haofeng and Upschulte, Eric and Dickscheid, Timo and de Almeida, José Guilherme and Wang, Yixin and Han, Lin and Yang, Xin and Labagnara, Marco and Rahi, Sahand Jamal and Kempster, Carly and Pollitt, Alice and Espinosa, Leon and Mignot, Tâm and Middeke, Jan Moritz and Eckardt, Jan-Niklas and Li, Wangkai and Li, Zhaoyang and Cai, Xiaochen and Bai, Bizhe and Greenwald, Noah F. and Van Valen, David and Weisbart, Erin and Cimini, Beth A. and Li, Zhuoshi and Zuo, Chao and Brück, Oscar and Bader, Gary D. and Wang, Bo},
	month = aug,
	year = {2023},
}

@misc{ma_multi-modality_2023-1,
	title = {The {Multi}-modality {Cell} {Segmentation} {Challenge}: {Towards} {Universal} {Solutions}},
	shorttitle = {The {Multi}-modality {Cell} {Segmentation} {Challenge}},
	url = {https://arxiv.org/abs/2308.05864},
	doi = {10.48550/arXiv.2308.05864},
	abstract = {Cell segmentation is a critical step for quantitative single-cell analysis in microscopy images. Existing cell segmentation methods are often tailored to specific modalities or require manual interventions to specify hyperparameters in different experimental settings. Here, we present a multi-modality cell segmentation benchmark, comprising over 1500 labeled images derived from more than 50 diverse biological experiments. The top participants developed a Transformer-based deep-learning algorithm that not only exceeds existing methods, but can also be applied to diverse microscopy images across imaging platforms and tissue types without manual parameter adjustments. This benchmark and the improved algorithm offer promising avenues for more accurate and versatile cell analysis in microscopy imaging.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Ma, Jun and Xie, Ronald and Ayyadhury, Shamini and Ge, Cheng and Gupta, Anubha and Gupta, Ritu and Gu, Song and Zhang, Yao and Lee, Gihun and Kim, Joonkee and Lou, Wei and Li, Haofeng and Upschulte, Eric and Dickscheid, Timo and de Almeida, José Guilherme and Wang, Yixin and Han, Lin and Yang, Xin and Labagnara, Marco and Rahi, Sahand Jamal and Kempster, Carly and Pollitt, Alice and Espinosa, Leon and Mignot, Tâm and Middeke, Jan Moritz and Eckardt, Jan-Niklas and Li, Wangkai and Li, Zhaoyang and Cai, Xiaochen and Bai, Bizhe and Greenwald, Noah F. and Van Valen, David and Weisbart, Erin and Cimini, Beth A. and Li, Zhuoshi and Zuo, Chao and Brück, Oscar and Bader, Gary D. and Wang, Bo},
	month = aug,
	year = {2023},
	note = {arXiv:2308.05864 [cs, eess, q-bio]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Quantitative Biology - Quantitative Methods},
	annote = {Comment: NeurIPS22 Cell Segmentation Challenge: https://neurips22-cellseg.grand-challenge.org/},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/M2BRR42L/Ma et al. - 2023 - The Multi-modality Cell Segmentation Challenge Towards Universal Solutions.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/GS7UWLKI/2308.html:text/html},
}

@article{wang_use_2024,
	title = {On the use of deep learning for phase recovery},
	volume = {13},
	copyright = {2024 The Author(s)},
	issn = {2047-7538},
	url = {https://www.nature.com/articles/s41377-023-01340-x},
	doi = {10.1038/s41377-023-01340-x},
	abstract = {Phase recovery (PR) refers to calculating the phase of the light field from its intensity measurements. As exemplified from quantitative phase imaging and coherent diffraction imaging to adaptive optics, PR is essential for reconstructing the refractive index distribution or topography of an object and correcting the aberration of an imaging system. In recent years, deep learning (DL), often implemented through deep neural networks, has provided unprecedented support for computational imaging, leading to more efficient solutions for various PR problems. In this review, we first briefly introduce conventional methods for PR. Then, we review how DL provides support for PR from the following three stages, namely, pre-processing, in-processing, and post-processing. We also review how DL is used in phase image processing. Finally, we summarize the work in DL for PR and provide an outlook on how to better use DL to improve the reliability and efficiency of PR. Furthermore, we present a live-updating resource (https://github.com/kqwang/phase-recovery) for readers to learn more about PR.},
	language = {en},
	number = {1},
	urldate = {2024-01-15},
	journal = {Light: Science \& Applications},
	author = {Wang, Kaiqiang and Song, Li and Wang, Chutian and Ren, Zhenbo and Zhao, Guangyuan and Dou, Jiazhen and Di, Jianglei and Barbastathis, George and Zhou, Renjie and Zhao, Jianlin and Lam, Edmund Y.},
	month = jan,
	year = {2024},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Imaging and sensing, Optical metrology},
	pages = {4},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/6GRCISQD/Wang et al. - 2024 - On the use of deep learning for phase recovery.pdf:application/pdf},
}

@misc{center_use_nodate,
	title = {The use of deep learning for phase recovery},
	url = {https://phys.org/news/2024-01-deep-phase-recovery.html},
	abstract = {Light, as an electromagnetic field, has two essential components: amplitude and phase. However, optical detectors, usually relying on photon-to-electron conversion (such as charge-coupled device sensors and the human eye), cannot capture the phase of the light field because of their limited sampling frequency.},
	language = {en},
	urldate = {2024-01-15},
	author = {Center, Light Publishing and Optics, Changchun Institute of and Mechanics, Fine and {Physics} and {CAS}},
	file = {Snapshot:/home/zwerg/Zotero/storage/9Z9L6M84/2024-01-deep-phase-recovery.html:text/html},
}

@misc{rafailov_direct_2023-1,
	title = {Direct {Preference} {Optimization}: {Your} {Language} {Model} is {Secretly} a {Reward} {Model}},
	shorttitle = {Direct {Preference} {Optimization}},
	url = {http://arxiv.org/abs/2305.18290},
	doi = {10.48550/arXiv.2305.18290},
	abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
	month = dec,
	year = {2023},
	note = {arXiv:2305.18290 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/JS485HB4/Rafailov et al. - 2023 - Direct Preference Optimization Your Language Model is Secretly a Reward Model.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/MMFA3S5Q/2305.html:text/html},
}

@article{vong_grounded_2024,
	title = {Grounded language acquisition through the eyes and ears of a single child},
	volume = {383},
	url = {https://www.science.org/doi/10.1126/science.adi1374},
	doi = {10.1126/science.adi1374},
	abstract = {Starting around 6 to 9 months of age, children begin acquiring their first words, linking spoken words to their visual counterparts. How much of this knowledge is learnable from sensory input with relatively generic learning mechanisms, and how much requires stronger inductive biases? Using longitudinal head-mounted camera recordings from one child aged 6 to 25 months, we trained a relatively generic neural network on 61 hours of correlated visual-linguistic data streams, learning feature-based representations and cross-modal associations. Our model acquires many word-referent mappings present in the child’s everyday experience, enables zero-shot generalization to new visual referents, and aligns its visual and linguistic conceptual systems. These results show how critical aspects of grounded word meaning are learnable through joint representation and associative learning from one child’s input.},
	number = {6682},
	urldate = {2024-02-02},
	journal = {Science},
	author = {Vong, Wai Keen and Wang, Wentao and Orhan, A. Emin and Lake, Brenden M.},
	month = feb,
	year = {2024},
	pages = {504--511},
}

@misc{vu_how_2024,
	title = {How to {Learn} {AI} on {Your} {Own} (a self-study guide)},
	url = {https://towardsdatascience.com/how-to-learn-ai-on-your-own-a-self-study-guide-a67e23350c24},
	abstract = {If your hands touch a keyboard for work, Artificial Intelligence is going to change your job in the next few years.},
	language = {en},
	urldate = {2024-02-06},
	journal = {Medium},
	author = {Vu, Thu},
	month = feb,
	year = {2024},
	file = {Snapshot:/home/zwerg/Zotero/storage/JQX9Q6YK/how-to-learn-ai-on-your-own-a-self-study-guide-a67e23350c24.html:text/html},
}


@misc{choi_retain_2017,
	title = {{RETAIN}: {An} {Interpretable} {Predictive} {Model} for {Healthcare} using {Reverse} {Time} {Attention} {Mechanism}},
	shorttitle = {{RETAIN}},
	url = {http://arxiv.org/abs/1608.05745},
	doi = {10.48550/arXiv.1608.05745},
	abstract = {Accuracy and interpretability are two dominant features of successful predictive models. Typically, a choice must be made in favor of complex black box models such as recurrent neural networks (RNN) for accuracy versus less accurate but more interpretable traditional models such as logistic regression. This tradeoff poses challenges in medicine where both accuracy and interpretability are important. We addressed this challenge by developing the REverse Time AttentIoN model (RETAIN) for application to Electronic Health Records (EHR) data. RETAIN achieves high accuracy while remaining clinically interpretable and is based on a two-level neural attention model that detects influential past visits and significant clinical variables within those visits (e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that recent clinical visits are likely to receive higher attention. RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K patients over an 8 year period and demonstrated predictive accuracy and computational scalability comparable to state-of-the-art methods such as RNN, and ease of interpretability comparable to traditional models.},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Choi, Edward and Bahadori, Mohammad Taha and Kulas, Joshua A. and Schuetz, Andy and Stewart, Walter F. and Sun, Jimeng},
	month = feb,
	year = {2017},
	note = {arXiv:1608.05745 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Accepted at Neural Information Processing Systems (NIPS) 2016},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/UYWBJ44X/Choi et al. - 2017 - RETAIN An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/7EEUWH3N/1608.html:text/html},
}

@inproceedings{mullenbach-etal-2018-explainable,
    title = "Explainable Prediction of Medical Codes from Clinical Text",
    author = "Mullenbach, James  and
      Wiegreffe, Sarah  and
      Duke, Jon  and
      Sun, Jimeng  and
      Eisenstein, Jacob",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1100",
    doi = "10.18653/v1/N18-1100",
    pages = "1101--1111",
    abstract = "Clinical notes are text documents that are created by clinicians for each patient encounter. They are typically accompanied by medical codes, which describe the diagnosis and treatment. Annotating these codes is labor intensive and error prone; furthermore, the connection between the codes and the text is not annotated, obscuring the reasons and details behind specific diagnoses and treatments. We present an attentional convolutional network that predicts medical codes from clinical text. Our method aggregates information across the document using a convolutional neural network, and uses an attention mechanism to select the most relevant segments for each of the thousands of possible codes. The method is accurate, achieving precision@8 of 0.71 and a Micro-F1 of 0.54, which are both better than the prior state of the art. Furthermore, through an interpretability evaluation by a physician, we show that the attention mechanism identifies meaningful explanations for each code assignment.",
}


@inproceedings{prangemeier_attention-based_2020,
	title = {Attention-{Based} {Transformers} for {Instance} {Segmentation} of {Cells} in {Microstructures}},
	url = {http://arxiv.org/abs/2011.09763},
	doi = {10.1109/BIBM49941.2020.9313305},
	abstract = {Detecting and segmenting object instances is a common task in biomedical applications. Examples range from detecting lesions on functional magnetic resonance images, to the detection of tumours in histopathological images and extracting quantitative single-cell information from microscopy imagery, where cell segmentation is a major bottleneck. Attention-based transformers are state-of-the-art in a range of deep learning fields. They have recently been proposed for segmentation tasks where they are beginning to outperforming other methods. We present a novel attention-based cell detection transformer (Cell-DETR) for direct end-to-end instance segmentation. While the segmentation performance is on par with a state-of-the-art instance segmentation method, Cell-DETR is simpler and faster. We showcase the method's contribution in a the typical use case of segmenting yeast in microstructured environments, commonly employed in systems or synthetic biology. For the specific use case, the proposed method surpasses the state-of-the-art tools for semantic segmentation and additionally predicts the individual object instances. The fast and accurate instance segmentation performance increases the experimental information yield for a posteriori data processing and makes online monitoring of experiments and closed-loop optimal experimental design feasible.},
	urldate = {2024-02-18},
	booktitle = {2020 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Prangemeier, Tim and Reich, Christoph and Koeppl, Heinz},
	month = dec,
	year = {2020},
	note = {arXiv:2011.09763 [physics, q-bio]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Signal Processing, Physics - Instrumentation and Detectors, Quantitative Biology - Quantitative Methods},
	pages = {700--707},
	annote = {Comment: IEEE BIBM 2020 (accepted)},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/HDJPVEXH/Prangemeier et al. - 2020 - Attention-Based Transformers for Instance Segmentation of Cells in Microstructures.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/9JD7J7D6/2011.html:text/html},
}

@article{gu_domain-specific_2022,
	title = {Domain-{Specific} {Language} {Model} {Pretraining} for {Biomedical} {Natural} {Language} {Processing}},
	volume = {3},
	issn = {2691-1957, 2637-8051},
	url = {http://arxiv.org/abs/2007.15779},
	doi = {10.1145/3458754},
	abstract = {Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this paper, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly-available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition (NER). To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding \& Reasoning Benchmark) at https://aka.ms/BLURB.},
	number = {1},
	urldate = {2022-10-02},
	journal = {ACM Transactions on Computing for Healthcare},
	author = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
	month = jan,
	year = {2022},
	note = {arXiv:2007.15779 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	pages = {1--23},
	annote = {Comment: ACM Transactions on Computing for Healthcare (HEALTH)},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/Q927UQE9/2007.html:text/html;Gu et al. - 2022 - Domain-Specific Language Model Pretraining for Bio.pdf:/home/zwerg/Zotero/storage/7S9LETYT/Gu et al. - 2022 - Domain-Specific Language Model Pretraining for Bio.pdf:application/pdf},
}


@article{lee_biobert_2019,
	title = {{BioBERT}: a pre-trained biomedical language representation model for biomedical text mining},
	issn = {1367-4803, 1460-2059},
	shorttitle = {{BioBERT}},
	url = {http://arxiv.org/abs/1901.08746},
	doi = {10.1093/bioinformatics/btz682},
	abstract = {Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62\% F1 score improvement), biomedical relation extraction (2.80\% F1 score improvement) and biomedical question answering (12.24\% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.},
	urldate = {2022-10-02},
	journal = {Bioinformatics},
	author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
	month = sep,
	year = {2019},
	note = {arXiv:1901.08746 [cs]},
	keywords = {Computer Science - Computation and Language},
	pages = {btz682},
	annote = {Comment: Bioinformatics},
	file = {arXiv.org Snapshot:/home/zwerg/Zotero/storage/82ES6S4K/1901.html:text/html;Lee et al. - 2019 - BioBERT a pre-trained biomedical language represe.pdf:/home/zwerg/Zotero/storage/LBRX5NHM/Lee et al. - 2019 - BioBERT a pre-trained biomedical language represe.pdf:application/pdf},
}


@misc{rusch_survey_2023,
	title = {A {Survey} on {Oversmoothing} in {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2303.10993},
	doi = {10.48550/arXiv.2303.10993},
	abstract = {Node features of graph neural networks (GNNs) tend to become more similar with the increase of the network depth. This effect is known as over-smoothing, which we axiomatically define as the exponential convergence of suitable similarity measures on the node features. Our definition unifies previous approaches and gives rise to new quantitative measures of over-smoothing. Moreover, we empirically demonstrate this behavior for several over-smoothing measures on different graphs (small-, medium-, and large-scale). We also review several approaches for mitigating over-smoothing and empirically test their effectiveness on real-world graph datasets. Through illustrative examples, we demonstrate that mitigating over-smoothing is a necessary but not sufficient condition for building deep GNNs that are expressive on a wide range of graph learning tasks. Finally, we extend our definition of over-smoothing to the rapidly emerging field of continuous-time GNNs.},
	urldate = {2024-03-12},
	publisher = {arXiv},
	author = {Rusch, T. Konstantin and Bronstein, Michael M. and Mishra, Siddhartha},
	month = mar,
	year = {2023},
	note = {arXiv:2303.10993 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/zwerg/Zotero/storage/9T9E83XK/Rusch et al. - 2023 - A Survey on Oversmoothing in Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/zwerg/Zotero/storage/7PZ4YPGY/2303.html:text/html},
}


@misc{hur_unifying_2022,
	title = {Unifying {Heterogeneous} {Electronic} {Health} {Records} {Systems} via {Text}-{Based} {Code} {Embedding}},
	url = {http://arxiv.org/abs/2108.03625},
	doi = {10.48550/arXiv.2108.03625},
	abstract = {Substantial increase in the use of Electronic Health Records (EHRs) has opened new frontiers for predictive healthcare. However, while EHR systems are nearly ubiquitous, they lack a unified code system for representing medical concepts. Heterogeneous formats of EHR present a substantial barrier for the training and deployment of state-of-the-art deep learning models at scale. To overcome this problem, we introduce Description-based Embedding, DescEmb, a code-agnostic description-based representation learning framework for predictive modeling on EHR. DescEmb takes advantage of the flexibility of neural language understanding models while maintaining a neutral approach that can be combined with prior frameworks for task-specific representation learning or predictive modeling. We tested our model's capacity on various experiments including prediction tasks, transfer learning and pooled learning. DescEmb shows higher performance in overall experiments compared to code-based approach, opening the door to a text-based approach in predictive healthcare research that is not constrained by EHR structure nor special domain knowledge.},
	urldate = {2024-02-24},
	author = {Hur, Kyunghoon and Lee, Jiyoung and Oh, Jungwoo and Price, Wesley and Kim, Young-Hak and Choi, Edward},
	month = mar,
	year = {2022},
	note = {arXiv:2108.03625 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Hur et al. - 2022 - Unifying Heterogeneous Electronic Health Records Systems via Text-Based Code Embedding.pdf:/home/zwerg/Zotero/storage/NUEDF4HY/Hur et al. - 2022 - Unifying Heterogeneous Electronic Health Records Systems via Text-Based Code Embedding.pdf:application/pdf},
}


@misc{huang_clinicalbert_2020,
	title = {{ClinicalBERT}: {Modeling} {Clinical} {Notes} and {Predicting} {Hospital} {Readmission}},
	shorttitle = {{ClinicalBERT}},
	url = {http://arxiv.org/abs/1904.05342},
	abstract = {Clinical notes contain information about patients beyond structured data such as lab values or medications. However, clinical notes have been underused relative to structured data, because notes are highdimensional and sparse. We aim to develop and evaluate a continuous representation of clinical notes. Given this representation, our goal is to predict 30-day hospital readmission at various timepoints of admission, including early stages and at discharge. We apply bidirectional encoder representations from transformers (bert) to clinical text. Publicly-released bert parameters are trained on standard corpora such as Wikipedia and BookCorpus, which differ from clinical text. We therefore pre-train bert using clinical notes and finetune the network for the task of predicting hospital readmission. This defines ClinicalBERT. ClinicalBERT uncovers high-quality relationships between medical concepts, as judged by physicians. ClinicalBERT outperforms various baselines on 30-day hospital readmission prediction using both discharge summaries and the first few days of notes in the intensive care unit on various clinically-motivated metrics. The attention weights of ClinicalBERT can also be used to interpret predictions. To facilitate research, we open-source model parameters, and scripts for training and evaluation. ClinicalBERT is a flexible framework to represent clinical notes. It improves on previous clinical text processing methods and with little engineering can be adapted to other clinical predictive tasks.},
	language = {en},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Huang, Kexin and Altosaar, Jaan and Ranganath, Rajesh},
	month = nov,
	year = {2020},
	note = {arXiv:1904.05342 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: CHIL 2020 Workshop},
	file = {Huang et al. - 2020 - ClinicalBERT Modeling Clinical Notes and Predicti.pdf:/home/zwerg/Zotero/storage/AETH8IS4/Huang et al. - 2020 - ClinicalBERT Modeling Clinical Notes and Predicti.pdf:application/pdf},
}



@article{johnson_mimic-iii_2016,
	title = {{MIMIC}-{III}, a freely accessible critical care database},
	volume = {3},
	copyright = {2016 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201635},
	doi = {10.1038/sdata.2016.35},
	abstract = {MIMIC-III (‘Medical Information Mart for Intensive Care’) is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
	language = {en},
	number = {1},
	urldate = {2024-03-23},
	journal = {Scientific Data},
	author = {Johnson, Alistair E. W. and Pollard, Tom J. and Shen, Lu and Lehman, Li-wei H. and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G.},
	month = may,
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	keywords = {Diagnosis, Health care, Medical research, Outcomes research, Prognosis},
	pages = {160035},
	file = {Full Text PDF:/home/zwerg/Zotero/storage/2VLVKE9B/Johnson et al. - 2016 - MIMIC-III, a freely accessible critical care database.pdf:application/pdf},
}

@misc{johnson_mimic-iii_2015,
	title = {{MIMIC}-{III} {Clinical} {Database}},
	url = {https://physionet.org/content/mimiciii/1.4/},
	doi = {10.13026/C2XW26},
	abstract = {MIMIC-III is a large, freely-available database comprising deidentified
health-related data associated with over forty thousand patients who stayed in
critical care units of the Beth Israel Deaconess Medical Center between 2001
and 2012. The database includes information such as demographics, vital sign
measurements made at the bedside ({\textasciitilde}1 data point per hour), laboratory test
results, procedures, medications, caregiver notes, imaging reports, and
mortality (including post-hospital discharge).

MIMIC supports a diverse range of analytic studies spanning epidemiology,
clinical decision-rule improvement, and electronic tool development. It is
notable for three factors: it is freely available to researchers worldwide; it
encompasses a diverse and very large population of ICU patients; and it
contains highly granular data, including vital signs, laboratory results, and
medications.},
	urldate = {2024-03-23},
	publisher = {[object Object]},
	author = {Johnson, Alistair and Pollard, Tom and Mark, Roger},
	year = {2015},
}

@article{pollard_eicu_2018,
	title = {The {eICU} {Collaborative} {Research} {Database}, a freely available multi-center database for critical care research},
	volume = {5},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata2018178},
	doi = {10.1038/sdata.2018.178},
	abstract = {Abstract
            Critical care patients are monitored closely through the course of their illness. As a result of this monitoring, large amounts of data are routinely collected for these patients. Philips Healthcare has developed a telehealth system, the eICU Program, which leverages these data to support management of critically ill patients. Here we describe the eICU Collaborative Research Database, a multi-center intensive care unit (ICU)database with high granularity data for over 200,000 admissions to ICUs monitored by eICU Programs across the United States. The database is deidentified, and includes vital sign measurements, care plan documentation, severity of illness measures, diagnosis information, treatment information, and more. Data are publicly available after registration, including completion of a training course in research with human subjects and signing of a data use agreement mandating responsible handling of the data and adhering to the principle of collaborative research. The freely available nature of the data will support a number of applications including the development of machine learning algorithms, decision support tools, and clinical research.},
	language = {en},
	number = {1},
	urldate = {2024-03-23},
	journal = {Scientific Data},
	author = {Pollard, Tom J. and Johnson, Alistair E. W. and Raffa, Jesse D. and Celi, Leo A. and Mark, Roger G. and Badawi, Omar},
	month = sep,
	year = {2018},
	pages = {180178},
	file = {Full Text:/home/zwerg/Zotero/storage/TETEXFZP/Pollard et al. - 2018 - The eICU Collaborative Research Database, a freely available multi-center database for critical care.pdf:application/pdf},
}

@article{casson2023transformerflops,
  author={Adam Casson},
  title={Transformer FLOPs},
  year={2023},
  url={https://adamcasson.com/posts/transformer-flops}
}
