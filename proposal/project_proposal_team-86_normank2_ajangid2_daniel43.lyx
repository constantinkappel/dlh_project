#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{enumitem}
\usepackage[usenames,dvipsnames]{xcolor}

\usepackage{titlesec}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{array}
\usepackage{siunitx}
%%%%%%%%%%%%%%%
%% FEEL FREE TO ADD MORE PACKAGES %%% 
%%%%%%
%%% LaTeX 
%% https://www.overleaf.com/learn/latex/Learn_LaTeX_in_30_minutes
%% https://www.overleaf.com/learn/latex/Positioning_images_and_tables 

%%% TODO: change TOPIC to your title
\title{CS447 Literature Review: Biomedical Information Extraction}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8
\fontencoding default
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "mathptmx" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref section
\pdf_pdfusetitle false
\pdf_quoted_options "allcolors=MidnightBlue"
\papersize letterpaper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 0
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.1in
\topmargin 1in
\rightmargin 1.1in
\bottommargin 1.1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
● Missing citation to the original paper (-1) ● Missing general problem
 (-2) ● Missing specific approach (-2) ● Missing hypotheses to be tested
 (-2) ● Missing ablations planned (-2) ● Missing description of how you
 will access the data (-2) ● Missing discussion of the feasibility of the
 computation (-2) ● Missing statement of whether you will use the existing
 code or not (-2) ● Exceed page limits (-1)
\end_layout

\end_inset


\end_layout

\begin_layout Title
CS 598 DLH Project Proposal 
\end_layout

\begin_layout Author
Amit Jangid, Constantin Kappel, Daniel Sanchez
\end_layout

\begin_layout Date
March 24th 2024
\end_layout

\begin_layout Address
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
hspace{0cm}
\end_layout

\end_inset

ajangid2@illinois.edu, normank2@illinois.edu, daniel43@illinois.edu
\end_layout

\begin_layout Section
Citation to original Paper
\end_layout

\begin_layout Standard
We picked a publication by Hur et al.
 
\begin_inset CommandInset citation
LatexCommand citep
key "hur_unifying_2022"
literal "false"

\end_inset

 titled 
\begin_inset Quotes eld
\end_inset


\emph on
Unifying Heterogeneous Electronic Health Records Systems via Text-Based
 Code Embedding
\emph default

\begin_inset Quotes erd
\end_inset

.
 
\end_layout

\begin_layout Section
General Problem
\end_layout

\begin_layout Standard
While EHR (Electronic Health Records) provide an attractive data source
 to do research on exposures and disease, there are many heterogenous medical
 code formats used by different healthcare providers.
 Thus, clinical studies using EHRs are difficult to scale up due to data
 incompatibilities.
 The technical hurdle is that previous systems learned hidden representations
 (latent embeddings) for each code system which projected the same medical
 concept with different encodings into different, incompatible semantic
 spaces.
 The paper 
\begin_inset CommandInset citation
LatexCommand citep
key "hur_unifying_2022"
literal "false"

\end_inset

 learns from the unstructured textual descriptions instead of the medcodes
 themselves, a strategy which the authors termed 
\emph on
description embedding
\emph default
 (
\emph on
DescEmb
\emph default
).
 Using 
\emph on
DescEmb
\emph default
 the authors were able to pool differently structured EHRs, namely 
\emph on
MIMIC-III 
\emph default
and 
\emph on
eICU
\emph default
, into one larger dataset and achieved higher accuracy.
 
\end_layout

\begin_layout Section
Scientific Approach
\begin_inset CommandInset label
LatexCommand label
name "sec:Scientific-Approach"

\end_inset


\end_layout

\begin_layout Enumerate
First, we will try to replicate the results of 
\begin_inset CommandInset citation
LatexCommand citep
key "hur_unifying_2022"
literal "false"

\end_inset

 as a baseline.
\end_layout

\begin_layout Enumerate
We plan on using 
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Verbatim

pyhealth
\end_layout

\end_inset

where applicable.
 Especially for handling EHR data this should be very helpful.
 
\end_layout

\begin_layout Enumerate
Thirdly, we intend to test a hypothesis described in more detail in section
 
\begin_inset CommandInset ref
LatexCommand vref
reference "sec:Hypotheses-to-be"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In short, we would like to replace the BERT
\begin_inset Foot
status open

\begin_layout Plain Layout
Bidirectional Encoder Representations from Transformers 
\begin_inset CommandInset citation
LatexCommand citet
key "devlin_bert_2019"
literal "false"

\end_inset


\end_layout

\end_inset

-derived encoders by one which was pretrained from scratch on biomedical
 data.
 
\end_layout

\begin_layout Standard
As we outlined in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Feasibility-of-the"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we will not do comprehensive pre-training from scratch, but work with pre-train
ed architectures, which we may fine-tune for our experiments.
 
\end_layout

\begin_layout Section
Hypotheses to be tested
\begin_inset CommandInset label
LatexCommand label
name "sec:Hypotheses-to-be"

\end_inset


\end_layout

\begin_layout Standard
The authors of 
\begin_inset CommandInset citation
LatexCommand citep
key "hur_unifying_2022"
literal "false"

\end_inset

 conducted experiments with several 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Put citations for all these!
\end_layout

\end_inset

 BERT-based architectures, such as BioBERT 
\begin_inset CommandInset citation
LatexCommand citet
key "lee_biobert_2019"
literal "false"

\end_inset

, ClinicalBERT 
\begin_inset CommandInset citation
LatexCommand citet
key "huang_clinicalbert_2020"
literal "false"

\end_inset

 and BlueBERT, which were partially trained or fine-tuned on medical literature.
 In contrast, Gu et al.
 
\begin_inset CommandInset citation
LatexCommand citep
key "gu_domain-specific_2022"
literal "false"

\end_inset

 found that pre-training on generic NLP corpi, such as derived from Wikipedia,
 actually perform worse than ones which have been initialized with random
 values and only pre-trained on PubMed.
 They called their model PubMedBERT and were able to demonstrate superior
 performance to the models employed by 
\begin_inset CommandInset citation
LatexCommand citep
key "hur_unifying_2022"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
We would like to test if PubMedBERT is capable of matching or even superceding
 the performance of the other models utilized, which were not fully pre-trained
 on pure corpi from the biomedical domain.
 
\end_layout

\begin_layout Section
Ablations planned
\end_layout

\begin_layout Standard
A true ablation study with PubMedBERT and e.g.
 ClinicalBERT or BioBERT would mean we would have to pre-train all these
 architectures from scratch on both, generic NLP corpi as well as PubMed.
 This will not be computationally feasible.
 Instead, we will cite the work of 
\begin_inset CommandInset citation
LatexCommand citep
key "gu_domain-specific_2022"
literal "false"

\end_inset

 where applicable and rather try to replicate their results with different,
 already pre-trained architectures.
 
\end_layout

\begin_layout Section
Data access 
\end_layout

\begin_layout Standard
The two datasets we plan on utilizing, namely MIMIC-III 
\begin_inset CommandInset citation
LatexCommand citet
key "johnson_mimic-iii_2015,johnson_mimic-iii_2016"
literal "false"

\end_inset

 and eICU 
\begin_inset CommandInset citation
LatexCommand citet
key "pollard_eicu_2018"
literal "false"

\end_inset

, are both publicly available.
 We will need to take into account some regulations and license conditions
 concerning data safety and privacy.
\end_layout

\begin_layout Section
Feasibility of the computation
\begin_inset CommandInset label
LatexCommand label
name "sec:Feasibility-of-the"

\end_inset


\end_layout

\begin_layout Standard
As far as computational cost is concernced let's consider the size of the
 data and the size of the model architecture we need to train.
 
\end_layout

\begin_layout Subsection
Model architecture
\end_layout

\begin_layout Standard
According to 
\begin_inset CommandInset citation
LatexCommand citet
key "hur_unifying_2022"
literal "false"

\end_inset

 p.
 4, bottom right paragraph, the largest architecture the authors trained
 was BERT-base, which has 110 M (
\begin_inset ERT
status open

\begin_layout Plain Layout

$10^6$
\end_layout

\end_inset

) parameters.
 Other models the authors experimented with include BioBERT and ClinicalBERT,
 which are based on BERT-base, too.
 Let's do a quick estimate
\begin_inset Foot
status open

\begin_layout Plain Layout
Assuming 32-bit single precision computation
\end_layout

\end_inset

 of how much GPU memory is needed to load and train such a model: 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Bytes per parameter
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model parameters
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Adam optimizer
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Gradients
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Activations and temp memory
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
TOTAL
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
24
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Thus, a model with 110 M parameters would consume 
\begin_inset ERT
status open

\begin_layout Plain Layout

$110 
\backslash
cdot 10^6 
\backslash
times 24 bytes =  2.46 GB$
\end_layout

\end_inset

 to just load the model.
 This managable on modern-day GPUs.
 One of the authors has a GTX 3080 with 12 GB of VRAM available for full
 training and a GTX 1070 with 8 GB is available to do some code testing
 on small batches.
 As mentioned in sections 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Scientific-Approach"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Hypotheses-to-be"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we are not planning to do pretraining.
 While Hur et al.
 
\begin_inset CommandInset citation
LatexCommand citet
key "hur_unifying_2022"
literal "false"

\end_inset

 did not explicitely provide information on training time, the authors of
 BioBERT mentioned that pre-training took 10 days on a V100 GPU.
 Given the iterative nature of the doing model training and the amount of
 time we can spend, the cost for pre-training from scratch would be prohibitive
 for us.
 So, we will only do inferencing with available pre-trained architectures
 and fine-tuning where applicable.
 
\end_layout

\begin_layout Subsection
Datasets
\end_layout

\begin_layout Standard
We will use two datasets 
\begin_inset Note Note
status open

\begin_layout Plain Layout
literature
\end_layout

\end_inset

: MIMIC-III and eICU.
 According to 
\begin_inset CommandInset citation
LatexCommand citet
key "johnson_mimic-iii_2016"
literal "false"

\end_inset

 the MIMIC-III dataset is a large, de-identified, and publicly-available
 collection of medical records.
 It consists of 112,000 clinical reports records with an average length
 of 709.3 tokens.
 The dataset includes 1,159 top-level ICD-9 codes, and each report is assigned
 to 7.6 codes on average.
 This makes for about 78056000 tokens in MIMIC-III.
 We can make a quick estimate of the amount of compute necessary by using
 the calculation summarized by Adam Casson 
\begin_inset CommandInset citation
LatexCommand citet
key "casson2023transformerflops"
literal "false"

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
Note: The calculation presented there was originally made for decoder-only
 architectures like GPT, while BERT is an encoder-only, bidirectional architectu
re.
 For the sake of a rough estimate we disregarded that fact here.
\end_layout

\end_inset

: Using the data above we get about 600 M FLOPs per token.
 At the number of tokens that amounts to about 46800 TFLOPs for the whole
 MIMIC-III dataset (that is per epoch).
 On a GTX 3080 with about 30 TFLOPs/s one training epoch would thus take
 about 26 min.
 In original BERT paper
\begin_inset Foot
status open

\begin_layout Plain Layout
According to 
\begin_inset CommandInset citation
LatexCommand citet
key "devlin_bert_2019"
literal "false"

\end_inset

 the size of the token embeddings is 768, the number of transformer blocks
 is 12 and the number of self-attention heads is 12.
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "devlin_bert_2019"
literal "false"

\end_inset

 the pre-training was done for 40 epochs and fine-tuning was done for 2-3
 epochs on some more specialized NLP tasks.
 40 epochs would then take about 17-18 hours.
 We conclude that for MIMIC-III fine-tuning or some amount of training over
 the course of several weeks we have availalbe should be possible on our
 available hardware.
 
\begin_inset Newline newline
\end_inset

The eICU Collaborative Research Database holds data associated with over
 200,000 patient visits.
 If we assume the same average length per visit (we didn't find any average
 number in the paper) and we further assume that each report has the same
 average number of codes, the eICU dataset would comprise very roughly about
 twice the number of tokens compared to MIMIC-III.
 So, fine-tuning would still be possible, while many dozens of training
 rounds should be avoided or at least attempted only very few times.
 If possible we might want to limit most experiments to MIMIC-III.
 
\end_layout

\begin_layout Subsection
Pre-processing
\end_layout

\begin_layout Standard
On GitHub the authors mention that pre-processing took 1 hour on 128 cores
 and 60 GB of RAM.
 It will likely take us significantly longer and if RAM was limiting we
 might need to rent a cloud node for this part and download the processed
 data.
 
\end_layout

\begin_layout Section
Existing Code
\end_layout

\begin_layout Standard
The authors Hur et al.
 
\begin_inset CommandInset citation
LatexCommand citet
key "hur_unifying_2022"
literal "false"

\end_inset

 provide code on GitHub
\begin_inset Foot
status open

\begin_layout Plain Layout
https://github.com/hoon9405/DescEmb
\end_layout

\end_inset

 for pre-processing as well as training.
 We thus have a reference in case we get stuck with reproducing their results.
 We will try to rely on pyhealth as much as we can to keep the amount of
 code we have to write managable.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "Literature_Project_Ref"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
